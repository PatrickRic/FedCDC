do_assignment: None
seeds: [12]
name: naive-cifar100-feddf12
score_metric: contrloss
aggregation: <function fed_df at 0x73ac22ef2e50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=12
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[21, 69, 56, 23, 52, 30, 9, 28, 11, 81, 95, 63, 13, 4, 93, 32, 86, 31, 74, 80, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([21, 69, 56, 23, 52, 30,  9, 28, 11, 81, 95, 63, 13,  4, 93, 32, 86, 31,
        74, 80, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.0255, 0.11639465796947479)
DC 1, val_set_size=2000, COIs=[12, 68, 43, 14, 54, 36, 64, 55, 16, 72, 6, 90, 89, 51, 8, 41, 62, 42, 70, 49, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([12, 68, 43, 14, 54, 36, 64, 55, 16, 72,  6, 90, 89, 51,  8, 41, 62, 42,
        70, 49, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.028, 0.11640621066093444)
DC 2, val_set_size=2000, COIs=[78, 98, 59, 92, 3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22, 7, 20, 19, 24, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([78, 98, 59, 92,  3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22,  7, 20,
        19, 24, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.0265, 0.11638416111469269)
D00: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D01: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D02: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D03: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D04: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D05: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D06: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D07: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D08: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D09: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D010: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D011: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D012: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D013: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D014: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D015: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D016: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D017: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D018: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D019: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D020: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D021: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D022: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D023: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.3185, 0.07955377715826034) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2995, 0.08187433725595475) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3305, 0.07867564326524734) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.372, 0.07027425548434257) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3695, 0.0718573477268219) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3875, 0.06910195636749268) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4025, 0.06684382557868958) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.394, 0.06780641484260559) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3975, 0.06656640696525573) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.399, 0.0666924489736557) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3995, 0.06737062788009643) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.06731576311588287) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.419, 0.06850257882475853) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.41, 0.06941292816400528) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4395, 0.06662098595499992) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4135, 0.07097614964842797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.408, 0.07057151558995246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.06848313641548157) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.422, 0.07336168050765991) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.401, 0.07305146795511246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4295, 0.07193456798791885) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4275, 0.07523972707986831) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.409, 0.07647148954868317) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4175, 0.07398442697525025) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4235, 0.07888796997070313) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.403, 0.07981111419200897) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.07815053576231003) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4175, 0.0850370409488678) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3895, 0.08468946850299836) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.08412167412042618) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.414, 0.09006727829575539) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3935, 0.09400618913769722) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4105, 0.0896186733841896) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.09810493239760398) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3935, 0.09611588072776794) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.09668878090381622) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4115, 0.10046117091178894) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3975, 0.10598996776342393) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4125, 0.0991956044435501) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3985, 0.11707142797112464) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.386, 0.12084887319803238) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.1070841578245163) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4035, 0.11551657888293267) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.12907409131526948) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3945, 0.12496630728244781) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.406, 0.11839482456445694) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3815, 0.13203304940462113) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3925, 0.12236380010843277) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.399, 0.13387905567884445) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3675, 0.13484914481639862) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3945, 0.13169330424070358) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.399, 0.13142462486028672) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3755, 0.1418523685336113) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.401, 0.1366384129524231) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4005, 0.14440630251169204) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.14606542646884918) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4045, 0.1357548724412918) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.395, 0.13733465856313706) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3775, 0.14937904953956604) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3825, 0.14657538694143296) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.384, 0.13969425255060197) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3925, 0.1419027904868126) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3915, 0.14549145555496215) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3925, 0.1460995614528656) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.15586408758163453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3855, 0.15649691623449324) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.402, 0.14051249945163727) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.15308835101127624) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3965, 0.15785498082637786) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4065, 0.14382487988471984) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.361, 0.16777032005786896) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4075, 0.1576976945400238) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4035, 0.1498345645070076) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.16704403591156006) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3875, 0.1762493607401848) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.16917162626981735) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.163048843562603) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3965, 0.1660739163160324) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.389, 0.16203253090381622) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.17928140497207642) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3915, 0.1688502522110939) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3735, 0.1761092393398285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3655, 0.19253511101007462) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3745, 0.17641686302423476) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3945, 0.1702143264710903) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3765, 0.17180982851982116) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4025, 0.1670209027528763) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3895, 0.1757056515812874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.18432945895195008) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.17709181702136995) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.39, 0.17586073923110962) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3725, 0.19010606545209885) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4025, 0.17159583938121795) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.397, 0.17861058515310288) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.19431577920913695) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.2012536963224411) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.379, 0.18606853449344635) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.369, 0.18925907516479493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.19346568977832795) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.388, 0.17632615733146667) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3595, 0.19782873064279557) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3815, 0.19029989075660705) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3865, 0.1885506228208542) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3745, 0.20146762192249298) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.382, 0.19805919229984284) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.38, 0.18554047280550004) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3645, 0.20430378842353822) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3875, 0.18773563987016678) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3895, 0.18873358359932899) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.20214548909664154) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.406, 0.19683841574192046) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.397, 0.19122806549072266) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3755, 0.1929726703763008) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3755, 0.2110552068948746) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.373, 0.20858964264392854) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3775, 0.1998792004585266) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.388, 0.19609889805316924) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.385, 0.2049256353378296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3575, 0.22352566719055175) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3865, 0.20360616028308867) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3765, 0.21275597429275514) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.383, 0.20525616979598998) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3885, 0.20438349282741547) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.383, 0.19200536191463471) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.207764817237854) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3715, 0.2114275382757187) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3905, 0.20093095922470092) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3735, 0.2198504558801651) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.376, 0.2015259448289871) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3935, 0.194211084485054) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3865, 0.20977352023124696) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.383, 0.19371212857961656) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3945, 0.21635081386566163) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.375, 0.22526509404182435) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3845, 0.2173186422586441) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.392, 0.21144953572750091) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.2116138620376587) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3945, 0.21443114173412323) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.384, 0.2047293606996536) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.198123566031456) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3855, 0.23203328990936278) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3945, 0.22061600345373153) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3685, 0.21442827093601227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.393, 0.22443385243415834) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3785, 0.22098642480373382) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.375, 0.22280488562583922) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.394, 0.1987359435558319) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.392, 0.21293793964385987) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.21929406654834746) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3925, 0.22400246095657347) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.0255, 0.11639465796947479), (0.3185, 0.07955377715826034), (0.372, 0.07027425548434257), (0.4025, 0.06684382557868958), (0.399, 0.0666924489736557), (0.419, 0.06850257882475853), (0.4135, 0.07097614964842797), (0.422, 0.07336168050765991), (0.4275, 0.07523972707986831), (0.4235, 0.07888796997070313), (0.4175, 0.0850370409488678), (0.414, 0.09006727829575539), (0.41, 0.09810493239760398), (0.4115, 0.10046117091178894), (0.3985, 0.11707142797112464), (0.4035, 0.11551657888293267), (0.406, 0.11839482456445694), (0.399, 0.13387905567884445), (0.399, 0.13142462486028672), (0.4005, 0.14440630251169204), (0.395, 0.13733465856313706), (0.384, 0.13969425255060197), (0.3925, 0.1460995614528656), (0.402, 0.14051249945163727), (0.4065, 0.14382487988471984), (0.4035, 0.1498345645070076), (0.378, 0.16917162626981735), (0.389, 0.16203253090381622), (0.3735, 0.1761092393398285), (0.3945, 0.1702143264710903), (0.3895, 0.1757056515812874), (0.39, 0.17586073923110962), (0.397, 0.17861058515310288), (0.379, 0.18606853449344635), (0.388, 0.17632615733146667), (0.3865, 0.1885506228208542), (0.38, 0.18554047280550004), (0.3895, 0.18873358359932899), (0.397, 0.19122806549072266), (0.373, 0.20858964264392854), (0.385, 0.2049256353378296), (0.3765, 0.21275597429275514), (0.383, 0.19200536191463471), (0.3905, 0.20093095922470092), (0.3935, 0.194211084485054), (0.3945, 0.21635081386566163), (0.392, 0.21144953572750091), (0.384, 0.2047293606996536), (0.3945, 0.22061600345373153), (0.3785, 0.22098642480373382), (0.392, 0.21293793964385987)]
TEST: 
[(0.025, 0.11545719927549362), (0.331, 0.07877380961179734), (0.3705, 0.06927043214440345), (0.4005, 0.06561842769384384), (0.41475, 0.06513708809018134), (0.4345, 0.06535317265987396), (0.42475, 0.06874225521087647), (0.433, 0.07090103685855866), (0.4405, 0.07273493137955665), (0.4375, 0.07729810696840286), (0.42625, 0.08250733515620232), (0.425, 0.08601465621590615), (0.418, 0.09480249989032745), (0.429, 0.09861997973918915), (0.41025, 0.11287097707390785), (0.41025, 0.11217307728528976), (0.411, 0.11550538611412048), (0.4045, 0.12740615767240523), (0.4035, 0.12720368057489395), (0.41025, 0.138089157640934), (0.4195, 0.12720629996061325), (0.40725, 0.13612201392650605), (0.41725, 0.13877912807464599), (0.42075, 0.13323400062322616), (0.4165, 0.13929367303848267), (0.42325, 0.14258279484510422), (0.40075, 0.15881338369846343), (0.408, 0.15252305960655213), (0.39875, 0.16465204638242723), (0.41425, 0.15898193454742432), (0.40775, 0.16559819185733796), (0.39875, 0.17145461910963058), (0.4065, 0.17357727414369584), (0.39225, 0.1839108526110649), (0.40475, 0.16662975543737413), (0.39975, 0.17282080966234206), (0.40425, 0.16929665261507035), (0.4005, 0.17698108601570128), (0.40425, 0.18099476718902588), (0.39475, 0.19427564585208892), (0.403, 0.18296739625930786), (0.39025, 0.1981886640191078), (0.39725, 0.17720928072929382), (0.4045, 0.18366971564292908), (0.4055, 0.1815607561469078), (0.40575, 0.2010549778342247), (0.40175, 0.19688926994800568), (0.4105, 0.19389798158407212), (0.41075, 0.20924076551198958), (0.40125, 0.2103793989419937), (0.415, 0.20406124114990234)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.72      0.66      0.69       100
           1       0.56      0.44      0.49       100
           4       0.19      0.26      0.22       100
           9       0.43      0.45      0.44       100
          11       0.18      0.19      0.18       100
          13       0.33      0.32      0.32       100
          18       0.29      0.37      0.32       100
          21       0.56      0.46      0.51       100
          23       0.55      0.56      0.55       100
          28       0.53      0.60      0.56       100
          29       0.32      0.37      0.34       100
          30       0.54      0.38      0.45       100
          31       0.42      0.28      0.34       100
          32       0.34      0.28      0.31       100
          34       0.31      0.25      0.28       100
          35       0.24      0.30      0.27       100
          44       0.21      0.18      0.19       100
          47       0.43      0.59      0.50       100
          48       0.47      0.56      0.51       100
          52       0.64      0.52      0.57       100
          56       0.61      0.46      0.52       100
          58       0.58      0.33      0.42       100
          60       0.71      0.79      0.75       100
          61       0.65      0.65      0.65       100
          63       0.46      0.37      0.41       100
          67       0.41      0.35      0.38       100
          69       0.57      0.52      0.54       100
          71       0.60      0.58      0.59       100
          74       0.23      0.25      0.24       100
          76       0.53      0.56      0.55       100
          79       0.32      0.40      0.36       100
          80       0.14      0.20      0.17       100
          81       0.30      0.36      0.33       100
          82       0.77      0.70      0.73       100
          84       0.34      0.19      0.24       100
          85       0.54      0.35      0.42       100
          86       0.32      0.44      0.37       100
          88       0.31      0.36      0.33       100
          93       0.20      0.19      0.19       100
          95       0.39      0.53      0.45       100

    accuracy                           0.41      4000
   macro avg       0.43      0.42      0.42      4000
weighted avg       0.43      0.41      0.42      4000

No_Competition_DC_1
VAL: 
[(0.028, 0.11640621066093444), (0.2995, 0.08187433725595475), (0.3695, 0.0718573477268219), (0.394, 0.06780641484260559), (0.3995, 0.06737062788009643), (0.41, 0.06941292816400528), (0.408, 0.07057151558995246), (0.401, 0.07305146795511246), (0.409, 0.07647148954868317), (0.403, 0.07981111419200897), (0.3895, 0.08468946850299836), (0.3935, 0.09400618913769722), (0.3935, 0.09611588072776794), (0.3975, 0.10598996776342393), (0.386, 0.12084887319803238), (0.377, 0.12907409131526948), (0.3815, 0.13203304940462113), (0.3675, 0.13484914481639862), (0.3755, 0.1418523685336113), (0.377, 0.14606542646884918), (0.3775, 0.14937904953956604), (0.3925, 0.1419027904868126), (0.374, 0.15586408758163453), (0.37, 0.15308835101127624), (0.361, 0.16777032005786896), (0.374, 0.16704403591156006), (0.377, 0.163048843562603), (0.371, 0.17928140497207642), (0.3655, 0.19253511101007462), (0.3765, 0.17180982851982116), (0.364, 0.18432945895195008), (0.3725, 0.19010606545209885), (0.366, 0.19431577920913695), (0.369, 0.18925907516479493), (0.3595, 0.19782873064279557), (0.3745, 0.20146762192249298), (0.3645, 0.20430378842353822), (0.378, 0.20214548909664154), (0.3755, 0.1929726703763008), (0.3775, 0.1998792004585266), (0.3575, 0.22352566719055175), (0.383, 0.20525616979598998), (0.366, 0.207764817237854), (0.3735, 0.2198504558801651), (0.3865, 0.20977352023124696), (0.375, 0.22526509404182435), (0.366, 0.2116138620376587), (0.378, 0.198123566031456), (0.3685, 0.21442827093601227), (0.375, 0.22280488562583922), (0.378, 0.21929406654834746)]
TEST: 
[(0.02975, 0.11552399575710297), (0.309, 0.07964303588867187), (0.38125, 0.07031609305739403), (0.409, 0.06659635040163994), (0.423, 0.06526576209068298), (0.41625, 0.06798273068666458), (0.42325, 0.0693229169845581), (0.42875, 0.07120417535305024), (0.42425, 0.07422969409823418), (0.417, 0.0780954840183258), (0.4175, 0.08257081043720245), (0.40725, 0.09068245911598205), (0.41425, 0.09235501503944397), (0.40725, 0.10457762002944947), (0.4165, 0.11553717759251594), (0.41425, 0.12300312250852585), (0.41575, 0.12433493971824645), (0.39725, 0.12919098818302155), (0.41225, 0.13623924207687377), (0.40125, 0.14476568520069122), (0.39175, 0.14692706060409547), (0.413, 0.13933857476711273), (0.396, 0.1541023510694504), (0.39075, 0.15078991520404816), (0.39675, 0.1600803498029709), (0.39625, 0.1636390809416771), (0.39675, 0.16133731800317763), (0.3805, 0.17470096957683562), (0.3795, 0.18460136473178865), (0.392, 0.17090158462524413), (0.3815, 0.18313119930028915), (0.39775, 0.18596887642145157), (0.38625, 0.19049879312515258), (0.3885, 0.18525144922733308), (0.3855, 0.19067431133985518), (0.393, 0.19372252517938615), (0.3905, 0.19642038452625274), (0.387, 0.20098091226816178), (0.3895, 0.19451118618249894), (0.39225, 0.19428412675857543), (0.37775, 0.2160032331943512), (0.395, 0.19998361575603485), (0.38925, 0.20956379109621048), (0.385, 0.2149607402086258), (0.3905, 0.2094678574204445), (0.385, 0.22102755922079087), (0.37825, 0.20885415208339692), (0.388, 0.1944073783159256), (0.3875, 0.2113255583047867), (0.3925, 0.21396249395608902), (0.394, 0.21964337700605394)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.54      0.58      0.56       100
           1       0.50      0.40      0.44       100
           6       0.32      0.33      0.33       100
           8       0.31      0.39      0.35       100
          12       0.33      0.34      0.34       100
          14       0.36      0.31      0.33       100
          16       0.31      0.44      0.37       100
          18       0.31      0.25      0.28       100
          29       0.44      0.24      0.31       100
          34       0.32      0.32      0.32       100
          35       0.30      0.49      0.37       100
          36       0.36      0.35      0.36       100
          41       0.54      0.51      0.53       100
          42       0.24      0.28      0.26       100
          43       0.34      0.25      0.29       100
          44       0.27      0.25      0.26       100
          47       0.82      0.58      0.68       100
          48       0.51      0.56      0.53       100
          49       0.31      0.36      0.33       100
          51       0.29      0.27      0.28       100
          54       0.52      0.36      0.43       100
          55       0.18      0.31      0.23       100
          58       0.40      0.43      0.41       100
          60       0.77      0.68      0.72       100
          61       0.66      0.61      0.64       100
          62       0.44      0.48      0.46       100
          64       0.19      0.15      0.17       100
          67       0.57      0.39      0.46       100
          68       0.78      0.57      0.66       100
          70       0.33      0.29      0.31       100
          71       0.59      0.46      0.52       100
          72       0.21      0.21      0.21       100
          76       0.64      0.59      0.61       100
          79       0.40      0.36      0.38       100
          82       0.64      0.65      0.65       100
          84       0.26      0.18      0.21       100
          85       0.38      0.51      0.43       100
          88       0.21      0.24      0.22       100
          89       0.27      0.42      0.33       100
          90       0.34      0.37      0.35       100

    accuracy                           0.39      4000
   macro avg       0.41      0.39      0.40      4000
weighted avg       0.41      0.39      0.40      4000

No_Competition_DC_2
VAL: 
[(0.0265, 0.11638416111469269), (0.3305, 0.07867564326524734), (0.3875, 0.06910195636749268), (0.3975, 0.06656640696525573), (0.416, 0.06731576311588287), (0.4395, 0.06662098595499992), (0.422, 0.06848313641548157), (0.4295, 0.07193456798791885), (0.4175, 0.07398442697525025), (0.425, 0.07815053576231003), (0.428, 0.08412167412042618), (0.4105, 0.0896186733841896), (0.415, 0.09668878090381622), (0.4125, 0.0991956044435501), (0.417, 0.1070841578245163), (0.3945, 0.12496630728244781), (0.3925, 0.12236380010843277), (0.3945, 0.13169330424070358), (0.401, 0.1366384129524231), (0.4045, 0.1357548724412918), (0.3825, 0.14657538694143296), (0.3915, 0.14549145555496215), (0.3855, 0.15649691623449324), (0.3965, 0.15785498082637786), (0.4075, 0.1576976945400238), (0.3875, 0.1762493607401848), (0.3965, 0.1660739163160324), (0.3915, 0.1688502522110939), (0.3745, 0.17641686302423476), (0.4025, 0.1670209027528763), (0.395, 0.17709181702136995), (0.4025, 0.17159583938121795), (0.379, 0.2012536963224411), (0.387, 0.19346568977832795), (0.3815, 0.19029989075660705), (0.382, 0.19805919229984284), (0.3875, 0.18773563987016678), (0.406, 0.19683841574192046), (0.3755, 0.2110552068948746), (0.388, 0.19609889805316924), (0.3865, 0.20360616028308867), (0.3885, 0.20438349282741547), (0.3715, 0.2114275382757187), (0.376, 0.2015259448289871), (0.383, 0.19371212857961656), (0.3845, 0.2173186422586441), (0.3945, 0.21443114173412323), (0.3855, 0.23203328990936278), (0.393, 0.22443385243415834), (0.394, 0.1987359435558319), (0.3925, 0.22400246095657347)]
TEST: 
[(0.024, 0.1154811590909958), (0.30825, 0.08007451406121253), (0.36125, 0.0708968978524208), (0.38375, 0.0686992644071579), (0.38975, 0.06988792023062707), (0.4105, 0.06953559777140618), (0.402, 0.07106032827496529), (0.40875, 0.0737326290011406), (0.39975, 0.07720113959908485), (0.40125, 0.0811170037984848), (0.39775, 0.08730833604931831), (0.39675, 0.09188960999250412), (0.39325, 0.09867122292518615), (0.3765, 0.10539408504962922), (0.39, 0.11151976153254509), (0.37975, 0.12756788137555122), (0.382, 0.12607567259669303), (0.37825, 0.13873787871003151), (0.388, 0.14278516378998757), (0.387, 0.14416878974437713), (0.378, 0.14783571910858154), (0.385, 0.14917766529321672), (0.37275, 0.16229984033107758), (0.383, 0.16435693198442458), (0.37825, 0.16895624607801438), (0.3635, 0.1822923799753189), (0.36925, 0.1723306788802147), (0.3795, 0.17404602551460266), (0.3795, 0.17883384680747985), (0.37675, 0.17293812996149063), (0.36725, 0.18638232630491255), (0.3795, 0.17882432860136033), (0.375, 0.20889846819639207), (0.3815, 0.20038428556919097), (0.38, 0.19324831002950668), (0.3685, 0.2035294749736786), (0.38025, 0.1896641719341278), (0.38025, 0.20081228333711623), (0.365, 0.21303973144292832), (0.371, 0.20215269726514817), (0.38, 0.2062141741514206), (0.383, 0.2162674656510353), (0.3645, 0.21840953516960143), (0.368, 0.21600191682577133), (0.37875, 0.2078892824649811), (0.3745, 0.22380118483304978), (0.379, 0.22462936460971833), (0.37275, 0.23716758298873902), (0.38775, 0.23143300551176071), (0.3855, 0.20760555469989778), (0.37975, 0.22721194046735763)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.47      0.66      0.55       100
           1       0.36      0.54      0.44       100
           3       0.35      0.29      0.32       100
           7       0.38      0.24      0.29       100
          15       0.33      0.27      0.30       100
          18       0.28      0.38      0.32       100
          19       0.33      0.31      0.32       100
          20       0.77      0.62      0.69       100
          22       0.29      0.18      0.22       100
          24       0.59      0.48      0.53       100
          25       0.29      0.38      0.33       100
          26       0.31      0.22      0.26       100
          29       0.36      0.41      0.38       100
          34       0.19      0.30      0.24       100
          35       0.29      0.25      0.27       100
          38       0.20      0.18      0.19       100
          44       0.17      0.17      0.17       100
          47       0.54      0.50      0.52       100
          48       0.48      0.59      0.53       100
          50       0.22      0.17      0.19       100
          53       0.54      0.50      0.52       100
          58       0.44      0.44      0.44       100
          59       0.31      0.45      0.36       100
          60       0.86      0.66      0.75       100
          61       0.52      0.58      0.55       100
          67       0.37      0.37      0.37       100
          71       0.74      0.45      0.56       100
          73       0.36      0.35      0.36       100
          76       0.67      0.62      0.65       100
          78       0.18      0.20      0.19       100
          79       0.33      0.36      0.34       100
          82       0.64      0.64      0.64       100
          83       0.31      0.33      0.32       100
          84       0.18      0.15      0.16       100
          85       0.42      0.54      0.47       100
          88       0.24      0.23      0.23       100
          91       0.39      0.50      0.44       100
          92       0.36      0.30      0.33       100
          98       0.19      0.14      0.16       100
          99       0.24      0.24      0.24       100

    accuracy                           0.38      4000
   macro avg       0.39      0.38      0.38      4000
weighted avg       0.39      0.38      0.38      4000

Competition
DC 0, val_set_size=2000, COIs=[21, 69, 56, 23, 52, 30, 9, 28, 11, 81, 95, 63, 13, 4, 93, 32, 86, 31, 74, 80, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([21, 69, 56, 23, 52, 30,  9, 28, 11, 81, 95, 63, 13,  4, 93, 32, 86, 31,
        74, 80, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.0255, 0.11639465796947479)
DC 1, val_set_size=2000, COIs=[12, 68, 43, 14, 54, 36, 64, 55, 16, 72, 6, 90, 89, 51, 8, 41, 62, 42, 70, 49, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([12, 68, 43, 14, 54, 36, 64, 55, 16, 72,  6, 90, 89, 51,  8, 41, 62, 42,
        70, 49, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.028, 0.11640621066093444)
DC 2, val_set_size=2000, COIs=[78, 98, 59, 92, 3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22, 7, 20, 19, 24, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([78, 98, 59, 92,  3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22,  7, 20,
        19, 24, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.0265, 0.11638416111469269)
D00: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D01: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D02: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D03: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D04: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D05: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D06: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D07: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D08: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D09: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D010: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D011: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D012: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D013: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D014: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D015: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D016: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D017: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D018: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D019: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D020: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D021: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D022: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D023: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO5', '(DO2']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.219, 0.13730174821615218) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.1885, 0.1415414100587368) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2205, 0.14266999346017836) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2405, 0.18743348613381386) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2245, 0.18527058947086333) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.243, 0.1793551420867443) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.22257724460959435) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.228, 0.25215356081724166) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2615, 0.22928027477860452) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.26764985382556916) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.235, 0.3046052090227604) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.30099684005975724) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.32942507475614546) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.236, 0.36467203122377395) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.3399977630376816) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO0', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.26, 0.35889685863256454) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2405, 0.35658539420366286) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.35578673219680784) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.248, 0.3698793235123157) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.229, 0.34502209165692327) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2555, 0.37705166286230085) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2505, 0.37666396048665046) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.226, 0.3513768477886915) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2565, 0.3674626606106758) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.3796458009779453) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2285, 0.39078747284412385) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.37711558321118355) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2575, 0.38968313032388685) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2365, 0.39582130998373033) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2465, 0.3922221145033836) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO4', '(DO5']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO3', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2565, 0.39123574647307396) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2315, 0.40373215946555135) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2535, 0.3826443800330162) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.259, 0.41109378096461296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.228, 0.41665752723813054) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.244, 0.3978461412191391) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.38945994716882704) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2175, 0.4255713315308094) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.4235885660052299) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.255, 0.3963775791823864) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2245, 0.3986493557095528) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2395, 0.41749416434764863) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2505, 0.43341623452305794) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2235, 0.4349216730892658) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.3839154612123966) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO1', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.407700407654047) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.224, 0.403526687502861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.408456768989563) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2465, 0.3961767560839653) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.22, 0.4126270595192909) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2475, 0.3662030753493309) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2405, 0.36198157027363775) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.24, 0.3877560318410397) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.39642900651693347) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.36653490966558455) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2305, 0.36903824961185455) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2405, 0.3726944007277489) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.3441430480182171) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2345, 0.36554273384809494) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2525, 0.3639912531375885) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO5', '(DO4']
DC 1 --> ['(DO1', '(DO2']
DC 2 --> ['(DO3', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.246, 0.3458677853047848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2255, 0.34736170744895933) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2385, 0.37598169773817064) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.34340765079855917) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.216, 0.3501162357926369) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2395, 0.35871525466442106) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.252, 0.3208338456749916) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2345, 0.334916372269392) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.3348879901766777) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.254, 0.3325786467194557) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.223, 0.3571399468481541) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.242, 0.3449926847219467) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.3138535348176956) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.228, 0.3178378139734268) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.33205407136678694) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO2', '(DO5']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO0', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.3206817120909691) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.218, 0.32135415595769884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2355, 0.3388170346617699) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.32528610438108446) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.23, 0.29974481123685837) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.3202605471611023) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.252, 0.29761527448892594) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2135, 0.3247835774719715) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2315, 0.3120222427248955) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2515, 0.3125991702079773) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2225, 0.31756450587511065) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.232, 0.3081979269981384) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.30561007928848266) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2285, 0.2898052510917187) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2385, 0.3085130519866943) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO5', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.254, 0.3134304138123989) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2275, 0.28800752902030946) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2315, 0.3033507896065712) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.259, 0.2926619341671467) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2255, 0.2732460272908211) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2375, 0.3017188655734062) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.248, 0.311131250500679) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.23, 0.28808221405744555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.244, 0.31392496621608734) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.249, 0.29989927738904953) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2325, 0.32398322561383247) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.27415647077560423) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.29945608365535736) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.232, 0.31428557747602465) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.2814850552678108) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO5', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2405, 0.3051927060186863) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.227, 0.2921913977265358) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2375, 0.26903420186042787) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2445, 0.30145113039016724) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2245, 0.30357318645715714) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.237, 0.26601174151897433) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2415, 0.28518463227152824) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.229, 0.29023729315400126) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2475, 0.26953581178188324) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.247, 0.3012410047054291) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2335, 0.28242791873216627) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2555, 0.27738071674108505) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.30033576250076294) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.24, 0.30431120908260345) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.27051833319664004) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO5', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2365, 0.29055984610319135) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.236, 0.2859846568703651) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2505, 0.2956529206633568) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.32612013322114947) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2305, 0.2705038837194443) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.3085367951989174) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.2846533807814121) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2355, 0.2770156669616699) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.28975106245279314) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.253, 0.2833356527090073) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.238, 0.2778631972968578) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.2686551134586334) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.27241447070240976) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.241, 0.2942182040214539) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2555, 0.27896672654151916) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO5', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.26930982476472853) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.245, 0.2787553162872791) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2455, 0.29733750462532044) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.255, 0.28397246545553206) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.253, 0.2831862810254097) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.256, 0.3081096867918968) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.255, 0.2875100002884865) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2445, 0.2950829060077667) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.2914231597185135) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.243, 0.3118785762190819) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2485, 0.26237873613834384) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2505, 0.28022513937950133) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2505, 0.28179307210445403) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2425, 0.2819098170399666) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.2963893817663193) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.0255, 0.11639465796947479), (0.219, 0.13730174821615218), (0.2405, 0.18743348613381386), (0.245, 0.22257724460959435), (0.2475, 0.26764985382556916), (0.2495, 0.32942507475614546), (0.26, 0.35889685863256454), (0.248, 0.3698793235123157), (0.2505, 0.37666396048665046), (0.257, 0.3796458009779453), (0.2575, 0.38968313032388685), (0.2565, 0.39123574647307396), (0.259, 0.41109378096461296), (0.2475, 0.38945994716882704), (0.255, 0.3963775791823864), (0.2505, 0.43341623452305794), (0.2495, 0.407700407654047), (0.2465, 0.3961767560839653), (0.2405, 0.36198157027363775), (0.2495, 0.36653490966558455), (0.251, 0.3441430480182171), (0.246, 0.3458677853047848), (0.2475, 0.34340765079855917), (0.252, 0.3208338456749916), (0.254, 0.3325786467194557), (0.251, 0.3138535348176956), (0.2475, 0.3206817120909691), (0.245, 0.32528610438108446), (0.252, 0.29761527448892594), (0.2515, 0.3125991702079773), (0.257, 0.30561007928848266), (0.254, 0.3134304138123989), (0.259, 0.2926619341671467), (0.248, 0.311131250500679), (0.249, 0.29989927738904953), (0.25, 0.29945608365535736), (0.2405, 0.3051927060186863), (0.2445, 0.30145113039016724), (0.2415, 0.28518463227152824), (0.247, 0.3012410047054291), (0.245, 0.30033576250076294), (0.2365, 0.29055984610319135), (0.244, 0.32612013322114947), (0.2595, 0.2846533807814121), (0.253, 0.2833356527090073), (0.2555, 0.27241447070240976), (0.257, 0.26930982476472853), (0.255, 0.28397246545553206), (0.255, 0.2875100002884865), (0.243, 0.3118785762190819), (0.2505, 0.28179307210445403)]
TEST: 
[(0.025, 0.11545719927549362), (0.21975, 0.13614332771301269), (0.24375, 0.18486670392751695), (0.2555, 0.21891169381141662), (0.26275, 0.2625189605951309), (0.26175, 0.3207554442882538), (0.26275, 0.34376634430885317), (0.26175, 0.3621557585000992), (0.261, 0.3617424391508102), (0.266, 0.3624600139856339), (0.25875, 0.3783736239671707), (0.26475, 0.38001897239685056), (0.261, 0.4010335029363632), (0.257, 0.38088699662685394), (0.256, 0.3885032958984375), (0.25825, 0.4180042977333069), (0.25725, 0.39113714480400086), (0.26, 0.38231639432907105), (0.2515, 0.35071204113960264), (0.25225, 0.3534874484539032), (0.2595, 0.3353268642425537), (0.25175, 0.3339359471797943), (0.24625, 0.3294436534643173), (0.25075, 0.3085758193731308), (0.252, 0.31794891667366026), (0.2545, 0.3006760756969452), (0.25325, 0.3066831657886505), (0.2495, 0.3118307167291641), (0.258, 0.2837346396446228), (0.24575, 0.30504656279087067), (0.253, 0.2968752248287201), (0.245, 0.30022975683212283), (0.25675, 0.27984105157852174), (0.25025, 0.2982658829689026), (0.25125, 0.2871602272987366), (0.2465, 0.2852894846200943), (0.247, 0.2951927796602249), (0.253, 0.2860225534439087), (0.248, 0.274956974029541), (0.252, 0.29351585257053375), (0.24875, 0.29200096321105956), (0.251, 0.2796746934652328), (0.251, 0.3074137227535248), (0.26125, 0.27361401593685153), (0.2555, 0.27368658912181854), (0.2595, 0.26514158487319944), (0.2615, 0.25860213887691497), (0.262, 0.2719325091838837), (0.25925, 0.26984012389183043), (0.2635, 0.2972840200662613), (0.2545, 0.2735970124006271)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           1       1.00      0.01      0.02       100
           4       0.20      0.32      0.25       100
           9       0.25      0.50      0.33       100
          11       0.19      0.48      0.27       100
          13       0.18      0.54      0.26       100
          18       0.00      0.00      0.00       100
          21       0.47      0.69      0.56       100
          23       0.32      0.64      0.43       100
          28       0.41      0.60      0.48       100
          29       0.00      0.00      0.00       100
          30       0.30      0.44      0.36       100
          31       0.20      0.50      0.29       100
          32       0.18      0.35      0.24       100
          34       0.00      0.00      0.00       100
          35       0.00      0.00      0.00       100
          44       0.20      0.01      0.02       100
          47       0.00      0.00      0.00       100
          48       0.00      0.00      0.00       100
          52       0.45      0.90      0.60       100
          56       0.33      0.51      0.40       100
          58       0.12      0.02      0.03       100
          60       0.85      0.11      0.19       100
          61       0.83      0.05      0.09       100
          63       0.32      0.40      0.36       100
          67       0.22      0.02      0.04       100
          69       0.37      0.58      0.45       100
          71       0.62      0.08      0.14       100
          74       0.17      0.38      0.23       100
          76       0.00      0.00      0.00       100
          79       0.75      0.03      0.06       100
          80       0.15      0.33      0.20       100
          81       0.23      0.36      0.28       100
          82       0.00      0.00      0.00       100
          84       0.00      0.00      0.00       100
          85       0.00      0.00      0.00       100
          86       0.33      0.51      0.40       100
          88       0.00      0.00      0.00       100
          93       0.10      0.36      0.16       100
          95       0.27      0.46      0.34       100

    accuracy                           0.25      4000
   macro avg       0.25      0.25      0.19      4000
weighted avg       0.25      0.25      0.19      4000

Competition_DC_1
VAL: 
[(0.028, 0.11640621066093444), (0.1885, 0.1415414100587368), (0.2245, 0.18527058947086333), (0.228, 0.25215356081724166), (0.235, 0.3046052090227604), (0.236, 0.36467203122377395), (0.2405, 0.35658539420366286), (0.229, 0.34502209165692327), (0.226, 0.3513768477886915), (0.2285, 0.39078747284412385), (0.2365, 0.39582130998373033), (0.2315, 0.40373215946555135), (0.228, 0.41665752723813054), (0.2175, 0.4255713315308094), (0.2245, 0.3986493557095528), (0.2235, 0.4349216730892658), (0.224, 0.403526687502861), (0.22, 0.4126270595192909), (0.24, 0.3877560318410397), (0.2305, 0.36903824961185455), (0.2345, 0.36554273384809494), (0.2255, 0.34736170744895933), (0.216, 0.3501162357926369), (0.2345, 0.334916372269392), (0.223, 0.3571399468481541), (0.228, 0.3178378139734268), (0.218, 0.32135415595769884), (0.23, 0.29974481123685837), (0.2135, 0.3247835774719715), (0.2225, 0.31756450587511065), (0.2285, 0.2898052510917187), (0.2275, 0.28800752902030946), (0.2255, 0.2732460272908211), (0.23, 0.28808221405744555), (0.2325, 0.32398322561383247), (0.232, 0.31428557747602465), (0.227, 0.2921913977265358), (0.2245, 0.30357318645715714), (0.229, 0.29023729315400126), (0.2335, 0.28242791873216627), (0.24, 0.30431120908260345), (0.236, 0.2859846568703651), (0.2305, 0.2705038837194443), (0.2355, 0.2770156669616699), (0.238, 0.2778631972968578), (0.241, 0.2942182040214539), (0.245, 0.2787553162872791), (0.253, 0.2831862810254097), (0.2445, 0.2950829060077667), (0.2485, 0.26237873613834384), (0.2425, 0.2819098170399666)]
TEST: 
[(0.02975, 0.11552399575710297), (0.20075, 0.13972690379619598), (0.21975, 0.18266826754808427), (0.22775, 0.24902315425872804), (0.2305, 0.2975272392034531), (0.23825, 0.3592861168384552), (0.23825, 0.35323522055149076), (0.2315, 0.341927216053009), (0.23625, 0.34776059484481814), (0.2255, 0.3877877976894379), (0.23575, 0.3971103423833847), (0.228, 0.40400904178619385), (0.22575, 0.41715996122360227), (0.225, 0.41763987636566163), (0.22975, 0.3943672912120819), (0.21925, 0.4322980120182037), (0.22425, 0.3998159993886948), (0.21775, 0.40562381303310396), (0.2255, 0.38450598120689394), (0.22725, 0.3700269447565079), (0.21725, 0.3652709761857986), (0.22325, 0.3402060819864273), (0.214, 0.34082430374622347), (0.22225, 0.328163161277771), (0.2235, 0.34788368463516234), (0.22025, 0.30576515316963193), (0.22125, 0.3158774793148041), (0.23125, 0.2964285957813263), (0.2175, 0.3197640173435211), (0.2275, 0.3109115459918976), (0.23325, 0.28333625435829163), (0.22025, 0.28607066535949704), (0.229, 0.26842244493961337), (0.233, 0.27903215765953066), (0.22925, 0.31826494216918944), (0.23075, 0.300025794506073), (0.2345, 0.2837222172021866), (0.22425, 0.30372020590305326), (0.2265, 0.2806050683259964), (0.22975, 0.2734310046434402), (0.237, 0.2973936402797699), (0.23375, 0.2756078482866287), (0.232, 0.26257796931266786), (0.2315, 0.2751519203186035), (0.243, 0.2709396539926529), (0.24025, 0.28707776963710785), (0.23675, 0.2759850888252258), (0.24175, 0.275454914689064), (0.2365, 0.28575880694389344), (0.24, 0.25843383312225343), (0.24, 0.27460123467445374)]
DETAILED: 
              precision    recall  f1-score   support

           0       1.00      0.07      0.13       100
           1       1.00      0.01      0.02       100
           6       0.23      0.47      0.31       100
           8       0.30      0.55      0.39       100
          12       0.22      0.39      0.28       100
          14       0.21      0.28      0.24       100
          16       0.27      0.59      0.37       100
          18       0.20      0.01      0.02       100
          29       0.00      0.00      0.00       100
          34       0.67      0.02      0.04       100
          35       0.00      0.00      0.00       100
          36       0.19      0.36      0.25       100
          41       0.47      0.59      0.52       100
          42       0.18      0.44      0.26       100
          43       0.24      0.44      0.31       100
          44       0.00      0.00      0.00       100
          47       1.00      0.02      0.04       100
          48       0.00      0.00      0.00       100
          49       0.21      0.57      0.30       100
          51       0.19      0.32      0.24       100
          54       0.33      0.59      0.42       100
          55       0.10      0.21      0.13       100
          58       0.00      0.00      0.00       100
          60       0.00      0.00      0.00       100
          61       0.40      0.02      0.04       100
          62       0.39      0.55      0.46       100
          64       0.10      0.28      0.15       100
          67       0.67      0.02      0.04       100
          68       0.37      0.76      0.50       100
          70       0.32      0.52      0.39       100
          71       0.64      0.09      0.16       100
          72       0.12      0.27      0.17       100
          76       0.57      0.08      0.14       100
          79       0.00      0.00      0.00       100
          82       0.85      0.22      0.35       100
          84       0.00      0.00      0.00       100
          85       1.00      0.05      0.10       100
          88       0.00      0.00      0.00       100
          89       0.21      0.45      0.28       100
          90       0.19      0.36      0.25       100

    accuracy                           0.24      4000
   macro avg       0.32      0.24      0.18      4000
weighted avg       0.32      0.24      0.18      4000

Competition_DC_2
VAL: 
[(0.0265, 0.11638416111469269), (0.2205, 0.14266999346017836), (0.243, 0.1793551420867443), (0.2615, 0.22928027477860452), (0.265, 0.30099684005975724), (0.2575, 0.3399977630376816), (0.254, 0.35578673219680784), (0.2555, 0.37705166286230085), (0.2565, 0.3674626606106758), (0.2515, 0.37711558321118355), (0.2465, 0.3922221145033836), (0.2535, 0.3826443800330162), (0.244, 0.3978461412191391), (0.252, 0.4235885660052299), (0.2395, 0.41749416434764863), (0.254, 0.3839154612123966), (0.25, 0.408456768989563), (0.2475, 0.3662030753493309), (0.245, 0.39642900651693347), (0.2405, 0.3726944007277489), (0.2525, 0.3639912531375885), (0.2385, 0.37598169773817064), (0.2395, 0.35871525466442106), (0.239, 0.3348879901766777), (0.242, 0.3449926847219467), (0.2485, 0.33205407136678694), (0.2355, 0.3388170346617699), (0.238, 0.3202605471611023), (0.2315, 0.3120222427248955), (0.232, 0.3081979269981384), (0.2385, 0.3085130519866943), (0.2315, 0.3033507896065712), (0.2375, 0.3017188655734062), (0.244, 0.31392496621608734), (0.238, 0.27415647077560423), (0.239, 0.2814850552678108), (0.2375, 0.26903420186042787), (0.237, 0.26601174151897433), (0.2475, 0.26953581178188324), (0.2555, 0.27738071674108505), (0.2595, 0.27051833319664004), (0.2505, 0.2956529206633568), (0.2485, 0.3085367951989174), (0.2485, 0.28975106245279314), (0.258, 0.2686551134586334), (0.2555, 0.27896672654151916), (0.2455, 0.29733750462532044), (0.256, 0.3081096867918968), (0.254, 0.2914231597185135), (0.2505, 0.28022513937950133), (0.2625, 0.2963893817663193)]
TEST: 
[(0.024, 0.1154811590909958), (0.20425, 0.1427670359015465), (0.228, 0.17892999589443206), (0.23975, 0.22770351254940033), (0.25, 0.2983880100250244), (0.244, 0.3394016674757004), (0.247, 0.35002038967609406), (0.242, 0.36617136216163637), (0.24025, 0.3612870533466339), (0.23575, 0.3729569517374039), (0.2355, 0.38615557515621185), (0.23425, 0.3776211497783661), (0.2325, 0.3927415132522583), (0.234, 0.41280815947055816), (0.23175, 0.4077090511322021), (0.24275, 0.37446468925476073), (0.2335, 0.40530787324905393), (0.231, 0.36324940764904023), (0.2265, 0.3866496798992157), (0.23125, 0.36494627249240874), (0.23875, 0.3569483585357666), (0.2275, 0.3688995672464371), (0.2345, 0.35321638453006743), (0.23225, 0.33214508390426634), (0.23925, 0.3425625092983246), (0.23325, 0.3292568871974945), (0.2305, 0.3338235821723938), (0.2315, 0.3144266756772995), (0.232, 0.3095242886543274), (0.23325, 0.3051777728796005), (0.23875, 0.30239406502246857), (0.23275, 0.29853064012527464), (0.23475, 0.2943755997419357), (0.23175, 0.3063782058954239), (0.23375, 0.2732639811038971), (0.24025, 0.28118668818473813), (0.23375, 0.26396150267124174), (0.239, 0.2603816612958908), (0.2425, 0.26581150901317596), (0.247, 0.27269967830181124), (0.244, 0.27076997590065005), (0.2455, 0.29402535617351533), (0.235, 0.30710632479190825), (0.2495, 0.29105172300338744), (0.24725, 0.27210835039615633), (0.24675, 0.28234576523303984), (0.2495, 0.29718925309181216), (0.25075, 0.30565873003005983), (0.2405, 0.2900572959184647), (0.247, 0.2772346669435501), (0.24775, 0.29429016447067263)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.78      0.14      0.24       100
           1       0.50      0.01      0.02       100
           3       0.22      0.28      0.25       100
           7       0.27      0.42      0.33       100
          15       0.16      0.28      0.21       100
          18       0.33      0.01      0.02       100
          19       0.19      0.32      0.24       100
          20       0.57      0.68      0.62       100
          22       0.23      0.42      0.29       100
          24       0.41      0.72      0.52       100
          25       0.21      0.41      0.28       100
          26       0.14      0.37      0.20       100
          29       0.00      0.00      0.00       100
          34       0.00      0.00      0.00       100
          35       0.75      0.03      0.06       100
          38       0.19      0.41      0.26       100
          44       0.33      0.01      0.02       100
          47       0.00      0.00      0.00       100
          48       0.81      0.13      0.22       100
          50       0.15      0.31      0.20       100
          53       0.39      0.54      0.45       100
          58       0.75      0.03      0.06       100
          59       0.24      0.65      0.35       100
          60       0.96      0.24      0.38       100
          61       0.67      0.02      0.04       100
          67       0.00      0.00      0.00       100
          71       0.67      0.02      0.04       100
          73       0.26      0.63      0.37       100
          76       1.00      0.04      0.08       100
          78       0.14      0.36      0.20       100
          79       0.33      0.01      0.02       100
          82       1.00      0.03      0.06       100
          83       0.22      0.61      0.32       100
          84       0.00      0.00      0.00       100
          85       0.75      0.03      0.06       100
          88       1.00      0.01      0.02       100
          91       0.29      0.62      0.39       100
          92       0.28      0.46      0.35       100
          98       0.18      0.31      0.23       100
          99       0.23      0.35      0.28       100

    accuracy                           0.25      4000
   macro avg       0.39      0.25      0.19      4000
weighted avg       0.39      0.25      0.19      4000

Collaboration
DC 0, val_set_size=2000, COIs=[21, 69, 56, 23, 52, 30, 9, 28, 11, 81, 95, 63, 13, 4, 93, 32, 86, 31, 74, 80, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([21, 69, 56, 23, 52, 30,  9, 28, 11, 81, 95, 63, 13,  4, 93, 32, 86, 31,
        74, 80, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.0255, 0.11639465796947479)
DC 1, val_set_size=2000, COIs=[12, 68, 43, 14, 54, 36, 64, 55, 16, 72, 6, 90, 89, 51, 8, 41, 62, 42, 70, 49, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([12, 68, 43, 14, 54, 36, 64, 55, 16, 72,  6, 90, 89, 51,  8, 41, 62, 42,
        70, 49, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.028, 0.11640621066093444)
DC 2, val_set_size=2000, COIs=[78, 98, 59, 92, 3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22, 7, 20, 19, 24, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([78, 98, 59, 92,  3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22,  7, 20,
        19, 24, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.0265, 0.11638416111469269)
D00: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D01: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D02: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D03: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D04: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D05: 1000 samples from classes {0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88}
D06: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D07: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D08: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D09: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D010: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D011: 1000 samples from classes {4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95}
D012: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D013: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D014: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D015: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D016: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D017: 1000 samples from classes {6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90}
D018: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D019: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D020: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D021: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D022: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
D023: 1000 samples from classes {3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO0', '(DO4']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.2025, 0.1396341785788536) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2095, 0.14246558374166488) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.218, 0.13960152095556258) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.17384512490034104) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.234, 0.19523358756303788) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.231, 0.18852761551737784) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.22678148692846298) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2435, 0.2470117070376873) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.2519578774869442) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.267, 0.29112414771318434) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2515, 0.29844838693737985) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.246, 0.30815341413021086) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.263, 0.3547761492133141) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2405, 0.3360487785935402) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.255, 0.3898617000281811) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO4', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2625, 0.3693101988732815) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2505, 0.36220408163964746) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.3632071052789688) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.27, 0.36375497180223465) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.239, 0.3566945753097534) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.4001330908834934) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2605, 0.36243046113848687) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2465, 0.3663783219754696) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.3929723858237267) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.3806731471717358) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2385, 0.3778376592695713) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.3840735909938812) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2465, 0.41661606314778327) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.241, 0.3878095577955246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.246, 0.38261088597774506) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[0, 1, 18, 29, 34, 35, 44, 47, 48, 58, 60, 61, 67, 71, 76, 79, 82, 84, 85, 88], M=tensor([ 0,  1,  3,  4,  6,  7,  8,  9, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21,
        22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 34, 35, 36, 38, 41, 42, 43, 44,
        47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 67,
        68, 69, 70, 71, 72, 73, 74, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88,
        89, 90, 91, 92, 93, 95, 98, 99], device='cuda:0'), Initial Performance: (0.476, 0.06210807263851166)
DC Expert-0, val_set_size=1000, COIs=[4, 9, 11, 13, 21, 23, 28, 30, 31, 32, 52, 56, 63, 69, 74, 80, 81, 86, 93, 95], M=tensor([21, 69, 56, 23, 52, 30,  9, 28, 11, 81, 95, 63, 13,  4, 93, 32, 86, 31,
        74, 80, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.493, 0.079680484354496)
DC Expert-1, val_set_size=1000, COIs=[6, 8, 12, 14, 16, 36, 41, 42, 43, 49, 51, 54, 55, 62, 64, 68, 70, 72, 89, 90], M=tensor([12, 68, 43, 14, 54, 36, 64, 55, 16, 72,  6, 90, 89, 51,  8, 41, 62, 42,
        70, 49, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.482, 0.07461467230319976)
DC Expert-2, val_set_size=1000, COIs=[3, 7, 15, 19, 20, 22, 24, 25, 26, 38, 50, 53, 59, 73, 78, 83, 91, 92, 98, 99], M=tensor([78, 98, 59, 92,  3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22,  7, 20,
        19, 24, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), Initial Performance: (0.492, 0.07146959602832795)
SUPER-DC 0, val_set_size=2000, COIs=[21, 69, 56, 23, 52, 30, 9, 28, 11, 81, 95, 63, 13, 4, 93, 32, 86, 31, 74, 80, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([21, 69, 56, 23, 52, 30,  9, 28, 11, 81, 95, 63, 13,  4, 93, 32, 86, 31,
        74, 80, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[12, 68, 43, 14, 54, 36, 64, 55, 16, 72, 6, 90, 89, 51, 8, 41, 62, 42, 70, 49, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([12, 68, 43, 14, 54, 36, 64, 55, 16, 72,  6, 90, 89, 51,  8, 41, 62, 42,
        70, 49, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[78, 98, 59, 92, 3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22, 7, 20, 19, 24, 60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 88, 76, 29, 71, 0, 79], M=tensor([78, 98, 59, 92,  3, 15, 26, 38, 53, 99, 25, 73, 83, 91, 50, 22,  7, 20,
        19, 24, 60, 34, 84, 67, 85, 44, 18, 48,  1, 47, 61, 35, 82, 58, 88, 76,
        29, 71,  0, 79], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.501, 0.08856143605709076) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.46, 0.09180805933475494) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.485, 0.08958015716075897) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.559, 0.045569996227820715) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2955, 0.16305423258244992) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.26, 0.17124188551306724) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.15983555275201797) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.493, 0.10338157701492309) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.487, 0.09872845470905305) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.477, 0.10927810335159302) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.589, 0.04445866866906484) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3055, 0.13115771371126175) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.268, 0.1299026376605034) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.27, 0.12606158369779588) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.514, 0.10568374627828599) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.45, 0.11529359358549118) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.10580619239807129) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.609, 0.046331208010514574) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2845, 0.13341173404455184) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.261, 0.14149500972032547) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2905, 0.11421157842874527) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.51, 0.11401696622371674) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.452, 0.11785577070713044) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.487, 0.110682302236557) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.6066666666666667, 0.047936589012543364) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.11269904324412346) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2895, 0.12006784647703171) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.299, 0.10709306982159615) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.1166495589017868) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.463, 0.11769809281826019) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.476, 0.12029709327220917) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5926666666666667, 0.052933115462462106) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3185, 0.09760361018776893) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2805, 0.11468296325206756) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2995, 0.10103928518295288) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.518, 0.11579699951410294) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.474, 0.12464934110641479) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.474, 0.1258743406534195) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5873333333333334, 0.05750720539689064) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3185, 0.10951962357759476) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.11872919219732285) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3, 0.10417731708288193) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.505, 0.1278417393565178) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.469, 0.12219669204950333) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.5, 0.12145374500751495) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.6133333333333333, 0.0565255764623483) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3285, 0.10488397204875946) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2905, 0.10848292523622513) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.09933929336071014) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.499, 0.11952839887142182) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.459, 0.14295991587638854) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.466, 0.13914189100265503) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.599, 0.06731421933571498) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.09766841503977776) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.317, 0.1006041789650917) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.10132768952846527) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.509, 0.12525301277637482) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.463, 0.12556378710269928) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.12510318160057068) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5923333333333334, 0.07596384062369664) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.347, 0.09593567484617234) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.31, 0.09871640080213546) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.09499946367740632) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.501, 0.1244205561876297) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.466, 0.1392769159078598) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.472, 0.13767185652256012) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.592, 0.08258548428614934) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3545, 0.09448779448866844) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.303, 0.10448838424682617) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.345, 0.09446475583314895) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.481, 0.13958971786499025) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.45, 0.15372389233112335) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.481, 0.1513783692121506) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.6063333333333333, 0.08302147519588471) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.363, 0.09458858233690262) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.326, 0.0987089546918869) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3405, 0.09713697308301926) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.13523055136203765) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.469, 0.14321641027927398) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.492, 0.13190614247322083) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5966666666666667, 0.08705078359444936) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.373, 0.09260129964351654) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3335, 0.1010262719988823) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3405, 0.0962335974574089) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.51, 0.1386627379655838) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.461, 0.13837775421142579) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.14847862005233764) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5846666666666667, 0.09761129390696685) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.361, 0.09978493636846543) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3095, 0.108042988717556) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3315, 0.10352064549922943) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.513, 0.1368538419008255) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.445, 0.14691100233793258) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.472, 0.15226254630088806) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.6053333333333333, 0.09012688378492992) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.359, 0.10055581896007061) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.313, 0.10474380308389664) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3585, 0.09870842427015304) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.499, 0.14954925656318666) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.441, 0.14218809002637864) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.484, 0.15585236072540284) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5996666666666667, 0.1016158456603686) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.332, 0.10613022792339324) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.322, 0.10452910178899764) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3305, 0.10818422019481659) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.499, 0.146096240401268) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.445, 0.15533442759513855) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.463, 0.1567324149608612) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5813333333333334, 0.09630457664529482) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.343, 0.10330162727832794) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.327, 0.10414780700206756) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.342, 0.10158941596746444) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.492, 0.15293458020687103) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.458, 0.1531833564043045) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.502, 0.15168623054027558) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5766666666666667, 0.09935802798469862) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.336, 0.10671421870589257) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.322, 0.1102931489944458) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.329, 0.10923418664932251) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.497, 0.14758128476142884) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.474, 0.15985933297872543) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.461, 0.16798088991642) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.6, 0.090466263204813) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3235, 0.108226726770401) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2975, 0.11472890603542328) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3335, 0.10611387419700623) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.15273045563697815) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.443, 0.16386590254306793) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.464, 0.1662807900905609) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5786666666666667, 0.10489702761173249) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3465, 0.10679171082377434) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.32, 0.11456726425886155) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.327, 0.10822786915302277) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.5, 0.15486979973316192) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.45, 0.1619635968208313) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.472, 0.16645578634738922) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.586, 0.09514818731447061) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3215, 0.11181282278895378) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.313, 0.11249021971225738) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3605, 0.10153717118501664) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.507, 0.15605989670753478) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.457, 0.17714608013629912) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.461, 0.17343247389793395) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5966666666666667, 0.1032215194106102) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3375, 0.10601152968406677) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.11409299820661545) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.11007321780920029) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.493, 0.15201952433586122) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.454, 0.1649511184692383) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.466, 0.16757416093349456) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5853333333333334, 0.10079030832648277) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3265, 0.11486566162109375) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.11412253355979919) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.326, 0.10857479697465897) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.15989437878131865) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.444, 0.1809297388792038) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.485, 0.1817183974981308) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5776666666666667, 0.11125461533665656) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3295, 0.11361449754238129) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.294, 0.11852102023363113) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.318, 0.11422479712963104) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.507, 0.1725298036336899) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.459, 0.16252781748771666) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.468, 0.1719088624715805) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5966666666666667, 0.10561087662478288) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3395, 0.11321396255493164) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.327, 0.11335321193933487) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3275, 0.11070296293497085) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.494, 0.17183338379859925) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.458, 0.17244231390953063) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.488, 0.17745033717155456) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5906666666666667, 0.11005514593919118) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3495, 0.11070339587330819) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.31, 0.1232845053076744) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.10917081421613693) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.489, 0.1628816021680832) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.467, 0.18378667092323303) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.467, 0.1721564745903015) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5736666666666667, 0.12129360868533452) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3355, 0.10984601706266403) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2935, 0.12526276749372484) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3385, 0.11112240815162659) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.498, 0.16554756200313567) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.458, 0.17543105173110962) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.484, 0.1809877418279648) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5886666666666667, 0.11719946298996607) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.353, 0.10845523276925087) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.32, 0.1205093240737915) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.332, 0.11435329413414001) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.495, 0.1704342588186264) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.457, 0.18297618699073792) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.468, 0.1651481454372406) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5923333333333334, 0.11934302757183711) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3345, 0.11450187188386918) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3125, 0.1256185069680214) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3235, 0.11990446373820304) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.493, 0.1702741065621376) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.446, 0.20303792095184325) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.1705863143801689) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.58, 0.11429270124435424) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.331, 0.11614342424273491) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.296, 0.12937979143857956) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.339, 0.11586746430397034) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.493, 0.17932232952117919) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.439, 0.17288029849529266) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.502, 0.17753120946884154) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5876666666666667, 0.1172694720228513) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3325, 0.11543170219659805) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3175, 0.12283494609594345) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3435, 0.11496234595775605) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.17171954941749573) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.466, 0.16464646011590958) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.472, 0.18473450684547424) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5936666666666667, 0.13396887654066086) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.332, 0.11900923776626587) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.322, 0.12253244924545288) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.333, 0.11749280047416687) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.18059696447849274) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.456, 0.18443257236480712) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.465, 0.17832473969459534) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5806666666666667, 0.12699243531624477) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.11958591669797897) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2895, 0.12225672531127929) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3285, 0.11882516866922378) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.17965094542503357) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.454, 0.20278494477272033) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.468, 0.19580637168884277) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5976666666666667, 0.12786201359828314) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.35, 0.11588191890716552) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.12878836566209792) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.11766881483793258) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.498, 0.17534431219100952) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.446, 0.21142090332508087) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.486, 0.1732539952993393) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5893333333333334, 0.13302089468638104) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3425, 0.11782586854696274) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2915, 0.1295878061056137) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.337, 0.11564349937438965) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.489, 0.18765721356868745) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.464, 0.1995098570585251) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.468, 0.20154108333587648) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5803333333333334, 0.13339819840590159) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.11857918953895569) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3155, 0.12590353614091873) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.31, 0.12222143822908402) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.504, 0.189402113199234) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.449, 0.20258050978183748) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.486, 0.19115262460708618) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5873333333333334, 0.11781164473295212) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.334, 0.12077798676490784) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3085, 0.12585563325881957) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3375, 0.11942440378665924) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.478, 0.19542355513572693) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.443, 0.20215557432174683) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.47, 0.19534592437744142) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.594, 0.1209876497387886) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.322, 0.1219291276037693) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3055, 0.12743037343025207) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.326, 0.12522324645519256) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.498, 0.1923508073091507) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.459, 0.21187268567085266) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.471, 0.20669854617118835) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.595, 0.12666069347659747) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3375, 0.1213521567583084) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3215, 0.12576925677061082) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.338, 0.1240690735578537) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.492, 0.1979749219417572) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.469, 0.20696395897865297) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.473, 0.19633871054649352) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5863333333333334, 0.13880481280883153) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.346, 0.11973199921846389) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.13325466430187224) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.1195682052373886) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.463, 0.2075256552696228) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.469, 0.20289278173446657) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.467, 0.20992076516151428) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5823333333333334, 0.14185209925969441) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.337, 0.11843196654319763) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3265, 0.13355769580602647) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3305, 0.1261541746854782) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.0255, 0.11639465796947479), (0.2025, 0.1396341785788536), (0.242, 0.17384512490034104), (0.2555, 0.22678148692846298), (0.267, 0.29112414771318434), (0.263, 0.3547761492133141), (0.2625, 0.3693101988732815), (0.27, 0.36375497180223465), (0.2605, 0.36243046113848687), (0.257, 0.3806731471717358), (0.2465, 0.41661606314778327), (0.2955, 0.16305423258244992), (0.3055, 0.13115771371126175), (0.2845, 0.13341173404455184), (0.3155, 0.11269904324412346), (0.3185, 0.09760361018776893), (0.3185, 0.10951962357759476), (0.3285, 0.10488397204875946), (0.3485, 0.09766841503977776), (0.347, 0.09593567484617234), (0.3545, 0.09448779448866844), (0.363, 0.09458858233690262), (0.373, 0.09260129964351654), (0.361, 0.09978493636846543), (0.359, 0.10055581896007061), (0.332, 0.10613022792339324), (0.343, 0.10330162727832794), (0.336, 0.10671421870589257), (0.3235, 0.108226726770401), (0.3465, 0.10679171082377434), (0.3215, 0.11181282278895378), (0.3375, 0.10601152968406677), (0.3265, 0.11486566162109375), (0.3295, 0.11361449754238129), (0.3395, 0.11321396255493164), (0.3495, 0.11070339587330819), (0.3355, 0.10984601706266403), (0.353, 0.10845523276925087), (0.3345, 0.11450187188386918), (0.331, 0.11614342424273491), (0.3325, 0.11543170219659805), (0.332, 0.11900923776626587), (0.345, 0.11958591669797897), (0.35, 0.11588191890716552), (0.3425, 0.11782586854696274), (0.3485, 0.11857918953895569), (0.334, 0.12077798676490784), (0.322, 0.1219291276037693), (0.3375, 0.1213521567583084), (0.346, 0.11973199921846389), (0.337, 0.11843196654319763)]
TEST: 
[(0.025, 0.11545719927549362), (0.20425, 0.13799779897928238), (0.23875, 0.17136693567037584), (0.25025, 0.22336445331573487), (0.2565, 0.28720094680786135), (0.2585, 0.34438405895233154), (0.2675, 0.3530717772245407), (0.25925, 0.35215890407562256), (0.2615, 0.3524987479448318), (0.25425, 0.3688524482250214), (0.24875, 0.4050538383722305), (0.28575, 0.15867310297489165), (0.29575, 0.12985395884513856), (0.29275, 0.13017053455114364), (0.312, 0.1096806637942791), (0.31525, 0.09663069331645965), (0.3075, 0.1074906200170517), (0.32125, 0.10413835936784745), (0.33125, 0.09607350051403046), (0.33425, 0.09483123403787613), (0.34425, 0.09333729976415635), (0.3485, 0.09356449061632156), (0.3445, 0.09440436777472495), (0.33125, 0.09908768758177758), (0.35075, 0.10007828563451766), (0.336, 0.10331410619616509), (0.34025, 0.09976523348689079), (0.31975, 0.10541568452119827), (0.31575, 0.10646367204189301), (0.3325, 0.10583532917499543), (0.32275, 0.109405099183321), (0.341, 0.10342693278193474), (0.32775, 0.11300608521699905), (0.32975, 0.11272269022464752), (0.33075, 0.11050315719842911), (0.339, 0.1100046045780182), (0.31875, 0.11026253813505173), (0.32525, 0.10687355804443359), (0.3355, 0.11078091138601304), (0.331, 0.11277224433422088), (0.3075, 0.1146188023686409), (0.31225, 0.11738936626911163), (0.32775, 0.11733261424303054), (0.34525, 0.11549997454881668), (0.3255, 0.11536181658506393), (0.321, 0.11951272636651993), (0.3235, 0.12055205738544464), (0.305, 0.12062760809063912), (0.32375, 0.11786634254455566), (0.33925, 0.1149245188832283), (0.3325, 0.11473291057348252)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.74      0.55      0.63       100
           1       0.37      0.40      0.38       100
           4       0.17      0.20      0.18       100
           9       0.38      0.30      0.34       100
          11       0.18      0.16      0.17       100
          13       0.26      0.28      0.27       100
          18       0.25      0.19      0.22       100
          21       0.41      0.55      0.47       100
          23       0.42      0.61      0.50       100
          28       0.33      0.58      0.42       100
          29       0.22      0.29      0.25       100
          30       0.31      0.18      0.23       100
          31       0.28      0.26      0.27       100
          32       0.15      0.21      0.17       100
          34       0.25      0.29      0.27       100
          35       0.26      0.34      0.29       100
          44       0.18      0.09      0.12       100
          47       0.34      0.47      0.40       100
          48       0.61      0.25      0.35       100
          52       0.58      0.49      0.53       100
          56       0.58      0.39      0.47       100
          58       0.30      0.21      0.25       100
          60       0.68      0.72      0.70       100
          61       0.49      0.59      0.53       100
          63       0.34      0.21      0.26       100
          67       0.24      0.53      0.33       100
          69       0.46      0.47      0.47       100
          71       0.53      0.28      0.37       100
          74       0.13      0.24      0.17       100
          76       0.53      0.52      0.52       100
          79       0.26      0.27      0.26       100
          80       0.15      0.20      0.17       100
          81       0.35      0.28      0.31       100
          82       0.76      0.54      0.63       100
          84       0.23      0.09      0.13       100
          85       0.32      0.39      0.35       100
          86       0.40      0.20      0.27       100
          88       0.26      0.18      0.21       100
          93       0.10      0.10      0.10       100
          95       0.34      0.20      0.25       100

    accuracy                           0.33      4000
   macro avg       0.35      0.33      0.33      4000
weighted avg       0.35      0.33      0.33      4000

Collaboration_DC_1
VAL: 
[(0.028, 0.11640621066093444), (0.2095, 0.14246558374166488), (0.234, 0.19523358756303788), (0.2435, 0.2470117070376873), (0.2515, 0.29844838693737985), (0.2405, 0.3360487785935402), (0.2505, 0.36220408163964746), (0.239, 0.3566945753097534), (0.2465, 0.3663783219754696), (0.2385, 0.3778376592695713), (0.241, 0.3878095577955246), (0.26, 0.17124188551306724), (0.268, 0.1299026376605034), (0.261, 0.14149500972032547), (0.2895, 0.12006784647703171), (0.2805, 0.11468296325206756), (0.2745, 0.11872919219732285), (0.2905, 0.10848292523622513), (0.317, 0.1006041789650917), (0.31, 0.09871640080213546), (0.303, 0.10448838424682617), (0.326, 0.0987089546918869), (0.3335, 0.1010262719988823), (0.3095, 0.108042988717556), (0.313, 0.10474380308389664), (0.322, 0.10452910178899764), (0.327, 0.10414780700206756), (0.322, 0.1102931489944458), (0.2975, 0.11472890603542328), (0.32, 0.11456726425886155), (0.313, 0.11249021971225738), (0.33, 0.11409299820661545), (0.3165, 0.11412253355979919), (0.294, 0.11852102023363113), (0.327, 0.11335321193933487), (0.31, 0.1232845053076744), (0.2935, 0.12526276749372484), (0.32, 0.1205093240737915), (0.3125, 0.1256185069680214), (0.296, 0.12937979143857956), (0.3175, 0.12283494609594345), (0.322, 0.12253244924545288), (0.2895, 0.12225672531127929), (0.305, 0.12878836566209792), (0.2915, 0.1295878061056137), (0.3155, 0.12590353614091873), (0.3085, 0.12585563325881957), (0.3055, 0.12743037343025207), (0.3215, 0.12576925677061082), (0.305, 0.13325466430187224), (0.3265, 0.13355769580602647)]
TEST: 
[(0.02975, 0.11552399575710297), (0.2045, 0.14155085688829422), (0.22775, 0.19356339591741561), (0.23975, 0.2431593145132065), (0.24325, 0.2956870549917221), (0.244, 0.3317790707349777), (0.24675, 0.3614808803796768), (0.24025, 0.35887203919887545), (0.2465, 0.36442826199531553), (0.24125, 0.3783414441347122), (0.24175, 0.38329387271404264), (0.26325, 0.16271261930465697), (0.277, 0.12589090114831925), (0.26, 0.13566761291027069), (0.294, 0.11487399500608445), (0.29275, 0.1085046157836914), (0.285, 0.11174981158971786), (0.3025, 0.10188406303524972), (0.3295, 0.09533732283115387), (0.328, 0.09333844116330146), (0.32, 0.1007914663553238), (0.32975, 0.09506994843482972), (0.33, 0.09925561660528183), (0.3295, 0.10386144948005677), (0.331, 0.09961464321613311), (0.3475, 0.10009330958127975), (0.337, 0.10265553736686707), (0.331, 0.10652384388446808), (0.3075, 0.10937306553125381), (0.326, 0.10820303112268448), (0.3325, 0.10455364561080932), (0.34875, 0.10749402737617493), (0.3305, 0.10790091919898986), (0.3135, 0.11150644862651825), (0.32125, 0.10862940615415573), (0.336, 0.11680988413095474), (0.31275, 0.11932343417406083), (0.32725, 0.1149086412191391), (0.3365, 0.11864018374681473), (0.3135, 0.12347523033618928), (0.3215, 0.11801469653844833), (0.34225, 0.11608034002780915), (0.3105, 0.11638912063837051), (0.31125, 0.12298260307312012), (0.3, 0.1250815903544426), (0.32725, 0.12239588445425034), (0.318, 0.12284287810325623), (0.318, 0.12198024117946625), (0.3235, 0.11883304703235627), (0.32425, 0.1281831314563751), (0.33275, 0.1309505934715271)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.51      0.59      0.55       100
           1       0.30      0.33      0.31       100
           6       0.24      0.28      0.26       100
           8       0.34      0.44      0.39       100
          12       0.31      0.26      0.28       100
          14       0.29      0.21      0.24       100
          16       0.27      0.43      0.33       100
          18       0.20      0.27      0.23       100
          29       0.23      0.31      0.27       100
          34       0.22      0.24      0.23       100
          35       0.28      0.28      0.28       100
          36       0.31      0.15      0.20       100
          41       0.46      0.56      0.51       100
          42       0.27      0.26      0.27       100
          43       0.27      0.28      0.28       100
          44       0.18      0.22      0.20       100
          47       0.67      0.64      0.65       100
          48       0.52      0.15      0.23       100
          49       0.33      0.18      0.23       100
          51       0.24      0.23      0.23       100
          54       0.44      0.40      0.42       100
          55       0.12      0.14      0.13       100
          58       0.46      0.29      0.36       100
          60       0.55      0.50      0.52       100
          61       0.47      0.64      0.54       100
          62       0.32      0.49      0.39       100
          64       0.22      0.20      0.21       100
          67       0.41      0.40      0.41       100
          68       0.44      0.62      0.51       100
          70       0.43      0.29      0.35       100
          71       0.43      0.53      0.48       100
          72       0.19      0.15      0.17       100
          76       0.52      0.58      0.55       100
          79       0.29      0.33      0.31       100
          82       0.60      0.12      0.20       100
          84       0.18      0.12      0.14       100
          85       0.34      0.48      0.40       100
          88       0.17      0.16      0.16       100
          89       0.29      0.27      0.28       100
          90       0.30      0.29      0.29       100

    accuracy                           0.33      4000
   macro avg       0.34      0.33      0.33      4000
weighted avg       0.34      0.33      0.33      4000

Collaboration_DC_2
VAL: 
[(0.0265, 0.11638416111469269), (0.218, 0.13960152095556258), (0.231, 0.18852761551737784), (0.249, 0.2519578774869442), (0.246, 0.30815341413021086), (0.255, 0.3898617000281811), (0.254, 0.3632071052789688), (0.2515, 0.4001330908834934), (0.249, 0.3929723858237267), (0.249, 0.3840735909938812), (0.246, 0.38261088597774506), (0.2625, 0.15983555275201797), (0.27, 0.12606158369779588), (0.2905, 0.11421157842874527), (0.299, 0.10709306982159615), (0.2995, 0.10103928518295288), (0.3, 0.10417731708288193), (0.308, 0.09933929336071014), (0.314, 0.10132768952846527), (0.334, 0.09499946367740632), (0.345, 0.09446475583314895), (0.3405, 0.09713697308301926), (0.3405, 0.0962335974574089), (0.3315, 0.10352064549922943), (0.3585, 0.09870842427015304), (0.3305, 0.10818422019481659), (0.342, 0.10158941596746444), (0.329, 0.10923418664932251), (0.3335, 0.10611387419700623), (0.327, 0.10822786915302277), (0.3605, 0.10153717118501664), (0.353, 0.11007321780920029), (0.326, 0.10857479697465897), (0.318, 0.11422479712963104), (0.3275, 0.11070296293497085), (0.355, 0.10917081421613693), (0.3385, 0.11112240815162659), (0.332, 0.11435329413414001), (0.3235, 0.11990446373820304), (0.339, 0.11586746430397034), (0.3435, 0.11496234595775605), (0.333, 0.11749280047416687), (0.3285, 0.11882516866922378), (0.334, 0.11766881483793258), (0.337, 0.11564349937438965), (0.31, 0.12222143822908402), (0.3375, 0.11942440378665924), (0.326, 0.12522324645519256), (0.338, 0.1240690735578537), (0.355, 0.1195682052373886), (0.3305, 0.1261541746854782)]
TEST: 
[(0.024, 0.1154811590909958), (0.19725, 0.14007272905111312), (0.21875, 0.18753812950849533), (0.23375, 0.25023877316713333), (0.24525, 0.3026774104833603), (0.241, 0.3860662338733673), (0.2435, 0.35877887547016146), (0.2435, 0.3991609377861023), (0.2425, 0.38770058906078336), (0.24275, 0.37744122636318206), (0.241, 0.3769912008047104), (0.2555, 0.16183334761857987), (0.27325, 0.1283777221441269), (0.28225, 0.1153597531914711), (0.27825, 0.11041104352474213), (0.285, 0.1035539150238037), (0.2825, 0.10674812650680542), (0.289, 0.1020078940987587), (0.29125, 0.10470300096273422), (0.31375, 0.09790388363599777), (0.32625, 0.09773028895258903), (0.32675, 0.10094528177380561), (0.32625, 0.09916169017553329), (0.31475, 0.10576103270053863), (0.33875, 0.10295646965503692), (0.31475, 0.10962893676757812), (0.325, 0.10540400862693787), (0.31525, 0.11159739726781845), (0.32225, 0.10946984905004502), (0.319, 0.10972175353765487), (0.345, 0.1040720872282982), (0.32725, 0.11437920320034027), (0.31825, 0.11166557145118713), (0.309, 0.11861203354597091), (0.31975, 0.11354945939779282), (0.329, 0.11266056841611863), (0.31975, 0.11490886157751083), (0.31475, 0.11864576023817063), (0.31625, 0.12114769637584687), (0.3225, 0.11803381860256196), (0.334, 0.11601662245392799), (0.331, 0.12092533665895462), (0.31675, 0.12296915876865387), (0.31, 0.1214574339389801), (0.3265, 0.11964090198278426), (0.291, 0.1252600622177124), (0.31025, 0.12270736819505691), (0.31625, 0.127157624989748), (0.3215, 0.12561389946937562), (0.33275, 0.12469949305057526), (0.319, 0.12730607402324676)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.45      0.57      0.50       100
           1       0.33      0.28      0.30       100
           3       0.28      0.16      0.20       100
           7       0.23      0.43      0.30       100
          15       0.22      0.17      0.19       100
          18       0.26      0.09      0.13       100
          19       0.26      0.27      0.27       100
          20       0.60      0.53      0.56       100
          22       0.38      0.26      0.31       100
          24       0.49      0.52      0.50       100
          25       0.23      0.29      0.25       100
          26       0.23      0.19      0.21       100
          29       0.24      0.26      0.25       100
          34       0.15      0.08      0.10       100
          35       0.26      0.22      0.24       100
          38       0.25      0.31      0.27       100
          44       0.19      0.21      0.20       100
          47       0.54      0.25      0.34       100
          48       0.55      0.06      0.11       100
          50       0.14      0.20      0.16       100
          53       0.48      0.57      0.52       100
          58       0.42      0.30      0.35       100
          59       0.31      0.57      0.40       100
          60       0.69      0.61      0.65       100
          61       0.36      0.63      0.46       100
          67       0.31      0.46      0.37       100
          71       0.66      0.51      0.58       100
          73       0.27      0.22      0.24       100
          76       0.54      0.43      0.48       100
          78       0.22      0.29      0.25       100
          79       0.21      0.16      0.18       100
          82       0.65      0.33      0.44       100
          83       0.22      0.39      0.28       100
          84       0.15      0.11      0.13       100
          85       0.36      0.48      0.41       100
          88       0.14      0.06      0.08       100
          91       0.36      0.47      0.41       100
          92       0.33      0.37      0.35       100
          98       0.18      0.19      0.18       100
          99       0.23      0.26      0.25       100

    accuracy                           0.32      4000
   macro avg       0.33      0.32      0.31      4000
weighted avg       0.33      0.32      0.31      4000

do_assignment: None
seeds: [23]
name: naive-cifar100-feddf23
score_metric: contrloss
aggregation: <function fed_df at 0x7335d6f84e50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=23
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[18, 14, 81, 69, 5, 83, 71, 60, 11, 33, 7, 62, 29, 35, 61, 86, 47, 55, 26, 96, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([18, 14, 81, 69,  5, 83, 71, 60, 11, 33,  7, 62, 29, 35, 61, 86, 47, 55,
        26, 96, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.026, 0.1168324224948883)
DC 1, val_set_size=2000, COIs=[12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59, 22, 90, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59,
        22, 90, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.027, 0.11678374063968658)
DC 2, val_set_size=2000, COIs=[17, 13, 15, 65, 6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74, 19, 9, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([17, 13, 15, 65,  6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74,
        19,  9, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.03, 0.1163102513551712)
D00: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D01: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D02: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D03: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D04: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D05: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D06: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D07: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D08: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D09: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D010: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D011: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D012: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D013: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D014: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D015: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D016: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D017: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D018: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D019: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D020: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D021: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D022: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D023: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.2965, 0.08304540932178497) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3025, 0.08283185774087906) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.291, 0.08510912036895751) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3335, 0.07606647872924804) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.344, 0.07439301985502243) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.35, 0.07577457424998284) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.359, 0.07417174780368804) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.0712102769613266) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3635, 0.07359418213367462) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.07472364634275436) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.404, 0.06940093161165714) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.375, 0.07368431177735328) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.37, 0.07651571941375733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.394, 0.06949020798504353) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.0755655959546566) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3855, 0.07844983994960784) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.396, 0.07438804438710213) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3695, 0.0801240611076355) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.08136602133512497) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4135, 0.07448900976777077) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3805, 0.08376747950911521) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3865, 0.08165753442049027) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4075, 0.07813697770237922) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.08633324426412582) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3885, 0.08293413102626801) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.403, 0.08292951409518719) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.09166408360004424) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3845, 0.08857041889429093) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4005, 0.08674113361537457) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.0952511540055275) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4005, 0.09450159940123558) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4045, 0.09615065144002438) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3695, 0.09877769750356674) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.39, 0.10178457951545715) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3845, 0.1082008766233921) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.10724845284223557) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3835, 0.10978017899394035) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3785, 0.11516427305340766) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3595, 0.12162922817468644) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3745, 0.11761525040864944) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3805, 0.11943034729361535) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3575, 0.12759154152870178) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3925, 0.1155079396367073) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3875, 0.13406366670131684) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3595, 0.13724281948804856) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3835, 0.13455770087242128) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3675, 0.15741648188233376) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3605, 0.14147038707137108) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3605, 0.1396191930770874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.383, 0.13392065347731114) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.1454553722143173) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3715, 0.14892106676101685) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.14288440537452698) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3625, 0.15199402433633805) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.372, 0.15271776711940765) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3715, 0.15535514688491822) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.348, 0.16852254432439803) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.379, 0.1510284805893898) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.383, 0.16088800412416457) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.357, 0.16257860308885574) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.363, 0.1569812023639679) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3685, 0.1665576154589653) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3495, 0.17583034217357635) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3705, 0.15762226432561874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3635, 0.1554289875626564) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3345, 0.18179273736476897) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.16291657081246377) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.17560933697223663) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.17143698865175247) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.37, 0.16602276661992074) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.372, 0.17170972537994383) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.341, 0.17641650676727294) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3625, 0.17089764720201492) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3655, 0.17970738184452056) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.347, 0.18278876012563705) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.375, 0.17428994172811507) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.17961323243379593) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3425, 0.1913810243308544) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.371, 0.17447743368148805) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3775, 0.18485216385126113) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.342, 0.1871262288093567) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.353, 0.1873200364112854) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.19401397240161897) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.18433329612016677) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3645, 0.1832180165052414) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3635, 0.18658855336904526) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.351, 0.19465078276395797) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.361, 0.18892344957590104) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3605, 0.20152096098661423) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.342, 0.2040906755924225) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.36, 0.18691177368164064) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.1979251513928175) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.1945108544230461) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3705, 0.1866745219230652) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3735, 0.2048140111863613) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.339, 0.2286914110183716) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.372, 0.19168820345401763) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3625, 0.2188408883213997) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3475, 0.21258753591775895) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3685, 0.2013568839430809) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.22193484038114547) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3425, 0.21613886326551437) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3605, 0.19189072835445403) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.2136591114103794) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.332, 0.21985015892982482) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.374, 0.19724444377422332) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.21595349502563477) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3415, 0.2250384691953659) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.363, 0.20449384433031081) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3715, 0.21180257654190063) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3535, 0.22088587778806687) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3665, 0.2061238671541214) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.23191678643226624) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.21374833184480668) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.364, 0.2002830617427826) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3665, 0.22821684831380845) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3435, 0.22335919207334518) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.376, 0.21144743633270263) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.36, 0.2310977092385292) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3425, 0.23302406233549117) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.20402168011665345) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.2372950207591057) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3465, 0.2397658583521843) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.369, 0.2103334082365036) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.22667852717638015) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.35, 0.2248565612435341) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.21529162108898162) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3715, 0.23157547330856323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.343, 0.23289103507995607) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.371, 0.2008060301542282) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3785, 0.22054344528913497) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.25274548041820527) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3765, 0.1998407779932022) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3795, 0.22445481759309768) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.352, 0.22972109192609788) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.22002633261680604) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3745, 0.22706951141357423) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.329, 0.24859958732128143) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.367, 0.21977895724773408) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.23722237151861192) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.2582133547067642) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3755, 0.2347539768218994) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3595, 0.24585436648130418) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.347, 0.25278237807750703) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.23193707156181337) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.2434698232412338) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3475, 0.23746972858905793) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.369, 0.22812347328662871) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.249736492395401) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.2494199547767639) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.026, 0.1168324224948883), (0.2965, 0.08304540932178497), (0.3335, 0.07606647872924804), (0.359, 0.07417174780368804), (0.354, 0.07472364634275436), (0.37, 0.07651571941375733), (0.3855, 0.07844983994960784), (0.378, 0.08136602133512497), (0.3865, 0.08165753442049027), (0.3885, 0.08293413102626801), (0.3845, 0.08857041889429093), (0.4005, 0.09450159940123558), (0.39, 0.10178457951545715), (0.3835, 0.10978017899394035), (0.3745, 0.11761525040864944), (0.3925, 0.1155079396367073), (0.3835, 0.13455770087242128), (0.3605, 0.1396191930770874), (0.3715, 0.14892106676101685), (0.372, 0.15271776711940765), (0.379, 0.1510284805893898), (0.363, 0.1569812023639679), (0.3705, 0.15762226432561874), (0.3725, 0.16291657081246377), (0.37, 0.16602276661992074), (0.3625, 0.17089764720201492), (0.375, 0.17428994172811507), (0.371, 0.17447743368148805), (0.353, 0.1873200364112854), (0.3645, 0.1832180165052414), (0.361, 0.18892344957590104), (0.36, 0.18691177368164064), (0.3705, 0.1866745219230652), (0.372, 0.19168820345401763), (0.3685, 0.2013568839430809), (0.3605, 0.19189072835445403), (0.374, 0.19724444377422332), (0.363, 0.20449384433031081), (0.3665, 0.2061238671541214), (0.364, 0.2002830617427826), (0.376, 0.21144743633270263), (0.3695, 0.20402168011665345), (0.369, 0.2103334082365036), (0.3695, 0.21529162108898162), (0.371, 0.2008060301542282), (0.3765, 0.1998407779932022), (0.3695, 0.22002633261680604), (0.367, 0.21977895724773408), (0.3755, 0.2347539768218994), (0.362, 0.23193707156181337), (0.369, 0.22812347328662871)]
TEST: 
[(0.023, 0.11593683683872223), (0.298, 0.08229166674613952), (0.3415, 0.07364748355746269), (0.3685, 0.07098437482118607), (0.3695, 0.07155200546979903), (0.386, 0.07231014966964722), (0.38625, 0.07422318893671036), (0.396, 0.07616555124521256), (0.39725, 0.07700066167116165), (0.39925, 0.07871709641814231), (0.39025, 0.08591697525978088), (0.393, 0.0904759484231472), (0.38725, 0.09774386072158814), (0.38375, 0.10772576460242271), (0.3855, 0.11733546993136405), (0.39175, 0.11279528763890266), (0.38975, 0.12678540301322938), (0.3715, 0.13406619310379028), (0.371, 0.14359093433618544), (0.373, 0.1507601336836815), (0.37225, 0.1511396301984787), (0.375, 0.15374127846956254), (0.3715, 0.1537630668282509), (0.36725, 0.16141999351978303), (0.36725, 0.1619666806459427), (0.367, 0.16857256442308427), (0.36525, 0.17006687286496164), (0.3785, 0.16806666952371596), (0.3645, 0.18272515797615052), (0.3735, 0.17857570642232895), (0.36675, 0.18779231661558152), (0.36725, 0.18142975974082948), (0.371, 0.17932205855846406), (0.36575, 0.19133281832933427), (0.37225, 0.19368408554792405), (0.37475, 0.1857637522816658), (0.3745, 0.19047166150808334), (0.37025, 0.1948377285003662), (0.36975, 0.202948533475399), (0.36775, 0.2006914033293724), (0.37925, 0.2022097560763359), (0.36775, 0.19840509355068206), (0.36725, 0.20685616278648378), (0.3795, 0.2053061764240265), (0.3615, 0.2007358527779579), (0.3745, 0.1926799378991127), (0.37525, 0.2187641103863716), (0.37575, 0.2131342885494232), (0.37575, 0.23637678068876267), (0.3645, 0.2275472948551178), (0.36875, 0.22400725769996643)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.40      0.43      0.42       100
           2       0.18      0.21      0.19       100
           3       0.27      0.25      0.26       100
           5       0.38      0.42      0.40       100
           7       0.33      0.32      0.32       100
          10       0.26      0.30      0.28       100
          11       0.24      0.22      0.23       100
          14       0.23      0.25      0.24       100
          16       0.43      0.41      0.42       100
          18       0.30      0.23      0.26       100
          24       0.48      0.64      0.55       100
          26       0.26      0.22      0.24       100
          28       0.60      0.50      0.54       100
          29       0.30      0.26      0.28       100
          33       0.31      0.46      0.37       100
          34       0.29      0.32      0.30       100
          35       0.22      0.24      0.23       100
          37       0.37      0.43      0.40       100
          39       0.49      0.39      0.44       100
          45       0.15      0.14      0.15       100
          47       0.50      0.40      0.44       100
          48       0.62      0.40      0.49       100
          54       0.33      0.44      0.38       100
          55       0.10      0.13      0.11       100
          56       0.43      0.49      0.46       100
          58       0.38      0.47      0.42       100
          60       0.70      0.76      0.73       100
          61       0.58      0.44      0.50       100
          62       0.31      0.37      0.34       100
          67       0.48      0.43      0.46       100
          69       0.56      0.58      0.57       100
          71       0.71      0.67      0.69       100
          75       0.67      0.46      0.54       100
          79       0.33      0.39      0.36       100
          81       0.25      0.39      0.31       100
          83       0.40      0.30      0.34       100
          86       0.56      0.35      0.43       100
          92       0.27      0.23      0.25       100
          96       0.31      0.25      0.28       100
          99       0.25      0.16      0.20       100

    accuracy                           0.37      4000
   macro avg       0.38      0.37      0.37      4000
weighted avg       0.38      0.37      0.37      4000

No_Competition_DC_1
VAL: 
[(0.027, 0.11678374063968658), (0.3025, 0.08283185774087906), (0.344, 0.07439301985502243), (0.3705, 0.0712102769613266), (0.404, 0.06940093161165714), (0.394, 0.06949020798504353), (0.396, 0.07438804438710213), (0.4135, 0.07448900976777077), (0.4075, 0.07813697770237922), (0.403, 0.08292951409518719), (0.4005, 0.08674113361537457), (0.4045, 0.09615065144002438), (0.3845, 0.1082008766233921), (0.3785, 0.11516427305340766), (0.3805, 0.11943034729361535), (0.3875, 0.13406366670131684), (0.3675, 0.15741648188233376), (0.383, 0.13392065347731114), (0.382, 0.14288440537452698), (0.3715, 0.15535514688491822), (0.383, 0.16088800412416457), (0.3685, 0.1665576154589653), (0.3635, 0.1554289875626564), (0.3705, 0.17560933697223663), (0.372, 0.17170972537994383), (0.3655, 0.17970738184452056), (0.371, 0.17961323243379593), (0.3775, 0.18485216385126113), (0.364, 0.19401397240161897), (0.3635, 0.18658855336904526), (0.3605, 0.20152096098661423), (0.368, 0.1979251513928175), (0.3735, 0.2048140111863613), (0.3625, 0.2188408883213997), (0.384, 0.22193484038114547), (0.391, 0.2136591114103794), (0.378, 0.21595349502563477), (0.3715, 0.21180257654190063), (0.376, 0.23191678643226624), (0.3665, 0.22821684831380845), (0.36, 0.2310977092385292), (0.373, 0.2372950207591057), (0.376, 0.22667852717638015), (0.3715, 0.23157547330856323), (0.3785, 0.22054344528913497), (0.3795, 0.22445481759309768), (0.3745, 0.22706951141357423), (0.37, 0.23722237151861192), (0.3595, 0.24585436648130418), (0.368, 0.2434698232412338), (0.3705, 0.249736492395401)]
TEST: 
[(0.02575, 0.11581322306394577), (0.3215, 0.08047413620352745), (0.37625, 0.07167120814323426), (0.4005, 0.06768778312206268), (0.411, 0.06637062603235244), (0.41775, 0.06687169700860977), (0.42175, 0.0699919947385788), (0.42025, 0.07101811954379082), (0.4345, 0.07430679687857628), (0.42025, 0.07905330002307892), (0.4075, 0.08190123847126961), (0.4115, 0.0890464739203453), (0.39625, 0.09977763676643371), (0.405, 0.10519194573163987), (0.40675, 0.1092633028626442), (0.3945, 0.12495827209949494), (0.38075, 0.14602761632204056), (0.39125, 0.12310546928644181), (0.39325, 0.13234542679786682), (0.386, 0.14375316822528839), (0.3805, 0.14912942171096802), (0.3815, 0.15291822403669358), (0.38975, 0.14252458721399308), (0.38125, 0.16394327867031097), (0.383, 0.16041910713911056), (0.37325, 0.17163568425178527), (0.3845, 0.16707241010665894), (0.39125, 0.1686492450237274), (0.38175, 0.17833690744638442), (0.37775, 0.1737465210556984), (0.38025, 0.18780203419923783), (0.389, 0.18864437931776046), (0.38675, 0.19552485299110411), (0.38, 0.20335507291555405), (0.386, 0.20181654059886933), (0.392, 0.2002410792708397), (0.388, 0.19971502608060837), (0.38425, 0.19888829481601716), (0.3835, 0.21699912768602372), (0.37625, 0.2188133498430252), (0.38475, 0.21698891866207123), (0.3865, 0.22266648685932158), (0.38925, 0.2115835930109024), (0.38025, 0.22016513031721116), (0.38475, 0.211021508872509), (0.39425, 0.21562081968784333), (0.394, 0.21479624462127686), (0.3905, 0.22037886667251588), (0.38125, 0.22941531747579574), (0.3805, 0.2297763228416443), (0.3855, 0.23093493932485581)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.52      0.49      0.50       100
           2       0.26      0.27      0.27       100
           3       0.23      0.32      0.27       100
          10       0.36      0.28      0.32       100
          12       0.32      0.33      0.33       100
          16       0.25      0.40      0.31       100
          20       0.55      0.68      0.61       100
          21       0.42      0.43      0.43       100
          22       0.34      0.31      0.33       100
          24       0.67      0.61      0.64       100
          28       0.62      0.45      0.52       100
          34       0.30      0.24      0.27       100
          37       0.25      0.39      0.30       100
          38       0.19      0.21      0.20       100
          39       0.32      0.34      0.33       100
          45       0.16      0.15      0.15       100
          48       0.56      0.44      0.49       100
          50       0.17      0.17      0.17       100
          52       0.67      0.72      0.69       100
          54       0.45      0.41      0.43       100
          56       0.40      0.56      0.47       100
          58       0.49      0.36      0.41       100
          59       0.24      0.11      0.15       100
          63       0.39      0.36      0.37       100
          66       0.27      0.26      0.27       100
          67       0.29      0.25      0.27       100
          73       0.36      0.39      0.38       100
          75       0.62      0.45      0.52       100
          76       0.57      0.71      0.63       100
          78       0.20      0.24      0.22       100
          79       0.52      0.41      0.46       100
          82       0.64      0.70      0.67       100
          87       0.35      0.34      0.35       100
          89       0.32      0.36      0.34       100
          90       0.30      0.34      0.32       100
          92       0.40      0.40      0.40       100
          94       0.66      0.56      0.61       100
          95       0.49      0.38      0.43       100
          97       0.30      0.33      0.32       100
          99       0.34      0.27      0.30       100

    accuracy                           0.39      4000
   macro avg       0.39      0.39      0.39      4000
weighted avg       0.39      0.39      0.39      4000

No_Competition_DC_2
VAL: 
[(0.03, 0.1163102513551712), (0.291, 0.08510912036895751), (0.35, 0.07577457424998284), (0.3635, 0.07359418213367462), (0.375, 0.07368431177735328), (0.379, 0.0755655959546566), (0.3695, 0.0801240611076355), (0.3805, 0.08376747950911521), (0.38, 0.08633324426412582), (0.3615, 0.09166408360004424), (0.3615, 0.0952511540055275), (0.3695, 0.09877769750356674), (0.353, 0.10724845284223557), (0.3595, 0.12162922817468644), (0.3575, 0.12759154152870178), (0.3595, 0.13724281948804856), (0.3605, 0.14147038707137108), (0.3565, 0.1454553722143173), (0.3625, 0.15199402433633805), (0.348, 0.16852254432439803), (0.357, 0.16257860308885574), (0.3495, 0.17583034217357635), (0.3345, 0.18179273736476897), (0.355, 0.17143698865175247), (0.341, 0.17641650676727294), (0.347, 0.18278876012563705), (0.3425, 0.1913810243308544), (0.342, 0.1871262288093567), (0.3565, 0.18433329612016677), (0.351, 0.19465078276395797), (0.342, 0.2040906755924225), (0.3525, 0.1945108544230461), (0.339, 0.2286914110183716), (0.3475, 0.21258753591775895), (0.3425, 0.21613886326551437), (0.332, 0.21985015892982482), (0.3415, 0.2250384691953659), (0.3535, 0.22088587778806687), (0.353, 0.21374833184480668), (0.3435, 0.22335919207334518), (0.3425, 0.23302406233549117), (0.3465, 0.2397658583521843), (0.35, 0.2248565612435341), (0.343, 0.23289103507995607), (0.334, 0.25274548041820527), (0.352, 0.22972109192609788), (0.329, 0.24859958732128143), (0.334, 0.2582133547067642), (0.347, 0.25278237807750703), (0.3475, 0.23746972858905793), (0.354, 0.2494199547767639)]
TEST: 
[(0.02375, 0.11537219303846359), (0.291, 0.08391563254594803), (0.36175, 0.07407785853743554), (0.36675, 0.07202922558784484), (0.3885, 0.0719781256020069), (0.3855, 0.0734828300178051), (0.38025, 0.07864724665880203), (0.38325, 0.08129808402061463), (0.37875, 0.08432783886790275), (0.3675, 0.08959856364130973), (0.3725, 0.09244436061382294), (0.3735, 0.09778313994407654), (0.3645, 0.10274449795484543), (0.35625, 0.12050425404310226), (0.3655, 0.12556860882043838), (0.3495, 0.13425768733024598), (0.357, 0.1381195398569107), (0.3575, 0.13968673354387284), (0.36525, 0.14884301835298538), (0.35725, 0.1633667989373207), (0.3715, 0.15709809625148774), (0.35625, 0.17301829028129578), (0.348, 0.1742744773030281), (0.3615, 0.16809402567148207), (0.34625, 0.17581523233652116), (0.36525, 0.18071663653850556), (0.35325, 0.1898324292898178), (0.359, 0.18488767772912978), (0.3705, 0.1822247884273529), (0.358, 0.19484915834665298), (0.352, 0.20106586068868637), (0.35575, 0.19620516514778139), (0.35325, 0.22231222343444823), (0.35425, 0.20970731341838836), (0.3455, 0.21239040517807006), (0.35575, 0.2112521320581436), (0.3485, 0.21874726235866546), (0.35975, 0.2157336963415146), (0.365, 0.21008742439746858), (0.348, 0.21690875840187074), (0.34775, 0.22880449068546296), (0.34825, 0.23506329083442687), (0.35175, 0.22666682839393615), (0.35275, 0.23129042649269105), (0.35175, 0.24119779992103577), (0.357, 0.21707730430364608), (0.35675, 0.2310136808156967), (0.347, 0.2412845638990402), (0.34825, 0.24016821563243865), (0.35475, 0.22714313066005706), (0.35025, 0.24119197285175323)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.32      0.55      0.41       100
           2       0.20      0.25      0.22       100
           3       0.26      0.24      0.25       100
           6       0.29      0.41      0.34       100
           9       0.53      0.47      0.50       100
          10       0.30      0.28      0.29       100
          13       0.33      0.36      0.35       100
          15       0.21      0.21      0.21       100
          16       0.32      0.33      0.32       100
          17       0.47      0.54      0.50       100
          19       0.23      0.25      0.24       100
          23       0.62      0.47      0.53       100
          24       0.53      0.70      0.60       100
          27       0.26      0.16      0.20       100
          28       0.57      0.50      0.53       100
          32       0.35      0.27      0.31       100
          34       0.22      0.33      0.27       100
          37       0.27      0.27      0.27       100
          39       0.44      0.42      0.43       100
          41       0.63      0.57      0.60       100
          45       0.15      0.13      0.14       100
          46       0.25      0.36      0.30       100
          48       0.58      0.44      0.50       100
          53       0.63      0.53      0.58       100
          54       0.36      0.20      0.26       100
          56       0.43      0.46      0.45       100
          58       0.40      0.33      0.36       100
          65       0.27      0.14      0.18       100
          67       0.33      0.46      0.39       100
          68       0.74      0.70      0.72       100
          70       0.26      0.27      0.26       100
          74       0.23      0.26      0.24       100
          75       0.75      0.46      0.57       100
          79       0.29      0.38      0.33       100
          80       0.12      0.07      0.09       100
          84       0.19      0.19      0.19       100
          88       0.23      0.28      0.25       100
          91       0.48      0.34      0.40       100
          92       0.21      0.30      0.25       100
          99       0.25      0.13      0.17       100

    accuracy                           0.35      4000
   macro avg       0.36      0.35      0.35      4000
weighted avg       0.36      0.35      0.35      4000

Competition
DC 0, val_set_size=2000, COIs=[18, 14, 81, 69, 5, 83, 71, 60, 11, 33, 7, 62, 29, 35, 61, 86, 47, 55, 26, 96, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([18, 14, 81, 69,  5, 83, 71, 60, 11, 33,  7, 62, 29, 35, 61, 86, 47, 55,
        26, 96, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.026, 0.1168324224948883)
DC 1, val_set_size=2000, COIs=[12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59, 22, 90, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59,
        22, 90, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.027, 0.11678374063968658)
DC 2, val_set_size=2000, COIs=[17, 13, 15, 65, 6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74, 19, 9, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([17, 13, 15, 65,  6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74,
        19,  9, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.03, 0.1163102513551712)
D00: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D01: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D02: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D03: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D04: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D05: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D06: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D07: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D08: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D09: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D010: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D011: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D012: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D013: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D014: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D015: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D016: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D017: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D018: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D019: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D020: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D021: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D022: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D023: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO1']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.216, 0.14342691899836063) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2405, 0.1356917760372162) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2135, 0.1355247532725334) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.247, 0.18656919014453888) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.17221090734004973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2345, 0.17292525608092546) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.255, 0.24283494445681572) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2695, 0.22505520770698786) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.22483699671924115) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2615, 0.2976629647612572) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.279, 0.26898979499936104) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.275103870883584) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2625, 0.35112429291009906) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.3171641520932317) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.3326835628822446) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO4', '(DO5']
DC 2 --> ['(DO2', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2575, 0.3636056214570999) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.271, 0.3345565497428179) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2525, 0.3651900195106864) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.258, 0.3819810532331467) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.34239554576575754) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.36228471107780935) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.26, 0.4108043337762356) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2715, 0.35322176441550257) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.38777871708758177) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.4256083210706711) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.3678565436378121) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.382801822450012) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.258, 0.39804599416255954) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.268, 0.37827240811288354) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.248, 0.40026304134726526) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO1', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2525, 0.3741797292232513) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.4210525951385498) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.248, 0.3793878228366375) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2485, 0.40756386986374854) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.3852203460037708) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2465, 0.4361692093461752) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.4095104324221611) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2625, 0.4190098704844713) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2425, 0.4199488381147385) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2545, 0.42103124076128007) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.4032676024734974) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.241, 0.4245464803650975) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.37669437313079834) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2585, 0.38845119705796244) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.248, 0.4455502573400736) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO0', '(DO4']
DC 2 --> ['(DO3', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2465, 0.4030369937717915) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.269, 0.3840648172199726) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2505, 0.4128659730553627) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.3922475471198559) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.39057048147916795) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2445, 0.4367180672585964) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.249, 0.4055356286764145) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2645, 0.4011164846867323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.40868275832384826) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.37146446096897123) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.258, 0.383947775170207) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.40967758566141127) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2535, 0.3613130425810814) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.3813009446710348) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2395, 0.3980805960074067) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO0', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.255, 0.3720545482635498) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.3426082690358162) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2385, 0.38501544991135594) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.34275149697065355) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.259, 0.3547861011177301) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2385, 0.3649175650477409) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2585, 0.3607952667474747) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.249, 0.3677896673083305) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.244, 0.36912829396128655) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.3480403705239296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.30561278808116915) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2395, 0.3416158460974693) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2445, 0.3228305082321167) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.3588444672226906) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2435, 0.31703384909406307) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO2', '(DO5']
DC 1 --> ['(DO1', '(DO0']
DC 2 --> ['(DO3', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.256, 0.3272542552947998) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.261, 0.33967052975296974) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.24, 0.3588455178141594) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.3113488175868988) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.258, 0.340274385496974) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2435, 0.30351312055438756) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.30796398133039476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2575, 0.33164949898421764) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2445, 0.30711138370633123) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.246, 0.31352120560407637) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.249, 0.3158882312476635) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.24, 0.301270977139473) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2525, 0.28832598531246184) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2625, 0.32970688642561435) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.236, 0.2753350436091423) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO5', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.28705767917633057) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2715, 0.3022927533686161) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2455, 0.27229013067483904) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2565, 0.2995013778805733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.3053451600074768) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2375, 0.29150083124637605) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.3008626569509506) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2595, 0.2772141086012125) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.27547361980378626) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2585, 0.27227403631806374) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2615, 0.2995353860259056) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.27378126108646395) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2565, 0.28127973079681395) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.271, 0.28129836386442186) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2345, 0.2814202661216259) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO0', '(DO5']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO3', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.2854701761007309) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2695, 0.2857671298831701) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.2651493326127529) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.2707479503750801) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.29242908650636673) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.233, 0.2732945003211498) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2525, 0.2676883040070534) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.2654401848167181) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.237, 0.2816130879074335) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.28954066598415373) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.2824119353741407) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.27800668217241764) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.2708244723677635) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.2787455497682095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.247, 0.28324765250086786) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO0', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.259, 0.24378727334737776) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.27238303104043005) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2455, 0.28755899639613924) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2535, 0.26783641320466994) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.2799749159514904) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.2752544973492622) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.264, 0.27571551823616025) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.28753275656700134) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.3006300632655621) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2545, 0.28924836897850037) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.2603875749707222) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.243, 0.2729441220909357) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.2895165516138077) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.254, 0.2938576781004667) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.28872280709445475) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO2', '(DO0']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO4', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.2818432155251503) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.28512423834204675) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.255, 0.2580226414203644) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2575, 0.27785387623310087) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2565, 0.27940384048223493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2495, 0.2804775267392397) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.26, 0.2903012451529503) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2625, 0.2986274829953909) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.242, 0.3100309982597828) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.259, 0.26000786751508714) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2725, 0.2749856511950493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2375, 0.2862333755195141) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.254, 0.2928605116009712) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.264, 0.2742216892838478) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.2494155874401331) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.026, 0.1168324224948883), (0.216, 0.14342691899836063), (0.247, 0.18656919014453888), (0.255, 0.24283494445681572), (0.2615, 0.2976629647612572), (0.2625, 0.35112429291009906), (0.2575, 0.3636056214570999), (0.258, 0.3819810532331467), (0.26, 0.4108043337762356), (0.2555, 0.4256083210706711), (0.258, 0.39804599416255954), (0.2525, 0.3741797292232513), (0.2485, 0.40756386986374854), (0.25, 0.4095104324221611), (0.2545, 0.42103124076128007), (0.2555, 0.37669437313079834), (0.2465, 0.4030369937717915), (0.2495, 0.3922475471198559), (0.249, 0.4055356286764145), (0.245, 0.37146446096897123), (0.2535, 0.3613130425810814), (0.255, 0.3720545482635498), (0.2555, 0.34275149697065355), (0.2585, 0.3607952667474747), (0.2495, 0.3480403705239296), (0.2445, 0.3228305082321167), (0.256, 0.3272542552947998), (0.245, 0.3113488175868988), (0.2455, 0.30796398133039476), (0.246, 0.31352120560407637), (0.2525, 0.28832598531246184), (0.251, 0.28705767917633057), (0.2565, 0.2995013778805733), (0.2595, 0.3008626569509506), (0.2585, 0.27227403631806374), (0.2565, 0.28127973079681395), (0.2455, 0.2854701761007309), (0.251, 0.2707479503750801), (0.2525, 0.2676883040070534), (0.2495, 0.28954066598415373), (0.257, 0.2708244723677635), (0.259, 0.24378727334737776), (0.2535, 0.26783641320466994), (0.264, 0.27571551823616025), (0.2545, 0.28924836897850037), (0.2495, 0.2895165516138077), (0.2555, 0.2818432155251503), (0.2575, 0.27785387623310087), (0.26, 0.2903012451529503), (0.259, 0.26000786751508714), (0.254, 0.2928605116009712)]
TEST: 
[(0.023, 0.11593683683872223), (0.21775, 0.14079903483390807), (0.248, 0.18290046495199203), (0.2675, 0.23637906467914582), (0.26575, 0.29156484520435333), (0.27275, 0.341429651260376), (0.2675, 0.35250046324729917), (0.27075, 0.365230589389801), (0.26975, 0.39287601125240323), (0.27325, 0.40569794380664825), (0.2655, 0.379698624253273), (0.268, 0.35988650166988373), (0.254, 0.39370403015613553), (0.267, 0.40013837957382203), (0.26325, 0.41006519401073455), (0.26375, 0.36814234483242037), (0.26125, 0.3985185053348541), (0.25575, 0.38289954340457916), (0.251, 0.401927005648613), (0.2565, 0.3661448253393173), (0.26, 0.3494401248693466), (0.25775, 0.3625204744338989), (0.2565, 0.33476153695583344), (0.25275, 0.35502397978305816), (0.2615, 0.33690165042877196), (0.2495, 0.31406520891189577), (0.258, 0.31984540927410127), (0.25375, 0.302039541721344), (0.25425, 0.2981618137359619), (0.256, 0.3080788643360138), (0.2555, 0.2827910472154617), (0.25725, 0.2817666895389557), (0.25525, 0.29573370778560637), (0.25975, 0.2996773654222488), (0.2585, 0.2675812258720398), (0.267, 0.2739150243997574), (0.25375, 0.27952483415603635), (0.26375, 0.2634369900226593), (0.265, 0.26416606092453004), (0.25225, 0.2828054538965225), (0.26075, 0.2619350116252899), (0.2595, 0.24066259002685547), (0.265, 0.2553274207115173), (0.2685, 0.2631921578645706), (0.2635, 0.27670224285125733), (0.263, 0.2735405114889145), (0.2595, 0.2753967946767807), (0.26025, 0.27021350610256195), (0.26575, 0.2769465831518173), (0.25925, 0.2528557556271553), (0.263, 0.2879720162153244)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.67      0.02      0.04       100
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       100
           5       0.27      0.45      0.34       100
           7       0.25      0.56      0.35       100
          10       0.00      0.00      0.00       100
          11       0.10      0.31      0.15       100
          14       0.19      0.29      0.23       100
          16       0.67      0.06      0.11       100
          18       0.18      0.50      0.26       100
          24       0.67      0.02      0.04       100
          26       0.20      0.37      0.26       100
          28       0.75      0.03      0.06       100
          29       0.22      0.38      0.28       100
          33       0.26      0.41      0.32       100
          34       0.50      0.01      0.02       100
          35       0.17      0.38      0.24       100
          37       0.50      0.01      0.02       100
          39       0.67      0.02      0.04       100
          45       0.20      0.01      0.02       100
          47       0.37      0.59      0.46       100
          48       0.50      0.01      0.02       100
          54       0.00      0.00      0.00       100
          55       0.12      0.32      0.17       100
          56       0.71      0.05      0.09       100
          58       0.00      0.00      0.00       100
          60       0.57      0.76      0.65       100
          61       0.40      0.72      0.51       100
          62       0.29      0.52      0.37       100
          67       0.77      0.10      0.18       100
          69       0.42      0.70      0.52       100
          71       0.48      0.80      0.60       100
          75       0.00      0.00      0.00       100
          79       0.33      0.02      0.04       100
          81       0.24      0.63      0.35       100
          83       0.28      0.57      0.38       100
          86       0.30      0.58      0.40       100
          92       0.00      0.00      0.00       100
          96       0.21      0.32      0.26       100
          99       0.00      0.00      0.00       100

    accuracy                           0.26      4000
   macro avg       0.31      0.26      0.19      4000
weighted avg       0.31      0.26      0.19      4000

Competition_DC_1
VAL: 
[(0.027, 0.11678374063968658), (0.2405, 0.1356917760372162), (0.273, 0.17221090734004973), (0.2695, 0.22505520770698786), (0.279, 0.26898979499936104), (0.277, 0.3171641520932317), (0.271, 0.3345565497428179), (0.273, 0.34239554576575754), (0.2715, 0.35322176441550257), (0.273, 0.3678565436378121), (0.268, 0.37827240811288354), (0.266, 0.4210525951385498), (0.2655, 0.3852203460037708), (0.2625, 0.4190098704844713), (0.2665, 0.4032676024734974), (0.2585, 0.38845119705796244), (0.269, 0.3840648172199726), (0.2685, 0.39057048147916795), (0.2645, 0.4011164846867323), (0.258, 0.383947775170207), (0.265, 0.3813009446710348), (0.2635, 0.3426082690358162), (0.259, 0.3547861011177301), (0.249, 0.3677896673083305), (0.256, 0.30561278808116915), (0.2635, 0.3588444672226906), (0.261, 0.33967052975296974), (0.258, 0.340274385496974), (0.2575, 0.33164949898421764), (0.249, 0.3158882312476635), (0.2625, 0.32970688642561435), (0.2715, 0.3022927533686161), (0.2605, 0.3053451600074768), (0.2595, 0.2772141086012125), (0.2615, 0.2995353860259056), (0.271, 0.28129836386442186), (0.2695, 0.2857671298831701), (0.2605, 0.29242908650636673), (0.2665, 0.2654401848167181), (0.2665, 0.2824119353741407), (0.263, 0.2787455497682095), (0.2705, 0.27238303104043005), (0.265, 0.2799749159514904), (0.25, 0.28753275656700134), (0.2605, 0.2603875749707222), (0.254, 0.2938576781004667), (0.267, 0.28512423834204675), (0.2565, 0.27940384048223493), (0.2625, 0.2986274829953909), (0.2725, 0.2749856511950493), (0.264, 0.2742216892838478)]
TEST: 
[(0.02575, 0.11581322306394577), (0.24325, 0.13353086924552918), (0.265, 0.16941525626182558), (0.27475, 0.22116757547855379), (0.274, 0.26612696278095244), (0.2805, 0.3144462481737137), (0.2755, 0.3300800054073334), (0.27775, 0.34234440577030184), (0.27725, 0.3487243068218231), (0.27325, 0.36673125958442687), (0.27425, 0.3746287795305252), (0.26775, 0.4128129394054413), (0.27025, 0.3859584276676178), (0.259, 0.40631947612762453), (0.26025, 0.3971990189552307), (0.26375, 0.39210276675224304), (0.2675, 0.3859956934452057), (0.25725, 0.3881022427082062), (0.258, 0.4015549142360687), (0.26525, 0.3833112301826477), (0.257, 0.3788969316482544), (0.26125, 0.3416852900981903), (0.25725, 0.3507179145812988), (0.25975, 0.36403079080581663), (0.26275, 0.3082440159320831), (0.25625, 0.36311796498298643), (0.25775, 0.34501073670387267), (0.26175, 0.3434103273153305), (0.254, 0.32890381455421447), (0.2665, 0.31502384948730466), (0.26475, 0.33222167468070984), (0.268, 0.30330849933624265), (0.267, 0.3090204713344574), (0.26625, 0.2779990673065186), (0.263, 0.30200470781326294), (0.26375, 0.28155833601951596), (0.27, 0.2823022365570068), (0.26, 0.293084997177124), (0.267, 0.2609774111509323), (0.26825, 0.28177918910980226), (0.26875, 0.27846595966815946), (0.26675, 0.2742202709913254), (0.2625, 0.28724455392360687), (0.25075, 0.28858042669296263), (0.2615, 0.26288536620140074), (0.25425, 0.294005823969841), (0.26125, 0.2848948942422867), (0.26375, 0.2795739334821701), (0.263, 0.29647698080539703), (0.2695, 0.27298716676235196), (0.26475, 0.27327235221862795)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00       100
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          12       0.24      0.43      0.31       100
          16       0.00      0.00      0.00       100
          20       0.41      0.69      0.51       100
          21       0.32      0.65      0.43       100
          22       0.19      0.44      0.27       100
          24       0.79      0.11      0.19       100
          28       0.00      0.00      0.00       100
          34       0.00      0.00      0.00       100
          37       0.00      0.00      0.00       100
          38       0.19      0.37      0.25       100
          39       0.00      0.00      0.00       100
          45       0.00      0.00      0.00       100
          48       0.91      0.10      0.18       100
          50       0.14      0.35      0.20       100
          52       0.61      0.78      0.68       100
          54       0.75      0.03      0.06       100
          56       1.00      0.01      0.02       100
          58       0.00      0.00      0.00       100
          59       0.27      0.46      0.34       100
          63       0.30      0.51      0.38       100
          66       0.16      0.33      0.21       100
          67       0.00      0.00      0.00       100
          73       0.21      0.53      0.30       100
          75       1.00      0.03      0.06       100
          76       0.44      0.73      0.55       100
          78       0.13      0.26      0.18       100
          79       1.00      0.01      0.02       100
          82       0.44      0.78      0.57       100
          87       0.24      0.46      0.32       100
          89       0.18      0.49      0.27       100
          90       0.25      0.38      0.30       100
          92       0.00      0.00      0.00       100
          94       0.32      0.64      0.43       100
          95       0.26      0.57      0.36       100
          97       0.19      0.43      0.27       100
          99       0.67      0.02      0.04       100

    accuracy                           0.26      4000
   macro avg       0.29      0.26      0.19      4000
weighted avg       0.29      0.26      0.19      4000

Competition_DC_2
VAL: 
[(0.03, 0.1163102513551712), (0.2135, 0.1355247532725334), (0.2345, 0.17292525608092546), (0.252, 0.22483699671924115), (0.249, 0.275103870883584), (0.249, 0.3326835628822446), (0.2525, 0.3651900195106864), (0.25, 0.36228471107780935), (0.254, 0.38777871708758177), (0.2515, 0.382801822450012), (0.248, 0.40026304134726526), (0.248, 0.3793878228366375), (0.2465, 0.4361692093461752), (0.2425, 0.4199488381147385), (0.241, 0.4245464803650975), (0.248, 0.4455502573400736), (0.2505, 0.4128659730553627), (0.2445, 0.4367180672585964), (0.239, 0.40868275832384826), (0.245, 0.40967758566141127), (0.2395, 0.3980805960074067), (0.2385, 0.38501544991135594), (0.2385, 0.3649175650477409), (0.244, 0.36912829396128655), (0.2395, 0.3416158460974693), (0.2435, 0.31703384909406307), (0.24, 0.3588455178141594), (0.2435, 0.30351312055438756), (0.2445, 0.30711138370633123), (0.24, 0.301270977139473), (0.236, 0.2753350436091423), (0.2455, 0.27229013067483904), (0.2375, 0.29150083124637605), (0.252, 0.27547361980378626), (0.2515, 0.27378126108646395), (0.2345, 0.2814202661216259), (0.249, 0.2651493326127529), (0.233, 0.2732945003211498), (0.237, 0.2816130879074335), (0.249, 0.27800668217241764), (0.247, 0.28324765250086786), (0.2455, 0.28755899639613924), (0.2515, 0.2752544973492622), (0.2485, 0.3006300632655621), (0.243, 0.2729441220909357), (0.2575, 0.28872280709445475), (0.255, 0.2580226414203644), (0.2495, 0.2804775267392397), (0.242, 0.3100309982597828), (0.2375, 0.2862333755195141), (0.2595, 0.2494155874401331)]
TEST: 
[(0.02375, 0.11537219303846359), (0.2185, 0.13375587713718415), (0.248, 0.17028155183792115), (0.25125, 0.2221991572380066), (0.26025, 0.2712477890253067), (0.26425, 0.3253441696166992), (0.2605, 0.3604694055318832), (0.264, 0.3561545473337173), (0.2615, 0.3810759918689728), (0.25475, 0.37435234785079957), (0.25975, 0.39698289847373963), (0.256, 0.37523148357868197), (0.2535, 0.4337012506723404), (0.2555, 0.41794993591308593), (0.25675, 0.42520630049705505), (0.25125, 0.44249812960624696), (0.24775, 0.4163711705207825), (0.2565, 0.4285451011657715), (0.25075, 0.4080206100940704), (0.25425, 0.4075150945186615), (0.2485, 0.3889478688240051), (0.24825, 0.3803000236749649), (0.24875, 0.36099761402606967), (0.25125, 0.3643618233203888), (0.248, 0.3364263963699341), (0.25425, 0.31294575715065004), (0.24175, 0.35664564836025237), (0.2505, 0.302942630648613), (0.2515, 0.301160453915596), (0.247, 0.29651437640190126), (0.24625, 0.2721470158100128), (0.2495, 0.27021920251846315), (0.24575, 0.2870772943496704), (0.25325, 0.2728305815458298), (0.2485, 0.27202985489368436), (0.24625, 0.27973027539253237), (0.255, 0.2619871212244034), (0.24625, 0.2719445608854294), (0.2435, 0.28114958572387694), (0.24525, 0.27536608147621155), (0.2555, 0.2782322709560394), (0.249, 0.2849153300523758), (0.25675, 0.2672122871875763), (0.25725, 0.29717044448852536), (0.24475, 0.27197961246967317), (0.25625, 0.2855118236541748), (0.2615, 0.2561155492067337), (0.254, 0.2847920879125595), (0.252, 0.30807318472862244), (0.25325, 0.2876836093664169), (0.25875, 0.25074918413162234)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.25      0.01      0.02       100
           2       0.33      0.04      0.07       100
           3       0.14      0.01      0.02       100
           6       0.19      0.52      0.28       100
           9       0.32      0.58      0.41       100
          10       1.00      0.01      0.02       100
          13       0.21      0.61      0.32       100
          15       0.18      0.27      0.22       100
          16       0.58      0.07      0.13       100
          17       0.40      0.74      0.52       100
          19       0.22      0.34      0.27       100
          23       0.46      0.77      0.57       100
          24       0.72      0.13      0.22       100
          27       0.22      0.30      0.26       100
          28       0.50      0.02      0.04       100
          32       0.16      0.30      0.21       100
          34       0.00      0.00      0.00       100
          37       0.00      0.00      0.00       100
          39       0.50      0.05      0.09       100
          41       0.29      0.67      0.40       100
          45       1.00      0.01      0.02       100
          46       0.14      0.41      0.21       100
          48       0.50      0.01      0.02       100
          53       0.44      0.74      0.55       100
          54       0.50      0.03      0.06       100
          56       0.90      0.09      0.16       100
          58       0.00      0.00      0.00       100
          65       0.17      0.20      0.18       100
          67       1.00      0.05      0.10       100
          68       0.61      0.85      0.71       100
          70       0.26      0.77      0.39       100
          74       0.16      0.27      0.20       100
          75       0.67      0.02      0.04       100
          79       0.00      0.00      0.00       100
          80       0.10      0.17      0.13       100
          84       0.14      0.31      0.19       100
          88       0.21      0.43      0.29       100
          91       0.30      0.55      0.38       100
          92       0.00      0.00      0.00       100
          99       0.00      0.00      0.00       100

    accuracy                           0.26      4000
   macro avg       0.34      0.26      0.19      4000
weighted avg       0.34      0.26      0.19      4000

Collaboration
DC 0, val_set_size=2000, COIs=[18, 14, 81, 69, 5, 83, 71, 60, 11, 33, 7, 62, 29, 35, 61, 86, 47, 55, 26, 96, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([18, 14, 81, 69,  5, 83, 71, 60, 11, 33,  7, 62, 29, 35, 61, 86, 47, 55,
        26, 96, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.026, 0.1168324224948883)
DC 1, val_set_size=2000, COIs=[12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59, 22, 90, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59,
        22, 90, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.027, 0.11678374063968658)
DC 2, val_set_size=2000, COIs=[17, 13, 15, 65, 6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74, 19, 9, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([17, 13, 15, 65,  6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74,
        19,  9, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.03, 0.1163102513551712)
D00: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D01: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D02: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D03: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D04: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D05: 1000 samples from classes {1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99}
D06: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D07: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D08: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D09: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D010: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D011: 1000 samples from classes {5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96}
D012: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D013: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D014: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D015: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D016: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D017: 1000 samples from classes {12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97}
D018: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D019: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D020: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D021: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D022: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
D023: 1000 samples from classes {6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO1', '(DO3']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.215, 0.14424512842297554) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2365, 0.13455374562740327) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2185, 0.14087234008312224) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.246, 0.17184765410423278) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.16744887445867063) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2335, 0.187880967900157) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.2162867821753025) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2735, 0.21842123821377754) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2495, 0.23363769097626208) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.255, 0.2692396463006735) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.284, 0.2757407920062542) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2495, 0.30656975610554216) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2565, 0.30346167930960655) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2825, 0.31525708886981013) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2425, 0.35027559335529806) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO3', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2575, 0.33019606265425683) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.32660961578041314) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.3645418119058013) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.3626083550155163) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.3388645200133324) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2565, 0.3821042283773422) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.3545006158798933) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2775, 0.349484727114439) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.257, 0.37546128551661967) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.26, 0.3481304573714733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2785, 0.34653802585601806) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.261, 0.40233576291054485) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2505, 0.3809527167081833) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2805, 0.3435814156159759) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2545, 0.414278343513608) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[1, 2, 3, 10, 16, 24, 28, 34, 37, 39, 45, 48, 54, 56, 58, 67, 75, 79, 92, 99], M=tensor([ 1,  2,  3,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
        21, 22, 23, 24, 26, 27, 28, 29, 32, 33, 34, 35, 37, 38, 39, 41, 45, 46,
        47, 48, 50, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68,
        69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89,
        90, 91, 92, 94, 95, 96, 97, 99], device='cuda:0'), Initial Performance: (0.396, 0.07049971667925517)
DC Expert-0, val_set_size=1000, COIs=[5, 7, 11, 14, 18, 26, 29, 33, 35, 47, 55, 60, 61, 62, 69, 71, 81, 83, 86, 96], M=tensor([18, 14, 81, 69,  5, 83, 71, 60, 11, 33,  7, 62, 29, 35, 61, 86, 47, 55,
        26, 96, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.501, 0.07037255334854126)
DC Expert-1, val_set_size=1000, COIs=[12, 20, 21, 22, 38, 50, 52, 59, 63, 66, 73, 76, 78, 82, 87, 89, 90, 94, 95, 97], M=tensor([12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59,
        22, 90, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.561, 0.06089653222262859)
DC Expert-2, val_set_size=1000, COIs=[6, 9, 13, 15, 17, 19, 23, 27, 32, 41, 46, 53, 65, 68, 70, 74, 80, 84, 88, 91], M=tensor([17, 13, 15, 65,  6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74,
        19,  9, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), Initial Performance: (0.509, 0.07257326021790504)
SUPER-DC 0, val_set_size=2000, COIs=[18, 14, 81, 69, 5, 83, 71, 60, 11, 33, 7, 62, 29, 35, 61, 86, 47, 55, 26, 96, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([18, 14, 81, 69,  5, 83, 71, 60, 11, 33,  7, 62, 29, 35, 61, 86, 47, 55,
        26, 96, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59, 22, 90, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([12, 89, 52, 21, 78, 95, 66, 87, 50, 76, 73, 20, 94, 82, 63, 38, 97, 59,
        22, 90, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[17, 13, 15, 65, 6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74, 19, 9, 99, 37, 10, 2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56, 1, 28, 79, 58, 3], M=tensor([17, 13, 15, 65,  6, 41, 84, 46, 91, 27, 68, 23, 80, 70, 53, 32, 88, 74,
        19,  9, 99, 37, 10,  2, 75, 39, 54, 48, 67, 45, 16, 92, 24, 34, 56,  1,
        28, 79, 58,  3], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.0910652968287468) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.547, 0.07580992858111858) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.09257819786667824) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49966666666666665, 0.052325818141301474) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2525, 0.17773342901468278) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.294, 0.1721628216356039) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2635, 0.178363735049963) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.51, 0.09399146193265914) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.543, 0.09183972066640854) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.511, 0.09548321509361267) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.521, 0.05463186947504679) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2765, 0.125983003616333) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.309, 0.1338859230428934) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2715, 0.1504200202077627) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.507, 0.09367214459180832) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.539, 0.09474665892124176) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.09739789378643036) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5233333333333333, 0.056201774378617605) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.267, 0.13026682782173157) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3015, 0.12253775607049465) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.13397582711279393) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.481, 0.11071055150032043) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.533, 0.09910266411304473) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.10703779894113541) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.521, 0.059210011740525564) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.291, 0.1216766408085823) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2975, 0.1225713272690773) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.268, 0.13210893234610557) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.5, 0.11350753492116929) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.515, 0.10332633677124978) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.495, 0.11668767401576043) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5336666666666666, 0.061089266339937845) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2775, 0.12043241328001023) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3105, 0.10703027227520942) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.261, 0.14064332446455954) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.487, 0.11956261944770813) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.542, 0.09473507726192475) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.477, 0.12403795537352562) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5333333333333333, 0.06336370982726415) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.292, 0.1078910193145275) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.306, 0.11096869365870952) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.1351241491138935) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.484, 0.11605180037021637) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.10400506573915481) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.501, 0.11598569110035896) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5, 0.07677308851480484) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3025, 0.10341041004657746) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3185, 0.10512190103530884) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2835, 0.11479847395420074) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.488, 0.12777589464187622) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.563, 0.11267652198672294) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.1306784253716469) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.512, 0.08150818051894505) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3035, 0.10817255893349648) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.317, 0.10430449725687505) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.285, 0.11693844577670097) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.484, 0.13792625391483307) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.515, 0.13145038336515427) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.492, 0.13233349084854126) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5146666666666667, 0.08554349982738495) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.297, 0.10669103154540062) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.10590499177575112) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.29, 0.11850273689627647) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.493, 0.12825919795036317) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.559, 0.11480290289223194) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.13332607007026673) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5056666666666667, 0.09497216669718425) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3, 0.10241341578960418) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.325, 0.10002298620343208) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.277, 0.112840044349432) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.502, 0.1353044137954712) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.12650445273518562) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.474, 0.15148526000976562) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5026666666666667, 0.10261892263094584) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2845, 0.1128436524271965) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.319, 0.10821850143373013) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.271, 0.12246712745726109) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.509, 0.12215763902664184) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.557, 0.1214906687438488) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.498, 0.12595111301541329) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49633333333333335, 0.10648491609096528) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.311, 0.10769104295969009) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3115, 0.10741233482956886) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.297, 0.11001339350640774) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.494, 0.14261068189144135) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.551, 0.12507590913772584) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.479, 0.13989903315901756) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5013333333333333, 0.10770996475219727) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.296, 0.11393649512529373) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.331, 0.10165666881203651) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.303, 0.1082330942451954) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.509, 0.1417724688053131) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.534, 0.1347057515978813) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.464, 0.13831602829694747) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5123333333333333, 0.11233343915144603) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2975, 0.11261198955774307) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3055, 0.10910320076346397) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.29, 0.113739541888237) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.14845250618457795) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.515, 0.12934795923531056) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.14060411259531974) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5016666666666667, 0.12367166006565095) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3205, 0.10572947889566421) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3245, 0.1076271137446165) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3025, 0.11260262164473533) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.479, 0.15980017685890197) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.521, 0.1385879648923874) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.471, 0.14843269526958466) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5126666666666667, 0.11347348447640737) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2955, 0.1233308442234993) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.10656815645098686) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.303, 0.11187941971421242) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.484, 0.14794616401195526) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.542, 0.1309139878153801) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.15019619083404542) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5146666666666667, 0.1189728593826294) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.291, 0.12057281562685966) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.10905018863081932) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2945, 0.1165840217769146) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.497, 0.14956783187389375) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.534, 0.13518603152036668) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.493, 0.15695106995105743) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5216666666666666, 0.11523824592431386) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.289, 0.11578514194488525) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3185, 0.10997633945941925) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2965, 0.1166039491891861) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.52, 0.1482694857120514) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.138113445520401) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.15391790461540222) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.508, 0.12220250129699707) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.303, 0.11263095295429229) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3215, 0.1128575014770031) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2945, 0.12136885172128678) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.485, 0.14073492789268494) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.515, 0.1391155373454094) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.479, 0.1567017738223076) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49366666666666664, 0.13021422493457793) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.293, 0.11939539027214051) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3125, 0.114096082508564) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.1240524942278862) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.502, 0.14682483911514282) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.528, 0.13726835156977177) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.473, 0.1649189812541008) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5006666666666667, 0.13480763606230417) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.302, 0.12060170444846154) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3085, 0.11654982723295688) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.11543233668804169) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.496, 0.1586125175356865) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.532, 0.15063639771938325) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.47, 0.16120472621917725) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5113333333333333, 0.1297674376964569) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2855, 0.12268277162313461) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.308, 0.11767531833052636) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2845, 0.12120055973529816) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.478, 0.16398755711317062) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.527, 0.149838996052742) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.464, 0.1860811460018158) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.492, 0.14440932114919028) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.304, 0.12056416541337966) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3155, 0.11684350132942199) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.1268280907869339) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.483, 0.17878996515274048) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.522, 0.14958759163320065) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.472, 0.17997676384449005) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5136666666666667, 0.13248659416039785) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2795, 0.12894527459144592) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.313, 0.11443404251337051) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.302, 0.12241453398764134) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.503, 0.1577583371400833) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.14936320123076438) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.17488506931066514) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5043333333333333, 0.1396909054517746) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.297, 0.1203582449555397) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3265, 0.1140195826292038) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2965, 0.12768232053518294) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.51, 0.16010514968633652) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.548, 0.16512213212251664) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.473, 0.17711856055259706) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5003333333333333, 0.15393844850858052) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2875, 0.1291582253575325) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.315, 0.12405910408496856) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.13016818526387214) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.514, 0.16629353851079942) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.555, 0.1444322685599327) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.462, 0.18002616274356842) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.495, 0.15823005692164102) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2925, 0.12875220787525177) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.322, 0.1209759811758995) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2885, 0.13295420157909393) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.517, 0.16870482301712036) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.547, 0.16079075545072555) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.488, 0.17259036773443223) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49933333333333335, 0.14379574501514436) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.28, 0.14121614295244217) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2955, 0.12682855670154095) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2875, 0.13676109835505484) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.513, 0.18156908404827118) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.536, 0.16153296530246736) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.503, 0.17203043276071547) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49666666666666665, 0.1577513944307963) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2725, 0.1393347004055977) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3005, 0.13074023404717444) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.296, 0.12836387825012208) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.18182591533660888) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.548, 0.16133893153816461) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.504, 0.19527232007682324) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.503, 0.15478365683555603) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2945, 0.1344146538376808) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3265, 0.12194342321157456) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.302, 0.12832126969099045) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.48, 0.17698143935203553) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.543, 0.16805133813619613) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.478, 0.18678048193454744) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5016666666666667, 0.1564398250579834) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.299, 0.1374619107246399) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.311, 0.12475763255357743) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.286, 0.13505386453866958) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.492, 0.19915931165218353) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.534, 0.18561233228445054) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.485, 0.18157437413930894) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5086666666666667, 0.15514224962393444) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.287, 0.1372704160809517) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3105, 0.13081029173731804) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.14307613915205003) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.488, 0.17918102979660033) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.17554295724630356) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.496, 0.17974304860830306) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5036666666666667, 0.1699700565735499) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.13968929225206375) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.276, 0.1389446195065975) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.288, 0.13888156348466874) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.499, 0.18792255365848543) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.544, 0.16878772860765456) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.474, 0.18713868063688277) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5046666666666667, 0.16683159339427947) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2805, 0.14391516304016114) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.12413340118527412) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.13381984424591065) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.487, 0.19137877774238587) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.536, 0.168178475856781) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.1837460515499115) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5043333333333333, 0.16266431840260823) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.13865901958942414) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3225, 0.12148503425717354) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.13781633758544923) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.2031881425380707) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.538, 0.17087408632040024) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.484, 0.20906577616930008) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49333333333333335, 0.16108095542589823) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.29, 0.14168419831991197) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.334, 0.12531660223007202) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.294, 0.1380507791042328) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.1962389484643936) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.527, 0.16021884983778) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.507, 0.1971331747174263) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49066666666666664, 0.17519006741046905) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.307, 0.13522013729810714) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3215, 0.12985285884141923) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2945, 0.14764583548903465) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.499, 0.1793961795568466) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.523, 0.17083214735984803) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.487, 0.20152851927280427) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5136666666666667, 0.1663979020913442) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2955, 0.13733264774084092) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3175, 0.13233161744475364) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.302, 0.1345794471502304) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.2015908396244049) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.535, 0.1805086604654789) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.475, 0.19362994813919068) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49466666666666664, 0.17218104374408721) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.1366246943473816) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3045, 0.13255884134769438) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2995, 0.1366543686389923) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.474, 0.20010879051685332) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.539, 0.17818700848519803) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.498, 0.19592583712935446) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49633333333333335, 0.1712669998804728) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.267, 0.14483046531677246) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.314, 0.13285159519314765) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3135, 0.1352204185426235) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.026, 0.1168324224948883), (0.215, 0.14424512842297554), (0.246, 0.17184765410423278), (0.2595, 0.2162867821753025), (0.255, 0.2692396463006735), (0.2565, 0.30346167930960655), (0.2575, 0.33019606265425683), (0.257, 0.3626083550155163), (0.2595, 0.3545006158798933), (0.26, 0.3481304573714733), (0.2505, 0.3809527167081833), (0.2525, 0.17773342901468278), (0.2765, 0.125983003616333), (0.267, 0.13026682782173157), (0.291, 0.1216766408085823), (0.2775, 0.12043241328001023), (0.292, 0.1078910193145275), (0.3025, 0.10341041004657746), (0.3035, 0.10817255893349648), (0.297, 0.10669103154540062), (0.3, 0.10241341578960418), (0.2845, 0.1128436524271965), (0.311, 0.10769104295969009), (0.296, 0.11393649512529373), (0.2975, 0.11261198955774307), (0.3205, 0.10572947889566421), (0.2955, 0.1233308442234993), (0.291, 0.12057281562685966), (0.289, 0.11578514194488525), (0.303, 0.11263095295429229), (0.293, 0.11939539027214051), (0.302, 0.12060170444846154), (0.2855, 0.12268277162313461), (0.304, 0.12056416541337966), (0.2795, 0.12894527459144592), (0.297, 0.1203582449555397), (0.2875, 0.1291582253575325), (0.2925, 0.12875220787525177), (0.28, 0.14121614295244217), (0.2725, 0.1393347004055977), (0.2945, 0.1344146538376808), (0.299, 0.1374619107246399), (0.287, 0.1372704160809517), (0.2865, 0.13968929225206375), (0.2805, 0.14391516304016114), (0.2865, 0.13865901958942414), (0.29, 0.14168419831991197), (0.307, 0.13522013729810714), (0.2955, 0.13733264774084092), (0.2865, 0.1366246943473816), (0.267, 0.14483046531677246)]
TEST: 
[(0.023, 0.11593683683872223), (0.2185, 0.13976073026657104), (0.24675, 0.16809901034832), (0.25275, 0.21115041327476503), (0.26425, 0.2622025964260101), (0.2615, 0.29655004942417146), (0.26225, 0.3220256118774414), (0.262, 0.3527116991281509), (0.26325, 0.34339783453941347), (0.26575, 0.33677990424633025), (0.25475, 0.36908284163475036), (0.25625, 0.17422319507598877), (0.28275, 0.12292104023694993), (0.26975, 0.12552621710300446), (0.278, 0.11884472531080247), (0.27125, 0.11865631210803985), (0.28525, 0.10736414104700089), (0.3035, 0.10086725881695748), (0.29, 0.10334268403053283), (0.3005, 0.10333195477724075), (0.3005, 0.0991978731751442), (0.2905, 0.10694423922896386), (0.31125, 0.10250737935304642), (0.3005, 0.10884148812294006), (0.316, 0.10642204457521438), (0.3275, 0.10201914221048355), (0.311, 0.11528924971818924), (0.30175, 0.11281984251737595), (0.299, 0.10969089925289154), (0.32325, 0.10758493793010712), (0.2975, 0.1139054896235466), (0.30625, 0.11506635370850563), (0.28825, 0.11789367181062699), (0.30675, 0.11494976705312729), (0.28425, 0.12358846223354339), (0.301, 0.11310636830329895), (0.2975, 0.1210654143691063), (0.30225, 0.12107554030418397), (0.2885, 0.13256426507234573), (0.27975, 0.13119394528865813), (0.307, 0.12588009855151178), (0.30175, 0.12909458881616592), (0.29425, 0.12609631407260896), (0.2925, 0.13086771374940873), (0.28175, 0.13527503168582916), (0.2955, 0.13055134838819504), (0.2815, 0.13746418660879134), (0.302, 0.12819348311424256), (0.30075, 0.13076295733451843), (0.29325, 0.1287265511751175), (0.285, 0.13776724630594253)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.28      0.42      0.34       100
           2       0.22      0.20      0.21       100
           3       0.26      0.27      0.26       100
           5       0.30      0.23      0.26       100
           7       0.20      0.42      0.27       100
          10       0.24      0.07      0.11       100
          11       0.20      0.27      0.23       100
          14       0.30      0.16      0.21       100
          16       0.21      0.11      0.14       100
          18       0.23      0.12      0.16       100
          24       0.53      0.37      0.44       100
          26       0.15      0.19      0.17       100
          28       0.57      0.44      0.50       100
          29       0.26      0.34      0.29       100
          33       0.45      0.15      0.23       100
          34       0.20      0.20      0.20       100
          35       0.20      0.18      0.19       100
          37       0.17      0.25      0.21       100
          39       0.21      0.22      0.21       100
          45       0.11      0.18      0.14       100
          47       0.68      0.25      0.36       100
          48       0.43      0.16      0.23       100
          54       0.31      0.29      0.30       100
          55       0.16      0.24      0.19       100
          56       0.25      0.53      0.34       100
          58       0.43      0.23      0.30       100
          60       0.76      0.41      0.53       100
          61       0.39      0.59      0.47       100
          62       0.29      0.28      0.28       100
          67       0.35      0.41      0.38       100
          69       0.45      0.48      0.47       100
          71       0.66      0.45      0.54       100
          75       0.53      0.33      0.41       100
          79       0.24      0.47      0.32       100
          81       0.22      0.24      0.23       100
          83       0.31      0.33      0.32       100
          86       0.42      0.44      0.43       100
          92       0.20      0.26      0.23       100
          96       0.23      0.09      0.13       100
          99       0.18      0.13      0.15       100

    accuracy                           0.28      4000
   macro avg       0.32      0.28      0.28      4000
weighted avg       0.32      0.28      0.28      4000

Collaboration_DC_1
VAL: 
[(0.027, 0.11678374063968658), (0.2365, 0.13455374562740327), (0.2605, 0.16744887445867063), (0.2735, 0.21842123821377754), (0.284, 0.2757407920062542), (0.2825, 0.31525708886981013), (0.28, 0.32660961578041314), (0.277, 0.3388645200133324), (0.2775, 0.349484727114439), (0.2785, 0.34653802585601806), (0.2805, 0.3435814156159759), (0.294, 0.1721628216356039), (0.309, 0.1338859230428934), (0.3015, 0.12253775607049465), (0.2975, 0.1225713272690773), (0.3105, 0.10703027227520942), (0.306, 0.11096869365870952), (0.3185, 0.10512190103530884), (0.317, 0.10430449725687505), (0.323, 0.10590499177575112), (0.325, 0.10002298620343208), (0.319, 0.10821850143373013), (0.3115, 0.10741233482956886), (0.331, 0.10165666881203651), (0.3055, 0.10910320076346397), (0.3245, 0.1076271137446165), (0.323, 0.10656815645098686), (0.3165, 0.10905018863081932), (0.3185, 0.10997633945941925), (0.3215, 0.1128575014770031), (0.3125, 0.114096082508564), (0.3085, 0.11654982723295688), (0.308, 0.11767531833052636), (0.3155, 0.11684350132942199), (0.313, 0.11443404251337051), (0.3265, 0.1140195826292038), (0.315, 0.12405910408496856), (0.322, 0.1209759811758995), (0.2955, 0.12682855670154095), (0.3005, 0.13074023404717444), (0.3265, 0.12194342321157456), (0.311, 0.12475763255357743), (0.3105, 0.13081029173731804), (0.276, 0.1389446195065975), (0.3165, 0.12413340118527412), (0.3225, 0.12148503425717354), (0.334, 0.12531660223007202), (0.3215, 0.12985285884141923), (0.3175, 0.13233161744475364), (0.3045, 0.13255884134769438), (0.314, 0.13285159519314765)]
TEST: 
[(0.02575, 0.11581322306394577), (0.25, 0.13162367969751357), (0.27, 0.16438090354204177), (0.27325, 0.21483240008354187), (0.27775, 0.27224623787403107), (0.27275, 0.31437417781352994), (0.2715, 0.328253488779068), (0.27075, 0.33944172656536103), (0.27575, 0.35235757505893706), (0.27325, 0.34976228427886963), (0.27725, 0.34470517134666445), (0.28625, 0.1695251587629318), (0.2965, 0.13300975066423415), (0.302, 0.12029536473751068), (0.29775, 0.11796509557962417), (0.3115, 0.10193506479263306), (0.3055, 0.10661657005548478), (0.31625, 0.10210877448320389), (0.3235, 0.09999556487798691), (0.30875, 0.10314623183012009), (0.3255, 0.0972091061770916), (0.314, 0.10476671981811524), (0.31775, 0.10339380311965943), (0.328, 0.09885679948329926), (0.31825, 0.10504068797826767), (0.3295, 0.10362160700559617), (0.31975, 0.10315757101774216), (0.31025, 0.10439440527558327), (0.3175, 0.104539137840271), (0.3205, 0.10877371954917908), (0.31725, 0.10821651458740235), (0.32125, 0.10926250654459), (0.327, 0.11150637590885162), (0.326, 0.11075322011113167), (0.31425, 0.10918037837743759), (0.325, 0.10760698997974395), (0.33225, 0.1191449513733387), (0.3215, 0.11784686428308487), (0.3035, 0.12213979893922806), (0.30325, 0.1253243564069271), (0.3285, 0.11758323234319687), (0.3125, 0.11969844934344291), (0.31275, 0.12438464105129242), (0.302, 0.13201720571517944), (0.31725, 0.12077843010425568), (0.3305, 0.11714439401030541), (0.32475, 0.12206379839777946), (0.3205, 0.12564375281333923), (0.3245, 0.12835055249929428), (0.3225, 0.12725364112854004), (0.336, 0.12622866997122764)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.44      0.45      0.45       100
           2       0.28      0.25      0.26       100
           3       0.19      0.37      0.25       100
          10       0.27      0.17      0.21       100
          12       0.42      0.33      0.37       100
          16       0.13      0.12      0.13       100
          20       0.42      0.64      0.50       100
          21       0.30      0.24      0.27       100
          22       0.23      0.33      0.27       100
          24       0.57      0.59      0.58       100
          28       0.51      0.47      0.49       100
          34       0.23      0.30      0.26       100
          37       0.26      0.25      0.25       100
          38       0.23      0.27      0.25       100
          39       0.35      0.26      0.30       100
          45       0.16      0.18      0.17       100
          48       0.40      0.39      0.40       100
          50       0.24      0.12      0.16       100
          52       0.81      0.35      0.49       100
          54       0.41      0.44      0.42       100
          56       0.29      0.59      0.38       100
          58       0.31      0.25      0.28       100
          59       0.35      0.20      0.25       100
          63       0.30      0.31      0.30       100
          66       0.22      0.14      0.17       100
          67       0.29      0.12      0.17       100
          73       0.30      0.56      0.39       100
          75       0.52      0.50      0.51       100
          76       0.62      0.65      0.64       100
          78       0.18      0.21      0.20       100
          79       0.31      0.33      0.32       100
          82       0.52      0.64      0.57       100
          87       0.38      0.28      0.32       100
          89       0.29      0.30      0.30       100
          90       0.37      0.21      0.27       100
          92       0.31      0.38      0.34       100
          94       0.42      0.58      0.49       100
          95       0.43      0.47      0.45       100
          97       0.32      0.15      0.20       100
          99       0.12      0.05      0.07       100

    accuracy                           0.34      4000
   macro avg       0.34      0.34      0.33      4000
weighted avg       0.34      0.34      0.33      4000

Collaboration_DC_2
VAL: 
[(0.03, 0.1163102513551712), (0.2185, 0.14087234008312224), (0.2335, 0.187880967900157), (0.2495, 0.23363769097626208), (0.2495, 0.30656975610554216), (0.2425, 0.35027559335529806), (0.2585, 0.3645418119058013), (0.2565, 0.3821042283773422), (0.257, 0.37546128551661967), (0.261, 0.40233576291054485), (0.2545, 0.414278343513608), (0.2635, 0.178363735049963), (0.2715, 0.1504200202077627), (0.2655, 0.13397582711279393), (0.268, 0.13210893234610557), (0.261, 0.14064332446455954), (0.2585, 0.1351241491138935), (0.2835, 0.11479847395420074), (0.285, 0.11693844577670097), (0.29, 0.11850273689627647), (0.277, 0.112840044349432), (0.271, 0.12246712745726109), (0.297, 0.11001339350640774), (0.303, 0.1082330942451954), (0.29, 0.113739541888237), (0.3025, 0.11260262164473533), (0.303, 0.11187941971421242), (0.2945, 0.1165840217769146), (0.2965, 0.1166039491891861), (0.2945, 0.12136885172128678), (0.2915, 0.1240524942278862), (0.308, 0.11543233668804169), (0.2845, 0.12120055973529816), (0.2915, 0.1268280907869339), (0.302, 0.12241453398764134), (0.2965, 0.12768232053518294), (0.2985, 0.13016818526387214), (0.2885, 0.13295420157909393), (0.2875, 0.13676109835505484), (0.296, 0.12836387825012208), (0.302, 0.12832126969099045), (0.286, 0.13505386453866958), (0.2915, 0.14307613915205003), (0.288, 0.13888156348466874), (0.2915, 0.13381984424591065), (0.306, 0.13781633758544923), (0.294, 0.1380507791042328), (0.2945, 0.14764583548903465), (0.302, 0.1345794471502304), (0.2995, 0.1366543686389923), (0.3135, 0.1352204185426235)]
TEST: 
[(0.02375, 0.11537219303846359), (0.22175, 0.13830103945732117), (0.24175, 0.18480222982168198), (0.2525, 0.2307434833049774), (0.2565, 0.3045920248031616), (0.26275, 0.34884016823768615), (0.255, 0.36337438607215883), (0.25825, 0.37920470464229583), (0.25325, 0.3728022696971893), (0.2585, 0.39754661834239957), (0.256, 0.4095899958610535), (0.264, 0.18138485538959503), (0.27175, 0.14649603658914567), (0.2735, 0.13528736209869385), (0.269, 0.13142846632003785), (0.261, 0.14278219121694566), (0.2665, 0.13342970383167266), (0.291, 0.11440072298049926), (0.28075, 0.11608657836914063), (0.28725, 0.11776446717977523), (0.3045, 0.10981542783975601), (0.28275, 0.12077268505096436), (0.302, 0.10913875287771226), (0.30625, 0.1074068307876587), (0.2975, 0.11200614833831787), (0.30925, 0.11137143987417221), (0.3085, 0.11148062056303024), (0.3005, 0.1158609704375267), (0.30975, 0.11386382585763931), (0.2935, 0.12070909839868546), (0.29125, 0.12312828010320663), (0.321, 0.11465987771749496), (0.315, 0.11760911417007447), (0.293, 0.1252509708404541), (0.29275, 0.12196107280254365), (0.30725, 0.12375804853439332), (0.30625, 0.12581797355413438), (0.29475, 0.1294705323576927), (0.29625, 0.1357508003115654), (0.301, 0.12789775639772416), (0.31375, 0.1270457094311714), (0.2885, 0.13556282222270966), (0.30075, 0.14154001706838606), (0.29875, 0.13663981527090072), (0.3005, 0.1304127618074417), (0.304, 0.13641439116001128), (0.295, 0.13708177238702773), (0.2995, 0.14566636443138123), (0.30475, 0.13317402011156082), (0.30875, 0.13563583797216416), (0.313, 0.13375997859239577)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.39      0.44      0.41       100
           2       0.17      0.23      0.20       100
           3       0.27      0.34      0.30       100
           6       0.29      0.32      0.30       100
           9       0.47      0.47      0.47       100
          10       0.38      0.23      0.29       100
          13       0.28      0.40      0.33       100
          15       0.23      0.20      0.21       100
          16       0.32      0.22      0.26       100
          17       0.50      0.45      0.47       100
          19       0.27      0.18      0.22       100
          23       0.42      0.58      0.49       100
          24       0.61      0.25      0.35       100
          27       0.17      0.14      0.16       100
          28       0.50      0.49      0.49       100
          32       0.20      0.21      0.21       100
          34       0.21      0.27      0.23       100
          37       0.21      0.25      0.23       100
          39       0.32      0.17      0.22       100
          41       0.37      0.47      0.41       100
          45       0.09      0.12      0.10       100
          46       0.27      0.21      0.24       100
          48       0.38      0.26      0.31       100
          53       0.47      0.67      0.55       100
          54       0.31      0.40      0.35       100
          56       0.57      0.54      0.56       100
          58       0.36      0.17      0.23       100
          65       0.23      0.19      0.21       100
          67       0.28      0.27      0.28       100
          68       0.74      0.71      0.72       100
          70       0.29      0.36      0.32       100
          74       0.17      0.29      0.21       100
          75       0.65      0.46      0.54       100
          79       0.33      0.34      0.34       100
          80       0.17      0.17      0.17       100
          84       0.17      0.12      0.14       100
          88       0.15      0.20      0.17       100
          91       0.39      0.37      0.38       100
          92       0.23      0.30      0.26       100
          99       0.12      0.06      0.08       100

    accuracy                           0.31      4000
   macro avg       0.32      0.31      0.31      4000
weighted avg       0.32      0.31      0.31      4000

do_assignment: None
seeds: [31]
name: naive-cifar100-feddf31
score_metric: contrloss
aggregation: <function fed_df at 0x7dbc2b416e50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=31
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[63, 34, 90, 16, 22, 3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45, 82, 43, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([63, 34, 90, 16, 22,  3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45,
        82, 43,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.028, 0.11652796924114227)
DC 1, val_set_size=2000, COIs=[8, 99, 85, 20, 36, 13, 23, 15, 79, 38, 2, 10, 76, 53, 44, 70, 92, 72, 42, 37, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([ 8, 99, 85, 20, 36, 13, 23, 15, 79, 38,  2, 10, 76, 53, 44, 70, 92, 72,
        42, 37,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.032, 0.11632643985748291)
DC 2, val_set_size=2000, COIs=[30, 54, 32, 74, 9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11, 26, 19, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([30, 54, 32, 74,  9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11,
        26, 19,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.0265, 0.11660425519943238)
D00: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D01: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D02: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D03: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D04: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D05: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D06: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D07: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D08: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D09: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D010: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D011: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D012: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D013: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D014: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D015: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D016: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D017: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D018: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D019: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D020: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D021: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D022: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D023: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.305, 0.08305751579999923) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.08279483330249786) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.309, 0.08105104357004166) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.353, 0.0746876573562622) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.363, 0.0732594935297966) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3625, 0.07286672762036324) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.343, 0.07324748730659485) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.38, 0.07141345632076264) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.06827613133192062) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3735, 0.07254579657316208) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.07253216350078583) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.06933329039812088) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3715, 0.07471452441811562) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.38, 0.07509736081957817) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4105, 0.07136665534973144) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3735, 0.07980064460635185) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.07666078209877014) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.07252490428090096) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3835, 0.08135798722505569) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3745, 0.08259534481167793) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.07614702218770981) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.374, 0.08536098995804786) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3845, 0.08388426238298416) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4075, 0.08035665291547775) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3815, 0.08927234494686127) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3775, 0.08983018040657044) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.08503197658061981) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.09618362921476364) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3665, 0.09397412806749345) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4, 0.08762362763285637) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.371, 0.10090002653002739) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3685, 0.10145266848802567) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.09625120979547501) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3685, 0.10680034291744232) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.10854557681083679) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.399, 0.10891958373785018) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.1215054156780243) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.11342967680096626) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3835, 0.1206389753818512) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.373, 0.13339001780748366) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3635, 0.12179114758968353) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.394, 0.11582214260101319) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.35, 0.14042268991470336) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.35, 0.13712567257881164) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.394, 0.12684104204177857) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.14892560929059984) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.337, 0.1529367299079895) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.384, 0.13449508589506148) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3625, 0.1489866651892662) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.372, 0.13680716836452483) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.374, 0.14935966205596923) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3505, 0.1625590001940727) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3535, 0.14932063347101213) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3855, 0.14619736325740815) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.35, 0.1656361576318741) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.15339915931224823) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.16101564919948577) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.351, 0.16847140491008758) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.359, 0.155363787651062) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.386, 0.15459132516384125) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3505, 0.17258848267793656) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3515, 0.15402385884523392) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.16461516708135604) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3645, 0.17368979424238204) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.362, 0.16573227339982988) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3695, 0.15979580515623093) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.344, 0.17586973536014558) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.358, 0.16662060260772704) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.386, 0.16136669808626175) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.341, 0.19218628966808318) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3605, 0.17129075336456298) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.385, 0.17241813683509827) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.17617309230566025) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.36, 0.1665933619737625) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.393, 0.1681118989288807) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3335, 0.1899621568918228) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3745, 0.17639045107364654) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3835, 0.18763231968879698) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.333, 0.18991727715730666) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.1756763717532158) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3745, 0.17924464076757432) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.351, 0.1837557394504547) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3645, 0.17998550260066987) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3805, 0.18059758818149566) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3495, 0.19714829725027083) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3695, 0.1776847134232521) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.17610275828838348) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3385, 0.20659130704402923) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3575, 0.19200792932510377) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3745, 0.19676014649868012) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.358, 0.19013616394996644) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.18809560775756837) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.377, 0.19751718783378602) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3435, 0.19374257266521455) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.18554629182815552) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.19549053424596788) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.346, 0.1980145332813263) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3655, 0.19526589703559877) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.1918223160505295) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3445, 0.2153903172016144) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3735, 0.197521115899086) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.37, 0.20979416501522063) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.2171596131324768) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3635, 0.20115048825740814) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.378, 0.20174365961551666) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3525, 0.20458847141265868) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3675, 0.202585529088974) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3675, 0.22554937148094178) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3555, 0.21597709453105926) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3675, 0.19499372458457948) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3705, 0.2095540782213211) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.21534000074863432) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3675, 0.21237204396724702) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.359, 0.22019368433952333) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.351, 0.21429507076740265) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.19390198057889937) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.374, 0.2135649471282959) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.352, 0.22503547394275666) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.1994275997877121) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.376, 0.21120381879806518) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3445, 0.2320260250866413) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.361, 0.21060909682512283) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.373, 0.21369156050682067) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.347, 0.21298158419132232) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3685, 0.2009511013031006) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.362, 0.2228078556060791) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.341, 0.2293584523797035) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3825, 0.20732377922534942) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3755, 0.22433200585842134) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.352, 0.23786603319644928) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.223337288916111) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3775, 0.22670924603939058) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3595, 0.23201723766326904) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.22726041913032533) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3705, 0.22824638652801513) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3395, 0.2341706721186638) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3715, 0.2199211641550064) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3755, 0.2282580975294113) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.352, 0.2311433913707733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.23532653081417085) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.378, 0.23472813546657562) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3585, 0.22366881680488587) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.23444673204421998) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3765, 0.2285588346719742) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3575, 0.2515140686035156) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.359, 0.2417569537162781) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3825, 0.23458643412590027) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3405, 0.24115814346075057) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3625, 0.23425203812122344) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3785, 0.233934143781662) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.028, 0.11652796924114227), (0.305, 0.08305751579999923), (0.353, 0.0746876573562622), (0.343, 0.07324748730659485), (0.3735, 0.07254579657316208), (0.3715, 0.07471452441811562), (0.3735, 0.07980064460635185), (0.3835, 0.08135798722505569), (0.374, 0.08536098995804786), (0.3815, 0.08927234494686127), (0.3695, 0.09618362921476364), (0.371, 0.10090002653002739), (0.3685, 0.10680034291744232), (0.3695, 0.1215054156780243), (0.373, 0.13339001780748366), (0.35, 0.14042268991470336), (0.354, 0.14892560929059984), (0.3625, 0.1489866651892662), (0.3505, 0.1625590001940727), (0.35, 0.1656361576318741), (0.351, 0.16847140491008758), (0.3505, 0.17258848267793656), (0.3645, 0.17368979424238204), (0.344, 0.17586973536014558), (0.341, 0.19218628966808318), (0.3475, 0.17617309230566025), (0.3335, 0.1899621568918228), (0.333, 0.18991727715730666), (0.351, 0.1837557394504547), (0.3495, 0.19714829725027083), (0.3385, 0.20659130704402923), (0.358, 0.19013616394996644), (0.3435, 0.19374257266521455), (0.346, 0.1980145332813263), (0.3445, 0.2153903172016144), (0.3475, 0.2171596131324768), (0.3525, 0.20458847141265868), (0.3555, 0.21597709453105926), (0.3535, 0.21534000074863432), (0.351, 0.21429507076740265), (0.352, 0.22503547394275666), (0.3445, 0.2320260250866413), (0.347, 0.21298158419132232), (0.341, 0.2293584523797035), (0.352, 0.23786603319644928), (0.3595, 0.23201723766326904), (0.3395, 0.2341706721186638), (0.352, 0.2311433913707733), (0.3585, 0.22366881680488587), (0.3575, 0.2515140686035156), (0.3405, 0.24115814346075057)]
TEST: 
[(0.02625, 0.11564967942237854), (0.2925, 0.08255697518587113), (0.33125, 0.0744956402182579), (0.341, 0.07358722189068795), (0.36225, 0.07397507423162461), (0.36375, 0.07611946871876717), (0.3765, 0.08035889256000518), (0.36875, 0.08233787035942078), (0.36775, 0.08617332410812378), (0.36525, 0.09114944577217102), (0.3485, 0.09804414370656013), (0.352, 0.10160913181304931), (0.3685, 0.1076207081079483), (0.3545, 0.1223934308886528), (0.35975, 0.13444284057617187), (0.3435, 0.13975674468278884), (0.3305, 0.15190774583816527), (0.34625, 0.153649765253067), (0.3375, 0.16342926681041717), (0.332, 0.16404274660348891), (0.351, 0.16619568145275115), (0.335, 0.17272058844566346), (0.348, 0.1757416427731514), (0.33475, 0.18014891481399536), (0.337, 0.18757238429784776), (0.34525, 0.17362501215934753), (0.34075, 0.18956962674856187), (0.32625, 0.1903227362036705), (0.3365, 0.18346229952573775), (0.34825, 0.1925980772972107), (0.3435, 0.20340045881271362), (0.3375, 0.19430857354402542), (0.336, 0.19473995161056518), (0.34425, 0.2018537192940712), (0.3415, 0.2127185235619545), (0.33525, 0.21340064370632172), (0.347, 0.20284725677967072), (0.3445, 0.21880229449272157), (0.3355, 0.21746086978912355), (0.337, 0.21986888301372529), (0.3405, 0.23033271038532258), (0.33175, 0.2369103056192398), (0.34, 0.21807088565826416), (0.34475, 0.23101769906282424), (0.34225, 0.2382324537038803), (0.34725, 0.23435289150476454), (0.35, 0.23344496965408326), (0.3325, 0.23421364760398863), (0.35125, 0.22896188867092132), (0.33025, 0.2567569065093994), (0.3405, 0.24360167944431305)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.30      0.46      0.36       100
           3       0.23      0.18      0.20       100
           4       0.16      0.23      0.19       100
           5       0.34      0.28      0.31       100
           7       0.45      0.33      0.38       100
          14       0.24      0.24      0.24       100
          16       0.34      0.27      0.30       100
          17       0.54      0.61      0.58       100
          18       0.26      0.22      0.24       100
          22       0.38      0.30      0.34       100
          29       0.25      0.39      0.30       100
          33       0.32      0.48      0.39       100
          34       0.22      0.32      0.26       100
          35       0.26      0.24      0.25       100
          39       0.43      0.35      0.38       100
          43       0.39      0.17      0.24       100
          45       0.14      0.12      0.13       100
          50       0.14      0.16      0.15       100
          51       0.25      0.27      0.26       100
          52       0.60      0.79      0.68       100
          55       0.14      0.14      0.14       100
          57       0.28      0.30      0.29       100
          60       0.68      0.67      0.68       100
          61       0.66      0.45      0.54       100
          62       0.45      0.38      0.41       100
          63       0.38      0.40      0.39       100
          64       0.15      0.17      0.16       100
          67       0.43      0.24      0.31       100
          68       0.74      0.53      0.62       100
          82       0.60      0.59      0.60       100
          83       0.24      0.35      0.29       100
          84       0.27      0.18      0.22       100
          87       0.42      0.35      0.38       100
          90       0.31      0.38      0.34       100
          91       0.31      0.39      0.35       100
          93       0.47      0.24      0.32       100
          94       0.69      0.57      0.62       100
          96       0.45      0.33      0.38       100
          97       0.25      0.39      0.31       100
          98       0.14      0.16      0.15       100

    accuracy                           0.34      4000
   macro avg       0.36      0.34      0.34      4000
weighted avg       0.36      0.34      0.34      4000

No_Competition_DC_1
VAL: 
[(0.032, 0.11632643985748291), (0.312, 0.08279483330249786), (0.363, 0.0732594935297966), (0.38, 0.07141345632076264), (0.382, 0.07253216350078583), (0.38, 0.07509736081957817), (0.391, 0.07666078209877014), (0.3745, 0.08259534481167793), (0.3845, 0.08388426238298416), (0.3775, 0.08983018040657044), (0.3665, 0.09397412806749345), (0.3685, 0.10145266848802567), (0.364, 0.10854557681083679), (0.374, 0.11342967680096626), (0.3635, 0.12179114758968353), (0.35, 0.13712567257881164), (0.337, 0.1529367299079895), (0.372, 0.13680716836452483), (0.3535, 0.14932063347101213), (0.368, 0.15339915931224823), (0.359, 0.155363787651062), (0.3515, 0.15402385884523392), (0.362, 0.16573227339982988), (0.358, 0.16662060260772704), (0.3605, 0.17129075336456298), (0.36, 0.1665933619737625), (0.3745, 0.17639045107364654), (0.3705, 0.1756763717532158), (0.3645, 0.17998550260066987), (0.3695, 0.1776847134232521), (0.3575, 0.19200792932510377), (0.364, 0.18809560775756837), (0.366, 0.18554629182815552), (0.3655, 0.19526589703559877), (0.3735, 0.197521115899086), (0.3635, 0.20115048825740814), (0.3675, 0.202585529088974), (0.3675, 0.19499372458457948), (0.3675, 0.21237204396724702), (0.373, 0.19390198057889937), (0.371, 0.1994275997877121), (0.361, 0.21060909682512283), (0.3685, 0.2009511013031006), (0.3825, 0.20732377922534942), (0.3705, 0.223337288916111), (0.371, 0.22726041913032533), (0.3715, 0.2199211641550064), (0.366, 0.23532653081417085), (0.366, 0.23444673204421998), (0.359, 0.2417569537162781), (0.3625, 0.23425203812122344)]
TEST: 
[(0.02975, 0.1154598125219345), (0.3085, 0.08218351072072982), (0.364, 0.07251080250740051), (0.379, 0.07001599794626236), (0.384, 0.07110496467351914), (0.3825, 0.07418684896826744), (0.39125, 0.07486515328288078), (0.38175, 0.08130447766184806), (0.388, 0.08237132042646408), (0.3815, 0.08673784443736077), (0.37325, 0.09121443688869477), (0.37325, 0.09912452667951584), (0.3765, 0.10477278196811676), (0.3755, 0.11016174566745758), (0.3675, 0.11978524202108383), (0.35625, 0.13051094698905946), (0.33975, 0.14993641126155854), (0.37175, 0.13166972988843917), (0.35925, 0.14228372490406035), (0.3605, 0.15034944397211075), (0.35325, 0.151602401137352), (0.351, 0.15351012516021728), (0.35275, 0.16565438205003738), (0.35225, 0.1684346187710762), (0.3605, 0.1700386511683464), (0.359, 0.16538157576322557), (0.3555, 0.17543211430311204), (0.364, 0.1744726595878601), (0.359, 0.1756477445960045), (0.366, 0.1794493926167488), (0.356, 0.19126470404863358), (0.35375, 0.19118787443637847), (0.366, 0.1845153529047966), (0.3725, 0.19192466026544572), (0.36975, 0.1955115648508072), (0.3695, 0.19726687502861023), (0.37025, 0.20130416280031205), (0.365, 0.19884466367959977), (0.369, 0.2122119862437248), (0.35575, 0.19990246224403382), (0.363, 0.19713096916675568), (0.3635, 0.21131007015705108), (0.37225, 0.20002409821748734), (0.3585, 0.20822444760799408), (0.3635, 0.2194376242160797), (0.35225, 0.22507039070129395), (0.3535, 0.22267706739902496), (0.36275, 0.22904779350757598), (0.35025, 0.2305435842871666), (0.365, 0.23920069444179534), (0.35225, 0.2317186119556427)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.38      0.51      0.44       100
           2       0.33      0.18      0.23       100
           4       0.15      0.15      0.15       100
           5       0.24      0.34      0.28       100
           7       0.32      0.50      0.39       100
           8       0.43      0.21      0.28       100
          10       0.33      0.26      0.29       100
          13       0.41      0.32      0.36       100
          14       0.26      0.27      0.27       100
          15       0.23      0.21      0.22       100
          17       0.52      0.32      0.40       100
          18       0.26      0.32      0.29       100
          20       0.64      0.50      0.56       100
          23       0.62      0.58      0.60       100
          29       0.33      0.35      0.34       100
          36       0.57      0.27      0.37       100
          37       0.22      0.37      0.28       100
          38       0.26      0.18      0.21       100
          42       0.32      0.25      0.28       100
          44       0.17      0.17      0.17       100
          50       0.13      0.19      0.15       100
          53       0.67      0.44      0.53       100
          57       0.30      0.37      0.33       100
          60       0.71      0.72      0.72       100
          67       0.34      0.34      0.34       100
          68       0.64      0.61      0.62       100
          70       0.36      0.26      0.30       100
          72       0.18      0.27      0.22       100
          76       0.60      0.61      0.61       100
          79       0.34      0.31      0.32       100
          84       0.15      0.15      0.15       100
          85       0.38      0.54      0.45       100
          87       0.44      0.40      0.42       100
          91       0.36      0.41      0.38       100
          92       0.32      0.37      0.34       100
          93       0.25      0.21      0.23       100
          94       0.66      0.61      0.63       100
          96       0.58      0.45      0.51       100
          97       0.25      0.32      0.28       100
          99       0.28      0.25      0.27       100

    accuracy                           0.35      4000
   macro avg       0.37      0.35      0.35      4000
weighted avg       0.37      0.35      0.35      4000

No_Competition_DC_2
VAL: 
[(0.0265, 0.11660425519943238), (0.309, 0.08105104357004166), (0.3625, 0.07286672762036324), (0.395, 0.06827613133192062), (0.408, 0.06933329039812088), (0.4105, 0.07136665534973144), (0.415, 0.07252490428090096), (0.408, 0.07614702218770981), (0.4075, 0.08035665291547775), (0.403, 0.08503197658061981), (0.4, 0.08762362763285637), (0.39, 0.09625120979547501), (0.399, 0.10891958373785018), (0.3835, 0.1206389753818512), (0.394, 0.11582214260101319), (0.394, 0.12684104204177857), (0.384, 0.13449508589506148), (0.374, 0.14935966205596923), (0.3855, 0.14619736325740815), (0.3725, 0.16101564919948577), (0.386, 0.15459132516384125), (0.38, 0.16461516708135604), (0.3695, 0.15979580515623093), (0.386, 0.16136669808626175), (0.385, 0.17241813683509827), (0.393, 0.1681118989288807), (0.3835, 0.18763231968879698), (0.3745, 0.17924464076757432), (0.3805, 0.18059758818149566), (0.387, 0.17610275828838348), (0.3745, 0.19676014649868012), (0.377, 0.19751718783378602), (0.371, 0.19549053424596788), (0.38, 0.1918223160505295), (0.37, 0.20979416501522063), (0.378, 0.20174365961551666), (0.3675, 0.22554937148094178), (0.3705, 0.2095540782213211), (0.359, 0.22019368433952333), (0.374, 0.2135649471282959), (0.376, 0.21120381879806518), (0.373, 0.21369156050682067), (0.362, 0.2228078556060791), (0.3755, 0.22433200585842134), (0.3775, 0.22670924603939058), (0.3705, 0.22824638652801513), (0.3755, 0.2282580975294113), (0.378, 0.23472813546657562), (0.3765, 0.2285588346719742), (0.3825, 0.23458643412590027), (0.3785, 0.233934143781662)]
TEST: 
[(0.032, 0.11564855867624282), (0.30975, 0.0807784604728222), (0.35675, 0.07256186375021935), (0.39425, 0.06800216993689537), (0.39625, 0.06928727626800538), (0.40375, 0.07124797371029853), (0.40325, 0.07241798475384713), (0.4005, 0.07616615200042724), (0.40575, 0.08061762344837188), (0.393, 0.0843865856230259), (0.38975, 0.08730961194634437), (0.399, 0.09697661912441254), (0.39325, 0.10988564905524253), (0.37375, 0.12160538297891617), (0.38475, 0.11278431090712547), (0.38175, 0.12822451400756835), (0.3765, 0.13180731821060182), (0.37375, 0.1484281513094902), (0.3705, 0.14683179932832718), (0.3725, 0.15891658091545105), (0.37325, 0.1509961146712303), (0.3725, 0.15843174415826797), (0.37775, 0.15742391055822372), (0.374, 0.15871872794628145), (0.372, 0.17118661963939666), (0.3805, 0.16461630195379257), (0.36375, 0.18601240384578704), (0.371, 0.17327854949235916), (0.37575, 0.17676714873313903), (0.37725, 0.1719768083691597), (0.37775, 0.1934336770772934), (0.3785, 0.19522954076528548), (0.365, 0.1934560894370079), (0.364, 0.19064199179410934), (0.366, 0.20997091084718705), (0.3735, 0.20143739545345307), (0.37175, 0.21179393273591995), (0.36925, 0.21281724125146867), (0.3505, 0.22424706494808197), (0.36825, 0.20674417531490327), (0.374, 0.20740370148420334), (0.36375, 0.2092366770505905), (0.363, 0.2192963455915451), (0.36275, 0.21899021255970003), (0.37225, 0.22182106119394301), (0.3715, 0.2211879878640175), (0.36575, 0.22116216027736663), (0.3755, 0.23280575734376907), (0.3805, 0.22333288967609405), (0.38, 0.2256034494638443), (0.3795, 0.22898466676473617)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.42      0.46      0.44       100
           4       0.14      0.23      0.18       100
           5       0.39      0.34      0.36       100
           7       0.34      0.41      0.37       100
           9       0.62      0.45      0.52       100
          11       0.24      0.30      0.27       100
          14       0.30      0.31      0.31       100
          17       0.47      0.60      0.53       100
          18       0.32      0.20      0.25       100
          19       0.36      0.32      0.34       100
          21       0.44      0.55      0.49       100
          26       0.34      0.32      0.33       100
          27       0.28      0.26      0.27       100
          29       0.39      0.31      0.34       100
          30       0.41      0.39      0.40       100
          32       0.33      0.29      0.31       100
          40       0.33      0.34      0.33       100
          41       0.77      0.50      0.61       100
          47       0.50      0.50      0.50       100
          49       0.40      0.55      0.46       100
          50       0.27      0.09      0.14       100
          54       0.46      0.49      0.48       100
          57       0.33      0.34      0.34       100
          58       0.40      0.34      0.37       100
          60       0.80      0.72      0.76       100
          67       0.47      0.20      0.28       100
          68       0.78      0.59      0.67       100
          69       0.64      0.56      0.60       100
          74       0.17      0.23      0.19       100
          77       0.18      0.21      0.20       100
          78       0.19      0.35      0.24       100
          84       0.20      0.22      0.21       100
          87       0.51      0.41      0.45       100
          89       0.44      0.36      0.40       100
          91       0.40      0.47      0.43       100
          93       0.26      0.22      0.24       100
          94       0.58      0.65      0.61       100
          95       0.44      0.47      0.45       100
          96       0.34      0.33      0.34       100
          97       0.28      0.30      0.29       100

    accuracy                           0.38      4000
   macro avg       0.40      0.38      0.38      4000
weighted avg       0.40      0.38      0.38      4000

Competition
DC 0, val_set_size=2000, COIs=[63, 34, 90, 16, 22, 3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45, 82, 43, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([63, 34, 90, 16, 22,  3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45,
        82, 43,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.028, 0.11652796924114227)
DC 1, val_set_size=2000, COIs=[8, 99, 85, 20, 36, 13, 23, 15, 79, 38, 2, 10, 76, 53, 44, 70, 92, 72, 42, 37, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([ 8, 99, 85, 20, 36, 13, 23, 15, 79, 38,  2, 10, 76, 53, 44, 70, 92, 72,
        42, 37,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.032, 0.11632643985748291)
DC 2, val_set_size=2000, COIs=[30, 54, 32, 74, 9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11, 26, 19, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([30, 54, 32, 74,  9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11,
        26, 19,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.0265, 0.11660425519943238)
D00: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D01: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D02: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D03: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D04: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D05: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D06: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D07: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D08: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D09: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D010: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D011: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D012: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D013: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D014: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D015: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D016: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D017: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D018: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D019: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D020: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D021: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D022: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D023: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO0', '(DO2']
DC 2 --> ['(DO5', '(DO4']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.201, 0.1360903970003128) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.225, 0.13515009021759034) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2165, 0.14034129482507707) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.1724819296002388) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2295, 0.17842501321434975) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2355, 0.18283687591552733) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2375, 0.21655402204394342) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.254, 0.21536052727699279) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.23222217267751694) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.238, 0.2850736349672079) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2465, 0.2713924720585346) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.2875493269264698) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.3079529513716698) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.31631595626473424) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.3426038283407688) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO4']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO0', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.225, 0.3193999870121479) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.254, 0.3234565255343914) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.3400157999098301) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2385, 0.3049556414783001) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.247, 0.34021400851011274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.3700522485822439) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.34729144340753554) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2465, 0.35142993158102037) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2555, 0.38624285365641114) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.34813025406003) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.35313763621449473) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.3919358174055815) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.39056425765156744) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2505, 0.386965971827507) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.4063312220722437) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO1', '(DO2']
DC 2 --> ['(DO3', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.237, 0.37009901866316797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.244, 0.3803440786451101) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2525, 0.37947599655389785) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2205, 0.4140500556230545) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2455, 0.3955391386151314) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.259, 0.3936798993647099) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.228, 0.40273119807243346) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2345, 0.3867731237113476) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.40605911873281003) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2295, 0.4235153877139092) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.237, 0.37939738485217095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.4025634140372276) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.4151183165311813) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.237, 0.364233627140522) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2565, 0.3853981539607048) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO3', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.40416891252994536) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.243, 0.37544650375843047) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2505, 0.39581853960454466) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.229, 0.381754833817482) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2515, 0.3857079335451126) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2465, 0.41173231476545336) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.224, 0.4215553268194199) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2465, 0.38424409824609757) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2455, 0.381751005679369) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.212, 0.4344401593208313) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.244, 0.3731420093178749) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.36966392081975935) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.223, 0.3954435364305973) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.247, 0.4002383649945259) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.243, 0.37672717171907427) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO2', '(DO5']
DC 1 --> ['(DO1', '(DO4']
DC 2 --> ['(DO0', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2295, 0.39427091252803803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2415, 0.37944178700447084) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.3470905670970678) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.222, 0.3927877932786942) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2375, 0.37149711817502973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2355, 0.33326071670651436) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2225, 0.404640291929245) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.236, 0.3494814625680447) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.241, 0.3086959166526794) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2195, 0.36402074360847475) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2485, 0.3224647151529789) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2425, 0.3074909439980984) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.221, 0.3534953351914883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2435, 0.3730064832270145) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.3214925107061863) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO0', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.23, 0.33900158911943434) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2395, 0.3248579670339823) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.3085984494388104) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2165, 0.3454385335445404) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.244, 0.33255260932445524) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.246, 0.3158068011105061) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2225, 0.33516234719753263) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2395, 0.32185529282689096) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2475, 0.3164999683499336) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2145, 0.3489092805683613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2475, 0.32337207782268523) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.3219132966697216) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.221, 0.3268485080599785) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.242, 0.3007626261115074) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.321059306293726) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO5', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.3489630881845951) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.239, 0.30826111024618147) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.3241062150001526) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2235, 0.31698570227622985) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2545, 0.27972531485557556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2525, 0.31417942062020304) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.222, 0.35144030636548995) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2375, 0.2886037465929985) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.3144437828063965) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2215, 0.3167281104922295) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.238, 0.30057359910011294) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.30517583483457567) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.219, 0.3244075635075569) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.241, 0.2742335956692696) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2635, 0.30852732113003734) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO0', '(DO3']
DC 1 --> ['(DO1', '(DO2']
DC 2 --> ['(DO5', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2215, 0.3113977527022362) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2445, 0.28112059515714644) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.30174398973584177) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.217, 0.31618596482276917) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.247, 0.26960949462652206) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.28439123910665515) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2245, 0.31517504411935804) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2455, 0.2721973270177841) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.263, 0.3105849977433682) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.224, 0.336913890004158) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.244, 0.2726586386561394) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.29692021262645724) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2215, 0.29848788505792617) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2465, 0.2615684265494347) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2555, 0.3153376507461071) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO3', '(DO0']
DC 2 --> ['(DO4', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2225, 0.3120651537179947) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2455, 0.2635267629027367) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.2901798727810383) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.216, 0.3151608789861202) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.242, 0.2730887432098389) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.29243625766038894) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2185, 0.3116022197008133) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.267296605527401) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.262, 0.29004334872961046) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.219, 0.29693233436346056) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.2598440079689026) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.28755742263793943) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.222, 0.32522926527261736) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2565, 0.2551361850500107) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.262, 0.3088364949822426) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO2', '(DO5']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO4', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2255, 0.28306151711940764) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.2664666026830673) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.31205379420518875) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2225, 0.31330293613672255) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.254, 0.27470805728435516) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.3079133983850479) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.3087439751625061) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.243, 0.26200006288290023) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.256, 0.27373855471611025) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2185, 0.32252452850341795) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2505, 0.2724767670631409) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.242, 0.3040704430937767) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.225, 0.3100569682121277) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.247, 0.27150699508190157) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.261, 0.2816352234482765) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.028, 0.11652796924114227), (0.201, 0.1360903970003128), (0.227, 0.1724819296002388), (0.2375, 0.21655402204394342), (0.238, 0.2850736349672079), (0.241, 0.3079529513716698), (0.225, 0.3193999870121479), (0.2385, 0.3049556414783001), (0.233, 0.34729144340753554), (0.233, 0.34813025406003), (0.227, 0.39056425765156744), (0.237, 0.37009901866316797), (0.2205, 0.4140500556230545), (0.228, 0.40273119807243346), (0.2295, 0.4235153877139092), (0.22, 0.4151183165311813), (0.227, 0.40416891252994536), (0.229, 0.381754833817482), (0.224, 0.4215553268194199), (0.212, 0.4344401593208313), (0.223, 0.3954435364305973), (0.2295, 0.39427091252803803), (0.222, 0.3927877932786942), (0.2225, 0.404640291929245), (0.2195, 0.36402074360847475), (0.221, 0.3534953351914883), (0.23, 0.33900158911943434), (0.2165, 0.3454385335445404), (0.2225, 0.33516234719753263), (0.2145, 0.3489092805683613), (0.221, 0.3268485080599785), (0.22, 0.3489630881845951), (0.2235, 0.31698570227622985), (0.222, 0.35144030636548995), (0.2215, 0.3167281104922295), (0.219, 0.3244075635075569), (0.2215, 0.3113977527022362), (0.217, 0.31618596482276917), (0.2245, 0.31517504411935804), (0.224, 0.336913890004158), (0.2215, 0.29848788505792617), (0.2225, 0.3120651537179947), (0.216, 0.3151608789861202), (0.2185, 0.3116022197008133), (0.219, 0.29693233436346056), (0.222, 0.32522926527261736), (0.2255, 0.28306151711940764), (0.2225, 0.31330293613672255), (0.22, 0.3087439751625061), (0.2185, 0.32252452850341795), (0.225, 0.3100569682121277)]
TEST: 
[(0.02625, 0.11564967942237854), (0.20475, 0.1340780227780342), (0.23375, 0.1707777316570282), (0.234, 0.2147365777492523), (0.23675, 0.2831582199335098), (0.23825, 0.304330127120018), (0.234, 0.3134162529706955), (0.243, 0.2997327628135681), (0.2355, 0.33897527277469636), (0.235, 0.3437932472229004), (0.24225, 0.38570377457141874), (0.23825, 0.3642382961511612), (0.229, 0.41268920719623564), (0.23925, 0.39841586017608643), (0.233, 0.42172546982765197), (0.23975, 0.4079079098701477), (0.23775, 0.39770257961750033), (0.236, 0.37862710666656496), (0.233, 0.41580096530914307), (0.23075, 0.4254427543878555), (0.23725, 0.3914456901550293), (0.2375, 0.38864720714092255), (0.23275, 0.384957687497139), (0.23, 0.39959631514549254), (0.2265, 0.35746684885025026), (0.23225, 0.3465976668596268), (0.228, 0.3309641361236572), (0.22425, 0.33784429407119754), (0.23875, 0.3275534017086029), (0.2215, 0.3417054167985916), (0.2405, 0.31991830897331236), (0.232, 0.3362924411296844), (0.237, 0.30957955384254454), (0.23625, 0.34264128291606905), (0.24025, 0.3051854112148285), (0.2365, 0.3167934989929199), (0.238, 0.3028997359275818), (0.2335, 0.31464232623577115), (0.23625, 0.31288958156108854), (0.22975, 0.33113469326496126), (0.23575, 0.2922045308351517), (0.2365, 0.30570854580402373), (0.22975, 0.3068471440076828), (0.22975, 0.306662871837616), (0.23, 0.2926152601242065), (0.22925, 0.31642286241054535), (0.23325, 0.27714146828651426), (0.22525, 0.3063358886241913), (0.23575, 0.29251500868797303), (0.23825, 0.3076675488948822), (0.23675, 0.29523795878887177)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00       100
           3       0.18      0.33      0.23       100
           4       0.00      0.00      0.00       100
           5       1.00      0.01      0.02       100
           7       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          16       0.20      0.42      0.27       100
          17       0.33      0.01      0.02       100
          18       0.00      0.00      0.00       100
          22       0.26      0.46      0.34       100
          29       1.00      0.01      0.02       100
          33       0.18      0.59      0.28       100
          34       0.21      0.46      0.29       100
          35       0.16      0.25      0.20       100
          39       0.21      0.53      0.30       100
          43       0.23      0.35      0.27       100
          45       0.09      0.28      0.14       100
          50       0.00      0.00      0.00       100
          51       0.28      0.49      0.35       100
          52       0.47      0.85      0.60       100
          55       0.12      0.25      0.16       100
          57       0.00      0.00      0.00       100
          60       0.83      0.05      0.09       100
          61       0.44      0.55      0.49       100
          62       0.32      0.63      0.42       100
          63       0.26      0.44      0.32       100
          64       0.15      0.24      0.19       100
          67       0.00      0.00      0.00       100
          68       1.00      0.05      0.10       100
          82       0.48      0.77      0.59       100
          83       0.25      0.53      0.34       100
          84       0.00      0.00      0.00       100
          87       0.00      0.00      0.00       100
          90       0.19      0.52      0.27       100
          91       0.00      0.00      0.00       100
          93       0.00      0.00      0.00       100
          94       0.87      0.13      0.23       100
          96       0.25      0.01      0.02       100
          97       0.00      0.00      0.00       100
          98       0.20      0.26      0.23       100

    accuracy                           0.24      4000
   macro avg       0.25      0.24      0.17      4000
weighted avg       0.25      0.24      0.17      4000

Competition_DC_1
VAL: 
[(0.032, 0.11632643985748291), (0.225, 0.13515009021759034), (0.2295, 0.17842501321434975), (0.254, 0.21536052727699279), (0.2465, 0.2713924720585346), (0.252, 0.31631595626473424), (0.254, 0.3234565255343914), (0.247, 0.34021400851011274), (0.2465, 0.35142993158102037), (0.2525, 0.35313763621449473), (0.2505, 0.386965971827507), (0.244, 0.3803440786451101), (0.2455, 0.3955391386151314), (0.2345, 0.3867731237113476), (0.237, 0.37939738485217095), (0.237, 0.364233627140522), (0.243, 0.37544650375843047), (0.2515, 0.3857079335451126), (0.2465, 0.38424409824609757), (0.244, 0.3731420093178749), (0.247, 0.4002383649945259), (0.2415, 0.37944178700447084), (0.2375, 0.37149711817502973), (0.236, 0.3494814625680447), (0.2485, 0.3224647151529789), (0.2435, 0.3730064832270145), (0.2395, 0.3248579670339823), (0.244, 0.33255260932445524), (0.2395, 0.32185529282689096), (0.2475, 0.32337207782268523), (0.242, 0.3007626261115074), (0.239, 0.30826111024618147), (0.2545, 0.27972531485557556), (0.2375, 0.2886037465929985), (0.238, 0.30057359910011294), (0.241, 0.2742335956692696), (0.2445, 0.28112059515714644), (0.247, 0.26960949462652206), (0.2455, 0.2721973270177841), (0.244, 0.2726586386561394), (0.2465, 0.2615684265494347), (0.2455, 0.2635267629027367), (0.242, 0.2730887432098389), (0.2525, 0.267296605527401), (0.2605, 0.2598440079689026), (0.2565, 0.2551361850500107), (0.2685, 0.2664666026830673), (0.254, 0.27470805728435516), (0.243, 0.26200006288290023), (0.2505, 0.2724767670631409), (0.247, 0.27150699508190157)]
TEST: 
[(0.02975, 0.1154598125219345), (0.21425, 0.1327170597910881), (0.23525, 0.17442617779970168), (0.2495, 0.2109612363576889), (0.2515, 0.26514919781684876), (0.249, 0.31019817781448367), (0.25325, 0.31753485202789306), (0.252, 0.3351934677362442), (0.255, 0.3472372388839722), (0.255, 0.35010258603096006), (0.24975, 0.38814094507694247), (0.25075, 0.3810916172266006), (0.2465, 0.3932956836223602), (0.2495, 0.385191314458847), (0.24925, 0.38021220147609713), (0.24525, 0.3624204834699631), (0.246, 0.37125400292873384), (0.2435, 0.38148663234710695), (0.24475, 0.38086785733699796), (0.24075, 0.36745214235782625), (0.24425, 0.39683465647697447), (0.24, 0.3828236094713211), (0.24425, 0.375530308008194), (0.2375, 0.3496274471282959), (0.244, 0.323503354549408), (0.23775, 0.3731077036857605), (0.2395, 0.32332559418678286), (0.242, 0.32872146117687223), (0.24675, 0.31576158225536344), (0.2405, 0.32018812143802644), (0.23575, 0.29874914276599884), (0.23825, 0.30231795871257783), (0.2455, 0.2721628268957138), (0.2435, 0.28525100243091583), (0.24175, 0.2963409780263901), (0.24425, 0.26566251027584076), (0.24775, 0.2711612370014191), (0.243, 0.2668659574985504), (0.24475, 0.2659115700721741), (0.24425, 0.2655185650587082), (0.242, 0.2579071122407913), (0.24975, 0.25899934852123263), (0.249, 0.26660462927818296), (0.25125, 0.2585359692573547), (0.258, 0.25503010642528534), (0.251, 0.2495644817352295), (0.24925, 0.2632946952581406), (0.24475, 0.27211119055747984), (0.24725, 0.2540353310108185), (0.25, 0.2734926916360855), (0.249, 0.2676087471246719)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.38      0.03      0.06       100
           2       0.15      0.33      0.20       100
           4       0.50      0.01      0.02       100
           5       0.00      0.00      0.00       100
           7       0.36      0.04      0.07       100
           8       0.30      0.43      0.35       100
          10       0.20      0.42      0.27       100
          13       0.26      0.45      0.33       100
          14       1.00      0.02      0.04       100
          15       0.22      0.40      0.28       100
          17       0.00      0.00      0.00       100
          18       0.33      0.01      0.02       100
          20       0.31      0.72      0.43       100
          23       0.40      0.70      0.51       100
          29       0.00      0.00      0.00       100
          36       0.29      0.36      0.32       100
          37       0.18      0.38      0.25       100
          38       0.17      0.30      0.22       100
          42       0.21      0.41      0.28       100
          44       0.11      0.33      0.17       100
          50       0.00      0.00      0.00       100
          53       0.42      0.66      0.51       100
          57       1.00      0.01      0.02       100
          60       0.86      0.12      0.21       100
          67       0.00      0.00      0.00       100
          68       0.88      0.14      0.24       100
          70       0.32      0.40      0.35       100
          72       0.14      0.21      0.17       100
          76       0.35      0.78      0.49       100
          79       0.23      0.53      0.32       100
          84       0.00      0.00      0.00       100
          85       0.32      0.70      0.43       100
          87       0.91      0.10      0.18       100
          91       1.00      0.01      0.02       100
          92       0.17      0.40      0.24       100
          93       0.25      0.01      0.02       100
          94       1.00      0.08      0.15       100
          96       0.33      0.04      0.07       100
          97       1.00      0.01      0.02       100
          99       0.19      0.42      0.26       100

    accuracy                           0.25      4000
   macro avg       0.37      0.25      0.19      4000
weighted avg       0.37      0.25      0.19      4000

Competition_DC_2
VAL: 
[(0.0265, 0.11660425519943238), (0.2165, 0.14034129482507707), (0.2355, 0.18283687591552733), (0.2575, 0.23222217267751694), (0.252, 0.2875493269264698), (0.2485, 0.3426038283407688), (0.2605, 0.3400157999098301), (0.252, 0.3700522485822439), (0.2555, 0.38624285365641114), (0.254, 0.3919358174055815), (0.2485, 0.4063312220722437), (0.2525, 0.37947599655389785), (0.259, 0.3936798993647099), (0.2575, 0.40605911873281003), (0.249, 0.4025634140372276), (0.2565, 0.3853981539607048), (0.2505, 0.39581853960454466), (0.2465, 0.41173231476545336), (0.2455, 0.381751005679369), (0.249, 0.36966392081975935), (0.243, 0.37672717171907427), (0.245, 0.3470905670970678), (0.2355, 0.33326071670651436), (0.241, 0.3086959166526794), (0.2425, 0.3074909439980984), (0.245, 0.3214925107061863), (0.25, 0.3085984494388104), (0.246, 0.3158068011105061), (0.2475, 0.3164999683499336), (0.253, 0.3219132966697216), (0.245, 0.321059306293726), (0.252, 0.3241062150001526), (0.2525, 0.31417942062020304), (0.2605, 0.3144437828063965), (0.2515, 0.30517583483457567), (0.2635, 0.30852732113003734), (0.2575, 0.30174398973584177), (0.258, 0.28439123910665515), (0.263, 0.3105849977433682), (0.2585, 0.29692021262645724), (0.2555, 0.3153376507461071), (0.26, 0.2901798727810383), (0.258, 0.29243625766038894), (0.262, 0.29004334872961046), (0.249, 0.28755742263793943), (0.262, 0.3088364949822426), (0.254, 0.31205379420518875), (0.2515, 0.3079133983850479), (0.256, 0.27373855471611025), (0.242, 0.3040704430937767), (0.261, 0.2816352234482765)]
TEST: 
[(0.032, 0.11564855867624282), (0.22425, 0.13786108762025834), (0.2535, 0.17937930792570114), (0.26425, 0.22872871112823487), (0.265, 0.28258803629875184), (0.26075, 0.33717015945911405), (0.26525, 0.33554424571990965), (0.26475, 0.35955883872509004), (0.263, 0.3778942075967789), (0.263, 0.38697451519966125), (0.2655, 0.4018861366510391), (0.258, 0.3764015915393829), (0.263, 0.39092350578308105), (0.25775, 0.3998217715024948), (0.26025, 0.39841112279891966), (0.25675, 0.3807909437417984), (0.254, 0.3885895720720291), (0.25325, 0.40567805123329165), (0.25325, 0.37374846315383914), (0.2535, 0.36644154381752014), (0.254, 0.37249764454364775), (0.25775, 0.3388902391195297), (0.2545, 0.32572864985466005), (0.2545, 0.30385994267463684), (0.253, 0.30212230324745176), (0.25175, 0.31779171884059904), (0.248, 0.3060936679840088), (0.2555, 0.31167669558525085), (0.253, 0.3101387326717377), (0.258, 0.31433756291866305), (0.24875, 0.3137156308889389), (0.25525, 0.31484310400485993), (0.26025, 0.3020425152778625), (0.256, 0.304369154214859), (0.2565, 0.2942220937013626), (0.261, 0.29942201495170595), (0.26225, 0.29646752214431765), (0.2605, 0.27649449729919434), (0.2565, 0.29639609158039093), (0.25875, 0.2855535786151886), (0.258, 0.30180036640167235), (0.26625, 0.2800150229930878), (0.2705, 0.28560210394859314), (0.26975, 0.2777848354578018), (0.2585, 0.2816161988973618), (0.25775, 0.29673837041854856), (0.2565, 0.3060634331703186), (0.25425, 0.2993837620019913), (0.26725, 0.2612680741548538), (0.26125, 0.2957950325012207), (0.26675, 0.2801737296581268)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           7       0.00      0.00      0.00       100
           9       0.26      0.63      0.37       100
          11       0.20      0.53      0.29       100
          14       0.00      0.00      0.00       100
          17       0.78      0.07      0.13       100
          18       0.00      0.00      0.00       100
          19       0.20      0.36      0.26       100
          21       0.30      0.68      0.42       100
          26       0.22      0.38      0.28       100
          27       0.22      0.32      0.26       100
          29       0.00      0.00      0.00       100
          30       0.26      0.51      0.35       100
          32       0.19      0.41      0.26       100
          40       0.26      0.39      0.31       100
          41       0.39      0.56      0.46       100
          47       0.41      0.80      0.54       100
          49       0.24      0.51      0.32       100
          50       0.00      0.00      0.00       100
          54       0.32      0.63      0.42       100
          57       0.00      0.00      0.00       100
          58       0.35      0.50      0.41       100
          60       0.77      0.30      0.43       100
          67       0.00      0.00      0.00       100
          68       0.88      0.14      0.24       100
          69       0.37      0.73      0.49       100
          74       0.16      0.31      0.21       100
          77       0.17      0.38      0.24       100
          78       0.15      0.42      0.22       100
          84       0.00      0.00      0.00       100
          87       0.00      0.00      0.00       100
          89       0.27      0.49      0.35       100
          91       0.90      0.09      0.16       100
          93       0.00      0.00      0.00       100
          94       1.00      0.01      0.02       100
          95       0.30      0.49      0.37       100
          96       0.33      0.01      0.02       100
          97       0.50      0.02      0.04       100

    accuracy                           0.27      4000
   macro avg       0.26      0.27      0.20      4000
weighted avg       0.26      0.27      0.20      4000

Collaboration
DC 0, val_set_size=2000, COIs=[63, 34, 90, 16, 22, 3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45, 82, 43, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([63, 34, 90, 16, 22,  3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45,
        82, 43,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.028, 0.11652796924114227)
DC 1, val_set_size=2000, COIs=[8, 99, 85, 20, 36, 13, 23, 15, 79, 38, 2, 10, 76, 53, 44, 70, 92, 72, 42, 37, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([ 8, 99, 85, 20, 36, 13, 23, 15, 79, 38,  2, 10, 76, 53, 44, 70, 92, 72,
        42, 37,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.032, 0.11632643985748291)
DC 2, val_set_size=2000, COIs=[30, 54, 32, 74, 9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11, 26, 19, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([30, 54, 32, 74,  9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11,
        26, 19,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.0265, 0.11660425519943238)
D00: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D01: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D02: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D03: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D04: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D05: 1000 samples from classes {1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97}
D06: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D07: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D08: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D09: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D010: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D011: 1000 samples from classes {3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98}
D012: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D013: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D014: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D015: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D016: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D017: 1000 samples from classes {2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99}
D018: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D019: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D020: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D021: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D022: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
D023: 1000 samples from classes {9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO0', '(DO2']
DC 2 --> ['(DO5', '(DO3']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.207, 0.13267525786161422) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.217, 0.1313484918475151) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.21, 0.13999190738797188) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.215, 0.1720471224784851) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.239, 0.17019950768351555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.1873358958363533) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.2246713495105505) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2455, 0.20993761107325554) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.23001324252784253) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2425, 0.2701809162050486) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.2707422410845757) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.29668021470308303) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2445, 0.286735528588295) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2435, 0.31053646305203436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.3275420639961958) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO5']
DC 1 --> ['(DO1', '(DO2']
DC 2 --> ['(DO3', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.32891177186369897) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2545, 0.34542896822094915) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.255, 0.3409604746997356) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2355, 0.3243708148598671) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.3502530733346939) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.3308991430699825) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2395, 0.33594149124622347) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.3552860699594021) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2615, 0.3536818906441331) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.236, 0.364378031283617) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.3724585956633091) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.37543862760066987) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2315, 0.36472533337771895) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2315, 0.3958821423649788) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.37022937592864036) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[1, 4, 5, 7, 14, 17, 18, 29, 50, 57, 60, 67, 68, 84, 87, 91, 93, 94, 96, 97], M=tensor([ 1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20,
        21, 22, 23, 26, 27, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,
        43, 44, 45, 47, 49, 50, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 64,
        67, 68, 69, 70, 72, 74, 76, 77, 78, 79, 82, 83, 84, 85, 87, 89, 90, 91,
        92, 93, 94, 95, 96, 97, 98, 99], device='cuda:0'), Initial Performance: (0.38033333333333336, 0.071589537302653)
DC Expert-0, val_set_size=1000, COIs=[3, 16, 22, 33, 34, 35, 39, 43, 45, 51, 52, 55, 61, 62, 63, 64, 82, 83, 90, 98], M=tensor([63, 34, 90, 16, 22,  3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45,
        82, 43,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.463, 0.08117087396979332)
DC Expert-1, val_set_size=1000, COIs=[2, 8, 10, 13, 15, 20, 23, 36, 37, 38, 42, 44, 53, 70, 72, 76, 79, 85, 92, 99], M=tensor([ 8, 99, 85, 20, 36, 13, 23, 15, 79, 38,  2, 10, 76, 53, 44, 70, 92, 72,
        42, 37,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.463, 0.08359926116466522)
DC Expert-2, val_set_size=1000, COIs=[9, 11, 19, 21, 26, 27, 30, 32, 40, 41, 47, 49, 54, 58, 69, 74, 77, 78, 89, 95], M=tensor([30, 54, 32, 74,  9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11,
        26, 19,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), Initial Performance: (0.528, 0.0669591868519783)
SUPER-DC 0, val_set_size=2000, COIs=[63, 34, 90, 16, 22, 3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45, 82, 43, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([63, 34, 90, 16, 22,  3, 62, 52, 33, 98, 51, 61, 55, 39, 83, 35, 64, 45,
        82, 43,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[8, 99, 85, 20, 36, 13, 23, 15, 79, 38, 2, 10, 76, 53, 44, 70, 92, 72, 42, 37, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([ 8, 99, 85, 20, 36, 13, 23, 15, 79, 38,  2, 10, 76, 53, 44, 70, 92, 72,
        42, 37,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[30, 54, 32, 74, 9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11, 26, 19, 1, 60, 14, 97, 50, 18, 87, 5, 17, 68, 29, 91, 96, 94, 4, 84, 7, 93, 57, 67], M=tensor([30, 54, 32, 74,  9, 41, 89, 47, 78, 77, 95, 40, 49, 69, 27, 21, 58, 11,
        26, 19,  1, 60, 14, 97, 50, 18, 87,  5, 17, 68, 29, 91, 96, 94,  4, 84,
         7, 93, 57, 67], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.467, 0.09592470401525498) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.482, 0.09095533388853073) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.518, 0.08171297866106034) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5213333333333333, 0.05078557256857554) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2515, 0.16868808338046073) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.16701046451926232) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.256, 0.16919519929587842) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.435, 0.11396939158439637) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.492, 0.09948022502660751) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.497, 0.0995613876581192) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.536, 0.05151605680584907) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2635, 0.1329599380493164) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.13759397709369658) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.292, 0.137433444917202) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.449, 0.1071932715177536) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.478, 0.10565101158618927) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.493, 0.10190964704751969) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.549, 0.051661004741986594) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2775, 0.11919548016786576) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.287, 0.12210689961910248) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2635, 0.13576778504252435) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.454, 0.12145344614982605) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.459, 0.1215991563796997) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.496, 0.10651189276576042) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5336666666666666, 0.05612463186184565) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2855, 0.12240124383568764) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.302, 0.12293084716796875) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2705, 0.12765928994119166) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.453, 0.12178345918655395) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.507, 0.11226512479782104) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.521, 0.10595282021164894) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5493333333333333, 0.058530283252398174) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2935, 0.11337517496943474) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2645, 0.11888779789209365) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.297, 0.11017352348566055) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.454, 0.12961208522319795) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.468, 0.12203346240520477) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.498, 0.11494084107875824) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5446666666666666, 0.0627374601662159) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2835, 0.1110892179608345) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2755, 0.11600842061638832) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2875, 0.12038076232373715) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.454, 0.1351235914826393) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.491, 0.12482273483276367) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.518, 0.11209675106406213) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5506666666666666, 0.07177256861329079) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.10452789092063904) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.10119515344500542) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.318, 0.1065254180431366) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.462, 0.1275752284526825) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.496, 0.12328226721286774) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.498, 0.12513617914915084) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5333333333333333, 0.08254421648383141) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.298, 0.10168884915113449) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.289, 0.10577853745222092) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.32, 0.10334944227337838) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.464, 0.13145825839042663) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.476, 0.14831094086170196) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.496, 0.12751785904169083) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.519, 0.09198752735058467) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.296, 0.1000212989449501) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.287, 0.10985991430282593) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.10441301673650742) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.464, 0.14190102779865266) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.473, 0.1416766476035118) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.513, 0.12621654373407365) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5166666666666667, 0.0909566710392634) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.10131297934055328) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.10274100244045258) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3255, 0.09907077157497406) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.451, 0.13584674507379532) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.477, 0.14025242805480956) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.494, 0.14623820501565934) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5443333333333333, 0.08663617371519407) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.303, 0.10261545276641845) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3185, 0.10029391968250274) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.327, 0.10562944903969765) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.443, 0.15607280337810517) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.482, 0.1527647672891617) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.12872501319646834) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5396666666666666, 0.10410979318618774) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3255, 0.1033110933303833) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.328, 0.10173303431272507) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3335, 0.10170114547014236) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.466, 0.14555313140153886) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.48, 0.146261093378067) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.478, 0.1477553607225418) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.528, 0.10775545859336853) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.318, 0.10346791642904282) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.318, 0.10607786643505096) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3055, 0.11163339322805405) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.447, 0.15006133723258971) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.483, 0.1581961463689804) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.501, 0.13558411687612534) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5423333333333333, 0.09692102640867234) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3205, 0.10424595844745636) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.303, 0.11536276286840438) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3285, 0.10157447975873947) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.447, 0.16463314712047578) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.485, 0.15419557547569274) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.488, 0.148456758081913) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5323333333333333, 0.11004446961482366) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3185, 0.1088402983546257) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3215, 0.1087567017674446) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3215, 0.10344963520765305) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.463, 0.16025922513008117) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.498, 0.15876692366600037) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.14174512511491774) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5233333333333333, 0.11865718563397726) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2915, 0.11373209679126739) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3195, 0.10919205957651139) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.322, 0.10554565590620041) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.453, 0.17772343325614928) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.495, 0.1583318476676941) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.1596973295211792) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5383333333333333, 0.10667852928241094) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2985, 0.11296353495121002) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3035, 0.11500004690885544) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3245, 0.11122202152013778) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.451, 0.16768108069896698) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.468, 0.16623161602020264) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.477, 0.1631289057135582) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.539, 0.11735831047097842) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3055, 0.11243853884935379) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.29, 0.11969411325454712) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3365, 0.10568822193145752) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.453, 0.16775935733318328) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.479, 0.15688611042499542) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.501, 0.1512657105922699) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5253333333333333, 0.1256489613155524) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3005, 0.11450959086418151) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.294, 0.11853343534469604) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3225, 0.11783645394444466) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.429, 0.17644089829921722) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.481, 0.16241454207897185) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.476, 0.15229100394248962) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.529, 0.1198116762638092) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3145, 0.11106307834386825) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3055, 0.11857332542538643) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.10862826347351075) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.463, 0.1704070395231247) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.475, 0.17005879604816437) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.1739008868932724) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5203333333333333, 0.11991854192813238) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.321, 0.10994097024202347) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3065, 0.11217029994726181) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3135, 0.1211224567592144) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.445, 0.1909039270877838) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.477, 0.17136284804344176) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.502, 0.16283024150133132) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.529, 0.12897136721014976) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.314, 0.11615431022644043) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3125, 0.11796934378147125) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3235, 0.11495410889387131) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.452, 0.18550944793224336) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.481, 0.1841881409883499) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.498, 0.16439702236652373) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5253333333333333, 0.13967481438318888) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3185, 0.11762289384007454) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.11897520655393601) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.339, 0.1185417084991932) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.451, 0.19154211568832397) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.472, 0.17459844088554383) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.48, 0.16325724267959596) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5323333333333333, 0.1314225802818934) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.314, 0.11842795473337174) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3135, 0.12217893719673156) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.328, 0.11434131813049317) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.449, 0.18203699898719788) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.465, 0.18817800879478455) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.171513569355011) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.528, 0.13588351897398632) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.311, 0.1205289996266365) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3, 0.1266606919169426) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.318, 0.11642417412996292) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.456, 0.1957682908773422) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.486, 0.15861619460582732) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.1778702608346939) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5413333333333333, 0.12482599979639053) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3305, 0.12002079540491103) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.12088163608312606) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.316, 0.11672151097655296) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.456, 0.18481640815734862) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.459, 0.1870842524766922) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.1805966904759407) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5233333333333333, 0.13102843721707663) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3105, 0.12006309866905213) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.12185615354776383) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.319, 0.12037796550989151) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.461, 0.17993666589260102) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.461, 0.18680758219957352) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.493, 0.18140561085939408) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5343333333333333, 0.13318757182359695) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.33, 0.12165722432732583) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2985, 0.12938257956504823) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.328, 0.12058417218923569) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.455, 0.18340949362516404) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.471, 0.1896343629360199) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.477, 0.1843740305900574) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5373333333333333, 0.1353499622742335) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.302, 0.1226813348531723) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.296, 0.12710063248872758) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.333, 0.12326905035972595) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.447, 0.2107821056842804) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.473, 0.1849220834374428) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.476, 0.1824060513973236) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.523, 0.13931117498874665) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2795, 0.12521401017904282) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3095, 0.12645544052124025) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3235, 0.11980820655822753) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.457, 0.18350799345970153) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.471, 0.19420711255073547) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.496, 0.1802760169506073) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.523, 0.15648976318041483) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.301, 0.12973355025053024) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3145, 0.1281670777797699) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3325, 0.12314916920661927) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.451, 0.19363530683517455) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.463, 0.19672118067741395) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.493, 0.17550056099891662) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5333333333333333, 0.13124475423494975) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.293, 0.12636737626791) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.307, 0.12429128777980804) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3275, 0.12159223392605782) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.448, 0.20253863513469697) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.457, 0.22211100029945374) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.5, 0.18373706018924713) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5236666666666666, 0.14713030538956323) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.311, 0.1253514506816864) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.288, 0.13900580775737761) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3245, 0.12808703017234802) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.455, 0.20146973764896392) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.474, 0.1989826431274414) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.494, 0.18222648656368257) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.524, 0.14866838566462198) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2915, 0.13309111332893372) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3025, 0.12840263772010804) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.1288219750523567) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.449, 0.19898843443393707) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.478, 0.2019990307688713) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.497, 0.18259422409534454) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5353333333333333, 0.147679508715868) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.311, 0.12912265157699584) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3135, 0.12756332468986512) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.1255185452103615) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.439, 0.22662709975242615) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.456, 0.2096563141345978) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.495, 0.1911096783876419) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5173333333333333, 0.1368474674622218) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.302, 0.12885164505243302) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3225, 0.1328148472905159) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3, 0.13900179663300513) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.46, 0.1961125500202179) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.482, 0.20345108807086945) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.482, 0.2015304319858551) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5333333333333333, 0.1465038571159045) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.302, 0.13013096076250077) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.307, 0.13339499753713607) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.13383646413683892) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.453, 0.22597289860248565) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.482, 0.1983307478427887) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.1879279714822769) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5183333333333333, 0.13817130613327028) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.29, 0.12940210044384004) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.13300282001495362) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.305, 0.13636987291276456) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.459, 0.2159626805782318) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.472, 0.21638121247291564) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.502, 0.17896566462516786) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5183333333333333, 0.15739315553506214) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.301, 0.1242250553369522) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.326, 0.13144640171527863) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3225, 0.12687020060420037) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.459, 0.2107534362077713) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.483, 0.2029126479625702) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.20673220801353454) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5353333333333333, 0.15452636174360912) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3, 0.13916079318523408) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.309, 0.1338257696032524) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3305, 0.12719819688796996) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.028, 0.11652796924114227), (0.207, 0.13267525786161422), (0.215, 0.1720471224784851), (0.227, 0.2246713495105505), (0.2425, 0.2701809162050486), (0.2445, 0.286735528588295), (0.227, 0.32891177186369897), (0.2355, 0.3243708148598671), (0.2395, 0.33594149124622347), (0.236, 0.364378031283617), (0.2315, 0.36472533337771895), (0.2515, 0.16868808338046073), (0.2635, 0.1329599380493164), (0.2775, 0.11919548016786576), (0.2855, 0.12240124383568764), (0.2935, 0.11337517496943474), (0.2835, 0.1110892179608345), (0.2895, 0.10452789092063904), (0.298, 0.10168884915113449), (0.296, 0.1000212989449501), (0.3155, 0.10131297934055328), (0.303, 0.10261545276641845), (0.3255, 0.1033110933303833), (0.318, 0.10346791642904282), (0.3205, 0.10424595844745636), (0.3185, 0.1088402983546257), (0.2915, 0.11373209679126739), (0.2985, 0.11296353495121002), (0.3055, 0.11243853884935379), (0.3005, 0.11450959086418151), (0.3145, 0.11106307834386825), (0.321, 0.10994097024202347), (0.314, 0.11615431022644043), (0.3185, 0.11762289384007454), (0.314, 0.11842795473337174), (0.311, 0.1205289996266365), (0.3305, 0.12002079540491103), (0.3105, 0.12006309866905213), (0.33, 0.12165722432732583), (0.302, 0.1226813348531723), (0.2795, 0.12521401017904282), (0.301, 0.12973355025053024), (0.293, 0.12636737626791), (0.311, 0.1253514506816864), (0.2915, 0.13309111332893372), (0.311, 0.12912265157699584), (0.302, 0.12885164505243302), (0.302, 0.13013096076250077), (0.29, 0.12940210044384004), (0.301, 0.1242250553369522), (0.3, 0.13916079318523408)]
TEST: 
[(0.02625, 0.11564967942237854), (0.20725, 0.13152211207151412), (0.219, 0.17068863713741303), (0.2305, 0.22407088375091552), (0.239, 0.2684257616996765), (0.238, 0.2842558201551437), (0.232, 0.3244755610227585), (0.241, 0.3198522630929947), (0.23675, 0.33263207244873044), (0.23975, 0.36305752956867215), (0.23525, 0.36281412208080294), (0.25275, 0.16657906073331832), (0.25875, 0.1333840310573578), (0.2755, 0.11815161812305451), (0.27225, 0.12454103755950928), (0.27675, 0.11224020361900329), (0.272, 0.11086373156309128), (0.28525, 0.10307800406217575), (0.285, 0.10171172672510147), (0.284, 0.10036835551261902), (0.2935, 0.10280012875795365), (0.28925, 0.10304482477903366), (0.30625, 0.1028776428103447), (0.30075, 0.10423782503604889), (0.31125, 0.10571074312925338), (0.3085, 0.10752609604597092), (0.2835, 0.11400609916448592), (0.2955, 0.11497267544269561), (0.30325, 0.11347852027416229), (0.30175, 0.1147770266532898), (0.3105, 0.10989602100849151), (0.32025, 0.11222282913327217), (0.309, 0.11939436012506485), (0.31, 0.11943528771400452), (0.30425, 0.12020719593763352), (0.3025, 0.12189958429336548), (0.3155, 0.12133962959051132), (0.29825, 0.12117146545648574), (0.315, 0.12215413242578506), (0.2975, 0.12475726872682572), (0.26975, 0.12807569634914398), (0.3005, 0.13119533532857894), (0.28125, 0.12891056752204896), (0.307, 0.12828322964906694), (0.29575, 0.13294292044639588), (0.3015, 0.13037461513280868), (0.298, 0.12886498481035233), (0.2925, 0.12974818748235703), (0.2855, 0.13236388313770295), (0.3075, 0.1285330914258957), (0.297, 0.1418511290550232)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.34      0.45      0.38       100
           3       0.15      0.06      0.09       100
           4       0.12      0.24      0.16       100
           5       0.42      0.30      0.35       100
           7       0.31      0.46      0.37       100
          14       0.19      0.20      0.19       100
          16       0.13      0.07      0.09       100
          17       0.50      0.47      0.48       100
          18       0.27      0.29      0.28       100
          22       0.22      0.11      0.15       100
          29       0.20      0.32      0.24       100
          33       0.26      0.52      0.35       100
          34       0.27      0.13      0.17       100
          35       0.19      0.05      0.08       100
          39       0.41      0.40      0.40       100
          43       0.25      0.24      0.24       100
          45       0.17      0.19      0.18       100
          50       0.14      0.29      0.19       100
          51       0.16      0.17      0.17       100
          52       0.67      0.76      0.71       100
          55       0.15      0.08      0.11       100
          57       0.18      0.43      0.26       100
          60       0.65      0.67      0.66       100
          61       0.56      0.34      0.42       100
          62       0.45      0.25      0.32       100
          63       0.34      0.31      0.33       100
          64       0.16      0.16      0.16       100
          67       0.46      0.39      0.42       100
          68       0.78      0.50      0.61       100
          82       0.70      0.33      0.45       100
          83       0.23      0.11      0.15       100
          84       0.21      0.20      0.21       100
          87       0.21      0.30      0.25       100
          90       0.26      0.27      0.27       100
          91       0.42      0.45      0.43       100
          93       0.31      0.20      0.24       100
          94       0.42      0.64      0.51       100
          96       0.27      0.12      0.17       100
          97       0.23      0.31      0.26       100
          98       0.23      0.10      0.14       100

    accuracy                           0.30      4000
   macro avg       0.32      0.30      0.29      4000
weighted avg       0.32      0.30      0.29      4000

Collaboration_DC_1
VAL: 
[(0.032, 0.11632643985748291), (0.217, 0.1313484918475151), (0.239, 0.17019950768351555), (0.2455, 0.20993761107325554), (0.2525, 0.2707422410845757), (0.2435, 0.31053646305203436), (0.2545, 0.34542896822094915), (0.252, 0.3502530733346939), (0.251, 0.3552860699594021), (0.25, 0.3724585956633091), (0.2315, 0.3958821423649788), (0.2665, 0.16701046451926232), (0.277, 0.13759397709369658), (0.287, 0.12210689961910248), (0.302, 0.12293084716796875), (0.2645, 0.11888779789209365), (0.2755, 0.11600842061638832), (0.3165, 0.10119515344500542), (0.289, 0.10577853745222092), (0.287, 0.10985991430282593), (0.305, 0.10274100244045258), (0.3185, 0.10029391968250274), (0.328, 0.10173303431272507), (0.318, 0.10607786643505096), (0.303, 0.11536276286840438), (0.3215, 0.1087567017674446), (0.3195, 0.10919205957651139), (0.3035, 0.11500004690885544), (0.29, 0.11969411325454712), (0.294, 0.11853343534469604), (0.3055, 0.11857332542538643), (0.3065, 0.11217029994726181), (0.3125, 0.11796934378147125), (0.323, 0.11897520655393601), (0.3135, 0.12217893719673156), (0.3, 0.1266606919169426), (0.304, 0.12088163608312606), (0.305, 0.12185615354776383), (0.2985, 0.12938257956504823), (0.296, 0.12710063248872758), (0.3095, 0.12645544052124025), (0.3145, 0.1281670777797699), (0.307, 0.12429128777980804), (0.288, 0.13900580775737761), (0.3025, 0.12840263772010804), (0.3135, 0.12756332468986512), (0.3225, 0.1328148472905159), (0.307, 0.13339499753713607), (0.301, 0.13300282001495362), (0.326, 0.13144640171527863), (0.309, 0.1338257696032524)]
TEST: 
[(0.02975, 0.1154598125219345), (0.21975, 0.12880311447381973), (0.24775, 0.16441020262241363), (0.25075, 0.20238461601734162), (0.26025, 0.26276558911800385), (0.2605, 0.300230770111084), (0.25675, 0.33620828568935396), (0.25925, 0.3441411294937134), (0.2575, 0.3477890659570694), (0.2545, 0.3645108243227005), (0.248, 0.39773525750637057), (0.27425, 0.15977502185106277), (0.29425, 0.13162407058477402), (0.29575, 0.11635071086883544), (0.30225, 0.11761249059438705), (0.295, 0.11355444556474685), (0.292, 0.11067862004041672), (0.3185, 0.09837871670722961), (0.3175, 0.10201006656885148), (0.304, 0.10581856071949006), (0.31825, 0.10179401585459709), (0.32975, 0.09742042809724807), (0.337, 0.09927813017368317), (0.32175, 0.1040455498099327), (0.31125, 0.11006253659725189), (0.33675, 0.10548728162050247), (0.32775, 0.10672026973962784), (0.319, 0.10880033993721008), (0.3185, 0.11484108328819274), (0.322, 0.11248145145177842), (0.34075, 0.11193248182535172), (0.33625, 0.10580122780799865), (0.33925, 0.11160792249441147), (0.32375, 0.11352218997478485), (0.32425, 0.11827024567127228), (0.317, 0.11887911355495454), (0.322, 0.11568697953224182), (0.323, 0.11551568549871445), (0.3255, 0.12153548133373261), (0.3205, 0.11988722926378251), (0.31875, 0.12030136227607727), (0.32425, 0.12131618964672089), (0.315, 0.11938734042644501), (0.29475, 0.13320113915205), (0.30425, 0.12437016797065735), (0.3155, 0.12404439288377762), (0.328, 0.1267179458141327), (0.3155, 0.12927349632978438), (0.307, 0.12805005413293838), (0.325, 0.12752011168003083), (0.32075, 0.12783400076627732)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.38      0.33      0.35       100
           2       0.30      0.19      0.23       100
           4       0.14      0.20      0.16       100
           5       0.17      0.15      0.16       100
           7       0.33      0.37      0.35       100
           8       0.35      0.28      0.31       100
          10       0.27      0.22      0.24       100
          13       0.37      0.39      0.38       100
          14       0.23      0.30      0.26       100
          15       0.31      0.16      0.21       100
          17       0.33      0.41      0.37       100
          18       0.26      0.36      0.30       100
          20       0.47      0.62      0.53       100
          23       0.42      0.63      0.50       100
          29       0.28      0.24      0.26       100
          36       0.33      0.22      0.26       100
          37       0.30      0.25      0.27       100
          38       0.13      0.06      0.08       100
          42       0.19      0.25      0.22       100
          44       0.17      0.09      0.12       100
          50       0.13      0.20      0.16       100
          53       0.53      0.30      0.38       100
          57       0.19      0.45      0.27       100
          60       0.81      0.47      0.59       100
          67       0.30      0.40      0.34       100
          68       0.54      0.65      0.59       100
          70       0.48      0.42      0.45       100
          72       0.21      0.22      0.21       100
          76       0.49      0.68      0.57       100
          79       0.39      0.19      0.26       100
          84       0.18      0.18      0.18       100
          85       0.56      0.53      0.54       100
          87       0.31      0.37      0.33       100
          91       0.39      0.38      0.38       100
          92       0.15      0.12      0.13       100
          93       0.24      0.24      0.24       100
          94       0.52      0.58      0.55       100
          96       0.53      0.37      0.44       100
          97       0.28      0.28      0.28       100
          99       0.23      0.08      0.12       100

    accuracy                           0.32      4000
   macro avg       0.33      0.32      0.32      4000
weighted avg       0.33      0.32      0.32      4000

Collaboration_DC_2
VAL: 
[(0.0265, 0.11660425519943238), (0.21, 0.13999190738797188), (0.238, 0.1873358958363533), (0.26, 0.23001324252784253), (0.252, 0.29668021470308303), (0.2595, 0.3275420639961958), (0.255, 0.3409604746997356), (0.26, 0.3308991430699825), (0.2615, 0.3536818906441331), (0.2575, 0.37543862760066987), (0.264, 0.37022937592864036), (0.256, 0.16919519929587842), (0.292, 0.137433444917202), (0.2635, 0.13576778504252435), (0.2705, 0.12765928994119166), (0.297, 0.11017352348566055), (0.2875, 0.12038076232373715), (0.318, 0.1065254180431366), (0.32, 0.10334944227337838), (0.306, 0.10441301673650742), (0.3255, 0.09907077157497406), (0.327, 0.10562944903969765), (0.3335, 0.10170114547014236), (0.3055, 0.11163339322805405), (0.3285, 0.10157447975873947), (0.3215, 0.10344963520765305), (0.322, 0.10554565590620041), (0.3245, 0.11122202152013778), (0.3365, 0.10568822193145752), (0.3225, 0.11783645394444466), (0.334, 0.10862826347351075), (0.3135, 0.1211224567592144), (0.3235, 0.11495410889387131), (0.339, 0.1185417084991932), (0.328, 0.11434131813049317), (0.318, 0.11642417412996292), (0.316, 0.11672151097655296), (0.319, 0.12037796550989151), (0.328, 0.12058417218923569), (0.333, 0.12326905035972595), (0.3235, 0.11980820655822753), (0.3325, 0.12314916920661927), (0.3275, 0.12159223392605782), (0.3245, 0.12808703017234802), (0.308, 0.1288219750523567), (0.334, 0.1255185452103615), (0.3, 0.13900179663300513), (0.306, 0.13383646413683892), (0.305, 0.13636987291276456), (0.3225, 0.12687020060420037), (0.3305, 0.12719819688796996)]
TEST: 
[(0.032, 0.11564855867624282), (0.21775, 0.13636042684316635), (0.2515, 0.18324162131547928), (0.265, 0.226009942650795), (0.2565, 0.2912429578304291), (0.2635, 0.3228433345556259), (0.26475, 0.3374829571247101), (0.269, 0.32678240025043487), (0.2695, 0.34450879538059237), (0.2665, 0.3709832774400711), (0.2645, 0.3648850393295288), (0.258, 0.16915332931280136), (0.29125, 0.13677732092142106), (0.27725, 0.1345069854259491), (0.279, 0.12356050515174866), (0.29625, 0.11096835052967072), (0.29325, 0.1194556400179863), (0.31225, 0.10548653680086136), (0.32325, 0.1032372390627861), (0.31275, 0.10378353410959244), (0.32775, 0.0990766449868679), (0.32075, 0.10567054679989815), (0.31975, 0.10165844574570655), (0.3145, 0.1095053978562355), (0.328, 0.10315128868818282), (0.3305, 0.10666369959712028), (0.3225, 0.10609359797835351), (0.32325, 0.11102224609255791), (0.3335, 0.10758865222334862), (0.319, 0.11598894864320755), (0.337, 0.10836545181274414), (0.328, 0.11694155925512315), (0.329, 0.11499790638685227), (0.327, 0.12021424293518067), (0.3075, 0.11671007674932479), (0.30875, 0.12018171101808547), (0.3115, 0.11947742199897766), (0.31, 0.12110976612567902), (0.32, 0.12286922216415405), (0.31425, 0.12559918242692947), (0.30925, 0.12217531722784042), (0.31325, 0.12393836009502411), (0.32675, 0.1219344071149826), (0.31225, 0.1273873656988144), (0.30675, 0.1285149992108345), (0.3105, 0.12632179868221283), (0.295, 0.13989697009325028), (0.30425, 0.13408766496181487), (0.312, 0.13727973932027818), (0.3255, 0.12664460706710814), (0.326, 0.1279317103624344)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.42      0.30      0.35       100
           4       0.15      0.13      0.14       100
           5       0.33      0.16      0.22       100
           7       0.36      0.20      0.26       100
           9       0.32      0.32      0.32       100
          11       0.29      0.41      0.34       100
          14       0.32      0.20      0.25       100
          17       0.49      0.34      0.40       100
          18       0.27      0.24      0.26       100
          19       0.16      0.26      0.20       100
          21       0.41      0.67      0.51       100
          26       0.17      0.21      0.19       100
          27       0.21      0.31      0.25       100
          29       0.37      0.13      0.19       100
          30       0.32      0.27      0.29       100
          32       0.26      0.34      0.29       100
          40       0.22      0.37      0.28       100
          41       0.48      0.46      0.47       100
          47       0.50      0.54      0.52       100
          49       0.40      0.35      0.37       100
          50       0.19      0.07      0.10       100
          54       0.42      0.64      0.51       100
          57       0.34      0.31      0.32       100
          58       0.30      0.43      0.36       100
          60       0.62      0.58      0.60       100
          67       0.35      0.33      0.34       100
          68       0.69      0.56      0.62       100
          69       0.41      0.52      0.46       100
          74       0.17      0.32      0.22       100
          77       0.22      0.18      0.20       100
          78       0.19      0.38      0.25       100
          84       0.16      0.03      0.05       100
          87       0.41      0.37      0.39       100
          89       0.30      0.46      0.36       100
          91       0.52      0.24      0.33       100
          93       0.19      0.14      0.16       100
          94       0.70      0.46      0.55       100
          95       0.33      0.52      0.40       100
          96       0.37      0.26      0.31       100
          97       0.21      0.03      0.05       100

    accuracy                           0.33      4000
   macro avg       0.34      0.33      0.32      4000
weighted avg       0.34      0.33      0.32      4000

do_assignment: None
seeds: [42]
name: naive-cifar100-feddf42
score_metric: contrloss
aggregation: <function fed_df at 0x773e2d16ae50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=42
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[36, 88, 66, 39, 72, 46, 0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60, 8, 32, 90, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([36, 88, 66, 39, 72, 46,  0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60,  8,
        32, 90, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.022, 0.11633801007270814)
DC 1, val_set_size=2000, COIs=[43, 63, 33, 85, 5, 79, 51, 58, 12, 45, 9, 59, 37, 68, 65, 44, 61, 22, 7, 95, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([43, 63, 33, 85,  5, 79, 51, 58, 12, 45,  9, 59, 37, 68, 65, 44, 61, 22,
         7, 95, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.02, 0.1165862237215042)
DC 2, val_set_size=2000, COIs=[42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53, 6, 52, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53,
         6, 52, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.028, 0.11644194757938385)
D00: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D01: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D02: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D03: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D04: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D05: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D06: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D07: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D08: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D09: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D010: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D011: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D012: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D013: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D014: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D015: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D016: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D017: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D018: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D019: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D020: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D021: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D022: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D023: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.263, 0.08685893893241882) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3025, 0.08263768512010575) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3135, 0.08083096405863761) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.335, 0.07629107996821403) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.369, 0.07232779136300087) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.07404920035600662) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.358, 0.07337314462661743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.06915001139044762) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3775, 0.07268938685953617) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3575, 0.07379234585165978) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3905, 0.06901451271772385) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.381, 0.07392782309651375) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.365, 0.07494602659344674) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4095, 0.07061366850137711) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3785, 0.07536883401870728) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3575, 0.07767624665796757) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.397, 0.07347020149230957) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.08115288917720317) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.357, 0.08158254197239875) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4, 0.07606485348939895) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.364, 0.0852726878374815) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.08627774658799171) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.399, 0.08308847931027412) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.365, 0.08915887996554375) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.351, 0.0879390222132206) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4035, 0.08145112743973731) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.358, 0.09701336987316608) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.356, 0.09251910370588302) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.0914055604338646) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.369, 0.10001455986499787) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3465, 0.10197835007309913) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3915, 0.0932833259999752) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.356, 0.10357287529110909) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.358, 0.1110189605653286) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3875, 0.10062810672819614) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.10945922313630581) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.335, 0.1166032443344593) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.11054905226826668) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.366, 0.1233040382862091) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.32, 0.13056039848923684) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.1188707676306367) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.12873335829377175) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.328, 0.13726856458187103) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.12890538302063942) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.37, 0.1429154340028763) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.14804380214214324) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3805, 0.13313590133190156) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3635, 0.1551351354420185) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3225, 0.1476523701250553) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.14079017996788024) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.16218094485998152) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.328, 0.15162757775187494) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.151369949311018) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.1603279784321785) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3235, 0.16267192423343657) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.1497211807370186) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3485, 0.16116933432221411) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.318, 0.17240584275126458) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3665, 0.16077689519524574) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.343, 0.17171928587555885) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3235, 0.17368549850583076) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.15155359968543053) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.16676142251491546) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3215, 0.18195772299170493) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.16576803463697434) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.17564708679914476) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.1828320413827896) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.16201920318603516) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3555, 0.1791620932519436) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.316, 0.186650494992733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3715, 0.16355009591579436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3545, 0.1900620121359825) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3195, 0.184985631108284) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3795, 0.16634490495920182) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3485, 0.18803459233045577) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3245, 0.1971060556769371) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3775, 0.17977765822410582) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3465, 0.20058782160282135) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3345, 0.19176108849048615) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.369, 0.1896104857325554) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3355, 0.20933255806565285) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.324, 0.2067003157734871) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3755, 0.18284440064430238) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.20243065044283867) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.327, 0.2001874813437462) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.19175962609052657) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.35, 0.19914609077572823) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3225, 0.22146694546937942) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3655, 0.18464850968122482) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.336, 0.21111237847805023) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.316, 0.21806159609556197) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3645, 0.18488990062475205) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3445, 0.20920118141174315) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3235, 0.2120787159204483) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3725, 0.1945259848833084) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3385, 0.2165579120963812) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3055, 0.22671486121416093) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3925, 0.19790161269903184) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3395, 0.21588270235061646) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.329, 0.2213972337245941) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.363, 0.19827781897783278) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.346, 0.22308022505044936) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3145, 0.22794584631919862) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3825, 0.18770680803060533) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3465, 0.22154407930374145) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.313, 0.22783125460147857) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.2083489436507225) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3435, 0.23334290581941605) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.326, 0.21457857620716095) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.21577253067493438) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.34, 0.22529237282276154) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.334, 0.23745489037036896) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.20108537214994432) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.342, 0.22540062853693962) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.321, 0.2524120808839798) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3785, 0.19682964128255845) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.35, 0.21813612219691275) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.329, 0.2441912500858307) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.19942360486090183) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3415, 0.23743736231327056) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.315, 0.24166539299488068) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.22529547756910323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3515, 0.23223958081007004) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3095, 0.24285002076625825) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3765, 0.22537450504302978) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.25552296340465547) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.313, 0.24318717956542968) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3765, 0.21037085275352002) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3395, 0.23428134095668793) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3125, 0.2499458178281784) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3865, 0.22022816866636277) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.339, 0.2287716672718525) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.316, 0.2533205502033234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3805, 0.22395257449150086) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.341, 0.23761762002110481) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.305, 0.23417635428905487) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3885, 0.214585619866848) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.23728936156630517) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3115, 0.24286404597759248) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3915, 0.21579551577568054) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.25325304189324377) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.311, 0.2497175281047821) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.2222726398706436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3475, 0.25117430490255355) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.25107523310184476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3695, 0.22556790149211883) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3455, 0.24926526981592179) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.326, 0.2469143643975258) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3745, 0.22527778780460359) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3395, 0.26135204049944877) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.022, 0.11633801007270814), (0.263, 0.08685893893241882), (0.335, 0.07629107996821403), (0.358, 0.07337314462661743), (0.3575, 0.07379234585165978), (0.365, 0.07494602659344674), (0.3575, 0.07767624665796757), (0.357, 0.08158254197239875), (0.3535, 0.08627774658799171), (0.351, 0.0879390222132206), (0.356, 0.09251910370588302), (0.3465, 0.10197835007309913), (0.358, 0.1110189605653286), (0.335, 0.1166032443344593), (0.32, 0.13056039848923684), (0.328, 0.13726856458187103), (0.338, 0.14804380214214324), (0.3225, 0.1476523701250553), (0.328, 0.15162757775187494), (0.3235, 0.16267192423343657), (0.318, 0.17240584275126458), (0.3235, 0.17368549850583076), (0.3215, 0.18195772299170493), (0.3155, 0.1828320413827896), (0.316, 0.186650494992733), (0.3195, 0.184985631108284), (0.3245, 0.1971060556769371), (0.3345, 0.19176108849048615), (0.324, 0.2067003157734871), (0.327, 0.2001874813437462), (0.3225, 0.22146694546937942), (0.316, 0.21806159609556197), (0.3235, 0.2120787159204483), (0.3055, 0.22671486121416093), (0.329, 0.2213972337245941), (0.3145, 0.22794584631919862), (0.313, 0.22783125460147857), (0.326, 0.21457857620716095), (0.334, 0.23745489037036896), (0.321, 0.2524120808839798), (0.329, 0.2441912500858307), (0.315, 0.24166539299488068), (0.3095, 0.24285002076625825), (0.313, 0.24318717956542968), (0.3125, 0.2499458178281784), (0.316, 0.2533205502033234), (0.305, 0.23417635428905487), (0.3115, 0.24286404597759248), (0.311, 0.2497175281047821), (0.3155, 0.25107523310184476), (0.326, 0.2469143643975258)]
TEST: 
[(0.0245, 0.11540564060211182), (0.2545, 0.08686540859937668), (0.34225, 0.0760566881597042), (0.35525, 0.07361567229032516), (0.3655, 0.07359258452057839), (0.372, 0.0749969952404499), (0.37075, 0.07736916151642799), (0.36775, 0.08159546327590943), (0.35625, 0.08560298907756805), (0.35425, 0.08799932894110679), (0.3535, 0.09404695501923561), (0.35125, 0.10372161757946015), (0.35325, 0.11339483594894409), (0.3515, 0.11625222843885422), (0.33075, 0.13386274009943008), (0.34525, 0.1369856896996498), (0.33675, 0.14743266642093658), (0.33475, 0.14957833087444306), (0.336, 0.15237260514497758), (0.3415, 0.1605654509663582), (0.336, 0.17082170701026916), (0.33525, 0.17392975252866744), (0.33375, 0.1822129544019699), (0.33725, 0.17833903640508653), (0.3195, 0.18719808995723725), (0.33925, 0.18591496980190278), (0.329, 0.19334588384628296), (0.3285, 0.1897850594520569), (0.3345, 0.20258024901151658), (0.33, 0.19449491250514983), (0.3225, 0.2111990926861763), (0.3475, 0.20808252280950545), (0.3375, 0.2035454289317131), (0.323, 0.21628258216381074), (0.33375, 0.2136387545466423), (0.33925, 0.21908617317676543), (0.33125, 0.21814709877967833), (0.33925, 0.2094320307970047), (0.33475, 0.22986620664596558), (0.3415, 0.23520229756832123), (0.34225, 0.23344923627376557), (0.33775, 0.2321120789051056), (0.325, 0.2405841320157051), (0.33025, 0.2370086544752121), (0.33275, 0.23922888493537903), (0.33325, 0.24919944655895232), (0.31, 0.2309328179359436), (0.32875, 0.2382137452363968), (0.3265, 0.2428924390077591), (0.3315, 0.2482196469306946), (0.331, 0.24556627118587493)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.76      0.65      0.70       100
           3       0.16      0.10      0.12       100
           4       0.22      0.13      0.16       100
           8       0.50      0.37      0.43       100
          11       0.35      0.27      0.31       100
          13       0.24      0.27      0.26       100
          14       0.23      0.31      0.26       100
          16       0.30      0.39      0.34       100
          17       0.49      0.56      0.52       100
          19       0.25      0.28      0.27       100
          25       0.24      0.28      0.26       100
          26       0.27      0.20      0.23       100
          27       0.23      0.34      0.27       100
          28       0.53      0.57      0.55       100
          29       0.37      0.34      0.35       100
          31       0.40      0.16      0.23       100
          32       0.24      0.27      0.25       100
          35       0.17      0.13      0.15       100
          36       0.26      0.34      0.30       100
          38       0.19      0.16      0.17       100
          39       0.36      0.38      0.37       100
          46       0.27      0.23      0.25       100
          54       0.39      0.59      0.47       100
          55       0.17      0.18      0.17       100
          60       0.62      0.78      0.69       100
          64       0.20      0.21      0.21       100
          66       0.17      0.33      0.23       100
          67       0.61      0.35      0.45       100
          69       0.64      0.56      0.60       100
          71       0.65      0.61      0.63       100
          72       0.09      0.12      0.11       100
          75       0.53      0.57      0.55       100
          77       0.22      0.22      0.22       100
          81       0.26      0.23      0.24       100
          86       0.37      0.22      0.28       100
          88       0.24      0.16      0.19       100
          89       0.26      0.38      0.31       100
          90       0.31      0.27      0.29       100
          93       0.33      0.28      0.30       100
          94       0.62      0.45      0.52       100

    accuracy                           0.33      4000
   macro avg       0.34      0.33      0.33      4000
weighted avg       0.34      0.33      0.33      4000

No_Competition_DC_1
VAL: 
[(0.02, 0.1165862237215042), (0.3025, 0.08263768512010575), (0.369, 0.07232779136300087), (0.39, 0.06915001139044762), (0.3905, 0.06901451271772385), (0.4095, 0.07061366850137711), (0.397, 0.07347020149230957), (0.4, 0.07606485348939895), (0.399, 0.08308847931027412), (0.4035, 0.08145112743973731), (0.391, 0.0914055604338646), (0.3915, 0.0932833259999752), (0.3875, 0.10062810672819614), (0.389, 0.11054905226826668), (0.384, 0.1188707676306367), (0.384, 0.12890538302063942), (0.3805, 0.13313590133190156), (0.3705, 0.14079017996788024), (0.37, 0.151369949311018), (0.37, 0.1497211807370186), (0.3665, 0.16077689519524574), (0.382, 0.15155359968543053), (0.371, 0.16576803463697434), (0.376, 0.16201920318603516), (0.3715, 0.16355009591579436), (0.3795, 0.16634490495920182), (0.3775, 0.17977765822410582), (0.369, 0.1896104857325554), (0.3755, 0.18284440064430238), (0.368, 0.19175962609052657), (0.3655, 0.18464850968122482), (0.3645, 0.18488990062475205), (0.3725, 0.1945259848833084), (0.3925, 0.19790161269903184), (0.363, 0.19827781897783278), (0.3825, 0.18770680803060533), (0.382, 0.2083489436507225), (0.378, 0.21577253067493438), (0.371, 0.20108537214994432), (0.3785, 0.19682964128255845), (0.381, 0.19942360486090183), (0.371, 0.22529547756910323), (0.3765, 0.22537450504302978), (0.3765, 0.21037085275352002), (0.3865, 0.22022816866636277), (0.3805, 0.22395257449150086), (0.3885, 0.214585619866848), (0.3915, 0.21579551577568054), (0.391, 0.2222726398706436), (0.3695, 0.22556790149211883), (0.3745, 0.22527778780460359)]
TEST: 
[(0.02475, 0.11565362548828124), (0.29325, 0.08324841576814651), (0.35875, 0.07381985926628112), (0.384, 0.07028335648775101), (0.3965, 0.06993695843219758), (0.39875, 0.07230319839715958), (0.39675, 0.07460544234514237), (0.39875, 0.0765686275959015), (0.38825, 0.0835183624625206), (0.39925, 0.08275532475113868), (0.392, 0.09148632866144181), (0.39575, 0.09332050353288651), (0.378, 0.1003938881456852), (0.381, 0.1101474098265171), (0.37625, 0.11954095834493637), (0.372, 0.13014627987146377), (0.366, 0.1339709123969078), (0.37575, 0.1388040474653244), (0.36875, 0.15036960971355437), (0.35725, 0.14937601971626283), (0.3705, 0.1604320845603943), (0.36875, 0.15329106050729752), (0.3575, 0.16453236830234527), (0.3595, 0.1572166475057602), (0.361, 0.1609286197423935), (0.38, 0.16497353202104567), (0.36425, 0.17772815328836442), (0.3535, 0.18619104373455048), (0.36725, 0.18177039486169816), (0.36275, 0.185376609146595), (0.371, 0.1790946586728096), (0.36825, 0.18083204990625382), (0.3675, 0.18988589131832123), (0.37475, 0.1902825226187706), (0.35775, 0.19563164526224136), (0.36325, 0.187101591527462), (0.3705, 0.20281757938861847), (0.36625, 0.21230440229177475), (0.368, 0.20047666150331497), (0.369, 0.19223953300714494), (0.375, 0.20083325231075286), (0.36975, 0.2170177277326584), (0.3655, 0.22261634302139283), (0.36925, 0.20960629999637603), (0.3775, 0.21748149800300598), (0.37, 0.22279437172412872), (0.36175, 0.2127516417503357), (0.3655, 0.21016435700654984), (0.365, 0.21907142442464828), (0.3555, 0.21950062108039856), (0.36825, 0.2215177645087242)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.22      0.24      0.23       100
           4       0.15      0.17      0.16       100
           5       0.28      0.36      0.32       100
           7       0.30      0.34      0.32       100
           9       0.42      0.50      0.46       100
          11       0.22      0.20      0.21       100
          12       0.49      0.23      0.31       100
          13       0.23      0.29      0.25       100
          14       0.24      0.33      0.28       100
          17       0.46      0.55      0.50       100
          22       0.36      0.24      0.29       100
          27       0.35      0.18      0.24       100
          28       0.56      0.47      0.51       100
          29       0.40      0.26      0.32       100
          31       0.33      0.29      0.31       100
          33       0.39      0.34      0.36       100
          35       0.23      0.21      0.22       100
          37       0.25      0.21      0.23       100
          43       0.26      0.40      0.32       100
          44       0.27      0.17      0.21       100
          45       0.19      0.31      0.23       100
          51       0.27      0.34      0.30       100
          54       0.48      0.63      0.55       100
          58       0.37      0.33      0.35       100
          59       0.55      0.42      0.48       100
          61       0.62      0.53      0.57       100
          63       0.33      0.40      0.36       100
          64       0.24      0.19      0.21       100
          65       0.18      0.15      0.16       100
          68       0.68      0.65      0.67       100
          69       0.59      0.60      0.59       100
          71       0.66      0.73      0.69       100
          75       0.73      0.54      0.62       100
          77       0.25      0.16      0.20       100
          79       0.33      0.38      0.35       100
          81       0.23      0.30      0.26       100
          85       0.49      0.45      0.47       100
          86       0.49      0.33      0.39       100
          94       0.62      0.61      0.62       100
          95       0.44      0.70      0.54       100

    accuracy                           0.37      4000
   macro avg       0.38      0.37      0.37      4000
weighted avg       0.38      0.37      0.37      4000

No_Competition_DC_2
VAL: 
[(0.028, 0.11644194757938385), (0.3135, 0.08083096405863761), (0.355, 0.07404920035600662), (0.3775, 0.07268938685953617), (0.381, 0.07392782309651375), (0.3785, 0.07536883401870728), (0.361, 0.08115288917720317), (0.364, 0.0852726878374815), (0.365, 0.08915887996554375), (0.358, 0.09701336987316608), (0.369, 0.10001455986499787), (0.356, 0.10357287529110909), (0.371, 0.10945922313630581), (0.366, 0.1233040382862091), (0.3725, 0.12873335829377175), (0.37, 0.1429154340028763), (0.3635, 0.1551351354420185), (0.355, 0.16218094485998152), (0.355, 0.1603279784321785), (0.3485, 0.16116933432221411), (0.343, 0.17171928587555885), (0.3525, 0.16676142251491546), (0.361, 0.17564708679914476), (0.3555, 0.1791620932519436), (0.3545, 0.1900620121359825), (0.3485, 0.18803459233045577), (0.3465, 0.20058782160282135), (0.3355, 0.20933255806565285), (0.3565, 0.20243065044283867), (0.35, 0.19914609077572823), (0.336, 0.21111237847805023), (0.3445, 0.20920118141174315), (0.3385, 0.2165579120963812), (0.3395, 0.21588270235061646), (0.346, 0.22308022505044936), (0.3465, 0.22154407930374145), (0.3435, 0.23334290581941605), (0.34, 0.22529237282276154), (0.342, 0.22540062853693962), (0.35, 0.21813612219691275), (0.3415, 0.23743736231327056), (0.3515, 0.23223958081007004), (0.334, 0.25552296340465547), (0.3395, 0.23428134095668793), (0.339, 0.2287716672718525), (0.341, 0.23761762002110481), (0.354, 0.23728936156630517), (0.334, 0.25325304189324377), (0.3475, 0.25117430490255355), (0.3455, 0.24926526981592179), (0.3395, 0.26135204049944877)]
TEST: 
[(0.02375, 0.11551799666881561), (0.31275, 0.08112451237440109), (0.3485, 0.07381684204936027), (0.36375, 0.07261217573285103), (0.37725, 0.0723487794995308), (0.3785, 0.07344979619979858), (0.37275, 0.0776224840581417), (0.37025, 0.08170472460985184), (0.3745, 0.0846015340089798), (0.36, 0.0923721039891243), (0.37025, 0.09706917187571526), (0.3655, 0.09928986176848412), (0.3815, 0.10366574588418007), (0.36925, 0.11703670689463616), (0.3705, 0.12544475397467614), (0.35825, 0.13791544693708419), (0.35825, 0.15182167899608612), (0.35325, 0.15528477436304092), (0.345, 0.15820384883880614), (0.34275, 0.15975275719165802), (0.33925, 0.17149215918779373), (0.36525, 0.16132321614027023), (0.3545, 0.16926653730869293), (0.3555, 0.17366142004728316), (0.35875, 0.18271917909383772), (0.35375, 0.17615917801856995), (0.3525, 0.18997651529312135), (0.346, 0.19933298832178115), (0.35475, 0.19401121878623961), (0.3665, 0.19841243302822112), (0.33925, 0.20479838621616364), (0.358, 0.20490496289730073), (0.34275, 0.21013365936279296), (0.3595, 0.20928204184770585), (0.34775, 0.21670646792650222), (0.36475, 0.20531165289878844), (0.3495, 0.2248071916103363), (0.357, 0.21404338586330413), (0.3515, 0.21568662267923355), (0.36025, 0.20684361410140992), (0.34875, 0.22986477881669998), (0.347, 0.22783498638868332), (0.34725, 0.24687076103687286), (0.35725, 0.22456254136562348), (0.358, 0.21577811992168427), (0.35425, 0.2287355068922043), (0.35825, 0.22355405765771866), (0.3515, 0.23673321223258972), (0.35, 0.2511124391555786), (0.34675, 0.23732384037971496), (0.34125, 0.2477474719285965)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.25      0.13      0.17       100
           4       0.22      0.21      0.21       100
           6       0.33      0.23      0.27       100
          11       0.29      0.19      0.23       100
          13       0.32      0.25      0.28       100
          14       0.22      0.27      0.24       100
          17       0.49      0.57      0.53       100
          18       0.21      0.28      0.24       100
          20       0.70      0.56      0.62       100
          23       0.47      0.52      0.50       100
          27       0.19      0.18      0.18       100
          28       0.44      0.49      0.46       100
          29       0.43      0.32      0.37       100
          31       0.36      0.30      0.33       100
          34       0.24      0.17      0.20       100
          35       0.23      0.27      0.25       100
          42       0.26      0.31      0.28       100
          49       0.39      0.38      0.38       100
          50       0.18      0.24      0.20       100
          52       0.61      0.81      0.70       100
          53       0.63      0.53      0.58       100
          54       0.23      0.48      0.31       100
          56       0.58      0.49      0.53       100
          57       0.41      0.24      0.30       100
          62       0.51      0.38      0.43       100
          64       0.16      0.19      0.18       100
          69       0.75      0.59      0.66       100
          70       0.37      0.07      0.12       100
          71       0.57      0.63      0.60       100
          74       0.12      0.20      0.15       100
          75       0.51      0.54      0.52       100
          77       0.26      0.19      0.22       100
          80       0.09      0.13      0.11       100
          81       0.31      0.33      0.32       100
          84       0.20      0.20      0.20       100
          86       0.35      0.43      0.39       100
          92       0.15      0.08      0.11       100
          94       0.56      0.63      0.59       100
          97       0.25      0.33      0.29       100
          99       0.31      0.31      0.31       100

    accuracy                           0.34      4000
   macro avg       0.35      0.34      0.34      4000
weighted avg       0.35      0.34      0.34      4000

Competition
DC 0, val_set_size=2000, COIs=[36, 88, 66, 39, 72, 46, 0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60, 8, 32, 90, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([36, 88, 66, 39, 72, 46,  0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60,  8,
        32, 90, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.022, 0.11633801007270814)
DC 1, val_set_size=2000, COIs=[43, 63, 33, 85, 5, 79, 51, 58, 12, 45, 9, 59, 37, 68, 65, 44, 61, 22, 7, 95, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([43, 63, 33, 85,  5, 79, 51, 58, 12, 45,  9, 59, 37, 68, 65, 44, 61, 22,
         7, 95, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.02, 0.1165862237215042)
DC 2, val_set_size=2000, COIs=[42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53, 6, 52, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53,
         6, 52, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.028, 0.11644194757938385)
D00: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D01: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D02: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D03: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D04: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D05: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D06: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D07: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D08: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D09: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D010: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D011: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D012: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D013: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D014: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D015: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D016: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D017: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D018: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D019: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D020: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D021: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D022: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D023: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO0', '(DO4']
DC 2 --> ['(DO1', '(DO5']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.178, 0.14167041933536528) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.216, 0.131617878139019) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.215, 0.1353839923515916) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2185, 0.17904610047489405) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2515, 0.17234433287382125) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2295, 0.18046997118741273) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2175, 0.22224480159580706) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.2139969868659973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.24, 0.22855863643437624) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2185, 0.2763577702790499) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.27415544191002844) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.2871927115097642) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2215, 0.3246790985241532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.33103532415628434) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2535, 0.32713108602166174) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO5']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO2', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.225, 0.33554251869022844) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.32986101141572) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2465, 0.3523557677194476) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.225, 0.34415052008628844) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.272, 0.3297788757383823) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.34855524507164953) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.219, 0.3448695615082979) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.34718808525800704) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2535, 0.35116689436137677) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2185, 0.3630648727118969) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.3504143563210964) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.385827423453331) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2135, 0.3790935801714659) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.35897237855196) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.4027065042592585) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO3', '(DO2']
DC 1 --> ['(DO4', '(DO5']
DC 2 --> ['(DO1', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.21, 0.4052915794402361) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2615, 0.373209490776062) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.38501417545974254) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.206, 0.4146924078809097) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.3737928787767887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.248, 0.38846931974589827) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2135, 0.4236398945748806) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.261, 0.374897216796875) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.3736820519268513) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.206, 0.4084960426092148) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2595, 0.3768949753046036) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.3934561689868569) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2155, 0.40752476231753826) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.255, 0.3871669113636017) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2335, 0.41724637284874916) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO0', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.204, 0.4385231821537018) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.37207087308168413) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.244, 0.38403149823099375) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.199, 0.40086317363381385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.3947452377080917) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.237, 0.3880714270323515) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2065, 0.37921748872846367) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.248, 0.37750227785110474) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2325, 0.3709920288622379) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.199, 0.3999499678388238) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.254, 0.3274382864832878) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.242, 0.35949536761641504) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2105, 0.3659510035216808) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.259, 0.3360710053443909) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2475, 0.3408808154761791) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO0', '(DO5']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO3', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2035, 0.3621420662254095) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2555, 0.3219350591301918) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.24, 0.35325312239676715) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2, 0.3707233045101166) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2555, 0.3238493933081627) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2355, 0.36290241877734664) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.206, 0.3485159102529287) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2495, 0.304456322491169) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.232, 0.35471196338534355) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.203, 0.3609523261785507) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.257, 0.3212920011281967) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2325, 0.367291718930006) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.204, 0.323904603600502) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.29576965487003326) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.3447221776545048) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO2', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2085, 0.34914124849438666) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.261, 0.30293127632141115) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.223, 0.34077364641427993) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.197, 0.34615879122912885) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2475, 0.30686735999584197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2435, 0.3192607790827751) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2025, 0.32251325775682926) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2585, 0.3013542751669884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2415, 0.3531566655635834) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.207, 0.32740609446167945) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.264, 0.30033884036540986) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2405, 0.32483594642579555) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.205, 0.31465539743751286) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.30319367635250094) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.232, 0.3206250577569008) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO0', '(DO1']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO4', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.208, 0.33363812245428565) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2545, 0.30385185277462007) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2395, 0.30411175978183747) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.1995, 0.32183733811974524) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.3015179949998856) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2335, 0.2998321137726307) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.215, 0.30731547737121584) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2565, 0.28938736581802366) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.232, 0.29205700784921645) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2, 0.33246833074092863) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2515, 0.30866591638326646) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2435, 0.3130331163704395) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2065, 0.304178909689188) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2615, 0.29329773169755935) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2375, 0.2821173602938652) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO0', '(DO3']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO1', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2115, 0.301370138078928) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2555, 0.2719566941857338) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.243, 0.2916534632742405) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2065, 0.31872499239444735) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.264, 0.2837249037027359) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.242, 0.2892443682551384) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.212, 0.3337700076401234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.2815970579981804) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2365, 0.3234429350346327) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2045, 0.32141040252149106) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2555, 0.29920338588953016) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.2868405274748802) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.208, 0.2954166921377182) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.2966598733663559) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.24, 0.2774394109547138) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO5', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.216, 0.30967940723896026) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.2686279039978981) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.2816559820771217) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.212, 0.2881789635568857) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.258, 0.27325297963619233) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.235, 0.281713959351182) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.21, 0.2849842507839203) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2575, 0.29579900699853895) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.232, 0.25335521472245454) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.201, 0.2949810633957386) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.28307582396268843) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2445, 0.2575795465409756) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2125, 0.2807970938384533) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.272, 0.26568223267793656) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2415, 0.2684158070385456) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO1', '(DO0']
DC 2 --> ['(DO3', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2135, 0.29517816722393037) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.272, 0.2791578451395035) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.246, 0.27723923318088056) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2055, 0.27803796499967576) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.27702421456575393) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.237, 0.28223915430903435) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.216, 0.2852920293211937) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2645, 0.27671587592363356) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.2550280845761299) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2085, 0.2954007326811552) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.2591751101016998) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.246, 0.27832840704917905) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.213, 0.28837843929231166) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.276, 0.2618619744181633) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2345, 0.2837882095873356) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.022, 0.11633801007270814), (0.178, 0.14167041933536528), (0.2185, 0.17904610047489405), (0.2175, 0.22224480159580706), (0.2185, 0.2763577702790499), (0.2215, 0.3246790985241532), (0.225, 0.33554251869022844), (0.225, 0.34415052008628844), (0.219, 0.3448695615082979), (0.2185, 0.3630648727118969), (0.2135, 0.3790935801714659), (0.21, 0.4052915794402361), (0.206, 0.4146924078809097), (0.2135, 0.4236398945748806), (0.206, 0.4084960426092148), (0.2155, 0.40752476231753826), (0.204, 0.4385231821537018), (0.199, 0.40086317363381385), (0.2065, 0.37921748872846367), (0.199, 0.3999499678388238), (0.2105, 0.3659510035216808), (0.2035, 0.3621420662254095), (0.2, 0.3707233045101166), (0.206, 0.3485159102529287), (0.203, 0.3609523261785507), (0.204, 0.323904603600502), (0.2085, 0.34914124849438666), (0.197, 0.34615879122912885), (0.2025, 0.32251325775682926), (0.207, 0.32740609446167945), (0.205, 0.31465539743751286), (0.208, 0.33363812245428565), (0.1995, 0.32183733811974524), (0.215, 0.30731547737121584), (0.2, 0.33246833074092863), (0.2065, 0.304178909689188), (0.2115, 0.301370138078928), (0.2065, 0.31872499239444735), (0.212, 0.3337700076401234), (0.2045, 0.32141040252149106), (0.208, 0.2954166921377182), (0.216, 0.30967940723896026), (0.212, 0.2881789635568857), (0.21, 0.2849842507839203), (0.201, 0.2949810633957386), (0.2125, 0.2807970938384533), (0.2135, 0.29517816722393037), (0.2055, 0.27803796499967576), (0.216, 0.2852920293211937), (0.2085, 0.2954007326811552), (0.213, 0.28837843929231166)]
TEST: 
[(0.0245, 0.11540564060211182), (0.1815, 0.1396234229207039), (0.2175, 0.17656735169887544), (0.2195, 0.2186760092973709), (0.2195, 0.2721193715333939), (0.2265, 0.32007163381576537), (0.23275, 0.3315294510126114), (0.22275, 0.33912570917606355), (0.2265, 0.33878236389160155), (0.22875, 0.3586651520729065), (0.218, 0.37370423793792723), (0.21925, 0.40218432557582856), (0.2195, 0.41117062771320345), (0.21325, 0.421357248544693), (0.21525, 0.405895348906517), (0.2185, 0.4048578855991364), (0.2135, 0.43701618134975434), (0.21175, 0.3944612227678299), (0.22075, 0.37711372554302214), (0.2185, 0.39545514464378356), (0.21975, 0.3637583780288696), (0.209, 0.35899011373519896), (0.20225, 0.3704098882675171), (0.2165, 0.3460981683731079), (0.2095, 0.3611894236803055), (0.20875, 0.32233925902843474), (0.21825, 0.3457752501964569), (0.20375, 0.34469987559318543), (0.21325, 0.3188485670089722), (0.21725, 0.3194477297067642), (0.21375, 0.30911507856845855), (0.2065, 0.33124963319301604), (0.2145, 0.3183950068950653), (0.216, 0.3140142661333084), (0.21575, 0.32876643896102903), (0.22025, 0.3044747096300125), (0.22025, 0.29962798714637756), (0.2185, 0.315455694437027), (0.21175, 0.33324705958366396), (0.21875, 0.315468386054039), (0.21825, 0.290609903216362), (0.2275, 0.3064835489988327), (0.219, 0.28589745712280273), (0.22675, 0.2817453901767731), (0.218, 0.2956916055679321), (0.22025, 0.2832424682378769), (0.22175, 0.2966720895767212), (0.221, 0.2778828911781311), (0.2255, 0.2812525174617767), (0.2195, 0.2939438323974609), (0.22925, 0.2837372031211853)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.56      0.77      0.65       100
           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           8       0.48      0.44      0.46       100
          11       0.00      0.00      0.00       100
          13       0.00      0.00      0.00       100
          14       0.00      0.00      0.00       100
          16       0.16      0.51      0.25       100
          17       0.60      0.06      0.11       100
          19       0.19      0.33      0.24       100
          25       0.19      0.40      0.25       100
          26       0.25      0.42      0.32       100
          27       0.00      0.00      0.00       100
          28       0.71      0.05      0.09       100
          29       0.00      0.00      0.00       100
          31       0.60      0.03      0.06       100
          32       0.18      0.29      0.22       100
          35       0.00      0.00      0.00       100
          36       0.28      0.37      0.32       100
          38       0.19      0.43      0.27       100
          39       0.30      0.57      0.39       100
          46       0.17      0.34      0.22       100
          54       0.60      0.03      0.06       100
          55       0.12      0.22      0.15       100
          60       0.50      0.89      0.64       100
          64       0.00      0.00      0.00       100
          66       0.15      0.37      0.22       100
          67       0.35      0.39      0.37       100
          69       0.50      0.01      0.02       100
          71       0.79      0.15      0.25       100
          72       0.07      0.18      0.10       100
          75       1.00      0.11      0.20       100
          77       0.00      0.00      0.00       100
          81       0.00      0.00      0.00       100
          86       0.40      0.02      0.04       100
          88       0.16      0.43      0.23       100
          89       0.24      0.52      0.33       100
          90       0.16      0.47      0.24       100
          93       0.18      0.30      0.23       100
          94       0.78      0.07      0.13       100

    accuracy                           0.23      4000
   macro avg       0.27      0.23      0.18      4000
weighted avg       0.27      0.23      0.18      4000

Competition_DC_1
VAL: 
[(0.02, 0.1165862237215042), (0.216, 0.131617878139019), (0.2515, 0.17234433287382125), (0.2665, 0.2139969868659973), (0.266, 0.27415544191002844), (0.2655, 0.33103532415628434), (0.267, 0.32986101141572), (0.272, 0.3297788757383823), (0.2635, 0.34718808525800704), (0.266, 0.3504143563210964), (0.263, 0.35897237855196), (0.2615, 0.373209490776062), (0.273, 0.3737928787767887), (0.261, 0.374897216796875), (0.2595, 0.3768949753046036), (0.255, 0.3871669113636017), (0.2655, 0.37207087308168413), (0.252, 0.3947452377080917), (0.248, 0.37750227785110474), (0.254, 0.3274382864832878), (0.259, 0.3360710053443909), (0.2555, 0.3219350591301918), (0.2555, 0.3238493933081627), (0.2495, 0.304456322491169), (0.257, 0.3212920011281967), (0.265, 0.29576965487003326), (0.261, 0.30293127632141115), (0.2475, 0.30686735999584197), (0.2585, 0.3013542751669884), (0.264, 0.30033884036540986), (0.263, 0.30319367635250094), (0.2545, 0.30385185277462007), (0.25, 0.3015179949998856), (0.2565, 0.28938736581802366), (0.2515, 0.30866591638326646), (0.2615, 0.29329773169755935), (0.2555, 0.2719566941857338), (0.264, 0.2837249037027359), (0.267, 0.2815970579981804), (0.2555, 0.29920338588953016), (0.2665, 0.2966598733663559), (0.266, 0.2686279039978981), (0.258, 0.27325297963619233), (0.2575, 0.29579900699853895), (0.267, 0.28307582396268843), (0.272, 0.26568223267793656), (0.272, 0.2791578451395035), (0.2675, 0.27702421456575393), (0.2645, 0.27671587592363356), (0.2635, 0.2591751101016998), (0.276, 0.2618619744181633)]
TEST: 
[(0.02475, 0.11565362548828124), (0.21775, 0.13079576390981673), (0.24875, 0.1702621773481369), (0.25775, 0.21204714322090149), (0.263, 0.27117718088626863), (0.26325, 0.33012304520606994), (0.26925, 0.32798559737205507), (0.2685, 0.32581017208099367), (0.2685, 0.34252636206150056), (0.2605, 0.3479382506608963), (0.25575, 0.36101858496665956), (0.2645, 0.3739547880887985), (0.2605, 0.37711544036865235), (0.25525, 0.37912246966362), (0.2575, 0.37916952252388003), (0.257, 0.3914715480804443), (0.25675, 0.3747736823558807), (0.252, 0.3932961478233337), (0.2565, 0.37441611433029176), (0.25575, 0.3258120361566544), (0.254, 0.3364446555376053), (0.25325, 0.3220768400430679), (0.2485, 0.32260868084430694), (0.248, 0.30285119616985323), (0.25275, 0.3197645958662033), (0.25525, 0.2992999223470688), (0.24675, 0.3054977436065674), (0.24725, 0.3076485335826874), (0.24325, 0.30244244313240054), (0.251, 0.30147880244255065), (0.25275, 0.306044775724411), (0.25325, 0.305929102063179), (0.24925, 0.300288123011589), (0.25425, 0.29104750549793246), (0.2505, 0.31126913583278654), (0.253, 0.30298602175712586), (0.25175, 0.27674017691612246), (0.25325, 0.2912091642618179), (0.2495, 0.291678337931633), (0.24525, 0.30284741199016574), (0.25, 0.304117785692215), (0.24925, 0.2725865329504013), (0.25125, 0.28000941216945646), (0.24875, 0.29780278956890105), (0.2565, 0.2921055634021759), (0.2595, 0.2742701094150543), (0.25675, 0.2872504975795746), (0.2545, 0.28679174888134), (0.25675, 0.28394896066188813), (0.24425, 0.2715322860479355), (0.25325, 0.2709452234506607)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       0.18      0.43      0.25       100
           7       0.25      0.48      0.33       100
           9       0.25      0.61      0.36       100
          11       0.00      0.00      0.00       100
          12       0.31      0.41      0.35       100
          13       0.50      0.02      0.04       100
          14       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          22       0.17      0.33      0.22       100
          27       0.17      0.01      0.02       100
          28       0.80      0.08      0.15       100
          29       0.60      0.09      0.16       100
          31       0.00      0.00      0.00       100
          33       0.28      0.49      0.35       100
          35       0.00      0.00      0.00       100
          37       0.15      0.43      0.22       100
          43       0.25      0.45      0.32       100
          44       0.16      0.34      0.22       100
          45       0.15      0.37      0.22       100
          51       0.18      0.47      0.26       100
          54       0.62      0.05      0.09       100
          58       0.28      0.48      0.36       100
          59       0.37      0.60      0.46       100
          61       0.55      0.46      0.50       100
          63       0.24      0.60      0.34       100
          64       0.00      0.00      0.00       100
          65       0.12      0.25      0.16       100
          68       0.51      0.71      0.60       100
          69       0.88      0.07      0.13       100
          71       0.91      0.10      0.18       100
          75       1.00      0.02      0.04       100
          77       0.00      0.00      0.00       100
          79       0.25      0.37      0.30       100
          81       0.00      0.00      0.00       100
          85       0.27      0.57      0.36       100
          86       0.11      0.01      0.02       100
          94       0.88      0.07      0.13       100
          95       0.45      0.76      0.56       100

    accuracy                           0.25      4000
   macro avg       0.30      0.25      0.19      4000
weighted avg       0.30      0.25      0.19      4000

Competition_DC_2
VAL: 
[(0.028, 0.11644194757938385), (0.215, 0.1353839923515916), (0.2295, 0.18046997118741273), (0.24, 0.22855863643437624), (0.238, 0.2871927115097642), (0.2535, 0.32713108602166174), (0.2465, 0.3523557677194476), (0.245, 0.34855524507164953), (0.2535, 0.35116689436137677), (0.2485, 0.385827423453331), (0.2485, 0.4027065042592585), (0.2485, 0.38501417545974254), (0.248, 0.38846931974589827), (0.249, 0.3736820519268513), (0.249, 0.3934561689868569), (0.2335, 0.41724637284874916), (0.244, 0.38403149823099375), (0.237, 0.3880714270323515), (0.2325, 0.3709920288622379), (0.242, 0.35949536761641504), (0.2475, 0.3408808154761791), (0.24, 0.35325312239676715), (0.2355, 0.36290241877734664), (0.232, 0.35471196338534355), (0.2325, 0.367291718930006), (0.245, 0.3447221776545048), (0.223, 0.34077364641427993), (0.2435, 0.3192607790827751), (0.2415, 0.3531566655635834), (0.2405, 0.32483594642579555), (0.232, 0.3206250577569008), (0.2395, 0.30411175978183747), (0.2335, 0.2998321137726307), (0.232, 0.29205700784921645), (0.2435, 0.3130331163704395), (0.2375, 0.2821173602938652), (0.243, 0.2916534632742405), (0.242, 0.2892443682551384), (0.2365, 0.3234429350346327), (0.239, 0.2868405274748802), (0.24, 0.2774394109547138), (0.239, 0.2816559820771217), (0.235, 0.281713959351182), (0.232, 0.25335521472245454), (0.2445, 0.2575795465409756), (0.2415, 0.2684158070385456), (0.246, 0.27723923318088056), (0.237, 0.28223915430903435), (0.245, 0.2550280845761299), (0.246, 0.27832840704917905), (0.2345, 0.2837882095873356)]
TEST: 
[(0.02375, 0.11551799666881561), (0.2275, 0.13283702421188354), (0.235, 0.17738934606313705), (0.24925, 0.2239143295288086), (0.2435, 0.2828302236795425), (0.2555, 0.3206452746391296), (0.25575, 0.34585112857818606), (0.25625, 0.3392211526632309), (0.25375, 0.3416809334754944), (0.2565, 0.3721164914369583), (0.26025, 0.3873162908554077), (0.25325, 0.3812016735076904), (0.2525, 0.3831026386022568), (0.251, 0.3687514855861664), (0.2535, 0.38296131539344785), (0.237, 0.40877873265743253), (0.249, 0.3756589208841324), (0.2425, 0.3802999904155731), (0.23975, 0.36683156394958494), (0.245, 0.35573143458366396), (0.25025, 0.3398510854244232), (0.252, 0.3463054494857788), (0.24425, 0.35283150148391723), (0.2445, 0.34527148604393004), (0.23775, 0.3577385619878769), (0.244, 0.334172949552536), (0.23175, 0.3353356792926788), (0.24725, 0.312437847495079), (0.23175, 0.35151761531829834), (0.2405, 0.3207941423654556), (0.2445, 0.31312916707992555), (0.2445, 0.29781861996650694), (0.23775, 0.29409664928913115), (0.25025, 0.2903480870723724), (0.244, 0.309130170583725), (0.24225, 0.2797745524644852), (0.24775, 0.2903840419054031), (0.24475, 0.2853528389930725), (0.23625, 0.31777736711502075), (0.2345, 0.28598136591911316), (0.24225, 0.27307635939121244), (0.239, 0.27454206538200376), (0.2445, 0.28137297749519347), (0.24425, 0.2518204299211502), (0.24725, 0.2582460207939148), (0.24075, 0.2654484158754349), (0.24925, 0.2730026648044586), (0.24025, 0.2813707444667816), (0.24575, 0.25830209445953367), (0.241, 0.2787968201637268), (0.245, 0.2752104098796844)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           6       0.20      0.46      0.28       100
          11       1.00      0.01      0.02       100
          13       1.00      0.01      0.02       100
          14       0.00      0.00      0.00       100
          17       1.00      0.01      0.02       100
          18       0.21      0.43      0.28       100
          20       0.39      0.71      0.51       100
          23       0.32      0.65      0.42       100
          27       0.00      0.00      0.00       100
          28       0.00      0.00      0.00       100
          29       0.00      0.00      0.00       100
          31       1.00      0.01      0.02       100
          34       0.22      0.31      0.26       100
          35       0.00      0.00      0.00       100
          42       0.17      0.42      0.25       100
          49       0.26      0.72      0.39       100
          50       0.15      0.41      0.21       100
          52       0.53      0.87      0.66       100
          53       0.39      0.58      0.47       100
          54       0.33      0.07      0.12       100
          56       0.37      0.58      0.45       100
          57       0.29      0.35      0.32       100
          62       0.42      0.41      0.41       100
          64       0.00      0.00      0.00       100
          69       1.00      0.01      0.02       100
          70       0.26      0.58      0.36       100
          71       0.00      0.00      0.00       100
          74       0.13      0.40      0.19       100
          75       0.50      0.03      0.06       100
          77       0.25      0.01      0.02       100
          80       0.14      0.26      0.18       100
          81       0.00      0.00      0.00       100
          84       0.11      0.28      0.16       100
          86       0.60      0.09      0.16       100
          92       0.18      0.25      0.21       100
          94       0.67      0.02      0.04       100
          97       0.21      0.41      0.28       100
          99       0.24      0.45      0.31       100

    accuracy                           0.24      4000
   macro avg       0.31      0.25      0.18      4000
weighted avg       0.31      0.24      0.18      4000

Collaboration
DC 0, val_set_size=2000, COIs=[36, 88, 66, 39, 72, 46, 0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60, 8, 32, 90, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([36, 88, 66, 39, 72, 46,  0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60,  8,
        32, 90, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.022, 0.11633801007270814)
DC 1, val_set_size=2000, COIs=[43, 63, 33, 85, 5, 79, 51, 58, 12, 45, 9, 59, 37, 68, 65, 44, 61, 22, 7, 95, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([43, 63, 33, 85,  5, 79, 51, 58, 12, 45,  9, 59, 37, 68, 65, 44, 61, 22,
         7, 95, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.02, 0.1165862237215042)
DC 2, val_set_size=2000, COIs=[42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53, 6, 52, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53,
         6, 52, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.028, 0.11644194757938385)
D00: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D01: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D02: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D03: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D04: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D05: 1000 samples from classes {3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94}
D06: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D07: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D08: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D09: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D010: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D011: 1000 samples from classes {0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93}
D012: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D013: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D014: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D015: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D016: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D017: 1000 samples from classes {5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95}
D018: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D019: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D020: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D021: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D022: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
D023: 1000 samples from classes {6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO3', '(DO4']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.1905, 0.14149007076025008) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.212, 0.13205300417542457) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.209, 0.1364609098881483) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2095, 0.18161420324444771) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2535, 0.17321361842751504) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2365, 0.1790789821371436) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2145, 0.2415544970035553) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.21004452913999558) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.237, 0.23449578948318958) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.217, 0.30043666014075276) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.2625376741886139) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2525, 0.28072402109205724) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2125, 0.33209875025227664) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.3076570853590965) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.251, 0.30715609215199946) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO0', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2195, 0.34326846423000096) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.269, 0.33029839977622033) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.247, 0.32515423818677663) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2225, 0.3432372513562441) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2615, 0.3442123855352402) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.3228883135765791) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.219, 0.3787151660211384) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2725, 0.3729322661459446) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.241, 0.3504827939495444) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2125, 0.38274408941343424) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.3844915807247162) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.247, 0.3561491595208645) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.218, 0.39636849831789733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2585, 0.3982699514627457) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2415, 0.3767030710577965) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[3, 4, 11, 13, 14, 17, 27, 28, 29, 31, 35, 54, 64, 69, 71, 75, 77, 81, 86, 94], M=tensor([ 0,  3,  4,  5,  6,  7,  8,  9, 11, 12, 13, 14, 16, 17, 18, 19, 20, 22,
        23, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44,
        45, 46, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,
        65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 77, 79, 80, 81, 84, 85, 86, 88,
        89, 90, 92, 93, 94, 95, 97, 99], device='cuda:0'), Initial Performance: (0.3893333333333333, 0.0694677353700002)
DC Expert-0, val_set_size=1000, COIs=[0, 8, 16, 19, 25, 26, 32, 36, 38, 39, 46, 55, 60, 66, 67, 72, 88, 89, 90, 93], M=tensor([36, 88, 66, 39, 72, 46,  0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60,  8,
        32, 90, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.436, 0.08651654689013957)
DC Expert-1, val_set_size=1000, COIs=[5, 7, 9, 12, 22, 33, 37, 43, 44, 45, 51, 58, 59, 61, 63, 65, 68, 79, 85, 95], M=tensor([43, 63, 33, 85,  5, 79, 51, 58, 12, 45,  9, 59, 37, 68, 65, 44, 61, 22,
         7, 95, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.517, 0.07154768633842468)
DC Expert-2, val_set_size=1000, COIs=[6, 18, 20, 23, 34, 42, 49, 50, 52, 53, 56, 57, 62, 70, 74, 80, 84, 92, 97, 99], M=tensor([42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53,
         6, 52, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), Initial Performance: (0.483, 0.07751318569481373)
SUPER-DC 0, val_set_size=2000, COIs=[36, 88, 66, 39, 72, 46, 0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60, 8, 32, 90, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([36, 88, 66, 39, 72, 46,  0, 26, 67, 55, 93, 25, 38, 89, 19, 16, 60,  8,
        32, 90, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[43, 63, 33, 85, 5, 79, 51, 58, 12, 45, 9, 59, 37, 68, 65, 44, 61, 22, 7, 95, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([43, 63, 33, 85,  5, 79, 51, 58, 12, 45,  9, 59, 37, 68, 65, 44, 61, 22,
         7, 95, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53, 6, 52, 81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 27, 29, 64, 77, 71], M=tensor([42, 50, 18, 99, 20, 70, 49, 80, 62, 97, 34, 84, 92, 56, 23, 57, 74, 53,
         6, 52, 81, 14,  3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54,  4, 27,
        29, 64, 77, 71], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.431, 0.10103238731622696) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.51, 0.08881991851329804) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.48, 0.09726584485918284) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5026666666666667, 0.05300044466058413) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.190173991702497) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2815, 0.16267396432161332) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.266, 0.19103664089739322) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.404, 0.11781245732307434) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.503, 0.09225307559967041) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.476, 0.10811152891325765) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.516, 0.05285381866494815) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.23, 0.14682868561148643) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.286, 0.14127882117033005) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.16063986690342427) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.416, 0.12603448144346477) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.51, 0.10231318402290344) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.448, 0.11751629945635796) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.523, 0.057128637154897056) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.1342844313830137) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2845, 0.12459951287508012) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.12951863698661328) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.425, 0.13441366083174944) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.517, 0.10939027607440949) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.485, 0.11614185159467161) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5186666666666667, 0.05985346611340841) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.24, 0.13291461798548698) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2875, 0.12377936002612114) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.274, 0.12555659785866738) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.414, 0.12765001805126666) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.492, 0.1140420732498169) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.466, 0.11607822025776841) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.533, 0.06121168491244316) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2605, 0.11212395989894867) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.10712059414386749) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2705, 0.10979389710724353) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.421, 0.14224975489079952) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.49, 0.12190263819694519) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.487, 0.12457244061026722) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5156666666666667, 0.07114500322937965) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2515, 0.11993616034090519) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.10473853099346161) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.293, 0.10555366696417331) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.428, 0.15095779061317444) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.496, 0.12610386300086976) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.474, 0.14182531600142828) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.505, 0.07841675312320391) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.11489733385294676) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.322, 0.10318250846862793) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.10510789189487696) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.418, 0.1406791764497757) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.515, 0.11992530155181885) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.472, 0.1413967975899577) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5146666666666667, 0.08107470392684142) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.263, 0.10657374352216721) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3285, 0.10069846907258034) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3005, 0.10604175743460655) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.433, 0.16270706201833673) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.505, 0.13241321289539337) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.47, 0.13781761940388243) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5093333333333333, 0.09707642417152723) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2725, 0.11042602372914553) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.10703565818071366) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.325, 0.09850513643026351) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.417, 0.16060849076509476) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.515, 0.12254518556594848) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.476, 0.13249425356835126) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.51, 0.09975406650702158) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.277, 0.10695786494016647) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3195, 0.10165138208866119) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3165, 0.10097863206267357) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.393, 0.17666002783179283) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.511, 0.13006640088558197) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.466, 0.14588296072022058) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5073333333333333, 0.09911833253502846) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2735, 0.1107787826359272) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3275, 0.09893121999502182) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2995, 0.10381475630402565) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.391, 0.17981626494741068) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.523, 0.12768275082111358) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.453, 0.14462993936706334) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5026666666666667, 0.11191230028867721) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.285, 0.11257070634514094) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.344, 0.10159741735458375) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3205, 0.10495390576124192) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.394, 0.17638805793225765) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.5, 0.13963615703582763) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.463, 0.1560425576942507) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5216666666666666, 0.11146269891659419) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.11200946041941642) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3245, 0.10143313002586364) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.106815968118608) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.412, 0.17906494855880736) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.507, 0.14672431588172913) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.467, 0.14390380575414746) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.502, 0.11950995915134748) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2825, 0.1127729690670967) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3375, 0.10196152383089066) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3335, 0.10428468561172485) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.432, 0.17510423296689986) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.495, 0.14135438776016235) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.473, 0.1402277386034584) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5013333333333333, 0.13112888036171597) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.289, 0.11524123027920723) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3425, 0.1008057073354721) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.32, 0.10730593413114548) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.417, 0.1953427203297615) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.503, 0.1581965194940567) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.466, 0.1634553205394186) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.502, 0.12854783342281978) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.292, 0.11408769105374814) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.342, 0.10716683822870254) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3185, 0.11459826792776584) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.397, 0.17794862828403712) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.514, 0.15055763328075408) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.47, 0.15650218772684457) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.509, 0.1265457613269488) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.292, 0.11660309636592865) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.341, 0.1033069457411766) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.11811062896251678) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.398, 0.1888580546528101) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.498, 0.1630724343061447) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.464, 0.17451158403416775) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49966666666666665, 0.128809811770916) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2735, 0.12211757338047027) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3225, 0.10892629486322403) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3135, 0.11875262174755336) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.411, 0.18425202947854996) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.511, 0.15227186095714568) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.47, 0.17431105937343092) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5013333333333333, 0.12178248773018518) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2715, 0.119231619104743) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.324, 0.10683744835853577) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.12015401965379716) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.405, 0.19612836509943007) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.516, 0.15259295558929442) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.465, 0.16340290921932318) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.504, 0.12293227410316468) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.11854679140448571) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.346, 0.10471393221616745) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.298, 0.11760634329915047) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.409, 0.1773535610437393) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.524, 0.1580121182203293) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.487, 0.16463451472087764) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49666666666666665, 0.14069331393639248) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.287, 0.12321407222747803) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.344, 0.11057047948241233) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3155, 0.12499805966019631) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.418, 0.2015072636306286) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.506, 0.17052717554569244) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.479, 0.18487700365597265) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.499, 0.15096408816178639) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2755, 0.12369408702850342) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.326, 0.11496948260068894) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.31, 0.12325191646814346) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.404, 0.20668507964909077) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.508, 0.18001858162879944) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.462, 0.17147660621628166) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5053333333333333, 0.14939568704366685) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2725, 0.12639727520942687) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.329, 0.11853077870607376) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3155, 0.12158426415920258) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.416, 0.2106991891413927) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.508, 0.17788365697860717) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.17061562106199563) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.496, 0.14859634796778362) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2755, 0.12610672250390054) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3215, 0.12074288183450699) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2995, 0.12351377993822098) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.404, 0.2083480673879385) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.499, 0.17571690368652343) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.479, 0.17940772092377302) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5036666666666667, 0.1489250246286392) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2695, 0.12831672160327434) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.337, 0.12236994072794914) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.12563893914222718) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.405, 0.20802861411869525) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.517, 0.17849563121795653) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.479, 0.18281694549196073) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5143333333333333, 0.1429199071129163) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.273, 0.12584804056584836) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3245, 0.11762091261148452) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.301, 0.12312339693307876) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.41, 0.2119406428411603) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.506, 0.1784423222541809) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.482, 0.18265371684776618) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49066666666666664, 0.1529713334242503) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2635, 0.1307779133617878) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3385, 0.11589007484912872) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3035, 0.12308367931842804) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.413, 0.21117170703411103) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.509, 0.1765169562101364) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.1815351720196195) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.503, 0.13706467231114705) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.13375718614459037) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3115, 0.11999446576833725) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3215, 0.12174200209975243) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.397, 0.21219895765185356) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.501, 0.1995115158557892) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.479, 0.19191400647163392) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5046666666666667, 0.14754400499661763) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2565, 0.13439727652072905) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3115, 0.12294775652885437) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.292, 0.12629328006505966) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.409, 0.21438821505010128) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.505, 0.19275521755218505) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.471, 0.18588033720385283) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49133333333333334, 0.15657729880015056) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.271, 0.1307316162288189) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3145, 0.12518766111135482) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2965, 0.1290366687476635) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.387, 0.22324103185534477) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.514, 0.18597959542274475) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.469, 0.19854127371311187) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5036666666666667, 0.15908765165011088) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2715, 0.13558042000234127) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3305, 0.12331624984741212) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2955, 0.13546736317873) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.4, 0.24178199006617068) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.51, 0.19185833549499512) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.465, 0.1989678008556366) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5063333333333333, 0.16402460968494414) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2635, 0.1383591624237597) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3185, 0.12178641670942307) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.296, 0.13415510165691374) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.411, 0.2353435395359993) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.506, 0.17850455403327942) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.469, 0.23017084273695945) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5063333333333333, 0.1759736972252528) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2785, 0.13442054015398025) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3155, 0.1294027220606804) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.13594448962807656) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.403, 0.23503221243433653) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.482, 0.20127739238739015) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.474, 0.19287630504369735) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.511, 0.15725835595528284) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2785, 0.1383973661996424) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.315, 0.12576845210790635) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3065, 0.13804026597738267) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.414, 0.2424475507789757) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.495, 0.18557870841026305) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.475, 0.20193175607919692) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49966666666666665, 0.15059641218185424) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2545, 0.1495489345639944) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.32, 0.12386514782905579) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.28, 0.14052291917800902) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.395, 0.2422597664669156) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.505, 0.182265567779541) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.474, 0.18850023449957365) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5013333333333333, 0.1548968322475751) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.261, 0.1337265138030052) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3315, 0.12177175652980804) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2995, 0.13112272334098815) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.406, 0.23554983079433442) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.512, 0.19274133467674257) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.19492364257573883) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5073333333333333, 0.1643384196162224) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.272, 0.13513121205568313) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.325, 0.12837404751777648) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.307, 0.13172533804178238) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.419, 0.22350098318513484) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.513, 0.20036623239517212) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.474, 0.2176396329551922) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5003333333333333, 0.1551178389787674) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2695, 0.13547570422291755) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3145, 0.12838344728946685) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.298, 0.14104426258802413) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.416, 0.2541335580945015) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.527, 0.20858037889003753) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.488, 0.2200810564471176) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.489, 0.16954818244775136) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2565, 0.14493568670749665) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3155, 0.13290287894010544) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2775, 0.14044215911626814) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.413, 0.23660952194035054) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.519, 0.20382908725738524) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.472, 0.2033063270001726) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.497, 0.1909791765610377) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2675, 0.1460983499661088) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3065, 0.13687466526031494) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.282, 0.14503140079975127) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.022, 0.11633801007270814), (0.1905, 0.14149007076025008), (0.2095, 0.18161420324444771), (0.2145, 0.2415544970035553), (0.217, 0.30043666014075276), (0.2125, 0.33209875025227664), (0.2195, 0.34326846423000096), (0.2225, 0.3432372513562441), (0.219, 0.3787151660211384), (0.2125, 0.38274408941343424), (0.218, 0.39636849831789733), (0.22, 0.190173991702497), (0.23, 0.14682868561148643), (0.241, 0.1342844313830137), (0.24, 0.13291461798548698), (0.2605, 0.11212395989894867), (0.2515, 0.11993616034090519), (0.2555, 0.11489733385294676), (0.263, 0.10657374352216721), (0.2725, 0.11042602372914553), (0.277, 0.10695786494016647), (0.2735, 0.1107787826359272), (0.285, 0.11257070634514094), (0.269, 0.11200946041941642), (0.2825, 0.1127729690670967), (0.289, 0.11524123027920723), (0.292, 0.11408769105374814), (0.292, 0.11660309636592865), (0.2735, 0.12211757338047027), (0.2715, 0.119231619104743), (0.275, 0.11854679140448571), (0.287, 0.12321407222747803), (0.2755, 0.12369408702850342), (0.2725, 0.12639727520942687), (0.2755, 0.12610672250390054), (0.2695, 0.12831672160327434), (0.273, 0.12584804056584836), (0.2635, 0.1307779133617878), (0.2595, 0.13375718614459037), (0.2565, 0.13439727652072905), (0.271, 0.1307316162288189), (0.2715, 0.13558042000234127), (0.2635, 0.1383591624237597), (0.2785, 0.13442054015398025), (0.2785, 0.1383973661996424), (0.2545, 0.1495489345639944), (0.261, 0.1337265138030052), (0.272, 0.13513121205568313), (0.2695, 0.13547570422291755), (0.2565, 0.14493568670749665), (0.2675, 0.1460983499661088)]
TEST: 
[(0.0245, 0.11540564060211182), (0.187, 0.13972374469041823), (0.21575, 0.17863342678546906), (0.2225, 0.23658936655521393), (0.226, 0.29347269213199617), (0.22625, 0.32314475274085996), (0.2245, 0.3333467617034912), (0.2215, 0.3362674956321716), (0.22725, 0.36641203224658964), (0.22425, 0.36844646191596986), (0.221, 0.3860514167547226), (0.23425, 0.18598876923322677), (0.2385, 0.14311437296867371), (0.25125, 0.13178433454036712), (0.25025, 0.13399428904056548), (0.272, 0.11005247700214386), (0.2655, 0.11781572830677033), (0.2605, 0.11369225424528122), (0.28, 0.10672882473468781), (0.28125, 0.11039427328109741), (0.286, 0.10898299765586852), (0.28075, 0.11164343529939652), (0.29125, 0.11084067666530609), (0.28825, 0.11320258754491806), (0.29625, 0.11402350574731827), (0.291, 0.11509541898965836), (0.29625, 0.11396000772714615), (0.30575, 0.1162810366153717), (0.28475, 0.12152112001180648), (0.28225, 0.11997858560085296), (0.28525, 0.12108549875020981), (0.283, 0.12767701834440232), (0.28125, 0.1265857056379318), (0.28525, 0.12841866540908814), (0.28125, 0.12961248129606248), (0.282, 0.13138969069719314), (0.27225, 0.1316366201043129), (0.27475, 0.13359434604644777), (0.26575, 0.13813293027877807), (0.26375, 0.13720230388641358), (0.27825, 0.13399030733108522), (0.27575, 0.13747213542461395), (0.268, 0.14019371271133424), (0.285, 0.1374240466952324), (0.27075, 0.13838971692323684), (0.2675, 0.14851886093616484), (0.27275, 0.1362395522594452), (0.27075, 0.13859984427690505), (0.27025, 0.14183088892698287), (0.2615, 0.14701942896842957), (0.2685, 0.1477696487903595)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.59      0.59      0.59       100
           3       0.15      0.25      0.19       100
           4       0.12      0.13      0.12       100
           8       0.34      0.41      0.37       100
          11       0.12      0.07      0.09       100
          13       0.20      0.13      0.16       100
          14       0.19      0.24      0.21       100
          16       0.25      0.30      0.27       100
          17       0.33      0.25      0.28       100
          19       0.24      0.14      0.18       100
          25       0.19      0.33      0.24       100
          26       0.30      0.23      0.26       100
          27       0.13      0.34      0.19       100
          28       0.50      0.26      0.34       100
          29       0.35      0.23      0.28       100
          31       0.27      0.24      0.25       100
          32       0.30      0.18      0.22       100
          35       0.16      0.14      0.15       100
          36       0.29      0.33      0.31       100
          38       0.20      0.20      0.20       100
          39       0.38      0.38      0.38       100
          46       0.24      0.23      0.24       100
          54       0.41      0.40      0.41       100
          55       0.07      0.06      0.06       100
          60       0.44      0.87      0.59       100
          64       0.12      0.20      0.15       100
          66       0.14      0.05      0.07       100
          67       0.32      0.26      0.29       100
          69       0.64      0.48      0.55       100
          71       0.24      0.09      0.13       100
          72       0.09      0.06      0.07       100
          75       0.45      0.57      0.50       100
          77       0.20      0.12      0.15       100
          81       0.18      0.18      0.18       100
          86       0.32      0.39      0.35       100
          88       0.21      0.25      0.23       100
          89       0.45      0.25      0.32       100
          90       0.17      0.27      0.21       100
          93       0.25      0.15      0.19       100
          94       0.63      0.49      0.55       100

    accuracy                           0.27      4000
   macro avg       0.28      0.27      0.26      4000
weighted avg       0.28      0.27      0.26      4000

Collaboration_DC_1
VAL: 
[(0.02, 0.1165862237215042), (0.212, 0.13205300417542457), (0.2535, 0.17321361842751504), (0.266, 0.21004452913999558), (0.266, 0.2625376741886139), (0.2685, 0.3076570853590965), (0.269, 0.33029839977622033), (0.2615, 0.3442123855352402), (0.2725, 0.3729322661459446), (0.2685, 0.3844915807247162), (0.2585, 0.3982699514627457), (0.2815, 0.16267396432161332), (0.286, 0.14127882117033005), (0.2845, 0.12459951287508012), (0.2875, 0.12377936002612114), (0.305, 0.10712059414386749), (0.304, 0.10473853099346161), (0.322, 0.10318250846862793), (0.3285, 0.10069846907258034), (0.3165, 0.10703565818071366), (0.3195, 0.10165138208866119), (0.3275, 0.09893121999502182), (0.344, 0.10159741735458375), (0.3245, 0.10143313002586364), (0.3375, 0.10196152383089066), (0.3425, 0.1008057073354721), (0.342, 0.10716683822870254), (0.341, 0.1033069457411766), (0.3225, 0.10892629486322403), (0.324, 0.10683744835853577), (0.346, 0.10471393221616745), (0.344, 0.11057047948241233), (0.326, 0.11496948260068894), (0.329, 0.11853077870607376), (0.3215, 0.12074288183450699), (0.337, 0.12236994072794914), (0.3245, 0.11762091261148452), (0.3385, 0.11589007484912872), (0.3115, 0.11999446576833725), (0.3115, 0.12294775652885437), (0.3145, 0.12518766111135482), (0.3305, 0.12331624984741212), (0.3185, 0.12178641670942307), (0.3155, 0.1294027220606804), (0.315, 0.12576845210790635), (0.32, 0.12386514782905579), (0.3315, 0.12177175652980804), (0.325, 0.12837404751777648), (0.3145, 0.12838344728946685), (0.3155, 0.13290287894010544), (0.3065, 0.13687466526031494)]
TEST: 
[(0.02475, 0.11565362548828124), (0.22425, 0.1314951414465904), (0.2535, 0.1725775439143181), (0.267, 0.20803201568126678), (0.269, 0.26035673296451567), (0.272, 0.3049136437177658), (0.27, 0.3270742623806), (0.263, 0.3403617917299271), (0.26675, 0.3641204352378845), (0.2635, 0.37643841600418093), (0.2635, 0.3904705801010132), (0.289, 0.15818609553575516), (0.2865, 0.1415276694893837), (0.29025, 0.12487024229764938), (0.28125, 0.12402328193187713), (0.2965, 0.10646135383844375), (0.30825, 0.10403706941008568), (0.31, 0.10395176500082016), (0.329, 0.10110047057271004), (0.325, 0.10766253638267517), (0.311, 0.10376304718852043), (0.316, 0.09925242453813553), (0.33125, 0.10192100685834884), (0.3195, 0.10337497422099114), (0.32175, 0.10236664399504662), (0.3245, 0.10390860283374787), (0.327, 0.10857856857776642), (0.328, 0.10514601701498032), (0.32525, 0.10922082620859146), (0.32125, 0.10815048986673355), (0.334, 0.10847719329595566), (0.3235, 0.11532251983880996), (0.31, 0.11787613886594772), (0.31375, 0.11950537872314453), (0.30725, 0.11922468918561935), (0.313, 0.12285830861330033), (0.3295, 0.11812666958570481), (0.32075, 0.11797210013866424), (0.30975, 0.12067215961217881), (0.31, 0.12662362396717072), (0.31125, 0.12452695393562317), (0.314, 0.12464567744731903), (0.31125, 0.12207416343688965), (0.3085, 0.13033129578828812), (0.329, 0.12524350666999817), (0.3125, 0.12495005077123642), (0.315, 0.12182428735494613), (0.32475, 0.1287188680768013), (0.32025, 0.12805788290500641), (0.31175, 0.13139321345090865), (0.31475, 0.1372388846874237)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.17      0.22      0.19       100
           4       0.17      0.15      0.16       100
           5       0.30      0.38      0.34       100
           7       0.45      0.27      0.34       100
           9       0.40      0.38      0.39       100
          11       0.28      0.11      0.16       100
          12       0.32      0.49      0.39       100
          13       0.19      0.12      0.15       100
          14       0.20      0.33      0.25       100
          17       0.33      0.18      0.23       100
          22       0.25      0.25      0.25       100
          27       0.19      0.36      0.25       100
          28       0.50      0.31      0.38       100
          29       0.26      0.19      0.22       100
          31       0.25      0.21      0.23       100
          33       0.41      0.28      0.33       100
          35       0.19      0.15      0.17       100
          37       0.19      0.22      0.21       100
          43       0.34      0.39      0.36       100
          44       0.14      0.18      0.16       100
          45       0.17      0.21      0.19       100
          51       0.30      0.30      0.30       100
          54       0.41      0.47      0.44       100
          58       0.33      0.39      0.36       100
          59       0.39      0.61      0.47       100
          61       0.54      0.40      0.46       100
          63       0.33      0.29      0.31       100
          64       0.28      0.15      0.20       100
          65       0.22      0.15      0.18       100
          68       0.62      0.55      0.59       100
          69       0.69      0.33      0.45       100
          71       0.53      0.33      0.41       100
          75       0.44      0.57      0.49       100
          77       0.23      0.16      0.19       100
          79       0.38      0.38      0.38       100
          81       0.13      0.19      0.16       100
          85       0.52      0.33      0.40       100
          86       0.28      0.44      0.35       100
          94       0.57      0.47      0.51       100
          95       0.40      0.70      0.51       100

    accuracy                           0.31      4000
   macro avg       0.33      0.31      0.31      4000
weighted avg       0.33      0.31      0.31      4000

Collaboration_DC_2
VAL: 
[(0.028, 0.11644194757938385), (0.209, 0.1364609098881483), (0.2365, 0.1790789821371436), (0.237, 0.23449578948318958), (0.2525, 0.28072402109205724), (0.251, 0.30715609215199946), (0.247, 0.32515423818677663), (0.249, 0.3228883135765791), (0.241, 0.3504827939495444), (0.247, 0.3561491595208645), (0.2415, 0.3767030710577965), (0.266, 0.19103664089739322), (0.25, 0.16063986690342427), (0.2655, 0.12951863698661328), (0.274, 0.12555659785866738), (0.2705, 0.10979389710724353), (0.293, 0.10555366696417331), (0.2985, 0.10510789189487696), (0.3005, 0.10604175743460655), (0.325, 0.09850513643026351), (0.3165, 0.10097863206267357), (0.2995, 0.10381475630402565), (0.3205, 0.10495390576124192), (0.317, 0.106815968118608), (0.3335, 0.10428468561172485), (0.32, 0.10730593413114548), (0.3185, 0.11459826792776584), (0.314, 0.11811062896251678), (0.3135, 0.11875262174755336), (0.314, 0.12015401965379716), (0.298, 0.11760634329915047), (0.3155, 0.12499805966019631), (0.31, 0.12325191646814346), (0.3155, 0.12158426415920258), (0.2995, 0.12351377993822098), (0.308, 0.12563893914222718), (0.301, 0.12312339693307876), (0.3035, 0.12308367931842804), (0.3215, 0.12174200209975243), (0.292, 0.12629328006505966), (0.2965, 0.1290366687476635), (0.2955, 0.13546736317873), (0.296, 0.13415510165691374), (0.306, 0.13594448962807656), (0.3065, 0.13804026597738267), (0.28, 0.14052291917800902), (0.2995, 0.13112272334098815), (0.307, 0.13172533804178238), (0.298, 0.14104426258802413), (0.2775, 0.14044215911626814), (0.282, 0.14503140079975127)]
TEST: 
[(0.02375, 0.11551799666881561), (0.213, 0.13418721491098404), (0.237, 0.17612586492300034), (0.2425, 0.23225529372692108), (0.254, 0.27712423539161685), (0.2515, 0.30133734607696533), (0.25575, 0.31987405252456663), (0.257, 0.31766111302375794), (0.25975, 0.34391832983493803), (0.25175, 0.35258198595046997), (0.2455, 0.37250326228141784), (0.26725, 0.19463442558050156), (0.2595, 0.1633937889933586), (0.264, 0.12876722371578217), (0.285, 0.12656954365968703), (0.28825, 0.10888743025064468), (0.29225, 0.10368177288770676), (0.3, 0.10302048844099045), (0.306, 0.10311202478408814), (0.3275, 0.09518613770604134), (0.32575, 0.09996742948889732), (0.31025, 0.10269777864217758), (0.326, 0.102523963868618), (0.313, 0.10563008290529251), (0.3215, 0.10466880735754967), (0.31675, 0.10567288142442703), (0.31875, 0.11114316010475159), (0.309, 0.11255987396836281), (0.31725, 0.11517551177740097), (0.30125, 0.1164926050901413), (0.31475, 0.11481551367044449), (0.3035, 0.12372507059574127), (0.2985, 0.11900539702177047), (0.31325, 0.11694019424915314), (0.3055, 0.11904765945672989), (0.30175, 0.12103986757993698), (0.3055, 0.11918986934423446), (0.30225, 0.12183591592311858), (0.3115, 0.11852099895477294), (0.2885, 0.12347185206413269), (0.3135, 0.12192684265971183), (0.297, 0.12840130412578582), (0.29825, 0.1277496761083603), (0.31175, 0.12936497062444688), (0.31475, 0.13086497807502748), (0.2915, 0.1341435424089432), (0.29575, 0.12731773120164872), (0.31475, 0.12794240099191664), (0.30275, 0.13623980724811555), (0.28725, 0.13536619633436203), (0.287, 0.14202782589197158)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.16      0.19      0.18       100
           4       0.14      0.12      0.13       100
           6       0.27      0.21      0.24       100
          11       0.14      0.17      0.15       100
          13       0.29      0.14      0.19       100
          14       0.16      0.29      0.21       100
          17       0.44      0.26      0.33       100
          18       0.14      0.07      0.09       100
          20       0.48      0.32      0.39       100
          23       0.48      0.61      0.54       100
          27       0.21      0.39      0.28       100
          28       0.38      0.43      0.41       100
          29       0.22      0.30      0.25       100
          31       0.34      0.27      0.30       100
          34       0.28      0.10      0.15       100
          35       0.17      0.23      0.19       100
          42       0.20      0.13      0.16       100
          49       0.26      0.42      0.32       100
          50       0.24      0.30      0.26       100
          52       0.68      0.80      0.74       100
          53       0.86      0.32      0.47       100
          54       0.23      0.44      0.30       100
          56       0.50      0.47      0.48       100
          57       0.58      0.26      0.36       100
          62       0.61      0.17      0.27       100
          64       0.11      0.18      0.14       100
          69       0.60      0.38      0.47       100
          70       0.14      0.04      0.06       100
          71       0.47      0.45      0.46       100
          74       0.11      0.16      0.13       100
          75       0.64      0.55      0.59       100
          77       0.18      0.17      0.18       100
          80       0.10      0.05      0.07       100
          81       0.21      0.31      0.25       100
          84       0.13      0.13      0.13       100
          86       0.28      0.50      0.36       100
          92       0.20      0.10      0.13       100
          94       0.50      0.54      0.52       100
          97       0.26      0.25      0.26       100
          99       0.25      0.26      0.25       100

    accuracy                           0.29      4000
   macro avg       0.32      0.29      0.28      4000
weighted avg       0.32      0.29      0.28      4000

do_assignment: None
seeds: [46]
name: naive-cifar100-feddf46
score_metric: contrloss
aggregation: <function fed_df at 0x769b4c74de50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=46
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61, 57, 25, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61,
        57, 25,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0255, 0.1164030305147171)
DC 1, val_set_size=2000, COIs=[16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63, 34, 95, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63,
        34, 95,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0245, 0.11667730021476745)
DC 2, val_set_size=2000, COIs=[49, 23, 64, 92, 32, 94, 90, 31, 39, 2, 87, 7, 91, 88, 14, 99, 0, 53, 69, 30, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([49, 23, 64, 92, 32, 94, 90, 31, 39,  2, 87,  7, 91, 88, 14, 99,  0, 53,
        69, 30,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0265, 0.11653024744987488)
D00: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D01: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D02: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D03: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D04: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D05: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D06: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D07: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D08: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D09: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D010: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D011: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D012: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D013: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D014: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D015: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D016: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D017: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D018: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D019: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D020: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D021: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D022: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D023: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.286, 0.08508154690265655) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.307, 0.08330850949883462) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3275, 0.08056577867269515) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.326, 0.07853464126586913) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3665, 0.07358207201957703) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3545, 0.07506233531236649) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.359, 0.07607134586572648) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.06921040752530098) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3925, 0.07005354759097099) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.363, 0.07546975621581077) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.401, 0.06967835545539856) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.366, 0.07463871347904205) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.357, 0.08222037690877915) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.412, 0.07084361723065376) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.07344598048925399) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.08262941417098045) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.397, 0.07328822508454323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.391, 0.07809270989894868) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.358, 0.08603707000613213) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4085, 0.07572431045770645) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.08094535320997238) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.363, 0.09025295221805572) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.404, 0.07901995778083801) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3815, 0.08432681638002396) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3595, 0.09428911295533181) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4065, 0.08388991186022758) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.378, 0.08691960787773133) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.0996569572687149) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.406, 0.08846088409423829) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3875, 0.09529211956262588) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3565, 0.10135580587387084) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4015, 0.09315106210112571) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.369, 0.09850004404783248) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.363, 0.113412728369236) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.405, 0.09802533796429634) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.37, 0.10607460996508598) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3545, 0.12309857457876205) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.386, 0.11137167006731033) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3865, 0.11094816422462464) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3315, 0.13577773648500444) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3825, 0.11787499117851258) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3685, 0.12612756633758546) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.331, 0.13970734041929245) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3865, 0.12236586064100266) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.13057655423879624) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.15089242660999297) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.387, 0.13270683139562606) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3675, 0.14631521546840667) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.15449518662691117) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.392, 0.14150410601496696) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.13654946672916413) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.16409912133216858) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.14639849495887755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.365, 0.14889440590143205) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.16272402119636536) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3835, 0.16054080653190614) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3535, 0.14760052073001861) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3375, 0.17617114847898482) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3785, 0.15595348173379897) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.14748274993896485) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.346, 0.1756078704595566) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.393, 0.15394566875696183) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.367, 0.15485009783506393) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.347, 0.18699985545873643) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.15922561132907867) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3605, 0.1601561935544014) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3385, 0.18361004829406738) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.379, 0.17036247712373734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.364, 0.17293836456537245) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3255, 0.19319490015506743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.18093549585342408) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3585, 0.17562737160921096) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.32, 0.20165092360973358) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.16382878524065017) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3805, 0.1808819947838783) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.20172363448143005) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.18181198167800902) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.365, 0.18465530163049698) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3185, 0.20046971893310547) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3815, 0.17038019120693207) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.1842391676902771) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3265, 0.1991245326399803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.387, 0.1811333967447281) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.369, 0.190601318359375) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.343, 0.19041500878334044) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.18883589267730713) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3765, 0.18265978592634202) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.20097602933645248) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.19668078327178956) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.372, 0.1867055628299713) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3325, 0.20245856404304505) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.18998740041255952) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.375, 0.1846458555459976) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3305, 0.21587735199928285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.379, 0.19771514296531678) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.19708472311496736) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.321, 0.22598192012310028) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.359, 0.20836687773466112) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.18916791379451753) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.326, 0.21639641070365906) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3835, 0.20272263544797897) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.2021303390264511) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3235, 0.2202622172832489) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.375, 0.20540720522403716) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3585, 0.2034003481864929) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.22597699773311614) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3685, 0.208972292304039) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.20427685904502868) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3365, 0.23456877744197846) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3735, 0.21616780054569246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3485, 0.20036615467071534) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.318, 0.2328161225914955) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.379, 0.21058441603183747) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.372, 0.21147919714450836) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.334, 0.22328867387771606) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.2178798394203186) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3575, 0.2269165953397751) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3295, 0.22430531632900239) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.21022033846378327) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3655, 0.19996730506420135) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.327, 0.24497590810060502) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3745, 0.22065621465444565) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.19999120366573334) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.331, 0.24964807331562042) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.22526042526960374) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.362, 0.20494760227203368) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3305, 0.24869940066337584) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3735, 0.22435537981987) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3655, 0.21460332667827606) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.329, 0.2441526837348938) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3695, 0.2312698981165886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3635, 0.21198744374513626) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3285, 0.237021120429039) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.22728665900230408) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3595, 0.22954664957523346) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.327, 0.2483816465139389) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.365, 0.24051540011167527) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.375, 0.22289395117759706) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.34, 0.24591168701648713) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.353, 0.25092526626586914) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.365, 0.21799907052516937) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3445, 0.2395880570411682) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.2481931084394455) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.20627455317974092) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3435, 0.24798164343833923) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.2289582784771919) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.21814722549915314) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.333, 0.2366979260444641) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3645, 0.22355790150165558) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.359, 0.23256516528129578) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.0255, 0.1164030305147171), (0.286, 0.08508154690265655), (0.326, 0.07853464126586913), (0.359, 0.07607134586572648), (0.363, 0.07546975621581077), (0.357, 0.08222037690877915), (0.362, 0.08262941417098045), (0.358, 0.08603707000613213), (0.363, 0.09025295221805572), (0.3595, 0.09428911295533181), (0.3535, 0.0996569572687149), (0.3565, 0.10135580587387084), (0.363, 0.113412728369236), (0.3545, 0.12309857457876205), (0.3315, 0.13577773648500444), (0.331, 0.13970734041929245), (0.3475, 0.15089242660999297), (0.3485, 0.15449518662691117), (0.3475, 0.16409912133216858), (0.345, 0.16272402119636536), (0.3375, 0.17617114847898482), (0.346, 0.1756078704595566), (0.347, 0.18699985545873643), (0.3385, 0.18361004829406738), (0.3255, 0.19319490015506743), (0.32, 0.20165092360973358), (0.3155, 0.20172363448143005), (0.3185, 0.20046971893310547), (0.3265, 0.1991245326399803), (0.343, 0.19041500878334044), (0.338, 0.20097602933645248), (0.3325, 0.20245856404304505), (0.3305, 0.21587735199928285), (0.321, 0.22598192012310028), (0.326, 0.21639641070365906), (0.3235, 0.2202622172832489), (0.338, 0.22597699773311614), (0.3365, 0.23456877744197846), (0.318, 0.2328161225914955), (0.334, 0.22328867387771606), (0.3295, 0.22430531632900239), (0.327, 0.24497590810060502), (0.331, 0.24964807331562042), (0.3305, 0.24869940066337584), (0.329, 0.2441526837348938), (0.3285, 0.237021120429039), (0.327, 0.2483816465139389), (0.34, 0.24591168701648713), (0.3445, 0.2395880570411682), (0.3435, 0.24798164343833923), (0.333, 0.2366979260444641)]
TEST: 
[(0.0255, 0.11546436005830765), (0.28775, 0.08414646363258362), (0.32675, 0.07724595993757248), (0.35875, 0.07468911448121071), (0.37675, 0.0732753444314003), (0.35125, 0.08008609569072724), (0.359, 0.08082709175348282), (0.3615, 0.08352739802002906), (0.35575, 0.08880284765362739), (0.36025, 0.09205692100524902), (0.35125, 0.09806391894817353), (0.36375, 0.0994244921207428), (0.3555, 0.11193959718942642), (0.3525, 0.11880889195203781), (0.3425, 0.13056568717956543), (0.34675, 0.13519824796915053), (0.34575, 0.1472940611243248), (0.337, 0.15387305867671966), (0.343, 0.1611909329891205), (0.337, 0.15800956308841704), (0.335, 0.17375567972660064), (0.35, 0.17011436527967452), (0.3335, 0.1800826131105423), (0.3345, 0.17677491241693497), (0.33225, 0.18545784813165664), (0.329, 0.19584842818975448), (0.3375, 0.19328226816654206), (0.33725, 0.1966544235944748), (0.32125, 0.19564510536193847), (0.3435, 0.19008718955516815), (0.33775, 0.19851102662086487), (0.33525, 0.19590004539489747), (0.33675, 0.2081907376050949), (0.323, 0.2168661972284317), (0.33, 0.20954557478427888), (0.32975, 0.2165451311469078), (0.33825, 0.21574643099308013), (0.33625, 0.2274027373790741), (0.3205, 0.22825544083118438), (0.33225, 0.21763045966625214), (0.3285, 0.2161469720005989), (0.32925, 0.23735916060209275), (0.33, 0.24035185796022415), (0.3265, 0.2365544868707657), (0.34325, 0.22959212565422057), (0.332, 0.22810935044288635), (0.326, 0.23019424939155578), (0.34225, 0.23857418018579482), (0.33675, 0.23863233917951585), (0.3435, 0.23719290721416472), (0.3365, 0.23068014633655548)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.23      0.29      0.26       100
           4       0.23      0.24      0.23       100
           5       0.32      0.39      0.35       100
           6       0.28      0.28      0.28       100
           8       0.61      0.36      0.45       100
           9       0.40      0.49      0.44       100
          15       0.28      0.28      0.28       100
          17       0.50      0.53      0.51       100
          18       0.28      0.27      0.28       100
          19       0.29      0.24      0.26       100
          22       0.29      0.38      0.33       100
          25       0.30      0.20      0.24       100
          27       0.25      0.21      0.23       100
          29       0.33      0.36      0.34       100
          37       0.35      0.23      0.28       100
          38       0.23      0.18      0.20       100
          40       0.30      0.35      0.32       100
          51       0.23      0.26      0.24       100
          54       0.37      0.41      0.39       100
          55       0.15      0.14      0.15       100
          57       0.42      0.33      0.37       100
          58       0.51      0.41      0.46       100
          61       0.62      0.44      0.51       100
          62       0.44      0.35      0.39       100
          66       0.28      0.22      0.25       100
          67       0.46      0.59      0.52       100
          68       0.66      0.60      0.63       100
          70       0.36      0.56      0.44       100
          71       0.72      0.63      0.67       100
          72       0.11      0.07      0.09       100
          74       0.31      0.25      0.28       100
          75       0.75      0.56      0.64       100
          76       0.49      0.66      0.56       100
          77       0.15      0.17      0.16       100
          78       0.15      0.12      0.13       100
          79       0.32      0.36      0.34       100
          80       0.14      0.15      0.15       100
          81       0.27      0.37      0.31       100
          84       0.15      0.19      0.17       100
          89       0.26      0.34      0.29       100

    accuracy                           0.34      4000
   macro avg       0.34      0.34      0.34      4000
weighted avg       0.34      0.34      0.34      4000

No_Competition_DC_1
VAL: 
[(0.0245, 0.11667730021476745), (0.307, 0.08330850949883462), (0.3665, 0.07358207201957703), (0.391, 0.06921040752530098), (0.401, 0.06967835545539856), (0.412, 0.07084361723065376), (0.397, 0.07328822508454323), (0.4085, 0.07572431045770645), (0.404, 0.07901995778083801), (0.4065, 0.08388991186022758), (0.406, 0.08846088409423829), (0.4015, 0.09315106210112571), (0.405, 0.09802533796429634), (0.386, 0.11137167006731033), (0.3825, 0.11787499117851258), (0.3865, 0.12236586064100266), (0.387, 0.13270683139562606), (0.392, 0.14150410601496696), (0.389, 0.14639849495887755), (0.3835, 0.16054080653190614), (0.3785, 0.15595348173379897), (0.393, 0.15394566875696183), (0.378, 0.15922561132907867), (0.379, 0.17036247712373734), (0.368, 0.18093549585342408), (0.381, 0.16382878524065017), (0.39, 0.18181198167800902), (0.3815, 0.17038019120693207), (0.387, 0.1811333967447281), (0.368, 0.18883589267730713), (0.373, 0.19668078327178956), (0.381, 0.18998740041255952), (0.379, 0.19771514296531678), (0.359, 0.20836687773466112), (0.3835, 0.20272263544797897), (0.375, 0.20540720522403716), (0.3685, 0.208972292304039), (0.3735, 0.21616780054569246), (0.379, 0.21058441603183747), (0.384, 0.2178798394203186), (0.3705, 0.21022033846378327), (0.3745, 0.22065621465444565), (0.3705, 0.22526042526960374), (0.3735, 0.22435537981987), (0.3695, 0.2312698981165886), (0.374, 0.22728665900230408), (0.365, 0.24051540011167527), (0.353, 0.25092526626586914), (0.352, 0.2481931084394455), (0.364, 0.2289582784771919), (0.3645, 0.22355790150165558)]
TEST: 
[(0.02375, 0.1157863432765007), (0.30125, 0.08302165007591247), (0.372, 0.07268362560868263), (0.39375, 0.0691505137681961), (0.39375, 0.07040558436512948), (0.395, 0.07137692865729332), (0.3895, 0.07451015329360962), (0.39925, 0.07676471590995788), (0.3985, 0.07964338141679764), (0.40375, 0.08368963864445686), (0.3965, 0.08904242470860481), (0.39, 0.09466690012812615), (0.39725, 0.09883734506368637), (0.38025, 0.11258524882793426), (0.38225, 0.1196444079875946), (0.38275, 0.12458434879779816), (0.3825, 0.13460423916578293), (0.3765, 0.14603850996494294), (0.38525, 0.15115658581256866), (0.37275, 0.1615680595636368), (0.36725, 0.16071299934387206), (0.37275, 0.1585891632437706), (0.37825, 0.16145421755313874), (0.3765, 0.173412677526474), (0.362, 0.17902343678474425), (0.36875, 0.16704054123163223), (0.371, 0.1891593253016472), (0.37825, 0.1727487512230873), (0.3775, 0.18599258130788804), (0.36025, 0.19155023431777954), (0.36025, 0.19966231101751328), (0.36475, 0.2003190451860428), (0.3745, 0.1972536919116974), (0.367, 0.2088402413725853), (0.3675, 0.2053820892572403), (0.37925, 0.203355319917202), (0.36875, 0.2075090510249138), (0.37775, 0.21347765964269638), (0.367, 0.21193718701601028), (0.37875, 0.22010028469562531), (0.37525, 0.20770286017656325), (0.37175, 0.22143039035797119), (0.363, 0.23145176899433137), (0.3665, 0.22279236340522765), (0.37425, 0.2252396844625473), (0.3705, 0.226661528468132), (0.35975, 0.23694770336151122), (0.349, 0.2515197967290878), (0.3595, 0.24579531610012054), (0.36475, 0.2294113340973854), (0.361, 0.22767897254228592)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.24      0.21      0.22       100
           4       0.19      0.23      0.21       100
           5       0.25      0.28      0.26       100
           6       0.33      0.42      0.37       100
           8       0.40      0.38      0.39       100
           9       0.43      0.54      0.48       100
          10       0.36      0.36      0.36       100
          12       0.35      0.26      0.30       100
          13       0.32      0.41      0.36       100
          16       0.41      0.28      0.33       100
          17       0.35      0.51      0.42       100
          19       0.27      0.21      0.24       100
          21       0.45      0.41      0.43       100
          24       0.52      0.58      0.55       100
          28       0.44      0.44      0.44       100
          29       0.58      0.22      0.32       100
          33       0.34      0.32      0.33       100
          34       0.32      0.24      0.27       100
          35       0.14      0.18      0.16       100
          36       0.28      0.43      0.34       100
          37       0.35      0.18      0.24       100
          40       0.23      0.34      0.28       100
          41       0.59      0.52      0.55       100
          42       0.30      0.26      0.28       100
          46       0.25      0.17      0.20       100
          51       0.41      0.33      0.37       100
          59       0.45      0.32      0.37       100
          60       0.73      0.64      0.68       100
          63       0.30      0.32      0.31       100
          66       0.24      0.17      0.20       100
          67       0.38      0.50      0.43       100
          68       0.62      0.65      0.63       100
          70       0.49      0.50      0.50       100
          74       0.24      0.23      0.23       100
          75       0.61      0.52      0.56       100
          80       0.20      0.13      0.16       100
          81       0.30      0.31      0.30       100
          95       0.47      0.59      0.52       100
          96       0.43      0.39      0.41       100
          97       0.25      0.46      0.33       100

    accuracy                           0.36      4000
   macro avg       0.37      0.36      0.36      4000
weighted avg       0.37      0.36      0.36      4000

No_Competition_DC_2
VAL: 
[(0.0265, 0.11653024744987488), (0.3275, 0.08056577867269515), (0.3545, 0.07506233531236649), (0.3925, 0.07005354759097099), (0.366, 0.07463871347904205), (0.387, 0.07344598048925399), (0.391, 0.07809270989894868), (0.39, 0.08094535320997238), (0.3815, 0.08432681638002396), (0.378, 0.08691960787773133), (0.3875, 0.09529211956262588), (0.369, 0.09850004404783248), (0.37, 0.10607460996508598), (0.3865, 0.11094816422462464), (0.3685, 0.12612756633758546), (0.3665, 0.13057655423879624), (0.3675, 0.14631521546840667), (0.371, 0.13654946672916413), (0.365, 0.14889440590143205), (0.3535, 0.14760052073001861), (0.3615, 0.14748274993896485), (0.367, 0.15485009783506393), (0.3605, 0.1601561935544014), (0.364, 0.17293836456537245), (0.3585, 0.17562737160921096), (0.3805, 0.1808819947838783), (0.365, 0.18465530163049698), (0.361, 0.1842391676902771), (0.369, 0.190601318359375), (0.3765, 0.18265978592634202), (0.372, 0.1867055628299713), (0.375, 0.1846458555459976), (0.3565, 0.19708472311496736), (0.3565, 0.18916791379451753), (0.361, 0.2021303390264511), (0.3585, 0.2034003481864929), (0.355, 0.20427685904502868), (0.3485, 0.20036615467071534), (0.372, 0.21147919714450836), (0.3575, 0.2269165953397751), (0.3655, 0.19996730506420135), (0.3725, 0.19999120366573334), (0.362, 0.20494760227203368), (0.3655, 0.21460332667827606), (0.3635, 0.21198744374513626), (0.3595, 0.22954664957523346), (0.375, 0.22289395117759706), (0.365, 0.21799907052516937), (0.3615, 0.20627455317974092), (0.3725, 0.21814722549915314), (0.359, 0.23256516528129578)]
TEST: 
[(0.02925, 0.11562418437004089), (0.314, 0.08053867435455322), (0.3415, 0.07441438457369805), (0.383, 0.0698833228945732), (0.37625, 0.07367235910892486), (0.392, 0.07222084078192711), (0.393, 0.07662262478470802), (0.39575, 0.07859064659476281), (0.39225, 0.08263001605868339), (0.39075, 0.08516153439879418), (0.379, 0.09347232231497765), (0.381, 0.09749275258183479), (0.388, 0.10314490836858749), (0.379, 0.11077508014440536), (0.3715, 0.12399545258283615), (0.37225, 0.12654538917541505), (0.36925, 0.14100981152057648), (0.37375, 0.12933285003900527), (0.357, 0.14635625368356706), (0.375, 0.14339765375852584), (0.3815, 0.1458103266954422), (0.3755, 0.1545188479423523), (0.37175, 0.1543577392101288), (0.36625, 0.1724895172715187), (0.3645, 0.1701972214579582), (0.37225, 0.17906832748651505), (0.3635, 0.17937819528579713), (0.37675, 0.1806471591591835), (0.37025, 0.18028096348047257), (0.37375, 0.1794499989748001), (0.368, 0.1843223357796669), (0.366, 0.18712297517061233), (0.3635, 0.19184687823057175), (0.37025, 0.1821916387081146), (0.3695, 0.19916530644893646), (0.3655, 0.19976509147882462), (0.36525, 0.2000163933634758), (0.368, 0.19739240097999572), (0.375, 0.20390544164180754), (0.3575, 0.22547489804029464), (0.35475, 0.19567511212825775), (0.37325, 0.2011995068192482), (0.37, 0.20199067628383635), (0.364, 0.20986367005109788), (0.36975, 0.21664294338226317), (0.368, 0.22411523878574371), (0.3805, 0.2212553319334984), (0.37725, 0.21754086643457413), (0.376, 0.20779467844963073), (0.36625, 0.22110627007484435), (0.363, 0.23567825150489807)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.61      0.42      0.50       100
           2       0.28      0.29      0.29       100
           3       0.23      0.22      0.23       100
           4       0.14      0.14      0.14       100
           5       0.27      0.56      0.36       100
           6       0.32      0.37      0.34       100
           7       0.36      0.46      0.40       100
           8       0.51      0.37      0.43       100
           9       0.46      0.48      0.47       100
          14       0.25      0.23      0.24       100
          17       0.44      0.58      0.50       100
          19       0.24      0.13      0.17       100
          23       0.64      0.37      0.47       100
          29       0.34      0.36      0.35       100
          30       0.56      0.43      0.49       100
          31       0.27      0.25      0.26       100
          32       0.20      0.20      0.20       100
          37       0.34      0.25      0.29       100
          39       0.40      0.44      0.42       100
          40       0.43      0.35      0.38       100
          49       0.47      0.37      0.41       100
          51       0.27      0.29      0.28       100
          53       0.58      0.64      0.61       100
          64       0.21      0.29      0.24       100
          66       0.26      0.15      0.19       100
          67       0.38      0.49      0.43       100
          68       0.74      0.59      0.66       100
          69       0.52      0.59      0.55       100
          70       0.30      0.39      0.34       100
          74       0.42      0.20      0.27       100
          75       0.54      0.50      0.52       100
          80       0.16      0.19      0.18       100
          81       0.31      0.26      0.28       100
          87       0.38      0.37      0.38       100
          88       0.30      0.29      0.30       100
          90       0.24      0.37      0.29       100
          91       0.45      0.45      0.45       100
          92       0.28      0.31      0.30       100
          94       0.63      0.60      0.62       100
          99       0.31      0.28      0.29       100

    accuracy                           0.36      4000
   macro avg       0.38      0.36      0.36      4000
weighted avg       0.38      0.36      0.36      4000

Competition
DC 0, val_set_size=2000, COIs=[54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61, 57, 25, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61,
        57, 25,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0255, 0.1164030305147171)
DC 1, val_set_size=2000, COIs=[16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63, 34, 95, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63,
        34, 95,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0245, 0.11667730021476745)
DC 2, val_set_size=2000, COIs=[49, 23, 64, 92, 32, 94, 90, 31, 39, 2, 87, 7, 91, 88, 14, 99, 0, 53, 69, 30, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([49, 23, 64, 92, 32, 94, 90, 31, 39,  2, 87,  7, 91, 88, 14, 99,  0, 53,
        69, 30,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0265, 0.11653024744987488)
D00: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D01: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D02: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D03: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D04: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D05: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D06: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D07: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D08: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D09: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D010: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D011: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D012: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D013: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D014: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D015: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D016: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D017: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D018: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D019: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D020: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D021: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D022: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D023: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO5']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO4', '(DO3']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.209, 0.13667149060964584) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2425, 0.13221803154051304) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.13571873965859413) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2275, 0.18234787836670877) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.17257503247261047) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.17753749698400498) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.238, 0.2228823932260275) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.279, 0.19619079661369324) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2745, 0.22011992229521274) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2425, 0.2918963204175234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2865, 0.26175536029040813) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2795, 0.2666170956492424) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2395, 0.3469833356589079) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.278, 0.3002760012447834) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2835, 0.3337460019737482) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO0', '(DO2']
DC 2 --> ['(DO5', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.3488950601220131) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2785, 0.3460476665943861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2755, 0.34325029024481774) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.35605609700083735) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.275, 0.36827646078169346) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.33323404802381995) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.3652875915169716) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.3676055261641741) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2855, 0.3436477469801903) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.237, 0.390605333045125) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.3740835966914892) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.277, 0.36809008997678755) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.3981064717918634) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.268, 0.39707655444741247) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2765, 0.373545984596014) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO5', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2385, 0.43435284501314164) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.40553512589633467) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.283, 0.38397912031412124) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.4126874235868454) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.39601147203147413) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2725, 0.3941396115422249) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.4188062264621258) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.38444462433457377) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.279, 0.3903367047458887) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.4097906032502651) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.40826241837441923) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.275, 0.3887695264071226) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2315, 0.4326808821260929) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.4030158943682909) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.276, 0.39703110414743426) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO5', '(DO0']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO2', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2245, 0.44137152910232547) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2765, 0.3579549961388111) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.273, 0.37987144070863726) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.4307167705595493) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.269, 0.4012592939734459) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.3755036658942699) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.39391883805394173) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2585, 0.39916449841856955) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.3622671719193459) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.234, 0.4080255272090435) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.37118735164403915) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.274, 0.3669387241601944) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.4124281287789345) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.26, 0.37639079651236534) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2735, 0.33244815132021904) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO2', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2205, 0.3759666734039783) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.37490625858306886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2745, 0.33311891183257103) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.23, 0.39164270541071894) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.35771928617358206) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.269, 0.34009277060627935) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.4008545325398445) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.3323144878745079) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.282, 0.3103541284203529) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2245, 0.3604814087152481) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.269, 0.32123433488607406) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2715, 0.3308335363864899) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.36701346853375433) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.32286826851963996) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.277, 0.3057466411888599) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO5', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2235, 0.39942181915044783) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.3248551707267761) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.275, 0.2989218337237835) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.222, 0.39382308334112165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2575, 0.30159571817517283) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.269, 0.29418898543715477) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2305, 0.32374022334814073) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.31110725449025634) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2775, 0.2757127188444138) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.223, 0.33562248706817627) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2695, 0.2946329638659954) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.283, 0.26660630869865415) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2285, 0.33116112226247785) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.30946553456783293) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.274, 0.2721077931523323) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO0', '(DO1']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO2', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.34441184464097024) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.271, 0.2913791089951992) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.279, 0.2627951177954674) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.229, 0.3531176124811172) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.30018848806619647) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2745, 0.26738465040922166) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.3069561626911163) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.242780712723732) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.273, 0.2818756889104843) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2325, 0.32696567445993424) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2615, 0.2825730135142803) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.274, 0.2718834845423698) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.30627832767367363) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.269, 0.28852719897031787) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.272, 0.2587474466562271) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO3', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2375, 0.2842189656198025) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.26, 0.29372184473276136) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.269, 0.28085184121131895) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2335, 0.29690462538599965) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.2982044870555401) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2785, 0.2640199467241764) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.3002196092009544) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.28586897772550585) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2805, 0.2531720865666866) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.2784398218393326) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.27, 0.2868297461345792) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2755, 0.26393025675415993) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2345, 0.2919193167090416) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2755, 0.2687252068519592) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.287, 0.2639619252681732) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO2', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.23, 0.2770500205159187) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.2758861138820648) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2795, 0.2655235462486744) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2425, 0.26983123499155043) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.269, 0.27196348375082013) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.283, 0.2571157723665237) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.229, 0.2936413254737854) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.29137056252360344) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.281, 0.2600830494761467) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.3051998052000999) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2695, 0.2919833868145943) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2765, 0.2796524854898453) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.24, 0.27498157238960264) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.275, 0.2776153875291347) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.275, 0.2553583762049675) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO4', '(DO5']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO0', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2345, 0.2556841819882393) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.27, 0.29214110440015795) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2835, 0.26459084391593934) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.237, 0.2863071136474609) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.268, 0.2948977315425873) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2845, 0.2712650338411331) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.238, 0.2793220458030701) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.26333962827920915) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.287, 0.2687372550368309) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.2793676687479019) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.27960093623399734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2745, 0.2657068351507187) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.23, 0.29084723979234695) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.28682631981372836) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.28, 0.267739422082901) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.0255, 0.1164030305147171), (0.209, 0.13667149060964584), (0.2275, 0.18234787836670877), (0.238, 0.2228823932260275), (0.2425, 0.2918963204175234), (0.2395, 0.3469833356589079), (0.2475, 0.3488950601220131), (0.233, 0.35605609700083735), (0.242, 0.3652875915169716), (0.237, 0.390605333045125), (0.241, 0.3981064717918634), (0.2385, 0.43435284501314164), (0.235, 0.4126874235868454), (0.233, 0.4188062264621258), (0.235, 0.4097906032502651), (0.2315, 0.4326808821260929), (0.2245, 0.44137152910232547), (0.235, 0.4307167705595493), (0.235, 0.39391883805394173), (0.234, 0.4080255272090435), (0.22, 0.4124281287789345), (0.2205, 0.3759666734039783), (0.23, 0.39164270541071894), (0.227, 0.4008545325398445), (0.2245, 0.3604814087152481), (0.227, 0.36701346853375433), (0.2235, 0.39942181915044783), (0.222, 0.39382308334112165), (0.2305, 0.32374022334814073), (0.223, 0.33562248706817627), (0.2285, 0.33116112226247785), (0.22, 0.34441184464097024), (0.229, 0.3531176124811172), (0.233, 0.3069561626911163), (0.2325, 0.32696567445993424), (0.227, 0.30627832767367363), (0.2375, 0.2842189656198025), (0.2335, 0.29690462538599965), (0.235, 0.3002196092009544), (0.241, 0.2784398218393326), (0.2345, 0.2919193167090416), (0.23, 0.2770500205159187), (0.2425, 0.26983123499155043), (0.229, 0.2936413254737854), (0.235, 0.3051998052000999), (0.24, 0.27498157238960264), (0.2345, 0.2556841819882393), (0.237, 0.2863071136474609), (0.238, 0.2793220458030701), (0.233, 0.2793676687479019), (0.23, 0.29084723979234695)]
TEST: 
[(0.0255, 0.11546436005830765), (0.2085, 0.1343124722838402), (0.231, 0.17889616060256958), (0.2375, 0.218974622964859), (0.24275, 0.286272420167923), (0.24025, 0.3377014340162277), (0.2465, 0.3415281444787979), (0.239, 0.34946504974365233), (0.23875, 0.36049987471103667), (0.24375, 0.38292483282089235), (0.2345, 0.3910610363483429), (0.23275, 0.42502210116386413), (0.23675, 0.40512196969985964), (0.237, 0.4105856807231903), (0.234, 0.40750632083415983), (0.23075, 0.4282832627296448), (0.23325, 0.4302809293270111), (0.23275, 0.42568995320796965), (0.23675, 0.39048580479621886), (0.235, 0.4023853497505188), (0.225, 0.40400613284111025), (0.22725, 0.36985045492649077), (0.2325, 0.3874357842206955), (0.22375, 0.39078002429008485), (0.2255, 0.3572008271217346), (0.2295, 0.36305537128448484), (0.228, 0.3947419605255127), (0.22, 0.3880784775018692), (0.23, 0.31734794902801516), (0.22825, 0.3307712091207504), (0.23, 0.32156011986732486), (0.232, 0.3320803543329239), (0.23, 0.3407886914014816), (0.23375, 0.30160648429393766), (0.2285, 0.3227202149629593), (0.22375, 0.30264704179763796), (0.23375, 0.2813473070859909), (0.237, 0.2946230856180191), (0.231, 0.2989297158718109), (0.23775, 0.27565322041511536), (0.23625, 0.2916856082677841), (0.2245, 0.27296753001213075), (0.239, 0.26800482034683226), (0.23375, 0.29172708570957184), (0.23875, 0.3006350524425507), (0.242, 0.27239125633239747), (0.23625, 0.25397706031799316), (0.23175, 0.28424987745285035), (0.2365, 0.2767974569797516), (0.239, 0.27504113137722014), (0.236, 0.2870494693517685)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.40      0.02      0.04       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       0.25      0.01      0.02       100
           8       0.00      0.00      0.00       100
           9       1.00      0.03      0.06       100
          15       0.20      0.43      0.27       100
          17       0.75      0.06      0.11       100
          18       0.24      0.35      0.28       100
          19       0.00      0.00      0.00       100
          22       0.23      0.44      0.30       100
          25       0.17      0.43      0.25       100
          27       0.15      0.47      0.23       100
          29       1.00      0.02      0.04       100
          37       0.00      0.00      0.00       100
          38       0.12      0.30      0.17       100
          40       0.60      0.03      0.06       100
          51       0.00      0.00      0.00       100
          54       0.30      0.60      0.40       100
          55       0.12      0.19      0.15       100
          57       0.27      0.50      0.35       100
          58       0.33      0.60      0.43       100
          61       0.51      0.49      0.50       100
          62       0.35      0.61      0.44       100
          66       0.00      0.00      0.00       100
          67       0.68      0.15      0.25       100
          68       1.00      0.02      0.04       100
          70       0.38      0.09      0.15       100
          71       0.44      0.85      0.58       100
          72       0.12      0.23      0.16       100
          74       0.00      0.00      0.00       100
          75       1.00      0.01      0.02       100
          76       0.44      0.76      0.56       100
          77       0.13      0.22      0.16       100
          78       0.14      0.28      0.19       100
          79       0.25      0.46      0.33       100
          80       0.50      0.01      0.02       100
          81       0.14      0.01      0.02       100
          84       0.16      0.29      0.21       100
          89       0.19      0.48      0.27       100

    accuracy                           0.24      4000
   macro avg       0.31      0.24      0.18      4000
weighted avg       0.31      0.24      0.18      4000

Competition_DC_1
VAL: 
[(0.0245, 0.11667730021476745), (0.2425, 0.13221803154051304), (0.2685, 0.17257503247261047), (0.279, 0.19619079661369324), (0.2865, 0.26175536029040813), (0.278, 0.3002760012447834), (0.2785, 0.3460476665943861), (0.275, 0.36827646078169346), (0.2655, 0.3676055261641741), (0.2745, 0.3740835966914892), (0.268, 0.39707655444741247), (0.2745, 0.40553512589633467), (0.2705, 0.39601147203147413), (0.2655, 0.38444462433457377), (0.2675, 0.40826241837441923), (0.2705, 0.4030158943682909), (0.2765, 0.3579549961388111), (0.269, 0.4012592939734459), (0.2585, 0.39916449841856955), (0.2635, 0.37118735164403915), (0.26, 0.37639079651236534), (0.2685, 0.37490625858306886), (0.2745, 0.35771928617358206), (0.2635, 0.3323144878745079), (0.269, 0.32123433488607406), (0.2675, 0.32286826851963996), (0.2685, 0.3248551707267761), (0.2575, 0.30159571817517283), (0.256, 0.31110725449025634), (0.2695, 0.2946329638659954), (0.267, 0.30946553456783293), (0.271, 0.2913791089951992), (0.2675, 0.30018848806619647), (0.2655, 0.242780712723732), (0.2615, 0.2825730135142803), (0.269, 0.28852719897031787), (0.26, 0.29372184473276136), (0.263, 0.2982044870555401), (0.2705, 0.28586897772550585), (0.27, 0.2868297461345792), (0.2755, 0.2687252068519592), (0.2675, 0.2758861138820648), (0.269, 0.27196348375082013), (0.265, 0.29137056252360344), (0.2695, 0.2919833868145943), (0.275, 0.2776153875291347), (0.27, 0.29214110440015795), (0.268, 0.2948977315425873), (0.2705, 0.26333962827920915), (0.256, 0.27960093623399734), (0.2705, 0.28682631981372836)]
TEST: 
[(0.02375, 0.1157863432765007), (0.24175, 0.13015912461280824), (0.25675, 0.17000418984889984), (0.267, 0.19287890326976775), (0.2735, 0.2553897944688797), (0.271, 0.29318399596214295), (0.2705, 0.33723665463924407), (0.27525, 0.35885457003116605), (0.27575, 0.3538256287574768), (0.27475, 0.3624431890249252), (0.27025, 0.3838274824619293), (0.27675, 0.393588192820549), (0.273, 0.3888783783912659), (0.26975, 0.3821367380619049), (0.269, 0.4074390826225281), (0.2655, 0.4017168604135513), (0.26525, 0.3586328508853912), (0.27175, 0.39679605269432067), (0.2595, 0.4014473934173584), (0.269, 0.3691193702220917), (0.26475, 0.37825742065906526), (0.27075, 0.37927579271793366), (0.27075, 0.35387909412384033), (0.2675, 0.32729098892211916), (0.27725, 0.3130734716653824), (0.27075, 0.32033482801914215), (0.273, 0.3234905824661255), (0.26675, 0.29579778444766996), (0.26125, 0.31000735771656035), (0.26825, 0.2915506979227066), (0.26225, 0.3047000684738159), (0.26575, 0.29404801285266874), (0.26425, 0.3047063604593277), (0.27275, 0.2446347873210907), (0.26525, 0.28498575210571286), (0.263, 0.2894120808839798), (0.25675, 0.29833569598197934), (0.265, 0.3004189977645874), (0.2645, 0.28662782871723175), (0.261, 0.2831546026468277), (0.2715, 0.2701916514635086), (0.26725, 0.2757513916492462), (0.27175, 0.2717880457639694), (0.2625, 0.2993461354970932), (0.266, 0.296754568696022), (0.26575, 0.28286238956451415), (0.26475, 0.2972773430347443), (0.26425, 0.29310591340065), (0.26875, 0.2635569729804993), (0.26175, 0.2800029932260513), (0.26075, 0.28982072746753695)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       1.00      0.01      0.02       100
           6       0.50      0.01      0.02       100
           8       0.00      0.00      0.00       100
           9       0.30      0.03      0.05       100
          10       0.30      0.46      0.36       100
          12       0.15      0.59      0.23       100
          13       0.21      0.56      0.31       100
          16       0.18      0.35      0.23       100
          17       1.00      0.01      0.02       100
          19       0.00      0.00      0.00       100
          21       0.33      0.72      0.46       100
          24       0.38      0.71      0.49       100
          28       0.31      0.59      0.41       100
          29       0.00      0.00      0.00       100
          33       0.23      0.47      0.31       100
          34       0.21      0.48      0.29       100
          35       0.18      0.33      0.23       100
          36       0.20      0.32      0.25       100
          37       0.00      0.00      0.00       100
          40       0.32      0.08      0.13       100
          41       0.28      0.61      0.38       100
          42       0.17      0.39      0.24       100
          46       0.18      0.26      0.21       100
          51       0.00      0.00      0.00       100
          59       0.34      0.52      0.41       100
          60       0.50      0.88      0.64       100
          63       0.28      0.41      0.34       100
          66       0.00      0.00      0.00       100
          67       0.50      0.02      0.04       100
          68       1.00      0.02      0.04       100
          70       0.40      0.02      0.04       100
          74       0.00      0.00      0.00       100
          75       1.00      0.02      0.04       100
          80       0.00      0.00      0.00       100
          81       0.00      0.00      0.00       100
          95       0.34      0.80      0.48       100
          96       0.39      0.42      0.41       100
          97       0.23      0.34      0.27       100

    accuracy                           0.26      4000
   macro avg       0.29      0.26      0.18      4000
weighted avg       0.29      0.26      0.18      4000

Competition_DC_2
VAL: 
[(0.0265, 0.11653024744987488), (0.253, 0.13571873965859413), (0.2585, 0.17753749698400498), (0.2745, 0.22011992229521274), (0.2795, 0.2666170956492424), (0.2835, 0.3337460019737482), (0.2755, 0.34325029024481774), (0.2915, 0.33323404802381995), (0.2855, 0.3436477469801903), (0.277, 0.36809008997678755), (0.2765, 0.373545984596014), (0.283, 0.38397912031412124), (0.2725, 0.3941396115422249), (0.279, 0.3903367047458887), (0.275, 0.3887695264071226), (0.276, 0.39703110414743426), (0.273, 0.37987144070863726), (0.265, 0.3755036658942699), (0.2625, 0.3622671719193459), (0.274, 0.3669387241601944), (0.2735, 0.33244815132021904), (0.2745, 0.33311891183257103), (0.269, 0.34009277060627935), (0.282, 0.3103541284203529), (0.2715, 0.3308335363864899), (0.277, 0.3057466411888599), (0.275, 0.2989218337237835), (0.269, 0.29418898543715477), (0.2775, 0.2757127188444138), (0.283, 0.26660630869865415), (0.274, 0.2721077931523323), (0.279, 0.2627951177954674), (0.2745, 0.26738465040922166), (0.273, 0.2818756889104843), (0.274, 0.2718834845423698), (0.272, 0.2587474466562271), (0.269, 0.28085184121131895), (0.2785, 0.2640199467241764), (0.2805, 0.2531720865666866), (0.2755, 0.26393025675415993), (0.287, 0.2639619252681732), (0.2795, 0.2655235462486744), (0.283, 0.2571157723665237), (0.281, 0.2600830494761467), (0.2765, 0.2796524854898453), (0.275, 0.2553583762049675), (0.2835, 0.26459084391593934), (0.2845, 0.2712650338411331), (0.287, 0.2687372550368309), (0.2745, 0.2657068351507187), (0.28, 0.267739422082901)]
TEST: 
[(0.02925, 0.11562418437004089), (0.244, 0.13506957226991653), (0.26525, 0.17555271756649019), (0.27725, 0.21750785112380983), (0.2805, 0.2646669534444809), (0.2885, 0.33070629370212556), (0.285, 0.3410960419178009), (0.2865, 0.3311305892467499), (0.28125, 0.34439921963214876), (0.2795, 0.3682460427284241), (0.28625, 0.37415118038654327), (0.28125, 0.3869604107141495), (0.28275, 0.3924637814760208), (0.27825, 0.3955080876350403), (0.278, 0.39016337621212005), (0.28375, 0.3930741086006165), (0.2735, 0.38091427099704744), (0.2685, 0.37150372767448425), (0.2745, 0.36037415063381195), (0.2775, 0.3711511617898941), (0.272, 0.33566629922389984), (0.28075, 0.3317891154289246), (0.2705, 0.339387344956398), (0.2825, 0.308833913564682), (0.272, 0.3274895015954971), (0.279, 0.30832501769065856), (0.27225, 0.299385306596756), (0.2745, 0.29255620288848877), (0.2765, 0.276981689453125), (0.27825, 0.26626397466659546), (0.2765, 0.2718361411094666), (0.27325, 0.2666101852655411), (0.277, 0.2697340338230133), (0.27225, 0.28259570741653445), (0.2805, 0.27165345668792723), (0.27625, 0.2573556042909622), (0.269, 0.2784277057647705), (0.28225, 0.2643856729269028), (0.2815, 0.24970759165287018), (0.27825, 0.2626385254859924), (0.28375, 0.2623986474275589), (0.2805, 0.2628521499633789), (0.292, 0.25077925765514375), (0.2885, 0.24837042319774627), (0.27425, 0.2683865945339203), (0.2835, 0.24685722780227662), (0.28025, 0.2544825179576874), (0.28525, 0.2637553737163544), (0.28275, 0.2694611117839813), (0.28625, 0.2629811042547226), (0.288, 0.2636440658569336)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.50      0.71      0.59       100
           2       0.31      0.43      0.36       100
           3       0.00      0.00      0.00       100
           4       0.00      0.00      0.00       100
           5       0.00      0.00      0.00       100
           6       1.00      0.01      0.02       100
           7       0.22      0.63      0.32       100
           8       0.50      0.01      0.02       100
           9       0.89      0.08      0.15       100
          14       0.19      0.37      0.25       100
          17       0.00      0.00      0.00       100
          19       0.50      0.01      0.02       100
          23       0.49      0.65      0.56       100
          29       0.60      0.03      0.06       100
          30       0.40      0.68      0.50       100
          31       0.18      0.60      0.28       100
          32       0.20      0.39      0.27       100
          37       0.33      0.01      0.02       100
          39       0.37      0.51      0.43       100
          40       0.00      0.00      0.00       100
          49       0.29      0.63      0.40       100
          51       0.00      0.00      0.00       100
          53       0.53      0.62      0.57       100
          64       0.15      0.30      0.20       100
          66       0.00      0.00      0.00       100
          67       0.78      0.07      0.13       100
          68       0.82      0.18      0.30       100
          69       0.42      0.71      0.53       100
          70       0.33      0.03      0.06       100
          74       0.00      0.00      0.00       100
          75       0.80      0.08      0.15       100
          80       0.00      0.00      0.00       100
          81       0.25      0.01      0.02       100
          87       0.32      0.54      0.40       100
          88       0.18      0.46      0.26       100
          90       0.17      0.51      0.26       100
          91       0.35      0.54      0.43       100
          92       0.24      0.60      0.34       100
          94       0.44      0.68      0.53       100
          99       0.21      0.44      0.29       100

    accuracy                           0.29      4000
   macro avg       0.32      0.29      0.22      4000
weighted avg       0.32      0.29      0.22      4000

Collaboration
DC 0, val_set_size=2000, COIs=[54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61, 57, 25, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61,
        57, 25,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0255, 0.1164030305147171)
DC 1, val_set_size=2000, COIs=[16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63, 34, 95, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63,
        34, 95,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0245, 0.11667730021476745)
DC 2, val_set_size=2000, COIs=[49, 23, 64, 92, 32, 94, 90, 31, 39, 2, 87, 7, 91, 88, 14, 99, 0, 53, 69, 30, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([49, 23, 64, 92, 32, 94, 90, 31, 39,  2, 87,  7, 91, 88, 14, 99,  0, 53,
        69, 30,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.0265, 0.11653024744987488)
D00: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D01: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D02: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D03: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D04: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D05: 1000 samples from classes {3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81}
D06: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D07: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D08: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D09: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D010: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D011: 1000 samples from classes {15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89}
D012: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D013: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D014: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D015: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D016: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D017: 1000 samples from classes {10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97}
D018: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D019: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D020: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D021: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D022: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
D023: 1000 samples from classes {0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO4', '(DO3']
DC 2 --> ['(DO0', '(DO2']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.202, 0.1409693197607994) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.24, 0.14089651209115983) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.228, 0.13209930661320687) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2255, 0.18419047766923904) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2775, 0.17498260675370694) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2665, 0.1748268899023533) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.236, 0.23539444771409035) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2855, 0.2166133507490158) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2805, 0.2259693077802658) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2385, 0.28242079994082453) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.288, 0.2620709447786212) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.282, 0.27517466694116594) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.234, 0.3295216091573238) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2865, 0.3055267943590879) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.275, 0.30897353373467923) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO5']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO3', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.236, 0.34206185898184777) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2835, 0.32310113635659216) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.282, 0.3220739869475365) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2275, 0.3519208933711052) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.284, 0.34383644759654997) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.277, 0.3389838016331196) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.226, 0.3555838957130909) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2845, 0.3382292886823416) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2815, 0.35674322494864463) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.36672219878435136) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2735, 0.37581144222617147) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2725, 0.37959419986605647) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.23, 0.3747866123318672) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2735, 0.37991414946317675) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.274, 0.3896309501230717) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[3, 4, 5, 6, 8, 9, 17, 19, 29, 37, 40, 51, 66, 67, 68, 70, 74, 75, 80, 81], M=tensor([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 17, 18, 19,
        21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39,
        40, 41, 42, 46, 49, 51, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 66,
        67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 84, 87, 88, 89,
        90, 91, 92, 94, 95, 96, 97, 99], device='cuda:0'), Initial Performance: (0.36866666666666664, 0.07138921449581782)
DC Expert-0, val_set_size=1000, COIs=[15, 18, 22, 25, 27, 38, 54, 55, 57, 58, 61, 62, 71, 72, 76, 77, 78, 79, 84, 89], M=tensor([54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61,
        57, 25,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.46, 0.07966874182224273)
DC Expert-1, val_set_size=1000, COIs=[10, 12, 13, 16, 21, 24, 28, 33, 34, 35, 36, 41, 42, 46, 59, 60, 63, 95, 96, 97], M=tensor([16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63,
        34, 95,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.547, 0.06196619284152985)
DC Expert-2, val_set_size=1000, COIs=[0, 2, 7, 14, 23, 30, 31, 32, 39, 49, 53, 64, 69, 87, 88, 90, 91, 92, 94, 99], M=tensor([49, 23, 64, 92, 32, 94, 90, 31, 39,  2, 87,  7, 91, 88, 14, 99,  0, 53,
        69, 30,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), Initial Performance: (0.548, 0.06872459477186203)
SUPER-DC 0, val_set_size=2000, COIs=[54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61, 57, 25, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([54, 18, 62, 79, 58, 27, 78, 77, 15, 76, 89, 22, 71, 38, 55, 84, 72, 61,
        57, 25,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63, 34, 95, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([16, 12, 60, 36, 10, 97, 35, 46, 42, 21, 41, 33, 96, 24, 13, 59, 28, 63,
        34, 95,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[49, 23, 64, 92, 32, 94, 90, 31, 39, 2, 87, 7, 91, 88, 14, 99, 0, 53, 69, 30, 9, 51, 5, 75, 29, 81, 66, 19, 74, 68, 4, 3, 8, 40, 80, 6, 17, 70, 37, 67], M=tensor([49, 23, 64, 92, 32, 94, 90, 31, 39,  2, 87,  7, 91, 88, 14, 99,  0, 53,
        69, 30,  9, 51,  5, 75, 29, 81, 66, 19, 74, 68,  4,  3,  8, 40, 80,  6,
        17, 70, 37, 67], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.451, 0.10004281866550445) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.555, 0.07354259358346463) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.548, 0.07801750308275222) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.475, 0.05537742339571317) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.249, 0.1742353835105896) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3125, 0.1550880949795246) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.284, 0.15275873267650605) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.442, 0.11617375183105469) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.07595884461700916) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.545, 0.08453425008058547) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49733333333333335, 0.05449410094817479) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.247, 0.15744064807891844) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.307, 0.1285801739692688) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.283, 0.1389934655725956) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.44, 0.12468281948566437) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.536, 0.09570978945493698) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.542, 0.0897226964533329) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.504, 0.05700812438130379) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2545, 0.1469199201464653) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.314, 0.11616350397467613) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3005, 0.12983604615926742) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.465, 0.12251031184196472) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.542, 0.09890329086780549) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.545, 0.09635854732990265) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.501, 0.0625457501312097) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2575, 0.12857547706365585) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2915, 0.12402412170171738) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.291, 0.12397932589054107) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.436, 0.12793737840652467) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.561, 0.09075571500509977) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.554, 0.09967657965421677) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.509, 0.06390076950192451) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2695, 0.11668908429145813) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.317, 0.10472912114858628) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.287, 0.11596698594093323) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.459, 0.12983060348033906) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.1026387505158782) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.528, 0.11995555222034454) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.497, 0.0705487788816293) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.267, 0.115085931122303) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.309, 0.10465698421001435) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.11860467350482941) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.448, 0.13011756682395936) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.528, 0.10552280169725418) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.534, 0.10506360828876496) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.494, 0.0763449383576711) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.293, 0.10433760607242584) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.324, 0.09562921395897865) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3105, 0.10125028443336487) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.449, 0.13755268502235413) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.542, 0.10550587618350983) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.528, 0.11265579080581666) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49633333333333335, 0.08249283016721408) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.277, 0.10939805072546005) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3215, 0.09460829532146454) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3065, 0.09779721695184708) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.463, 0.14751321280002594) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.11218496137112378) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.563, 0.10476482546329498) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.492, 0.09016935010751088) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.288, 0.11221517533063889) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.32, 0.0980911580324173) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3035, 0.10297078883647919) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.442, 0.15591584026813507) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.544, 0.11231826621294022) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.518, 0.12336960780620575) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4696666666666667, 0.10661981662114461) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2945, 0.10694750386476516) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.333, 0.09759107428789139) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.319, 0.10252565604448319) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.443, 0.155822913646698) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.525, 0.11529141426086426) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.539, 0.12433670234680176) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48933333333333334, 0.10411413453022639) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.10897867059707642) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3385, 0.09669272619485855) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2995, 0.10973504322767258) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.444, 0.16042873561382293) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.1259219120144844) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.562, 0.12424433743953704) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.478, 0.10931616864601772) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.295, 0.11449104231595993) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.10377984529733658) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.312, 0.1053981648683548) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.456, 0.15859055256843566) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.539, 0.11461256957054138) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.539, 0.13275895309448243) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4856666666666667, 0.11184921890497207) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.11565786635875702) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.316, 0.0996008067727089) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.312, 0.11037249261140823) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.433, 0.16405591309070588) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.538, 0.1337636842727661) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.531, 0.13263355684280395) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.478, 0.12090576954682668) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2705, 0.12144082319736481) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.1023877689242363) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.10704442059993743) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.437, 0.15791680550575257) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.547, 0.12832376736402512) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.551, 0.12155369347333908) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4796666666666667, 0.11524513347943624) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.288, 0.11573314327001571) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.315, 0.1043920231461525) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3125, 0.10471887212991715) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.438, 0.16000085282325743) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.557, 0.13248025006055833) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.546, 0.12643339455127717) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4726666666666667, 0.11962515113751093) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.308, 0.11068995147943497) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.1049529994726181) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.31, 0.10910715585947037) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.446, 0.17706782984733582) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.544, 0.12556800705194474) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.548, 0.13436856496334076) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4746666666666667, 0.12322970565160116) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.299, 0.11417409640550613) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3285, 0.10574852496385574) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3275, 0.11189459735155105) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.421, 0.18637862968444824) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.553, 0.13474518251419068) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.539, 0.13460244739055632) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4713333333333333, 0.1379396524230639) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2855, 0.12259308582544327) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3375, 0.10732568970322609) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.298, 0.11733681905269623) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.444, 0.18006283402442932) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.12667472076416014) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.547, 0.14203015327453614) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47533333333333333, 0.1289941766858101) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.295, 0.12178330963850022) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3245, 0.10803318423032761) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3205, 0.11273664575815201) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.423, 0.19279308116436006) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.553, 0.12803941690921783) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.54, 0.1452153686285019) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4673333333333333, 0.14410080285867055) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.12660134810209275) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.329, 0.11099854409694672) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.307, 0.1174044957458973) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.434, 0.20702742445468902) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.544, 0.1479785087108612) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.518, 0.15358422255516052) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4723333333333333, 0.13800032456715902) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.1282656038403511) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.321, 0.11198917919397354) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3065, 0.11936545687913895) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.451, 0.17851990723609926) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.14930506563186646) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.544, 0.14814453023672103) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48233333333333334, 0.13734824911753338) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.12578851163387297) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.332, 0.11483707332611084) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3035, 0.12032943034172058) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.443, 0.19352116239070893) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.543, 0.14592168319225313) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.538, 0.15401075500249864) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4756666666666667, 0.14575469692548115) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.292, 0.12879550778865814) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.34, 0.11157598996162414) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.31, 0.12233161300420761) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.447, 0.19347207260131835) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.538, 0.1622837063074112) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.563, 0.1537024097442627) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.475, 0.14927130961418153) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.281, 0.13406939285993577) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.324, 0.11655195268988609) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.312, 0.12560383623838425) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.437, 0.19529978132247924) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.535, 0.16103403532505037) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.522, 0.15937112998962402) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.473, 0.15041684106985728) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2885, 0.13617937076091766) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3115, 0.11915834927558899) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.321, 0.12056135740876198) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.44, 0.20285267347097397) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.544, 0.15380113399028777) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.53, 0.1615872323513031) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.471, 0.15338505601882935) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.289, 0.1290321488380432) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3395, 0.11787844169139862) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.12375314092636108) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.445, 0.20400263917446138) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.51, 0.15965807223320008) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.536, 0.150258074760437) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.474, 0.155824059565862) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.288, 0.13484056687355042) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3045, 0.12210528919100762) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3055, 0.12537309646606445) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.44, 0.20253946149349214) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.55, 0.15694333803653718) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.536, 0.1550563974380493) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4716666666666667, 0.16091182589530945) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2845, 0.13323814058303832) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.1232570087313652) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2955, 0.12525915563106538) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.454, 0.1938822237253189) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.534, 0.1600250936150551) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.549, 0.1608188042640686) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4643333333333333, 0.15607218352953592) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2875, 0.1329959332346916) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.1264787631034851) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2945, 0.12525009071826934) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.441, 0.2165891135931015) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.15624289417266846) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.537, 0.15940819573402404) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47633333333333333, 0.16153011492888134) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2855, 0.13577819496393204) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3125, 0.12452465093135834) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.12709034496545793) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.428, 0.1923006660938263) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.551, 0.1519157508611679) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.542, 0.15697693037986754) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48033333333333333, 0.15047072664896646) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.299, 0.13817716199159621) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.309, 0.12450840157270432) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.11604886204004287) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.425, 0.22664693713188172) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.539, 0.16574216067790984) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.529, 0.16646242320537566) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4826666666666667, 0.16277533117930093) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2785, 0.14323392581939698) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2975, 0.1312096330523491) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3085, 0.13397766131162644) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.437, 0.215106308221817) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.523, 0.16107512152194978) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.524, 0.1642057585120201) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48633333333333334, 0.15273401258389155) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2795, 0.13956903034448623) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.13096565848588942) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3015, 0.13290143257379533) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.439, 0.22027416253089904) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.522, 0.1736349251270294) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.514, 0.18055958139896394) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49066666666666664, 0.157527714908123) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2965, 0.13859673738479614) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3085, 0.1251054012775421) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.13562376683950425) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.457, 0.20779988825321197) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.531, 0.17480323553085328) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.538, 0.18024044811725617) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.473, 0.1643500742514928) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2815, 0.1436365423798561) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.313, 0.1245477301478386) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3065, 0.13079687631130218) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.436, 0.22416346096992493) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.538, 0.18853447914123536) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.517, 0.18230247259140014) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.487, 0.14251910543441773) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.1450950381755829) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.302, 0.13247154361009597) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.294, 0.1345928241610527) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.41, 0.21357240533828736) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.547, 0.18302853190898896) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.533, 0.18473738494515418) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.485, 0.16363080461819968) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2675, 0.1460120422244072) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.306, 0.13035309213399887) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3045, 0.1395015991926193) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.443, 0.23159897661209106) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.519, 0.1832781524658203) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.542, 0.17343458353728056) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.491, 0.16902192171414693) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2875, 0.14453332620859147) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.295, 0.1383747027516365) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3095, 0.13285395401716232) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.44, 0.21911029970645904) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.528, 0.18153728544712067) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.53, 0.18123145521245898) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47533333333333333, 0.19137024148305257) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.283, 0.14328116565942764) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.292, 0.13433307868242264) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.316, 0.1391996577978134) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.44, 0.21115443015098573) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.517, 0.18780502033233643) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.545, 0.1770661048591137) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4806666666666667, 0.15888749543825786) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.297, 0.1433657359480858) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.303, 0.12935740852355956) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3, 0.14146618610620498) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.0255, 0.1164030305147171), (0.202, 0.1409693197607994), (0.2255, 0.18419047766923904), (0.236, 0.23539444771409035), (0.2385, 0.28242079994082453), (0.234, 0.3295216091573238), (0.236, 0.34206185898184777), (0.2275, 0.3519208933711052), (0.226, 0.3555838957130909), (0.233, 0.36672219878435136), (0.23, 0.3747866123318672), (0.249, 0.1742353835105896), (0.247, 0.15744064807891844), (0.2545, 0.1469199201464653), (0.2575, 0.12857547706365585), (0.2695, 0.11668908429145813), (0.267, 0.115085931122303), (0.293, 0.10433760607242584), (0.277, 0.10939805072546005), (0.288, 0.11221517533063889), (0.2945, 0.10694750386476516), (0.2865, 0.10897867059707642), (0.295, 0.11449104231595993), (0.2865, 0.11565786635875702), (0.2705, 0.12144082319736481), (0.288, 0.11573314327001571), (0.308, 0.11068995147943497), (0.299, 0.11417409640550613), (0.2855, 0.12259308582544327), (0.295, 0.12178330963850022), (0.2895, 0.12660134810209275), (0.269, 0.1282656038403511), (0.2865, 0.12578851163387297), (0.292, 0.12879550778865814), (0.281, 0.13406939285993577), (0.2885, 0.13617937076091766), (0.289, 0.1290321488380432), (0.288, 0.13484056687355042), (0.2845, 0.13323814058303832), (0.2875, 0.1329959332346916), (0.2855, 0.13577819496393204), (0.299, 0.13817716199159621), (0.2785, 0.14323392581939698), (0.2795, 0.13956903034448623), (0.2965, 0.13859673738479614), (0.2815, 0.1436365423798561), (0.269, 0.1450950381755829), (0.2675, 0.1460120422244072), (0.2875, 0.14453332620859147), (0.283, 0.14328116565942764), (0.297, 0.1433657359480858)]
TEST: 
[(0.0255, 0.11546436005830765), (0.20425, 0.13922780287265776), (0.2295, 0.18229064029455186), (0.23575, 0.2329094489812851), (0.2405, 0.27896194326877594), (0.24175, 0.32360210621356966), (0.2375, 0.3360581982135773), (0.241, 0.3409562067985535), (0.22925, 0.34628960192203523), (0.23775, 0.3559122678041458), (0.237, 0.36720366168022156), (0.255, 0.17330638229846954), (0.24425, 0.1578964871764183), (0.2535, 0.14686508959531783), (0.26825, 0.12715112084150315), (0.26775, 0.11608562016487121), (0.26775, 0.11407570254802704), (0.29025, 0.10363398987054825), (0.2745, 0.11020587682723999), (0.26725, 0.1136985986828804), (0.271, 0.10851394879817963), (0.28525, 0.10679655796289445), (0.2785, 0.11296710008382797), (0.2775, 0.1154277350306511), (0.2685, 0.12044738394021988), (0.28525, 0.11449632632732391), (0.2955, 0.11120673894882202), (0.27825, 0.11479949098825455), (0.27475, 0.12307514262199402), (0.28625, 0.12073189783096314), (0.28575, 0.12309671247005463), (0.281, 0.12563259518146516), (0.287, 0.12375201088190078), (0.285, 0.12577344870567322), (0.27525, 0.13381002312898635), (0.28, 0.13348378145694734), (0.2855, 0.12924818742275238), (0.29025, 0.1315508017539978), (0.2685, 0.13224863094091416), (0.27725, 0.13188055408000945), (0.2785, 0.13428558999300003), (0.28825, 0.13749550503492355), (0.276, 0.14007722049951554), (0.28, 0.1367322599887848), (0.29025, 0.13410020875930787), (0.27475, 0.13942098969221114), (0.26075, 0.1406868951320648), (0.2605, 0.1415232456922531), (0.279, 0.14039093619585039), (0.27625, 0.14098053175210953), (0.273, 0.14042591899633408)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.30      0.16      0.21       100
           4       0.09      0.04      0.06       100
           5       0.27      0.39      0.32       100
           6       0.26      0.12      0.16       100
           8       0.61      0.19      0.29       100
           9       0.39      0.26      0.31       100
          15       0.18      0.26      0.21       100
          17       0.48      0.51      0.50       100
          18       0.29      0.24      0.26       100
          19       0.12      0.06      0.08       100
          22       0.26      0.27      0.27       100
          25       0.23      0.18      0.20       100
          27       0.20      0.23      0.21       100
          29       0.33      0.14      0.20       100
          37       0.26      0.16      0.20       100
          38       0.12      0.17      0.14       100
          40       0.24      0.22      0.23       100
          51       0.26      0.11      0.15       100
          54       0.29      0.31      0.30       100
          55       0.08      0.15      0.10       100
          57       0.27      0.40      0.32       100
          58       0.40      0.25      0.31       100
          61       0.51      0.30      0.38       100
          62       0.47      0.19      0.27       100
          66       0.27      0.11      0.16       100
          67       0.30      0.56      0.39       100
          68       0.66      0.63      0.65       100
          70       0.32      0.70      0.43       100
          71       0.58      0.60      0.59       100
          72       0.09      0.09      0.09       100
          74       0.20      0.19      0.19       100
          75       0.54      0.38      0.44       100
          76       0.40      0.65      0.50       100
          77       0.11      0.13      0.12       100
          78       0.15      0.27      0.20       100
          79       0.27      0.38      0.32       100
          80       0.11      0.07      0.09       100
          81       0.27      0.32      0.29       100
          84       0.19      0.14      0.16       100
          89       0.23      0.39      0.29       100

    accuracy                           0.27      4000
   macro avg       0.29      0.27      0.26      4000
weighted avg       0.29      0.27      0.26      4000

Collaboration_DC_1
VAL: 
[(0.0245, 0.11667730021476745), (0.24, 0.14089651209115983), (0.2775, 0.17498260675370694), (0.2855, 0.2166133507490158), (0.288, 0.2620709447786212), (0.2865, 0.3055267943590879), (0.2835, 0.32310113635659216), (0.284, 0.34383644759654997), (0.2845, 0.3382292886823416), (0.2735, 0.37581144222617147), (0.2735, 0.37991414946317675), (0.3125, 0.1550880949795246), (0.307, 0.1285801739692688), (0.314, 0.11616350397467613), (0.2915, 0.12402412170171738), (0.317, 0.10472912114858628), (0.309, 0.10465698421001435), (0.324, 0.09562921395897865), (0.3215, 0.09460829532146454), (0.32, 0.0980911580324173), (0.333, 0.09759107428789139), (0.3385, 0.09669272619485855), (0.305, 0.10377984529733658), (0.316, 0.0996008067727089), (0.33, 0.1023877689242363), (0.315, 0.1043920231461525), (0.3165, 0.1049529994726181), (0.3285, 0.10574852496385574), (0.3375, 0.10732568970322609), (0.3245, 0.10803318423032761), (0.329, 0.11099854409694672), (0.321, 0.11198917919397354), (0.332, 0.11483707332611084), (0.34, 0.11157598996162414), (0.324, 0.11655195268988609), (0.3115, 0.11915834927558899), (0.3395, 0.11787844169139862), (0.3045, 0.12210528919100762), (0.323, 0.1232570087313652), (0.312, 0.1264787631034851), (0.3125, 0.12452465093135834), (0.309, 0.12450840157270432), (0.2975, 0.1312096330523491), (0.312, 0.13096565848588942), (0.3085, 0.1251054012775421), (0.313, 0.1245477301478386), (0.302, 0.13247154361009597), (0.306, 0.13035309213399887), (0.295, 0.1383747027516365), (0.292, 0.13433307868242264), (0.303, 0.12935740852355956)]
TEST: 
[(0.02375, 0.1157863432765007), (0.226, 0.13846950680017472), (0.27, 0.17275238758325576), (0.27925, 0.21144932818412782), (0.2785, 0.25662729251384736), (0.28175, 0.29760614812374114), (0.28025, 0.315565526843071), (0.27875, 0.3339323287010193), (0.2765, 0.3265263646841049), (0.2735, 0.35724844002723694), (0.27625, 0.361671516418457), (0.295, 0.15576082986593245), (0.298, 0.1297787948846817), (0.30425, 0.11583900609612464), (0.28975, 0.12502015775442124), (0.31525, 0.10692437356710434), (0.3105, 0.10497127795219421), (0.3335, 0.09387199607491493), (0.3355, 0.09378867563605309), (0.3285, 0.09910360276699066), (0.32925, 0.0979948687851429), (0.33275, 0.09978252547979355), (0.3125, 0.105364144384861), (0.3215, 0.10226324480772019), (0.3325, 0.10459852546453476), (0.3185, 0.10568003711104393), (0.3155, 0.10832628041505814), (0.325, 0.10815562683343888), (0.3255, 0.11063893288373947), (0.32175, 0.11247001618146896), (0.33375, 0.11653794819116592), (0.32675, 0.11560682249069214), (0.33025, 0.11572429460287094), (0.3205, 0.11709163373708725), (0.32225, 0.12184663280844689), (0.31375, 0.12417955949902534), (0.33025, 0.1249795692563057), (0.3085, 0.126512942135334), (0.32575, 0.1274147822856903), (0.3145, 0.13025941634178162), (0.309, 0.12671271353960037), (0.31375, 0.12578427004814147), (0.30625, 0.13387315022945404), (0.30925, 0.13164189326763154), (0.31875, 0.1254168557524681), (0.314, 0.12426074409484864), (0.31375, 0.1345188113451004), (0.31975, 0.1319700444340706), (0.30575, 0.13924641382694244), (0.3045, 0.13844405776262284), (0.32125, 0.13066185170412065)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.39      0.12      0.18       100
           4       0.13      0.05      0.07       100
           5       0.33      0.40      0.36       100
           6       0.43      0.30      0.35       100
           8       0.52      0.16      0.24       100
           9       0.54      0.25      0.34       100
          10       0.33      0.33      0.33       100
          12       0.23      0.40      0.29       100
          13       0.24      0.34      0.28       100
          16       0.36      0.35      0.36       100
          17       0.30      0.39      0.34       100
          19       0.30      0.17      0.22       100
          21       0.41      0.50      0.45       100
          24       0.49      0.56      0.52       100
          28       0.38      0.45      0.41       100
          29       0.41      0.16      0.23       100
          33       0.30      0.46      0.36       100
          34       0.32      0.25      0.28       100
          35       0.22      0.22      0.22       100
          36       0.32      0.25      0.28       100
          37       0.19      0.22      0.20       100
          40       0.24      0.24      0.24       100
          41       0.50      0.52      0.51       100
          42       0.18      0.26      0.21       100
          46       0.27      0.22      0.24       100
          51       0.25      0.24      0.25       100
          59       0.31      0.44      0.36       100
          60       0.53      0.28      0.37       100
          63       0.25      0.37      0.30       100
          66       0.23      0.15      0.18       100
          67       0.37      0.49      0.42       100
          68       0.46      0.57      0.51       100
          70       0.50      0.68      0.58       100
          74       0.15      0.14      0.15       100
          75       0.53      0.50      0.51       100
          80       0.11      0.07      0.08       100
          81       0.19      0.30      0.23       100
          95       0.42      0.50      0.46       100
          96       0.42      0.25      0.31       100
          97       0.23      0.30      0.26       100

    accuracy                           0.32      4000
   macro avg       0.33      0.32      0.31      4000
weighted avg       0.33      0.32      0.31      4000

Collaboration_DC_2
VAL: 
[(0.0265, 0.11653024744987488), (0.228, 0.13209930661320687), (0.2665, 0.1748268899023533), (0.2805, 0.2259693077802658), (0.282, 0.27517466694116594), (0.275, 0.30897353373467923), (0.282, 0.3220739869475365), (0.277, 0.3389838016331196), (0.2815, 0.35674322494864463), (0.2725, 0.37959419986605647), (0.274, 0.3896309501230717), (0.284, 0.15275873267650605), (0.283, 0.1389934655725956), (0.3005, 0.12983604615926742), (0.291, 0.12397932589054107), (0.287, 0.11596698594093323), (0.2915, 0.11860467350482941), (0.3105, 0.10125028443336487), (0.3065, 0.09779721695184708), (0.3035, 0.10297078883647919), (0.319, 0.10252565604448319), (0.2995, 0.10973504322767258), (0.312, 0.1053981648683548), (0.312, 0.11037249261140823), (0.317, 0.10704442059993743), (0.3125, 0.10471887212991715), (0.31, 0.10910715585947037), (0.3275, 0.11189459735155105), (0.298, 0.11733681905269623), (0.3205, 0.11273664575815201), (0.307, 0.1174044957458973), (0.3065, 0.11936545687913895), (0.3035, 0.12032943034172058), (0.31, 0.12233161300420761), (0.312, 0.12560383623838425), (0.321, 0.12056135740876198), (0.2985, 0.12375314092636108), (0.3055, 0.12537309646606445), (0.2955, 0.12525915563106538), (0.2945, 0.12525009071826934), (0.314, 0.12709034496545793), (0.317, 0.11604886204004287), (0.3085, 0.13397766131162644), (0.3015, 0.13290143257379533), (0.2985, 0.13562376683950425), (0.3065, 0.13079687631130218), (0.294, 0.1345928241610527), (0.3045, 0.1395015991926193), (0.3095, 0.13285395401716232), (0.316, 0.1391996577978134), (0.3, 0.14146618610620498)]
TEST: 
[(0.02925, 0.11562418437004089), (0.2255, 0.13116226786375046), (0.2635, 0.17216267031431198), (0.27125, 0.221975798368454), (0.28075, 0.26895166051387787), (0.273, 0.3026706259250641), (0.28575, 0.3150892233848572), (0.28225, 0.33161575770378116), (0.28, 0.3485282083749771), (0.2795, 0.3737938621044159), (0.271, 0.3809756041765213), (0.28425, 0.1546206477880478), (0.28625, 0.14005081361532212), (0.29325, 0.13095898979902268), (0.29175, 0.12250970011949538), (0.30325, 0.11379161828756332), (0.29375, 0.11853777372837067), (0.31525, 0.10169337230920791), (0.3145, 0.09808208668231964), (0.302, 0.10357027545571328), (0.31625, 0.10146831846237182), (0.3095, 0.10737574613094329), (0.31, 0.1055340719819069), (0.3035, 0.10878077721595764), (0.32275, 0.10559997808933258), (0.3175, 0.10467341098189353), (0.3135, 0.11067631465196609), (0.314, 0.11197680443525314), (0.2945, 0.11927770429849624), (0.30675, 0.11367014181613923), (0.29625, 0.11820172268152238), (0.301, 0.11706695592403411), (0.29925, 0.11875200551748276), (0.3185, 0.1198132780790329), (0.294, 0.1271558410525322), (0.31125, 0.12068780821561813), (0.3105, 0.12383684033155441), (0.29875, 0.12675876766443253), (0.306, 0.12485555517673493), (0.30625, 0.12775766533613206), (0.3145, 0.12792325508594513), (0.31375, 0.11902372759580612), (0.29225, 0.13531819808483123), (0.28525, 0.13187427985668182), (0.2905, 0.1384301598072052), (0.28875, 0.13403235679864883), (0.28625, 0.1329574989080429), (0.29125, 0.13944594490528106), (0.298, 0.1349762209057808), (0.30775, 0.13606986379623412), (0.3015, 0.13803844672441482)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.67      0.43      0.52       100
           2       0.35      0.32      0.33       100
           3       0.15      0.12      0.13       100
           4       0.13      0.07      0.09       100
           5       0.30      0.31      0.31       100
           6       0.28      0.26      0.27       100
           7       0.26      0.53      0.35       100
           8       0.35      0.11      0.17       100
           9       0.68      0.27      0.39       100
          14       0.16      0.13      0.14       100
          17       0.38      0.36      0.37       100
          19       0.24      0.10      0.14       100
          23       0.49      0.47      0.48       100
          29       0.45      0.14      0.21       100
          30       0.32      0.20      0.25       100
          31       0.21      0.36      0.27       100
          32       0.22      0.27      0.24       100
          37       0.24      0.16      0.19       100
          39       0.39      0.32      0.35       100
          40       0.27      0.17      0.21       100
          49       0.37      0.30      0.33       100
          51       0.24      0.14      0.18       100
          53       0.56      0.52      0.54       100
          64       0.12      0.25      0.17       100
          66       0.17      0.05      0.08       100
          67       0.26      0.54      0.35       100
          68       0.73      0.63      0.68       100
          69       0.43      0.60      0.50       100
          70       0.27      0.74      0.39       100
          74       0.17      0.13      0.15       100
          75       0.53      0.34      0.41       100
          80       0.10      0.09      0.10       100
          81       0.32      0.35      0.33       100
          87       0.26      0.35      0.30       100
          88       0.16      0.26      0.20       100
          90       0.21      0.28      0.24       100
          91       0.35      0.50      0.41       100
          92       0.08      0.02      0.03       100
          94       0.54      0.64      0.58       100
          99       0.24      0.23      0.23       100

    accuracy                           0.30      4000
   macro avg       0.32      0.30      0.29      4000
weighted avg       0.32      0.30      0.29      4000

do_assignment: None
seeds: [72]
name: naive-cifar100-feddf72
score_metric: contrloss
aggregation: <function fed_df at 0x7e32065bce50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=72
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[34, 70, 1, 68, 10, 46, 13, 77, 51, 2, 64, 38, 41, 55, 58, 99, 78, 65, 37, 86, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([34, 70,  1, 68, 10, 46, 13, 77, 51,  2, 64, 38, 41, 55, 58, 99, 78, 65,
        37, 86,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0215, 0.11645013678073883)
DC 1, val_set_size=2000, COIs=[24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21, 5, 6, 83, 85, 73, 35, 7, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21,  5,  6, 83, 85, 73,
        35,  7,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0275, 0.11640736758708954)
DC 2, val_set_size=2000, COIs=[25, 54, 11, 74, 82, 8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90, 26, 71, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([25, 54, 11, 74, 82,  8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90,
        26, 71,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0235, 0.1164862197637558)
D00: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D01: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D02: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D03: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D04: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D05: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D06: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D07: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D08: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D09: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D010: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D011: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D012: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D013: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D014: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D015: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D016: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D017: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D018: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D019: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D020: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D021: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D022: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D023: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.291, 0.08497226923704147) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.332, 0.07957457721233369) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2945, 0.08403059768676757) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.343, 0.07628667795658112) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3855, 0.07020996993780136) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.07511349219083786) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3585, 0.0740847366154194) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.414, 0.06687970146536827) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.0708142186999321) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.374, 0.07273315891623497) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4335, 0.0658234624862671) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3805, 0.07222982186079026) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3845, 0.07579068344831466) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.06705050879716873) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.07331929564476013) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.377, 0.07975218152999877) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4355, 0.07025927007198333) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3895, 0.07748385769128799) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3805, 0.08165366420149803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4255, 0.07267308932542801) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3855, 0.07690354523062706) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3845, 0.08291833147406578) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.432, 0.07444100111722946) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3995, 0.07976247310638428) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3605, 0.08823728621006012) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4355, 0.07796246835589409) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.08523234283924103) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3775, 0.09368679162859916) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.432, 0.08255261194705964) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3885, 0.09098758870363235) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3635, 0.10313660722970963) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4245, 0.08658368527889251) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.386, 0.09818302342295647) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3675, 0.11190314757823944) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.429, 0.09272546565532684) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3885, 0.10605366653203964) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3645, 0.1260758934020996) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.09910261601209641) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.12366936612129212) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3465, 0.1402354825735092) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4235, 0.10502498203516006) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.376, 0.12308341163396835) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3515, 0.1373621432185173) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3985, 0.1161362898349762) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3645, 0.12869939321279525) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.343, 0.15157052206993102) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.397, 0.12426843142509461) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.36, 0.13963903698325159) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3615, 0.15620428961515426) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.41, 0.12373245644569397) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.15126012831926347) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.367, 0.15163118773698805) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.404, 0.13566769921779634) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.375, 0.13551779860258104) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3425, 0.1668341619372368) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4095, 0.13017844879627227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.362, 0.1537670794725418) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3595, 0.16363984334468842) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4015, 0.15036268216371537) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3655, 0.1600914068222046) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.341, 0.1799900358915329) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3945, 0.15158034706115722) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3585, 0.169908312022686) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.1788586890101433) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4045, 0.1492008132338524) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.352, 0.17097209864854812) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.34, 0.17608931869268418) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.404, 0.14980660831928252) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3645, 0.17557603627443313) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3425, 0.19765738248825074) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.399, 0.15600233942270278) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3575, 0.17819725173711776) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.19030320101976395) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.394, 0.15981575310230256) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3835, 0.16568661737442017) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3495, 0.19327288967370987) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4045, 0.17112087225914002) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3655, 0.17405910104513167) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3515, 0.19562546199560166) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.403, 0.16394302570819855) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.364, 0.17927150225639343) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.1969534205198288) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3855, 0.17684953260421754) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.35, 0.18844600027799607) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3525, 0.1959137950539589) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.397, 0.17688196450471877) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3505, 0.18901989614963532) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.355, 0.21055620640516282) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.404, 0.17058906227350235) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.1994676352739334) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.346, 0.20622625613212586) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4105, 0.16068390434980392) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3575, 0.20609816968441008) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.341, 0.212841943860054) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4045, 0.18727786445617675) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.18867326843738555) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.20741249424219133) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3935, 0.1960975911617279) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3745, 0.20117612850666047) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.22077514564990997) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4075, 0.1836296173930168) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3755, 0.20451885449886323) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3545, 0.20992021214962006) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.18575429904460908) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3635, 0.21102650487422944) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.352, 0.2167677274942398) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4135, 0.17895757257938386) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3595, 0.21183044695854186) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3435, 0.22879884445667267) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.393, 0.18793128108978271) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3535, 0.21093521863222123) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.22101994282007217) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4025, 0.18818774557113646) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.366, 0.21674669295549392) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.349, 0.21790059041976928) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.402, 0.20397374176979066) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.21613496971130372) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3465, 0.2208312237262726) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3985, 0.20123677653074265) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3655, 0.21854001820087432) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.351, 0.22407706892490387) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.1968482599258423) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.364, 0.21182991141080856) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.2315271474123001) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3955, 0.20468918573856354) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.21802656924724578) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.24372756934165954) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.398, 0.20985576754808427) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.369, 0.21803600353002547) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3565, 0.23669272458553314) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3835, 0.21323282325267792) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.365, 0.23193722915649415) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.24384444332122804) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.21161544156074524) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.363, 0.21892670106887818) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3575, 0.2415339269042015) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.399, 0.20630923020839692) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3585, 0.23300156104564668) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.2406809378862381) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.2179243268966675) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.23868056166172028) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.23743738895654679) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.399, 0.2149672487974167) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3635, 0.24211785435676575) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3375, 0.24946901822090148) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3865, 0.21383706331253052) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.23176005816459655) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3635, 0.24184268736839296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.22188973999023437) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.25169710958004) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.0215, 0.11645013678073883), (0.291, 0.08497226923704147), (0.343, 0.07628667795658112), (0.3585, 0.0740847366154194), (0.374, 0.07273315891623497), (0.3845, 0.07579068344831466), (0.377, 0.07975218152999877), (0.3805, 0.08165366420149803), (0.3845, 0.08291833147406578), (0.3605, 0.08823728621006012), (0.3775, 0.09368679162859916), (0.3635, 0.10313660722970963), (0.3675, 0.11190314757823944), (0.3645, 0.1260758934020996), (0.3465, 0.1402354825735092), (0.3515, 0.1373621432185173), (0.343, 0.15157052206993102), (0.3615, 0.15620428961515426), (0.367, 0.15163118773698805), (0.3425, 0.1668341619372368), (0.3595, 0.16363984334468842), (0.341, 0.1799900358915329), (0.345, 0.1788586890101433), (0.34, 0.17608931869268418), (0.3425, 0.19765738248825074), (0.338, 0.19030320101976395), (0.3495, 0.19327288967370987), (0.3515, 0.19562546199560166), (0.3535, 0.1969534205198288), (0.3525, 0.1959137950539589), (0.355, 0.21055620640516282), (0.346, 0.20622625613212586), (0.341, 0.212841943860054), (0.345, 0.20741249424219133), (0.3485, 0.22077514564990997), (0.3545, 0.20992021214962006), (0.352, 0.2167677274942398), (0.3435, 0.22879884445667267), (0.3485, 0.22101994282007217), (0.349, 0.21790059041976928), (0.3465, 0.2208312237262726), (0.351, 0.22407706892490387), (0.354, 0.2315271474123001), (0.3535, 0.24372756934165954), (0.3565, 0.23669272458553314), (0.3535, 0.24384444332122804), (0.3575, 0.2415339269042015), (0.354, 0.2406809378862381), (0.3475, 0.23743738895654679), (0.3375, 0.24946901822090148), (0.3635, 0.24184268736839296)]
TEST: 
[(0.02275, 0.1154470106959343), (0.31, 0.08349424505233764), (0.35375, 0.07513207891583443), (0.367, 0.07273786610364914), (0.3785, 0.07122405529022217), (0.384, 0.07382093992829322), (0.383, 0.07709089893102646), (0.3925, 0.07901869502663612), (0.39225, 0.08105197349190713), (0.3885, 0.08542432880401611), (0.385, 0.0897522261440754), (0.3725, 0.09949414229393005), (0.37025, 0.10594706755876541), (0.36275, 0.1214669618010521), (0.3555, 0.13267365044355392), (0.35875, 0.13179253137111663), (0.3545, 0.14481150108575822), (0.3545, 0.15564941173791885), (0.368, 0.14848111218214036), (0.36, 0.1616519981622696), (0.34925, 0.1559842129945755), (0.35125, 0.17330323648452758), (0.358, 0.17259911048412324), (0.35625, 0.1671719157099724), (0.33575, 0.18827358627319335), (0.3515, 0.18431092041730882), (0.3655, 0.18492443692684174), (0.35425, 0.1929389330148697), (0.34675, 0.19343193221092225), (0.35225, 0.18939367425441742), (0.34775, 0.20444944119453431), (0.3445, 0.20019456177949904), (0.3475, 0.2052581744790077), (0.3445, 0.2011375761628151), (0.34575, 0.21728558802604675), (0.35075, 0.20483939599990844), (0.3485, 0.2073739002943039), (0.34825, 0.22206332910060883), (0.35925, 0.21793894499540328), (0.3465, 0.2192233925461769), (0.34875, 0.21731958311796187), (0.3525, 0.21626021510362625), (0.3645, 0.22598986452817918), (0.349, 0.24054714846611022), (0.34625, 0.23112240159511566), (0.34925, 0.23926577579975128), (0.34675, 0.2341459972858429), (0.35025, 0.23828775590658188), (0.338, 0.24700206875801087), (0.33075, 0.25616628491878507), (0.34825, 0.2395400093793869)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.46      0.43      0.45       100
           2       0.26      0.22      0.24       100
           3       0.20      0.27      0.23       100
           9       0.44      0.52      0.48       100
          10       0.34      0.18      0.24       100
          13       0.29      0.20      0.24       100
          16       0.28      0.29      0.28       100
          18       0.27      0.33      0.30       100
          23       0.56      0.59      0.58       100
          29       0.31      0.30      0.30       100
          34       0.22      0.22      0.22       100
          37       0.24      0.22      0.23       100
          38       0.19      0.18      0.18       100
          39       0.39      0.41      0.40       100
          41       0.69      0.51      0.59       100
          43       0.30      0.28      0.29       100
          44       0.17      0.15      0.16       100
          46       0.19      0.24      0.21       100
          47       0.71      0.63      0.67       100
          48       0.44      0.49      0.46       100
          51       0.37      0.19      0.25       100
          55       0.20      0.19      0.19       100
          58       0.35      0.37      0.36       100
          64       0.15      0.12      0.13       100
          65       0.20      0.17      0.19       100
          68       0.78      0.62      0.69       100
          69       0.51      0.52      0.52       100
          70       0.58      0.52      0.55       100
          76       0.54      0.57      0.55       100
          77       0.21      0.18      0.19       100
          78       0.20      0.26      0.23       100
          79       0.34      0.36      0.35       100
          81       0.29      0.30      0.29       100
          86       0.41      0.44      0.43       100
          87       0.32      0.44      0.37       100
          88       0.16      0.25      0.20       100
          89       0.38      0.37      0.37       100
          91       0.44      0.51      0.47       100
          94       0.61      0.57      0.59       100
          99       0.26      0.32      0.28       100

    accuracy                           0.35      4000
   macro avg       0.36      0.35      0.35      4000
weighted avg       0.36      0.35      0.35      4000

No_Competition_DC_1
VAL: 
[(0.0275, 0.11640736758708954), (0.332, 0.07957457721233369), (0.3855, 0.07020996993780136), (0.414, 0.06687970146536827), (0.4335, 0.0658234624862671), (0.446, 0.06705050879716873), (0.4355, 0.07025927007198333), (0.4255, 0.07267308932542801), (0.432, 0.07444100111722946), (0.4355, 0.07796246835589409), (0.432, 0.08255261194705964), (0.4245, 0.08658368527889251), (0.429, 0.09272546565532684), (0.416, 0.09910261601209641), (0.4235, 0.10502498203516006), (0.3985, 0.1161362898349762), (0.397, 0.12426843142509461), (0.41, 0.12373245644569397), (0.404, 0.13566769921779634), (0.4095, 0.13017844879627227), (0.4015, 0.15036268216371537), (0.3945, 0.15158034706115722), (0.4045, 0.1492008132338524), (0.404, 0.14980660831928252), (0.399, 0.15600233942270278), (0.394, 0.15981575310230256), (0.4045, 0.17112087225914002), (0.403, 0.16394302570819855), (0.3855, 0.17684953260421754), (0.397, 0.17688196450471877), (0.404, 0.17058906227350235), (0.4105, 0.16068390434980392), (0.4045, 0.18727786445617675), (0.3935, 0.1960975911617279), (0.4075, 0.1836296173930168), (0.391, 0.18575429904460908), (0.4135, 0.17895757257938386), (0.393, 0.18793128108978271), (0.4025, 0.18818774557113646), (0.402, 0.20397374176979066), (0.3985, 0.20123677653074265), (0.395, 0.1968482599258423), (0.3955, 0.20468918573856354), (0.398, 0.20985576754808427), (0.3835, 0.21323282325267792), (0.381, 0.21161544156074524), (0.399, 0.20630923020839692), (0.381, 0.2179243268966675), (0.399, 0.2149672487974167), (0.3865, 0.21383706331253052), (0.39, 0.22188973999023437)]
TEST: 
[(0.02725, 0.11549289643764496), (0.3265, 0.0798783357143402), (0.38575, 0.07019417241215706), (0.408, 0.06593622267246246), (0.41825, 0.06532183849811554), (0.4255, 0.06589274477958679), (0.422, 0.06919857779145241), (0.42375, 0.07165287908911705), (0.42575, 0.07419977793097496), (0.4205, 0.07804222291707992), (0.4155, 0.081672093719244), (0.41625, 0.08537640702724457), (0.42025, 0.09228158578276634), (0.42025, 0.09858135041594505), (0.416, 0.1038786444067955), (0.39225, 0.11403483045101166), (0.39125, 0.12347126483917237), (0.407, 0.12157744118571281), (0.3895, 0.13599883735179902), (0.402, 0.1318579021692276), (0.39525, 0.14660109055042267), (0.3875, 0.15202868634462358), (0.39175, 0.15613722449541093), (0.38625, 0.15102948915958406), (0.3885, 0.15892706966400147), (0.4005, 0.15769280195236207), (0.38525, 0.16940599393844605), (0.39625, 0.16219361263513565), (0.37275, 0.1784994143843651), (0.3965, 0.17863890755176545), (0.39325, 0.16921815651655198), (0.39275, 0.16277729749679565), (0.39675, 0.1828003266453743), (0.38925, 0.1890508616566658), (0.39625, 0.18019912874698638), (0.38125, 0.18448154628276825), (0.389, 0.1827015659213066), (0.39325, 0.1872260056734085), (0.39325, 0.19012947463989258), (0.386, 0.204429885327816), (0.3985, 0.1989578813314438), (0.38475, 0.1904098424911499), (0.3915, 0.20060450333356858), (0.38375, 0.21150191068649293), (0.38675, 0.2098924909234047), (0.386, 0.2096568620800972), (0.39125, 0.21112906354665756), (0.3905, 0.21494036149978638), (0.38075, 0.21535956472158432), (0.386, 0.21314321494102478), (0.39375, 0.22181373810768126)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.24      0.25      0.25       100
           5       0.38      0.39      0.38       100
           6       0.25      0.43      0.32       100
           7       0.34      0.31      0.32       100
           9       0.53      0.41      0.46       100
          15       0.33      0.15      0.21       100
          16       0.33      0.36      0.34       100
          18       0.29      0.21      0.24       100
          19       0.19      0.18      0.18       100
          21       0.42      0.58      0.49       100
          23       0.57      0.56      0.57       100
          24       0.49      0.57      0.53       100
          28       0.59      0.41      0.48       100
          29       0.33      0.29      0.31       100
          31       0.23      0.22      0.22       100
          32       0.29      0.25      0.27       100
          35       0.29      0.34      0.31       100
          39       0.51      0.48      0.49       100
          40       0.33      0.29      0.31       100
          43       0.38      0.23      0.29       100
          44       0.20      0.19      0.19       100
          47       0.34      0.60      0.43       100
          48       0.65      0.40      0.49       100
          49       0.48      0.53      0.50       100
          52       0.51      0.36      0.42       100
          59       0.27      0.21      0.23       100
          60       0.68      0.74      0.71       100
          69       0.58      0.54      0.56       100
          72       0.19      0.07      0.10       100
          73       0.57      0.55      0.56       100
          76       0.63      0.61      0.62       100
          79       0.36      0.44      0.40       100
          81       0.46      0.30      0.36       100
          83       0.40      0.62      0.49       100
          85       0.38      0.52      0.44       100
          87       0.39      0.37      0.38       100
          88       0.33      0.31      0.32       100
          89       0.28      0.45      0.35       100
          91       0.36      0.48      0.41       100
          94       0.63      0.55      0.59       100

    accuracy                           0.39      4000
   macro avg       0.40      0.39      0.39      4000
weighted avg       0.40      0.39      0.39      4000

No_Competition_DC_2
VAL: 
[(0.0235, 0.1164862197637558), (0.2945, 0.08403059768676757), (0.3525, 0.07511349219083786), (0.387, 0.0708142186999321), (0.3805, 0.07222982186079026), (0.403, 0.07331929564476013), (0.3895, 0.07748385769128799), (0.3855, 0.07690354523062706), (0.3995, 0.07976247310638428), (0.395, 0.08523234283924103), (0.3885, 0.09098758870363235), (0.386, 0.09818302342295647), (0.3885, 0.10605366653203964), (0.371, 0.12366936612129212), (0.376, 0.12308341163396835), (0.3645, 0.12869939321279525), (0.36, 0.13963903698325159), (0.3665, 0.15126012831926347), (0.375, 0.13551779860258104), (0.362, 0.1537670794725418), (0.3655, 0.1600914068222046), (0.3585, 0.169908312022686), (0.352, 0.17097209864854812), (0.3645, 0.17557603627443313), (0.3575, 0.17819725173711776), (0.3835, 0.16568661737442017), (0.3655, 0.17405910104513167), (0.364, 0.17927150225639343), (0.35, 0.18844600027799607), (0.3505, 0.18901989614963532), (0.3615, 0.1994676352739334), (0.3575, 0.20609816968441008), (0.354, 0.18867326843738555), (0.3745, 0.20117612850666047), (0.3755, 0.20451885449886323), (0.3635, 0.21102650487422944), (0.3595, 0.21183044695854186), (0.3535, 0.21093521863222123), (0.366, 0.21674669295549392), (0.355, 0.21613496971130372), (0.3655, 0.21854001820087432), (0.364, 0.21182991141080856), (0.3665, 0.21802656924724578), (0.369, 0.21803600353002547), (0.365, 0.23193722915649415), (0.363, 0.21892670106887818), (0.3585, 0.23300156104564668), (0.3665, 0.23868056166172028), (0.3635, 0.24211785435676575), (0.3665, 0.23176005816459655), (0.3615, 0.25169710958004)]
TEST: 
[(0.026, 0.11547955000400544), (0.30925, 0.08300889414548875), (0.362, 0.07392572778463363), (0.3895, 0.06897772151231765), (0.39325, 0.07068127378821373), (0.39875, 0.07178037282824516), (0.39475, 0.07533474490046502), (0.38775, 0.07527961137890815), (0.39375, 0.07894747474789619), (0.38825, 0.08360948768258095), (0.39, 0.0894540297985077), (0.38325, 0.09611470699310302), (0.3885, 0.10493373465538025), (0.362, 0.12371267294883728), (0.37475, 0.12042889964580536), (0.3705, 0.12466892910003662), (0.36625, 0.13522854256629943), (0.353, 0.15128652983903884), (0.36725, 0.1376228767633438), (0.35425, 0.15298307240009307), (0.35475, 0.15419574803113936), (0.3625, 0.16650536900758744), (0.35325, 0.17089863312244416), (0.35875, 0.17672407990694047), (0.35625, 0.17628878074884416), (0.3725, 0.16873781096935273), (0.37575, 0.17149567252397538), (0.35325, 0.18445661878585815), (0.3675, 0.18141183358430862), (0.35925, 0.18424234867095948), (0.371, 0.1970572226047516), (0.3545, 0.2026912187933922), (0.3665, 0.18762743985652924), (0.3655, 0.20186818301677703), (0.37725, 0.20231108552217483), (0.36375, 0.20484405303001404), (0.347, 0.21052469968795776), (0.35525, 0.21036438477039338), (0.36275, 0.21237123048305512), (0.35575, 0.21345858651399613), (0.3545, 0.217028013586998), (0.3635, 0.21225740349292754), (0.3705, 0.2186685990691185), (0.3545, 0.22021754640340804), (0.376, 0.22816479146480562), (0.36425, 0.21457692754268645), (0.3595, 0.23407310283184052), (0.367, 0.2334670833349228), (0.36525, 0.24010005331039427), (0.36125, 0.2330163881778717), (0.355, 0.25540109622478485)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.34      0.21      0.26       100
           8       0.51      0.43      0.46       100
           9       0.35      0.44      0.39       100
          11       0.31      0.31      0.31       100
          16       0.22      0.35      0.27       100
          18       0.34      0.25      0.29       100
          22       0.26      0.23      0.24       100
          23       0.44      0.66      0.53       100
          25       0.20      0.27      0.23       100
          26       0.18      0.28      0.22       100
          29       0.34      0.28      0.31       100
          36       0.33      0.31      0.32       100
          39       0.30      0.31      0.30       100
          42       0.25      0.13      0.17       100
          43       0.42      0.28      0.34       100
          44       0.20      0.23      0.21       100
          47       0.42      0.35      0.38       100
          48       0.55      0.53      0.54       100
          50       0.18      0.16      0.17       100
          54       0.40      0.32      0.36       100
          61       0.68      0.38      0.49       100
          66       0.28      0.23      0.25       100
          67       0.36      0.35      0.35       100
          69       0.52      0.45      0.48       100
          71       0.61      0.65      0.63       100
          74       0.29      0.17      0.21       100
          76       0.59      0.58      0.58       100
          79       0.32      0.36      0.34       100
          81       0.38      0.30      0.34       100
          82       0.67      0.56      0.61       100
          84       0.26      0.15      0.19       100
          87       0.44      0.38      0.41       100
          88       0.21      0.20      0.20       100
          89       0.30      0.32      0.31       100
          90       0.29      0.42      0.34       100
          91       0.42      0.48      0.45       100
          92       0.38      0.42      0.40       100
          93       0.26      0.32      0.29       100
          94       0.52      0.58      0.55       100
          96       0.29      0.57      0.39       100

    accuracy                           0.36      4000
   macro avg       0.36      0.35      0.35      4000
weighted avg       0.36      0.35      0.35      4000

Competition
DC 0, val_set_size=2000, COIs=[34, 70, 1, 68, 10, 46, 13, 77, 51, 2, 64, 38, 41, 55, 58, 99, 78, 65, 37, 86, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([34, 70,  1, 68, 10, 46, 13, 77, 51,  2, 64, 38, 41, 55, 58, 99, 78, 65,
        37, 86,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0215, 0.11645013678073883)
DC 1, val_set_size=2000, COIs=[24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21, 5, 6, 83, 85, 73, 35, 7, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21,  5,  6, 83, 85, 73,
        35,  7,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0275, 0.11640736758708954)
DC 2, val_set_size=2000, COIs=[25, 54, 11, 74, 82, 8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90, 26, 71, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([25, 54, 11, 74, 82,  8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90,
        26, 71,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0235, 0.1164862197637558)
D00: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D01: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D02: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D03: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D04: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D05: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D06: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D07: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D08: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D09: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D010: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D011: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D012: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D013: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D014: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D015: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D016: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D017: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D018: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D019: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D020: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D021: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D022: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D023: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO5', '(DO0']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.185, 0.1394462924897671) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.222, 0.13935426399111747) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.18, 0.13491899305582047) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.213, 0.17942911174893378) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.16746247287094593) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2205, 0.17120003962516786) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2165, 0.23395272874832154) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2805, 0.19949597765505314) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.2205458999723196) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.28582696156203746) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.285, 0.2471946467757225) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2445, 0.27404198637604715) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2195, 0.33815975838899615) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.29811707623302935) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2445, 0.3163145174086094) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO2', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2205, 0.34812394934892654) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.278, 0.32501332454383375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2465, 0.3261121627241373) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.219, 0.35236122843623163) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2735, 0.3312086882442236) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2475, 0.3333806322365999) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.225, 0.35829040582478044) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.33877271787822244) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2505, 0.34211264036595823) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2245, 0.3780776776075363) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.3588443279117346) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.34807383827865124) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2175, 0.3815228668451309) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2775, 0.3583696631938219) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2475, 0.36710547041893005) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO5', '(DO4']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO0', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.228, 0.3825542801320553) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.281, 0.3776941959261894) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.244, 0.38963437248766425) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2165, 0.4128353205323219) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.274, 0.40736699411273003) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2425, 0.3781762052178383) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.207, 0.41718874821066854) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.3859904466569424) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2435, 0.38954779537022116) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2165, 0.4090645455121994) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.3748312215656042) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2345, 0.3902101512402296) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.214, 0.40878739935159686) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.37677542120218277) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2435, 0.39192733745276925) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO4', '(DO5']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO3', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.209, 0.4097952565252781) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.3966861490756273) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.236, 0.3853566768318415) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2105, 0.39166604122519494) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.3589093612134457) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.24, 0.3817195177078247) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2135, 0.3911294265389442) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.37897317838668826) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.235, 0.3756011133044958) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.3990247991681099) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.271, 0.358011616140604) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.39528161742538215) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2125, 0.4086367009282112) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.3368848157525063) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2375, 0.3516854434609413) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO0', '(DO4']
DC 2 --> ['(DO1', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2015, 0.4024211955368519) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.33055195996165276) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.3500316118597984) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2115, 0.35632919135689733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.323097417473793) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2335, 0.3615770672559738) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.218, 0.38520609539747236) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.275, 0.32955744552612304) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.231, 0.34462098973989486) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.209, 0.3683128425627947) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.268, 0.3103841248005629) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2285, 0.3637440782189369) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.205, 0.37507420432567595) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.271, 0.29746592333912847) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2285, 0.3521324816942215) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO1', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2025, 0.35261729633808137) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.271, 0.2970067406594753) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.227, 0.34920141550898554) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.213, 0.3434788267016411) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.29422304731607435) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.239, 0.32142454069852827) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2125, 0.3348431220054626) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.2868510262966156) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.31801585978269575) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.205, 0.30661137130856514) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.275, 0.2822832220196724) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2385, 0.30138136154413225) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2135, 0.33735548782348634) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.2912707200050354) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.228, 0.3105791458636522) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2175, 0.31984028124809266) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2735, 0.2792799746990204) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.241, 0.3155825917944312) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2165, 0.34862056019902227) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.275, 0.2579727663397789) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.236, 0.3060592692494392) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.216, 0.3408636611402035) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.283, 0.2811203292906284) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2295, 0.3289744162708521) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2185, 0.3269304432272911) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.27390772113204004) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.236, 0.3151511668264866) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2125, 0.32620999413728713) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2755, 0.2665751968324184) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2365, 0.3138594026863575) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.224, 0.29628574174642563) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.27, 0.28640509384870527) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.231, 0.3366488032639027) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2175, 0.28898161777853965) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.27451632323861125) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2225, 0.28991836935281756) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2145, 0.33296107572317124) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.26175019061565397) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.235, 0.31859594719111917) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.212, 0.3133869397640228) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.28439075684547427) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2385, 0.31315754759311676) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2145, 0.30259317317605017) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.285371375143528) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2415, 0.3046124859452248) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO2', '(DO0']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO4', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.227, 0.2964507662951946) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2765, 0.25065843150019645) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2295, 0.32304437273740766) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2105, 0.31163718736171725) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.27230384692549703) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.243, 0.3094056808650494) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.215, 0.2943444458246231) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.2938342723548412) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.233, 0.3042507859170437) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2105, 0.30673252588510513) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.27456031531095504) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.31231916350126265) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.206, 0.2943505013138056) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.278, 0.2705152726173401) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.241, 0.2894870994091034) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO3', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2215, 0.32497838002443313) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.284, 0.28639283043146135) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.30717101350426673) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.32291182109713557) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.274, 0.2824349771738052) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2415, 0.2945760402083397) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2235, 0.320761921107769) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2835, 0.2615997050702572) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.231, 0.32298513513803484) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.218, 0.3282787028551102) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.27697792410850525) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.241, 0.2896116855740547) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2095, 0.3112420035004616) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.271833514213562) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2465, 0.3037042475938797) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.0215, 0.11645013678073883), (0.185, 0.1394462924897671), (0.213, 0.17942911174893378), (0.2165, 0.23395272874832154), (0.227, 0.28582696156203746), (0.2195, 0.33815975838899615), (0.2205, 0.34812394934892654), (0.219, 0.35236122843623163), (0.225, 0.35829040582478044), (0.2245, 0.3780776776075363), (0.2175, 0.3815228668451309), (0.228, 0.3825542801320553), (0.2165, 0.4128353205323219), (0.207, 0.41718874821066854), (0.2165, 0.4090645455121994), (0.214, 0.40878739935159686), (0.209, 0.4097952565252781), (0.2105, 0.39166604122519494), (0.2135, 0.3911294265389442), (0.22, 0.3990247991681099), (0.2125, 0.4086367009282112), (0.2015, 0.4024211955368519), (0.2115, 0.35632919135689733), (0.218, 0.38520609539747236), (0.209, 0.3683128425627947), (0.205, 0.37507420432567595), (0.2025, 0.35261729633808137), (0.213, 0.3434788267016411), (0.2125, 0.3348431220054626), (0.205, 0.30661137130856514), (0.2135, 0.33735548782348634), (0.2175, 0.31984028124809266), (0.2165, 0.34862056019902227), (0.216, 0.3408636611402035), (0.2185, 0.3269304432272911), (0.2125, 0.32620999413728713), (0.224, 0.29628574174642563), (0.2175, 0.28898161777853965), (0.2145, 0.33296107572317124), (0.212, 0.3133869397640228), (0.2145, 0.30259317317605017), (0.227, 0.2964507662951946), (0.2105, 0.31163718736171725), (0.215, 0.2943444458246231), (0.2105, 0.30673252588510513), (0.206, 0.2943505013138056), (0.2215, 0.32497838002443313), (0.22, 0.32291182109713557), (0.2235, 0.320761921107769), (0.218, 0.3282787028551102), (0.2095, 0.3112420035004616)]
TEST: 
[(0.02275, 0.1154470106959343), (0.1925, 0.13687621366977692), (0.211, 0.17572556984424592), (0.22275, 0.22978082340955736), (0.22475, 0.28121566617488863), (0.2255, 0.3326349620819092), (0.222, 0.3409484530687332), (0.22175, 0.34853160643577574), (0.22375, 0.35279385638237), (0.2225, 0.37341776096820833), (0.21825, 0.37870204532146456), (0.21875, 0.3784029405117035), (0.2185, 0.41093029475212095), (0.21125, 0.41046704137325285), (0.21725, 0.4060607353448868), (0.21375, 0.40725327444076537), (0.21375, 0.4078509258031845), (0.21075, 0.3911877270936966), (0.21825, 0.3872293254137039), (0.21825, 0.3974796723127365), (0.2145, 0.4084990538358688), (0.208, 0.4004278652667999), (0.2135, 0.35443701684474943), (0.211, 0.3877208142280579), (0.2095, 0.37182793343067166), (0.21425, 0.3764611750841141), (0.206, 0.3510202525854111), (0.212, 0.3409863121509552), (0.21225, 0.3305604614019394), (0.20925, 0.3047204657793045), (0.2065, 0.3354159091711044), (0.216, 0.32147516071796417), (0.21325, 0.34629387748241425), (0.20975, 0.3354393380880356), (0.21075, 0.3259725450277329), (0.21675, 0.3179135719537735), (0.222, 0.29117509466409686), (0.21925, 0.28576418113708496), (0.21875, 0.32569341230392457), (0.221, 0.30598976600170136), (0.218, 0.29345239233970644), (0.216, 0.28916848707199094), (0.21525, 0.3052206439971924), (0.2165, 0.28646756839752197), (0.21775, 0.2932446739673614), (0.22325, 0.28105981719493867), (0.22275, 0.315715732216835), (0.223, 0.31624056363105774), (0.22825, 0.3110350203514099), (0.227, 0.3174309046268463), (0.22025, 0.3069793425798416)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.32      0.56      0.41       100
           2       0.16      0.31      0.21       100
           3       0.75      0.03      0.06       100
           9       0.75      0.03      0.06       100
          10       0.20      0.37      0.26       100
          13       0.17      0.50      0.26       100
          16       0.25      0.01      0.02       100
          18       0.50      0.01      0.02       100
          23       0.71      0.05      0.09       100
          29       0.50      0.02      0.04       100
          34       0.18      0.33      0.23       100
          37       0.14      0.44      0.22       100
          38       0.12      0.29      0.17       100
          39       0.00      0.00      0.00       100
          41       0.42      0.57      0.48       100
          43       1.00      0.01      0.02       100
          44       0.50      0.01      0.02       100
          46       0.15      0.34      0.21       100
          47       1.00      0.03      0.06       100
          48       0.67      0.02      0.04       100
          51       0.26      0.33      0.29       100
          55       0.10      0.19      0.13       100
          58       0.29      0.59      0.39       100
          64       0.11      0.28      0.16       100
          65       0.15      0.31      0.20       100
          68       0.55      0.81      0.65       100
          69       1.00      0.03      0.06       100
          70       0.47      0.57      0.52       100
          76       0.00      0.00      0.00       100
          77       0.13      0.18      0.15       100
          78       0.19      0.40      0.25       100
          79       0.25      0.01      0.02       100
          81       0.33      0.01      0.02       100
          86       0.29      0.55      0.38       100
          87       0.00      0.00      0.00       100
          88       0.00      0.00      0.00       100
          89       0.00      0.00      0.00       100
          91       0.62      0.05      0.09       100
          94       1.00      0.12      0.21       100
          99       0.20      0.45      0.27       100

    accuracy                           0.22      4000
   macro avg       0.36      0.22      0.17      4000
weighted avg       0.36      0.22      0.17      4000

Competition_DC_1
VAL: 
[(0.0275, 0.11640736758708954), (0.222, 0.13935426399111747), (0.263, 0.16746247287094593), (0.2805, 0.19949597765505314), (0.285, 0.2471946467757225), (0.28, 0.29811707623302935), (0.278, 0.32501332454383375), (0.2735, 0.3312086882442236), (0.28, 0.33877271787822244), (0.273, 0.3588443279117346), (0.2775, 0.3583696631938219), (0.281, 0.3776941959261894), (0.274, 0.40736699411273003), (0.277, 0.3859904466569424), (0.28, 0.3748312215656042), (0.273, 0.37677542120218277), (0.2655, 0.3966861490756273), (0.2745, 0.3589093612134457), (0.28, 0.37897317838668826), (0.271, 0.358011616140604), (0.2655, 0.3368848157525063), (0.2705, 0.33055195996165276), (0.2685, 0.323097417473793), (0.275, 0.32955744552612304), (0.268, 0.3103841248005629), (0.271, 0.29746592333912847), (0.271, 0.2970067406594753), (0.265, 0.29422304731607435), (0.2685, 0.2868510262966156), (0.275, 0.2822832220196724), (0.2705, 0.2912707200050354), (0.2735, 0.2792799746990204), (0.275, 0.2579727663397789), (0.283, 0.2811203292906284), (0.277, 0.27390772113204004), (0.2755, 0.2665751968324184), (0.27, 0.28640509384870527), (0.2675, 0.27451632323861125), (0.277, 0.26175019061565397), (0.266, 0.28439075684547427), (0.28, 0.285371375143528), (0.2765, 0.25065843150019645), (0.277, 0.27230384692549703), (0.267, 0.2938342723548412), (0.2675, 0.27456031531095504), (0.278, 0.2705152726173401), (0.284, 0.28639283043146135), (0.274, 0.2824349771738052), (0.2835, 0.2615997050702572), (0.273, 0.27697792410850525), (0.2745, 0.271833514213562)]
TEST: 
[(0.02725, 0.11549289643764496), (0.2205, 0.13820855522155762), (0.25625, 0.16504749220609666), (0.27325, 0.19762511622905732), (0.2775, 0.24383626687526702), (0.274, 0.29450028109550475), (0.27075, 0.3221930960416794), (0.27675, 0.331216570854187), (0.27075, 0.337857274889946), (0.269, 0.3614157041311264), (0.2705, 0.3616106858253479), (0.27775, 0.3783372157812119), (0.26625, 0.4086187002658844), (0.26425, 0.38919502758979796), (0.2665, 0.37607214069366457), (0.26925, 0.37320445013046266), (0.25825, 0.39579830622673035), (0.2615, 0.3573423635959625), (0.25775, 0.3807336311340332), (0.26375, 0.35877647829055787), (0.264, 0.3355034029483795), (0.26425, 0.33166713285446164), (0.258, 0.32262685692310333), (0.25775, 0.3291945745944977), (0.263, 0.30733116328716276), (0.261, 0.3014100810289383), (0.259, 0.30287068951129914), (0.25075, 0.2963052327632904), (0.2605, 0.2838405697345734), (0.26925, 0.28412186551094054), (0.2665, 0.2962092329263687), (0.26475, 0.283113725066185), (0.2675, 0.26324037671089173), (0.27025, 0.28653424048423765), (0.26425, 0.2806350927352905), (0.2615, 0.27407865738868714), (0.266, 0.28896803629398343), (0.256, 0.27562144422531126), (0.2675, 0.26451299619674684), (0.256, 0.2892309707403183), (0.2655, 0.293692892074585), (0.27225, 0.25692865598201753), (0.2605, 0.27966085529327395), (0.25675, 0.2949872524738312), (0.264, 0.2751562405824661), (0.265, 0.27188085675239565), (0.26875, 0.2917725284099579), (0.25975, 0.28979009366035463), (0.27625, 0.2659011642932892), (0.27325, 0.28296233522892), (0.2725, 0.2757764081954956)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.14      0.02      0.04       100
           5       0.21      0.56      0.31       100
           6       0.19      0.50      0.28       100
           7       0.23      0.45      0.31       100
           9       0.62      0.08      0.14       100
          15       0.17      0.27      0.21       100
          16       0.00      0.00      0.00       100
          18       0.33      0.02      0.04       100
          19       0.16      0.39      0.22       100
          21       0.33      0.58      0.42       100
          23       0.00      0.00      0.00       100
          24       0.38      0.71      0.49       100
          28       0.30      0.61      0.40       100
          29       0.00      0.00      0.00       100
          31       0.22      0.29      0.25       100
          32       0.26      0.38      0.31       100
          35       0.22      0.40      0.28       100
          39       1.00      0.01      0.02       100
          40       0.17      0.42      0.24       100
          43       0.56      0.05      0.09       100
          44       0.00      0.00      0.00       100
          47       0.00      0.00      0.00       100
          48       1.00      0.01      0.02       100
          49       0.30      0.52      0.38       100
          52       0.41      0.85      0.55       100
          59       0.19      0.38      0.26       100
          60       0.60      0.89      0.71       100
          69       0.64      0.09      0.16       100
          72       0.14      0.28      0.18       100
          73       0.41      0.71      0.52       100
          76       0.00      0.00      0.00       100
          79       0.50      0.04      0.07       100
          81       0.38      0.05      0.09       100
          83       0.35      0.62      0.45       100
          85       0.28      0.62      0.39       100
          87       0.40      0.02      0.04       100
          88       0.33      0.01      0.02       100
          89       0.42      0.05      0.09       100
          91       0.00      0.00      0.00       100
          94       1.00      0.02      0.04       100

    accuracy                           0.27      4000
   macro avg       0.32      0.27      0.20      4000
weighted avg       0.32      0.27      0.20      4000

Competition_DC_2
VAL: 
[(0.0235, 0.1164862197637558), (0.18, 0.13491899305582047), (0.2205, 0.17120003962516786), (0.238, 0.2205458999723196), (0.2445, 0.27404198637604715), (0.2445, 0.3163145174086094), (0.2465, 0.3261121627241373), (0.2475, 0.3333806322365999), (0.2505, 0.34211264036595823), (0.245, 0.34807383827865124), (0.2475, 0.36710547041893005), (0.244, 0.38963437248766425), (0.2425, 0.3781762052178383), (0.2435, 0.38954779537022116), (0.2345, 0.3902101512402296), (0.2435, 0.39192733745276925), (0.236, 0.3853566768318415), (0.24, 0.3817195177078247), (0.235, 0.3756011133044958), (0.239, 0.39528161742538215), (0.2375, 0.3516854434609413), (0.239, 0.3500316118597984), (0.2335, 0.3615770672559738), (0.231, 0.34462098973989486), (0.2285, 0.3637440782189369), (0.2285, 0.3521324816942215), (0.227, 0.34920141550898554), (0.239, 0.32142454069852827), (0.238, 0.31801585978269575), (0.2385, 0.30138136154413225), (0.228, 0.3105791458636522), (0.241, 0.3155825917944312), (0.236, 0.3060592692494392), (0.2295, 0.3289744162708521), (0.236, 0.3151511668264866), (0.2365, 0.3138594026863575), (0.231, 0.3366488032639027), (0.2225, 0.28991836935281756), (0.235, 0.31859594719111917), (0.2385, 0.31315754759311676), (0.2415, 0.3046124859452248), (0.2295, 0.32304437273740766), (0.243, 0.3094056808650494), (0.233, 0.3042507859170437), (0.245, 0.31231916350126265), (0.241, 0.2894870994091034), (0.238, 0.30717101350426673), (0.2415, 0.2945760402083397), (0.231, 0.32298513513803484), (0.241, 0.2896116855740547), (0.2465, 0.3037042475938797)]
TEST: 
[(0.026, 0.11547955000400544), (0.18825, 0.13284773010015488), (0.22675, 0.16862347263097763), (0.23375, 0.21775271928310394), (0.23675, 0.27235924077033996), (0.24825, 0.31194968914985655), (0.24925, 0.32232148706912994), (0.247, 0.3327239851951599), (0.249, 0.33677486681938174), (0.24275, 0.3440355894565582), (0.24025, 0.36386017441749574), (0.245, 0.3822226431369781), (0.24475, 0.37293216955661773), (0.23725, 0.38467271530628205), (0.2335, 0.3810141500234604), (0.24275, 0.3826479511260986), (0.23725, 0.3806381834745407), (0.243, 0.375927787899971), (0.2385, 0.36900580644607545), (0.23375, 0.39067263996601104), (0.23725, 0.34788057100772857), (0.24025, 0.3451952339410782), (0.23525, 0.35467895209789274), (0.23825, 0.33673646557331083), (0.22975, 0.3557297500371933), (0.23475, 0.3417464098930359), (0.239, 0.33880529057979586), (0.2385, 0.31234523868560793), (0.2355, 0.31018161177635195), (0.2465, 0.2935012798309326), (0.238, 0.30625492382049563), (0.234, 0.3094057745933533), (0.23475, 0.2999221649169922), (0.2405, 0.3191183000802994), (0.23625, 0.3096161913871765), (0.23275, 0.3104427846670151), (0.23425, 0.3272051568031311), (0.22775, 0.28891612899303437), (0.23125, 0.3160402510166168), (0.2365, 0.3073866955041885), (0.229, 0.2950535649061203), (0.2325, 0.3127459154129028), (0.2355, 0.3012266696691513), (0.23025, 0.29546971344947814), (0.23225, 0.3060735514163971), (0.2435, 0.28236642920970917), (0.23775, 0.29764349448680877), (0.2335, 0.2873077437877655), (0.24, 0.31517018854618073), (0.241, 0.2828551436662674), (0.23325, 0.29839111721515654)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.00      0.00      0.00       100
           8       0.23      0.57      0.32       100
           9       0.00      0.00      0.00       100
          11       0.18      0.38      0.25       100
          16       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          22       0.15      0.50      0.23       100
          23       0.00      0.00      0.00       100
          25       0.22      0.33      0.26       100
          26       0.19      0.37      0.25       100
          29       0.00      0.00      0.00       100
          36       0.24      0.41      0.30       100
          39       0.00      0.00      0.00       100
          42       0.20      0.44      0.28       100
          43       0.00      0.00      0.00       100
          44       0.00      0.00      0.00       100
          47       0.50      0.02      0.04       100
          48       0.00      0.00      0.00       100
          50       0.12      0.28      0.17       100
          54       0.30      0.35      0.33       100
          61       0.49      0.52      0.50       100
          66       0.15      0.34      0.21       100
          67       0.28      0.54      0.37       100
          69       0.64      0.09      0.16       100
          71       0.43      0.82      0.57       100
          74       0.20      0.33      0.25       100
          76       1.00      0.01      0.02       100
          79       1.00      0.01      0.02       100
          81       0.00      0.00      0.00       100
          82       0.58      0.75      0.66       100
          84       0.20      0.24      0.22       100
          87       0.00      0.00      0.00       100
          88       0.00      0.00      0.00       100
          89       0.20      0.01      0.02       100
          90       0.20      0.48      0.28       100
          91       0.00      0.00      0.00       100
          92       0.27      0.50      0.35       100
          93       0.15      0.30      0.20       100
          94       0.60      0.03      0.06       100
          96       0.21      0.71      0.32       100

    accuracy                           0.23      4000
   macro avg       0.22      0.23      0.17      4000
weighted avg       0.22      0.23      0.17      4000

Collaboration
DC 0, val_set_size=2000, COIs=[34, 70, 1, 68, 10, 46, 13, 77, 51, 2, 64, 38, 41, 55, 58, 99, 78, 65, 37, 86, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([34, 70,  1, 68, 10, 46, 13, 77, 51,  2, 64, 38, 41, 55, 58, 99, 78, 65,
        37, 86,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0215, 0.11645013678073883)
DC 1, val_set_size=2000, COIs=[24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21, 5, 6, 83, 85, 73, 35, 7, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21,  5,  6, 83, 85, 73,
        35,  7,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0275, 0.11640736758708954)
DC 2, val_set_size=2000, COIs=[25, 54, 11, 74, 82, 8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90, 26, 71, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([25, 54, 11, 74, 82,  8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90,
        26, 71,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.0235, 0.1164862197637558)
D00: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D01: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D02: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D03: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D04: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D05: 1000 samples from classes {3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94}
D06: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D07: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D08: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D09: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D010: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D011: 1000 samples from classes {1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99}
D012: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D013: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D014: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D015: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D016: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D017: 1000 samples from classes {5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85}
D018: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D019: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D020: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D021: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D022: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
D023: 1000 samples from classes {8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO5']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO0', '(DO3']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.1895, 0.13782579535245895) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2375, 0.13082133403420448) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.207, 0.13924749711155893) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2105, 0.18758745054900647) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2565, 0.16275211694836617) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.233, 0.1802140417546034) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.223, 0.2380396123677492) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.20634453116357326) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.238, 0.22448100411891939) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.224, 0.3301523919850588) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.2551686302125454) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2475, 0.2847401274293661) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.232, 0.35310756269097326) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2765, 0.2942390876561403) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.248, 0.3263798767477274) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO5', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.226, 0.3397823686301708) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.30912854555249214) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2535, 0.34052205604314806) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2255, 0.3348869109451771) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2815, 0.3320896367579699) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2375, 0.336886304423213) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2255, 0.3611009502112865) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.274, 0.3377953835427761) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2565, 0.33766094157099724) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.224, 0.3734000595957041) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2765, 0.34611316686868665) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2465, 0.34167185884714124) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.22, 0.3906715538203716) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.3624727316200733) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2405, 0.36172971621155736) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[3, 9, 16, 18, 23, 29, 39, 43, 44, 47, 48, 69, 76, 79, 81, 87, 88, 89, 91, 94], M=tensor([ 1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 13, 15, 16, 18, 19, 21, 22, 23,
        24, 25, 26, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,
        46, 47, 48, 49, 50, 51, 52, 54, 55, 58, 59, 60, 61, 64, 65, 66, 67, 68,
        69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88,
        89, 90, 91, 92, 93, 94, 96, 99], device='cuda:0'), Initial Performance: (0.44566666666666666, 0.06714725104967753)
DC Expert-0, val_set_size=1000, COIs=[1, 2, 10, 13, 34, 37, 38, 41, 46, 51, 55, 58, 64, 65, 68, 70, 77, 78, 86, 99], M=tensor([34, 70,  1, 68, 10, 46, 13, 77, 51,  2, 64, 38, 41, 55, 58, 99, 78, 65,
        37, 86,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.44, 0.0809545070528984)
DC Expert-1, val_set_size=1000, COIs=[5, 6, 7, 15, 19, 21, 24, 28, 31, 32, 35, 40, 49, 52, 59, 60, 72, 73, 83, 85], M=tensor([24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21,  5,  6, 83, 85, 73,
        35,  7,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.537, 0.06017097920179367)
DC Expert-2, val_set_size=1000, COIs=[8, 11, 22, 25, 26, 36, 42, 50, 54, 61, 66, 67, 71, 74, 82, 84, 90, 92, 93, 96], M=tensor([25, 54, 11, 74, 82,  8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90,
        26, 71,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), Initial Performance: (0.481, 0.07306408388912677)
SUPER-DC 0, val_set_size=2000, COIs=[34, 70, 1, 68, 10, 46, 13, 77, 51, 2, 64, 38, 41, 55, 58, 99, 78, 65, 37, 86, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([34, 70,  1, 68, 10, 46, 13, 77, 51,  2, 64, 38, 41, 55, 58, 99, 78, 65,
        37, 86,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21, 5, 6, 83, 85, 73, 35, 7, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([24, 32, 60, 52, 72, 19, 28, 40, 59, 31, 15, 49, 21,  5,  6, 83, 85, 73,
        35,  7,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[25, 54, 11, 74, 82, 8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90, 26, 71, 9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29, 48, 18, 81, 3], M=tensor([25, 54, 11, 74, 82,  8, 93, 66, 36, 96, 84, 67, 42, 50, 61, 92, 22, 90,
        26, 71,  9, 76, 94, 23, 44, 89, 69, 79, 47, 39, 91, 87, 16, 88, 43, 29,
        48, 18, 81,  3], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.428, 0.09969223272800445) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.07475481951236725) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.469, 0.09637117049098015) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5193333333333333, 0.05159180787205696) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.247, 0.17578132650256156) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.295, 0.14801817965507508) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2845, 0.14344706356525422) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.448, 0.1011039783358574) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.546, 0.07799809926748276) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.478, 0.09685086032748222) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.555, 0.05053333926200867) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2425, 0.16163563129305838) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3045, 0.13834135511517526) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2845, 0.12772750452160836) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.433, 0.11743729960918427) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.54, 0.09626511377096177) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.456, 0.11530936974287033) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5526666666666666, 0.05300826396544774) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2585, 0.12434118828177453) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3055, 0.1190577967762947) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.283, 0.11777331748604775) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.447, 0.11968982660770416) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.53, 0.09544746088981629) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.487, 0.10948498707637191) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.549, 0.05576804131269455) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2705, 0.13024608133733273) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.296, 0.12994639375805855) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3035, 0.11626838579773903) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.438, 0.1323526057600975) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.529, 0.10883199679851532) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.464, 0.11153019976615906) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5436666666666666, 0.05796911060810089) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.26, 0.12484065076708793) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.11430452752113342) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2945, 0.103668128490448) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.43, 0.13206012028455735) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.0950181639790535) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.471, 0.12290583890676499) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5396666666666666, 0.0669364634156227) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.10799551478028298) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.099724016726017) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3335, 0.0982431928217411) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.426, 0.14495857894420625) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.518, 0.09945854890346527) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.488, 0.11585723835229873) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.504, 0.0823721819917361) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.298, 0.10979136681556702) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.324, 0.09883005928993226) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3255, 0.09429538187384605) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.439, 0.1319703586101532) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.09291540738940239) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.475, 0.13067745735496283) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5476666666666666, 0.07605569583177567) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.284, 0.10432359018921852) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.334, 0.09311572483181954) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.323, 0.09770113882422447) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.419, 0.14691696381568908) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.10519196605682372) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.473, 0.12493098901957274) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5286666666666666, 0.08591032894452413) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.317, 0.10729284873604775) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.361, 0.09333232676982879) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3345, 0.09651932227611541) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.43, 0.1505853762626648) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.554, 0.11108725225925445) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.477, 0.13487296774238347) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5336666666666666, 0.08574955006440481) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2835, 0.10869262245297431) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3555, 0.0960050950050354) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.09410994112491608) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.43, 0.1556630799174309) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.11770453178882599) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.12926305800676346) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5106666666666667, 0.10606078551212947) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3135, 0.10741734719276429) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3555, 0.09766013622283935) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.335, 0.09853456073999405) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.432, 0.17487757575511934) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.546, 0.11704813957214355) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.465, 0.13925788414478302) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.522, 0.1035762800971667) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3115, 0.10916899293661117) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.355, 0.10264477920532226) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.339, 0.10059615901112556) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.435, 0.16762293922901153) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.542, 0.11456693708896637) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.457, 0.15454682461917402) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5246666666666666, 0.10247733787695566) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.306, 0.10818999236822129) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3355, 0.09793754863739014) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.332, 0.10099457985162735) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.428, 0.15880970871448516) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.536, 0.12876512110233307) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.457, 0.1531917273402214) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5286666666666666, 0.1110083966255188) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3015, 0.11186663770675659) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3455, 0.09897365510463714) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.337, 0.10269659489393235) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.432, 0.16417161417007448) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.551, 0.11477104938030243) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.14230445025861263) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5183333333333333, 0.11342691210905712) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.303, 0.11051663354039193) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3465, 0.09802872961759568) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.341, 0.09991012102365494) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.431, 0.1677677037715912) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.13290252804756164) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.466, 0.15373192429542543) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.537, 0.11232475797335306) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2985, 0.11212312531471252) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3375, 0.11054299330711365) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.341, 0.10589533764123917) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.427, 0.17229463744163515) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.542, 0.12862094700336457) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.15037097233533858) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5363333333333333, 0.11611613277594249) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3305, 0.11045226234197617) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.334, 0.10852899289131164) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.346, 0.10662527459859848) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.443, 0.17694427055120468) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.553, 0.12542769587039948) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.462, 0.16476793286576868) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5186666666666667, 0.12746214695771535) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.306, 0.11796210348606109) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.10798496067523956) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.337, 0.10970414805412293) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.428, 0.17893474459648132) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.546, 0.12988173818588256) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.485, 0.15209754765033723) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.513, 0.1324811145067215) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3195, 0.11809947311878205) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.324, 0.11116711175441742) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3355, 0.10794003784656525) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.425, 0.19645635294914246) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.553, 0.123746364235878) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.458, 0.1603171015381813) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5256666666666666, 0.1215288293560346) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2935, 0.1219317826628685) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3315, 0.11127774024009704) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3405, 0.10703932964801788) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.439, 0.19898115694522858) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.559, 0.13236440342664718) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.478, 0.1726857945919037) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5236666666666666, 0.12511405181884766) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.29, 0.12261829787492752) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.343, 0.10928854411840438) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3345, 0.11181889045238495) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.448, 0.18979357481002807) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.557, 0.134488487303257) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.473, 0.16140371859073638) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5266666666666666, 0.12827916117509205) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.309, 0.12039704120159149) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3315, 0.11643593022227287) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.331, 0.1165556429028511) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.441, 0.18676130127906798) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.14065669572353362) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.454, 0.1889275851249695) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5333333333333333, 0.13919846761226654) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.303, 0.12320034056901932) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3295, 0.12062327349185943) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.11401873522996903) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.445, 0.1970436211824417) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.541, 0.1516507996916771) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.17487999397516252) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.525, 0.13069077622890474) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.277, 0.12642604026198387) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3405, 0.11083799797296524) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3285, 0.11454122793674469) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.437, 0.20307920265197754) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.543, 0.14371865546703338) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.459, 0.1850406211614609) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5266666666666666, 0.14012161817153296) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.301, 0.130775477707386) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.342, 0.11598887032270432) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3175, 0.11839073491096497) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.44, 0.2071991101503372) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.543, 0.16552843189239502) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.1738227744102478) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5266666666666666, 0.13701005844275158) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.281, 0.1234208647608757) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.32, 0.1214140232205391) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.334, 0.1119520769417286) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.425, 0.19583507227897645) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.536, 0.14638878470659256) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.17990185463428499) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.532, 0.13156026848157246) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3105, 0.12377605611085891) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.346, 0.11812134712934494) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3295, 0.11513468188047409) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.439, 0.22031085669994355) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.538, 0.14381651657819747) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.492, 0.18389271658658982) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5466666666666666, 0.13924756638209024) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2985, 0.12791312265396118) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3435, 0.1220049098432064) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3385, 0.1169197518825531) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.437, 0.19095018023252489) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.533, 0.15223627710342408) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.473, 0.1958250384479761) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.522, 0.15551597960789998) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.287, 0.13126467102766037) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3395, 0.12466756159067154) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.328, 0.1218078545331955) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.421, 0.21580934143066408) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.525, 0.16061732047796248) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.464, 0.1934596152305603) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5236666666666666, 0.14647189791997273) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.277, 0.1287547436952591) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3305, 0.12580695301294326) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3145, 0.12236873036623001) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.419, 0.2080895701646805) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.536, 0.1606297471523285) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.445, 0.21543039405345918) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.517, 0.1451293828090032) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2845, 0.13568150067329407) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.336, 0.11730559501051903) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.309, 0.12640906774997712) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.432, 0.21691302460432052) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.53, 0.1612407920360565) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.466, 0.19630875386297703) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5256666666666666, 0.1568872400522232) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.295, 0.12985455590486528) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3455, 0.12858724969625474) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.327, 0.12562541687488557) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.459, 0.20150422370433807) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.526, 0.15846462658047675) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.19568025364994537) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5136666666666667, 0.1558902649084727) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.291, 0.13383559900522232) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3415, 0.12340328031778336) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.328, 0.122196771889925) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.439, 0.19419463109970092) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.528, 0.16174075090885162) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.467, 0.19229124697172664) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.523, 0.15272138877709707) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.292, 0.13672101280093193) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3425, 0.11986846762895584) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.316, 0.1240831699371338) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.444, 0.2167714359164238) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.54, 0.1601955772638321) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.492, 0.1896117767095566) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.528, 0.15526543335119883) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.283, 0.13714448782801628) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.321, 0.12479942750930786) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3245, 0.12234173226356507) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.429, 0.21871306574344634) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.16081073784828187) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.19310643404722214) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5186666666666667, 0.1631455940405528) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2725, 0.13979853224754332) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3305, 0.12258405232429505) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3175, 0.1228704394698143) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.453, 0.21626332640647888) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.539, 0.1716492211818695) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.483, 0.19323819863051175) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5206666666666667, 0.1660280598004659) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.277, 0.1436740263402462) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.331, 0.12374022108316421) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.311, 0.12628512394428254) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.423, 0.21933045542240143) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.521, 0.18470867383480072) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.497, 0.2006133291348815) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.515, 0.1567319848537445) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.14003050750494003) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3325, 0.12455099242925644) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3235, 0.12315835201740265) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.442, 0.22453257691860198) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.529, 0.1763821551799774) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.477, 0.22306595540046692) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5323333333333333, 0.16082184390227) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2805, 0.13615725690126418) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.333, 0.1276157785654068) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3125, 0.12953081440925598) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.441, 0.21705164182186126) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.525, 0.1690611608028412) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.48, 0.19602889885706826) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.529, 0.15818663680553435) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2695, 0.14371102249622345) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.329, 0.1296389992237091) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.299, 0.12997225373983384) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.0215, 0.11645013678073883), (0.1895, 0.13782579535245895), (0.2105, 0.18758745054900647), (0.223, 0.2380396123677492), (0.224, 0.3301523919850588), (0.232, 0.35310756269097326), (0.226, 0.3397823686301708), (0.2255, 0.3348869109451771), (0.2255, 0.3611009502112865), (0.224, 0.3734000595957041), (0.22, 0.3906715538203716), (0.247, 0.17578132650256156), (0.2425, 0.16163563129305838), (0.2585, 0.12434118828177453), (0.2705, 0.13024608133733273), (0.26, 0.12484065076708793), (0.294, 0.10799551478028298), (0.298, 0.10979136681556702), (0.284, 0.10432359018921852), (0.317, 0.10729284873604775), (0.2835, 0.10869262245297431), (0.3135, 0.10741734719276429), (0.3115, 0.10916899293661117), (0.306, 0.10818999236822129), (0.3015, 0.11186663770675659), (0.303, 0.11051663354039193), (0.2985, 0.11212312531471252), (0.3305, 0.11045226234197617), (0.306, 0.11796210348606109), (0.3195, 0.11809947311878205), (0.2935, 0.1219317826628685), (0.29, 0.12261829787492752), (0.309, 0.12039704120159149), (0.303, 0.12320034056901932), (0.277, 0.12642604026198387), (0.301, 0.130775477707386), (0.281, 0.1234208647608757), (0.3105, 0.12377605611085891), (0.2985, 0.12791312265396118), (0.287, 0.13126467102766037), (0.277, 0.1287547436952591), (0.2845, 0.13568150067329407), (0.295, 0.12985455590486528), (0.291, 0.13383559900522232), (0.292, 0.13672101280093193), (0.283, 0.13714448782801628), (0.2725, 0.13979853224754332), (0.277, 0.1436740263402462), (0.2865, 0.14003050750494003), (0.2805, 0.13615725690126418), (0.2695, 0.14371102249622345)]
TEST: 
[(0.02275, 0.1154470106959343), (0.19225, 0.13582881081104278), (0.2155, 0.18423419219255446), (0.2225, 0.2346848179101944), (0.22575, 0.3252854828834534), (0.229, 0.3457900538444519), (0.23525, 0.33287377285957337), (0.22975, 0.3302689245939255), (0.231, 0.3558875535726547), (0.22825, 0.3649336032867432), (0.22625, 0.38156825709342956), (0.25175, 0.1736696642637253), (0.24875, 0.15776163452863692), (0.271, 0.12151975744962693), (0.2785, 0.12624010306596756), (0.27125, 0.1192719561457634), (0.31, 0.10439341962337494), (0.2935, 0.10581249469518661), (0.30125, 0.10114744943380356), (0.313, 0.10428362840414047), (0.2965, 0.10566629260778428), (0.31975, 0.1041935594677925), (0.32425, 0.10514138531684876), (0.307, 0.10602377772331238), (0.3035, 0.10800560873746871), (0.31075, 0.10745960342884063), (0.312, 0.10946278715133667), (0.33, 0.10819243729114532), (0.314, 0.11507729357481003), (0.316, 0.11599972987174988), (0.30375, 0.11497156494855881), (0.3095, 0.11764590036869049), (0.33425, 0.11432024681568145), (0.319, 0.11835180634260177), (0.30875, 0.12187156772613525), (0.29625, 0.12791663068532944), (0.302, 0.11806105250120164), (0.3215, 0.11815145641565322), (0.3005, 0.12446097069978714), (0.305, 0.12708296501636504), (0.29, 0.12477793139219284), (0.284, 0.1337854299545288), (0.30575, 0.12575665390491486), (0.2875, 0.13054502815008162), (0.299, 0.13249044358730316), (0.28825, 0.1348541085124016), (0.29625, 0.13630090552568436), (0.285, 0.13974553006887436), (0.286, 0.132983906686306), (0.30425, 0.13026148253679276), (0.28, 0.13751963067054748)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.39      0.38      0.39       100
           2       0.23      0.26      0.24       100
           3       0.21      0.15      0.18       100
           9       0.42      0.28      0.34       100
          10       0.21      0.35      0.26       100
          13       0.23      0.22      0.22       100
          16       0.22      0.15      0.18       100
          18       0.25      0.19      0.22       100
          23       0.46      0.31      0.37       100
          29       0.29      0.19      0.23       100
          34       0.21      0.21      0.21       100
          37       0.20      0.38      0.26       100
          38       0.14      0.12      0.13       100
          39       0.37      0.22      0.27       100
          41       0.49      0.54      0.51       100
          43       0.18      0.17      0.17       100
          44       0.14      0.25      0.18       100
          46       0.22      0.22      0.22       100
          47       0.66      0.69      0.67       100
          48       0.48      0.30      0.37       100
          51       0.22      0.17      0.19       100
          55       0.13      0.12      0.13       100
          58       0.30      0.31      0.31       100
          64       0.14      0.15      0.14       100
          65       0.11      0.07      0.08       100
          68       0.77      0.66      0.71       100
          69       0.40      0.42      0.41       100
          70       0.49      0.49      0.49       100
          76       0.57      0.16      0.25       100
          77       0.15      0.16      0.15       100
          78       0.22      0.27      0.24       100
          79       0.32      0.32      0.32       100
          81       0.21      0.20      0.20       100
          86       0.36      0.55      0.43       100
          87       0.37      0.26      0.31       100
          88       0.17      0.21      0.19       100
          89       0.22      0.26      0.24       100
          91       0.27      0.40      0.32       100
          94       0.77      0.10      0.18       100
          99       0.17      0.34      0.22       100

    accuracy                           0.28      4000
   macro avg       0.31      0.28      0.28      4000
weighted avg       0.31      0.28      0.28      4000

Collaboration_DC_1
VAL: 
[(0.0275, 0.11640736758708954), (0.2375, 0.13082133403420448), (0.2565, 0.16275211694836617), (0.2655, 0.20634453116357326), (0.28, 0.2551686302125454), (0.2765, 0.2942390876561403), (0.273, 0.30912854555249214), (0.2815, 0.3320896367579699), (0.274, 0.3377953835427761), (0.2765, 0.34611316686868665), (0.2685, 0.3624727316200733), (0.295, 0.14801817965507508), (0.3045, 0.13834135511517526), (0.3055, 0.1190577967762947), (0.296, 0.12994639375805855), (0.301, 0.11430452752113342), (0.33, 0.099724016726017), (0.324, 0.09883005928993226), (0.334, 0.09311572483181954), (0.361, 0.09333232676982879), (0.3555, 0.0960050950050354), (0.3555, 0.09766013622283935), (0.355, 0.10264477920532226), (0.3355, 0.09793754863739014), (0.3455, 0.09897365510463714), (0.3465, 0.09802872961759568), (0.3375, 0.11054299330711365), (0.334, 0.10852899289131164), (0.323, 0.10798496067523956), (0.324, 0.11116711175441742), (0.3315, 0.11127774024009704), (0.343, 0.10928854411840438), (0.3315, 0.11643593022227287), (0.3295, 0.12062327349185943), (0.3405, 0.11083799797296524), (0.342, 0.11598887032270432), (0.32, 0.1214140232205391), (0.346, 0.11812134712934494), (0.3435, 0.1220049098432064), (0.3395, 0.12466756159067154), (0.3305, 0.12580695301294326), (0.336, 0.11730559501051903), (0.3455, 0.12858724969625474), (0.3415, 0.12340328031778336), (0.3425, 0.11986846762895584), (0.321, 0.12479942750930786), (0.3305, 0.12258405232429505), (0.331, 0.12374022108316421), (0.3325, 0.12455099242925644), (0.333, 0.1276157785654068), (0.329, 0.1296389992237091)]
TEST: 
[(0.02725, 0.11549289643764496), (0.2385, 0.12954052031040192), (0.2535, 0.16111791652441024), (0.27, 0.20460673105716706), (0.27775, 0.25409573256969453), (0.27475, 0.2915184686183929), (0.2765, 0.30769705152511595), (0.27025, 0.32943087446689606), (0.27925, 0.331862082362175), (0.26875, 0.34469209563732145), (0.2675, 0.35885360193252563), (0.29025, 0.14594162994623183), (0.28725, 0.13922850102186202), (0.2905, 0.12086519861221313), (0.2865, 0.12832833582162856), (0.28525, 0.11454095649719238), (0.32475, 0.10138058179616928), (0.31425, 0.09706116515398025), (0.322, 0.09253523844480514), (0.341, 0.09357278457283974), (0.34575, 0.09408562704920768), (0.34075, 0.09708820009231567), (0.3475, 0.10346852597594261), (0.33, 0.09914445376396179), (0.3275, 0.10000039100646972), (0.337, 0.09786908388137817), (0.33125, 0.109149856030941), (0.336, 0.10615676438808441), (0.3275, 0.1080929073691368), (0.3235, 0.11113212305307388), (0.31425, 0.11339014500379563), (0.3375, 0.10954144728183747), (0.33125, 0.11455513739585876), (0.32075, 0.11884956246614456), (0.33025, 0.10817570573091508), (0.322, 0.1172184665799141), (0.31375, 0.11964169389009476), (0.3185, 0.11768082660436631), (0.324, 0.11932484322786331), (0.31575, 0.12392251634597778), (0.31875, 0.12361541873216629), (0.318, 0.11787488454580307), (0.32825, 0.12814106011390686), (0.31975, 0.12408722025156022), (0.3215, 0.11961621010303497), (0.30975, 0.1241121553182602), (0.325, 0.1206760722398758), (0.315, 0.12438611352443694), (0.31675, 0.12485409587621689), (0.3285, 0.12644050908088683), (0.31925, 0.12914596712589263)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.19      0.19      0.19       100
           5       0.24      0.38      0.29       100
           6       0.26      0.26      0.26       100
           7       0.24      0.30      0.26       100
           9       0.42      0.31      0.36       100
          15       0.18      0.26      0.21       100
          16       0.35      0.22      0.27       100
          18       0.24      0.26      0.25       100
          19       0.25      0.30      0.27       100
          21       0.42      0.58      0.49       100
          23       0.39      0.47      0.42       100
          24       0.54      0.49      0.51       100
          28       0.45      0.43      0.44       100
          29       0.23      0.18      0.20       100
          31       0.18      0.30      0.23       100
          32       0.30      0.41      0.34       100
          35       0.28      0.25      0.27       100
          39       0.51      0.25      0.34       100
          40       0.39      0.27      0.32       100
          43       0.25      0.15      0.19       100
          44       0.16      0.20      0.18       100
          47       0.31      0.70      0.43       100
          48       0.57      0.34      0.42       100
          49       0.40      0.37      0.38       100
          52       0.45      0.05      0.09       100
          59       0.17      0.21      0.19       100
          60       0.69      0.75      0.72       100
          69       0.47      0.55      0.51       100
          72       0.13      0.14      0.14       100
          73       0.53      0.24      0.33       100
          76       0.29      0.10      0.15       100
          79       0.35      0.29      0.32       100
          81       0.25      0.36      0.29       100
          83       0.38      0.51      0.44       100
          85       0.29      0.35      0.32       100
          87       0.44      0.34      0.38       100
          88       0.23      0.18      0.20       100
          89       0.30      0.28      0.29       100
          91       0.29      0.24      0.26       100
          94       0.67      0.31      0.42       100

    accuracy                           0.32      4000
   macro avg       0.34      0.32      0.31      4000
weighted avg       0.34      0.32      0.31      4000

Collaboration_DC_2
VAL: 
[(0.0235, 0.1164862197637558), (0.207, 0.13924749711155893), (0.233, 0.1802140417546034), (0.238, 0.22448100411891939), (0.2475, 0.2847401274293661), (0.248, 0.3263798767477274), (0.2535, 0.34052205604314806), (0.2375, 0.336886304423213), (0.2565, 0.33766094157099724), (0.2465, 0.34167185884714124), (0.2405, 0.36172971621155736), (0.2845, 0.14344706356525422), (0.2845, 0.12772750452160836), (0.283, 0.11777331748604775), (0.3035, 0.11626838579773903), (0.2945, 0.103668128490448), (0.3335, 0.0982431928217411), (0.3255, 0.09429538187384605), (0.323, 0.09770113882422447), (0.3345, 0.09651932227611541), (0.353, 0.09410994112491608), (0.335, 0.09853456073999405), (0.339, 0.10059615901112556), (0.332, 0.10099457985162735), (0.337, 0.10269659489393235), (0.341, 0.09991012102365494), (0.341, 0.10589533764123917), (0.346, 0.10662527459859848), (0.337, 0.10970414805412293), (0.3355, 0.10794003784656525), (0.3405, 0.10703932964801788), (0.3345, 0.11181889045238495), (0.331, 0.1165556429028511), (0.317, 0.11401873522996903), (0.3285, 0.11454122793674469), (0.3175, 0.11839073491096497), (0.334, 0.1119520769417286), (0.3295, 0.11513468188047409), (0.3385, 0.1169197518825531), (0.328, 0.1218078545331955), (0.3145, 0.12236873036623001), (0.309, 0.12640906774997712), (0.327, 0.12562541687488557), (0.328, 0.122196771889925), (0.316, 0.1240831699371338), (0.3245, 0.12234173226356507), (0.3175, 0.1228704394698143), (0.311, 0.12628512394428254), (0.3235, 0.12315835201740265), (0.3125, 0.12953081440925598), (0.299, 0.12997225373983384)]
TEST: 
[(0.026, 0.11547955000400544), (0.202, 0.13794700706005097), (0.22825, 0.17868261528015136), (0.2375, 0.22167891949415208), (0.242, 0.2809331146478653), (0.2495, 0.3195913562774658), (0.24575, 0.3335595564842224), (0.2385, 0.3317504849433899), (0.246, 0.33381770813465117), (0.24625, 0.33379373252391814), (0.24375, 0.3543761790990829), (0.26725, 0.1439940299987793), (0.27625, 0.12706685841083526), (0.27925, 0.11808897620439529), (0.29175, 0.1173496041893959), (0.2995, 0.10345781183242798), (0.32975, 0.09882026478648186), (0.3315, 0.09429085585474968), (0.33425, 0.09615131509304047), (0.336, 0.09646643349528312), (0.337, 0.09362916567921639), (0.32875, 0.0968620037138462), (0.32975, 0.09943829619884491), (0.3225, 0.10015803307294846), (0.3345, 0.10017389008402824), (0.3315, 0.09969532763957978), (0.33975, 0.10464824873209), (0.338, 0.10535809135437012), (0.33625, 0.10984785884618758), (0.333, 0.1083004886507988), (0.34275, 0.10747295027971268), (0.3305, 0.11140073180198669), (0.33325, 0.1156748053431511), (0.3335, 0.11341206246614456), (0.329, 0.11403762620687485), (0.31575, 0.12006501942873), (0.322, 0.1139770639538765), (0.32525, 0.11448101288080216), (0.32725, 0.11628664791584015), (0.324, 0.12178762882947922), (0.318, 0.12360387694835663), (0.30875, 0.12531205582618712), (0.3345, 0.12516058319807052), (0.322, 0.12420277667045593), (0.31525, 0.12691459423303605), (0.33075, 0.12112833499908447), (0.3115, 0.12396791553497315), (0.31975, 0.1255438779592514), (0.3175, 0.12445188236236572), (0.30025, 0.13170004844665528), (0.309, 0.1299960849881172)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.20      0.06      0.09       100
           8       0.29      0.37      0.32       100
           9       0.50      0.37      0.43       100
          11       0.21      0.19      0.20       100
          16       0.31      0.18      0.23       100
          18       0.21      0.09      0.13       100
          22       0.13      0.22      0.17       100
          23       0.40      0.35      0.37       100
          25       0.26      0.20      0.23       100
          26       0.25      0.29      0.27       100
          29       0.23      0.23      0.23       100
          36       0.35      0.29      0.32       100
          39       0.28      0.22      0.25       100
          42       0.14      0.22      0.17       100
          43       0.24      0.19      0.21       100
          44       0.17      0.25      0.20       100
          47       0.43      0.66      0.52       100
          48       0.55      0.31      0.40       100
          50       0.17      0.16      0.16       100
          54       0.41      0.45      0.43       100
          61       0.52      0.43      0.47       100
          66       0.23      0.37      0.29       100
          67       0.32      0.36      0.34       100
          69       0.56      0.59      0.58       100
          71       0.56      0.68      0.62       100
          74       0.12      0.26      0.17       100
          76       0.55      0.18      0.27       100
          79       0.39      0.26      0.31       100
          81       0.29      0.37      0.32       100
          82       0.54      0.77      0.64       100
          84       0.24      0.18      0.21       100
          87       0.36      0.31      0.33       100
          88       0.27      0.19      0.22       100
          89       0.30      0.40      0.34       100
          90       0.21      0.18      0.19       100
          91       0.30      0.40      0.34       100
          92       0.42      0.35      0.38       100
          93       0.21      0.12      0.15       100
          94       0.66      0.38      0.48       100
          96       0.22      0.28      0.25       100

    accuracy                           0.31      4000
   macro avg       0.32      0.31      0.31      4000
weighted avg       0.32      0.31      0.31      4000

do_assignment: None
seeds: [89]
name: naive-cifar100-feddf89
score_metric: contrloss
aggregation: <function fed_df at 0x7cdde59e9e50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=89
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[0, 36, 24, 93, 72, 6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99, 1, 69, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([ 0, 36, 24, 93, 72,  6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99,
         1, 69, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.021, 0.11636942088603973)
DC 1, val_set_size=2000, COIs=[20, 56, 29, 75, 11, 97, 39, 2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25, 58, 55, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([20, 56, 29, 75, 11, 97, 39,  2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25,
        58, 55, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.0275, 0.11640495622158051)
DC 2, val_set_size=2000, COIs=[63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70, 32, 92, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70,
        32, 92, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.026, 0.11662145912647247)
D00: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D01: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D02: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D03: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D04: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D05: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D06: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D07: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D08: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D09: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D010: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D011: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D012: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D013: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D014: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D015: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D016: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D017: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D018: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D019: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D020: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D021: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D022: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D023: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.2875, 0.08443887615203857) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.275, 0.08454234111309052) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2845, 0.08466723614931107) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.335, 0.07541801625490188) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3455, 0.07566316995024681) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.338, 0.07531152158975601) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.351, 0.07251882207393646) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.36, 0.07412789011001587) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3805, 0.07052193307876586) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.371, 0.07163156569004059) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3785, 0.07239485776424408) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.07077922615408898) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3825, 0.07333167755603791) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3865, 0.07223490238189698) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.07400428467988968) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3775, 0.07852338376641274) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.07574961245059968) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3755, 0.07457894930243492) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.388, 0.07750089544057846) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.08018331310153007) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.07594015076756477) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3895, 0.08002796941995621) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3695, 0.08198781710863114) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3915, 0.08002195438742638) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4005, 0.08440765428543091) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.08735266244411469) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3785, 0.08517707285284996) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3825, 0.09143208968639374) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.375, 0.09155650097131729) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.09675327891111374) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.09990198370814324) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3705, 0.09845161047577858) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.386, 0.09402467757463455) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.1070894593000412) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.367, 0.11074043482542038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.357, 0.1139927127957344) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.376, 0.11123972576856613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3615, 0.11738292378187179) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.11896553111076355) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.368, 0.12736304125189782) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3615, 0.12453840047121048) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.374, 0.11811828684806824) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3655, 0.13369477391242982) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3575, 0.12946749186515807) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.367, 0.12780753606557846) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3625, 0.13970318913459778) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.338, 0.15851248276233673) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.352, 0.14792551732063294) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.15719299912452697) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3495, 0.14176947164535522) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3635, 0.1373596593737602) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3735, 0.14515573993325234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3415, 0.1549889482855797) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.372, 0.14690099358558656) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3565, 0.16278630864620208) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.15249807858467102) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.337, 0.15547612249851228) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.1524028015136719) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.16918148934841157) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3545, 0.15701844561100006) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.353, 0.16531227302551268) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.34, 0.16173225945234299) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.36, 0.1595572355389595) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3565, 0.16025092548131942) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.336, 0.1794006732106209) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.15917272818088532) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.1650543938279152) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3455, 0.17857069236040116) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3325, 0.18384468388557434) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3705, 0.17336187493801117) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.338, 0.1941439588069916) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.1723806248307228) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.18488981449604033) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.351, 0.16814939814805985) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3555, 0.17150479906797408) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3685, 0.1666609577536583) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3335, 0.18944824206829072) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.18213949811458588) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.18186131727695465) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3565, 0.1779403206706047) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.1926639450788498) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3705, 0.18403362131118775) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.336, 0.18736293667554854) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.362, 0.19073678278923034) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3395, 0.2036721560359001) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.339, 0.20287973821163177) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.1889543459415436) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3545, 0.195047048330307) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3485, 0.2043226593732834) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3645, 0.18517648899555206) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.355, 0.19181943547725677) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.341, 0.21383908212184907) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.359, 0.19270536279678344) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3575, 0.19732110607624054) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.344, 0.19763596200942993) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3425, 0.2090698263645172) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.349, 0.20509330666065215) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3455, 0.20778013813495635) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3585, 0.20324669754505156) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3545, 0.21532167875766753) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.338, 0.21022054249048233) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3495, 0.19590411865711213) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.359, 0.20376348638534547) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.326, 0.22642674267292023) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3455, 0.21608438217639922) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.357, 0.2163274973630905) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.346, 0.2067247344851494) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3505, 0.19864649319648742) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.344, 0.220753293633461) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.332, 0.21439420819282531) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3535, 0.21047155106067658) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3615, 0.2095602783560753) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.342, 0.22797252237796783) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.21278256225585937) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3465, 0.20464521247148515) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.343, 0.22775320041179656) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3465, 0.19956117868423462) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.22358174514770507) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3475, 0.21580220437049866) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3455, 0.20717753148078918) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3525, 0.21846393120288848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3555, 0.22104812061786652) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.341, 0.2121177637577057) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.212670738697052) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.342, 0.22795488977432252) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3495, 0.21760942375659942) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.2178871750831604) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.347, 0.22904548549652098) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.22134272718429565) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.23437799894809722) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3555, 0.2320429253578186) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.22087150013446807) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.367, 0.229508116543293) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3515, 0.2510479143857956) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.218604052901268) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3655, 0.22206015336513518) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.36, 0.24473725652694703) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.346, 0.24256868743896484) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3555, 0.23405068290233613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3545, 0.2311002721786499) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3655, 0.21429029738903047) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.363, 0.22533930426836013) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3465, 0.25035485792160034) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.23042859518527983) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.366, 0.21869304913282395) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.365, 0.2316782044172287) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3515, 0.22565902853012085) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.346, 0.24613563334941865) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3425, 0.24944985103607178) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3575, 0.2336233366727829) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.021, 0.11636942088603973), (0.2875, 0.08443887615203857), (0.335, 0.07541801625490188), (0.351, 0.07251882207393646), (0.371, 0.07163156569004059), (0.3825, 0.07333167755603791), (0.3775, 0.07852338376641274), (0.388, 0.07750089544057846), (0.3895, 0.08002796941995621), (0.4005, 0.08440765428543091), (0.3825, 0.09143208968639374), (0.378, 0.09990198370814324), (0.3725, 0.1070894593000412), (0.376, 0.11123972576856613), (0.368, 0.12736304125189782), (0.3655, 0.13369477391242982), (0.3625, 0.13970318913459778), (0.345, 0.15719299912452697), (0.3735, 0.14515573993325234), (0.3565, 0.16278630864620208), (0.3475, 0.1524028015136719), (0.353, 0.16531227302551268), (0.3565, 0.16025092548131942), (0.3725, 0.1650543938279152), (0.3705, 0.17336187493801117), (0.362, 0.18488981449604033), (0.3685, 0.1666609577536583), (0.354, 0.18186131727695465), (0.3705, 0.18403362131118775), (0.3395, 0.2036721560359001), (0.3545, 0.195047048330307), (0.355, 0.19181943547725677), (0.3575, 0.19732110607624054), (0.349, 0.20509330666065215), (0.3545, 0.21532167875766753), (0.359, 0.20376348638534547), (0.357, 0.2163274973630905), (0.344, 0.220753293633461), (0.3615, 0.2095602783560753), (0.3465, 0.20464521247148515), (0.3485, 0.22358174514770507), (0.3525, 0.21846393120288848), (0.3535, 0.212670738697052), (0.3475, 0.2178871750831604), (0.354, 0.23437799894809722), (0.367, 0.229508116543293), (0.3655, 0.22206015336513518), (0.3555, 0.23405068290233613), (0.363, 0.22533930426836013), (0.366, 0.21869304913282395), (0.346, 0.24613563334941865)]
TEST: 
[(0.022, 0.11542808198928833), (0.28725, 0.08402793169021607), (0.34725, 0.07508051481842995), (0.373, 0.07141339483857155), (0.38475, 0.07025914564728737), (0.389, 0.0724946848154068), (0.3845, 0.0772546623647213), (0.38725, 0.07664794725179672), (0.38475, 0.07996488353610039), (0.385, 0.08255517590045928), (0.388, 0.08922929093241691), (0.378, 0.09676997736096382), (0.376, 0.10439085566997527), (0.38175, 0.10981941333413124), (0.36825, 0.126426284968853), (0.37075, 0.13443821060657501), (0.37, 0.13596446418762206), (0.3495, 0.1545129885673523), (0.3715, 0.1435103195309639), (0.361, 0.16071181631088258), (0.3675, 0.147895976126194), (0.36475, 0.15994790363311767), (0.3635, 0.1608793153166771), (0.37125, 0.16713017654418946), (0.3635, 0.1695690023303032), (0.3645, 0.1829668577313423), (0.36275, 0.17057231825590133), (0.35, 0.1830847271680832), (0.36125, 0.1830370956659317), (0.34625, 0.19163650918006897), (0.3485, 0.1930301673412323), (0.3675, 0.18769973117113115), (0.3545, 0.19555505847930907), (0.35175, 0.2003628734946251), (0.351, 0.21036250883340835), (0.37, 0.1992927160859108), (0.35275, 0.2122902166247368), (0.3645, 0.20952257084846496), (0.36025, 0.2025351886153221), (0.36425, 0.20079499709606172), (0.34625, 0.22082815766334535), (0.35575, 0.21285428935289383), (0.36225, 0.2059117555618286), (0.3505, 0.21617442888021468), (0.36125, 0.23507892245054246), (0.361, 0.2318077630996704), (0.36425, 0.22696668803691863), (0.355, 0.22985211312770842), (0.365, 0.22430497747659683), (0.36625, 0.21786266839504242), (0.35525, 0.24097189033031463)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.63      0.70      0.66       100
           1       0.33      0.39      0.36       100
           6       0.27      0.29      0.28       100
           8       0.58      0.30      0.39       100
           9       0.42      0.58      0.49       100
          10       0.32      0.43      0.37       100
          12       0.33      0.37      0.35       100
          13       0.41      0.42      0.42       100
          14       0.42      0.22      0.29       100
          17       0.47      0.56      0.51       100
          19       0.33      0.22      0.26       100
          21       0.46      0.43      0.44       100
          23       0.82      0.54      0.65       100
          24       0.63      0.64      0.64       100
          26       0.21      0.25      0.23       100
          33       0.45      0.34      0.39       100
          36       0.22      0.20      0.21       100
          38       0.22      0.16      0.19       100
          42       0.20      0.24      0.22       100
          44       0.18      0.20      0.19       100
          46       0.20      0.28      0.23       100
          52       0.67      0.60      0.63       100
          53       0.49      0.80      0.61       100
          54       0.41      0.55      0.47       100
          59       0.35      0.34      0.35       100
          64       0.18      0.20      0.19       100
          65       0.18      0.17      0.17       100
          69       0.60      0.62      0.61       100
          72       0.18      0.18      0.18       100
          73       0.39      0.45      0.42       100
          74       0.22      0.17      0.19       100
          77       0.25      0.15      0.19       100
          79       0.32      0.32      0.32       100
          87       0.47      0.49      0.48       100
          88       0.21      0.13      0.16       100
          89       0.27      0.39      0.32       100
          90       0.30      0.31      0.31       100
          93       0.20      0.18      0.19       100
          98       0.30      0.25      0.27       100
          99       0.24      0.15      0.19       100

    accuracy                           0.36      4000
   macro avg       0.36      0.36      0.35      4000
weighted avg       0.36      0.36      0.35      4000

No_Competition_DC_1
VAL: 
[(0.0275, 0.11640495622158051), (0.275, 0.08454234111309052), (0.3455, 0.07566316995024681), (0.36, 0.07412789011001587), (0.3785, 0.07239485776424408), (0.3865, 0.07223490238189698), (0.384, 0.07574961245059968), (0.378, 0.08018331310153007), (0.3695, 0.08198781710863114), (0.374, 0.08735266244411469), (0.375, 0.09155650097131729), (0.3705, 0.09845161047577858), (0.367, 0.11074043482542038), (0.3615, 0.11738292378187179), (0.3615, 0.12453840047121048), (0.3575, 0.12946749186515807), (0.338, 0.15851248276233673), (0.3495, 0.14176947164535522), (0.3415, 0.1549889482855797), (0.352, 0.15249807858467102), (0.352, 0.16918148934841157), (0.34, 0.16173225945234299), (0.336, 0.1794006732106209), (0.3455, 0.17857069236040116), (0.338, 0.1941439588069916), (0.351, 0.16814939814805985), (0.3335, 0.18944824206829072), (0.3565, 0.1779403206706047), (0.336, 0.18736293667554854), (0.339, 0.20287973821163177), (0.3485, 0.2043226593732834), (0.341, 0.21383908212184907), (0.344, 0.19763596200942993), (0.3455, 0.20778013813495635), (0.338, 0.21022054249048233), (0.326, 0.22642674267292023), (0.346, 0.2067247344851494), (0.332, 0.21439420819282531), (0.342, 0.22797252237796783), (0.343, 0.22775320041179656), (0.3475, 0.21580220437049866), (0.3555, 0.22104812061786652), (0.342, 0.22795488977432252), (0.347, 0.22904548549652098), (0.3555, 0.2320429253578186), (0.3515, 0.2510479143857956), (0.36, 0.24473725652694703), (0.3545, 0.2311002721786499), (0.3465, 0.25035485792160034), (0.365, 0.2316782044172287), (0.3425, 0.24944985103607178)]
TEST: 
[(0.026, 0.1154832991361618), (0.283, 0.08437816494703293), (0.33675, 0.07563970848917961), (0.36, 0.07423639237880707), (0.3775, 0.07261991187930107), (0.38075, 0.0730014121234417), (0.37625, 0.07638111132383346), (0.37475, 0.0806849085688591), (0.38175, 0.08142344206571579), (0.379, 0.08761409202218055), (0.37875, 0.09250658324360847), (0.37775, 0.09821522355079651), (0.37925, 0.10745496970415115), (0.36825, 0.11547731047868728), (0.36925, 0.12234228801727295), (0.359, 0.12881021571159362), (0.34125, 0.1545177857875824), (0.35825, 0.13819870978593826), (0.3495, 0.1504324408173561), (0.3535, 0.1500747106075287), (0.344, 0.16750427079200744), (0.3625, 0.15488513737916945), (0.36125, 0.17334882551431655), (0.35625, 0.1755800022482872), (0.35275, 0.18873657858371734), (0.3685, 0.163656174659729), (0.343, 0.1873461449146271), (0.358, 0.17641816043853759), (0.36, 0.180405988574028), (0.3475, 0.19670603692531585), (0.35225, 0.19776797556877138), (0.34175, 0.20963672226667404), (0.35975, 0.18993752694129945), (0.35175, 0.19786432343721388), (0.3515, 0.2080254487991333), (0.34725, 0.22104907977581023), (0.35375, 0.20762354618310927), (0.34375, 0.2137586299777031), (0.35675, 0.22304337137937547), (0.3565, 0.22305972814559938), (0.35625, 0.20697455435991288), (0.358, 0.21256760281324386), (0.3465, 0.22242836165428162), (0.36025, 0.22668672066926956), (0.36325, 0.22971774613857268), (0.363, 0.24270140147209168), (0.3555, 0.24140362548828126), (0.356, 0.22333208966255189), (0.33775, 0.24380802392959594), (0.364, 0.22832652813196183), (0.3415, 0.24071156215667724)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.27      0.12      0.17       100
           8       0.38      0.38      0.38       100
           9       0.44      0.54      0.49       100
          10       0.32      0.28      0.30       100
          11       0.12      0.11      0.11       100
          17       0.65      0.43      0.52       100
          19       0.29      0.30      0.30       100
          20       0.68      0.47      0.56       100
          21       0.56      0.44      0.49       100
          22       0.29      0.36      0.32       100
          25       0.22      0.29      0.25       100
          26       0.27      0.28      0.27       100
          29       0.34      0.32      0.33       100
          33       0.50      0.40      0.44       100
          35       0.12      0.09      0.10       100
          38       0.14      0.08      0.10       100
          39       0.41      0.44      0.43       100
          40       0.28      0.34      0.30       100
          42       0.19      0.10      0.13       100
          44       0.19      0.21      0.20       100
          53       0.54      0.59      0.56       100
          55       0.19      0.21      0.20       100
          56       0.57      0.47      0.51       100
          58       0.38      0.44      0.41       100
          59       0.37      0.50      0.43       100
          60       0.72      0.73      0.72       100
          66       0.18      0.27      0.22       100
          67       0.42      0.32      0.36       100
          71       0.67      0.58      0.62       100
          73       0.41      0.45      0.43       100
          75       0.55      0.53      0.54       100
          77       0.23      0.23      0.23       100
          79       0.41      0.22      0.29       100
          80       0.14      0.25      0.18       100
          83       0.43      0.48      0.45       100
          87       0.36      0.36      0.36       100
          88       0.22      0.26      0.24       100
          90       0.33      0.39      0.36       100
          97       0.22      0.24      0.23       100
          98       0.13      0.16      0.14       100

    accuracy                           0.34      4000
   macro avg       0.35      0.34      0.34      4000
weighted avg       0.35      0.34      0.34      4000

No_Competition_DC_2
VAL: 
[(0.026, 0.11662145912647247), (0.2845, 0.08466723614931107), (0.338, 0.07531152158975601), (0.3805, 0.07052193307876586), (0.371, 0.07077922615408898), (0.379, 0.07400428467988968), (0.3755, 0.07457894930243492), (0.396, 0.07594015076756477), (0.3915, 0.08002195438742638), (0.3785, 0.08517707285284996), (0.3725, 0.09675327891111374), (0.386, 0.09402467757463455), (0.357, 0.1139927127957344), (0.354, 0.11896553111076355), (0.374, 0.11811828684806824), (0.367, 0.12780753606557846), (0.352, 0.14792551732063294), (0.3635, 0.1373596593737602), (0.372, 0.14690099358558656), (0.337, 0.15547612249851228), (0.3545, 0.15701844561100006), (0.36, 0.1595572355389595), (0.361, 0.15917272818088532), (0.3325, 0.18384468388557434), (0.361, 0.1723806248307228), (0.3555, 0.17150479906797408), (0.3565, 0.18213949811458588), (0.3565, 0.1926639450788498), (0.362, 0.19073678278923034), (0.3725, 0.1889543459415436), (0.3645, 0.18517648899555206), (0.359, 0.19270536279678344), (0.3425, 0.2090698263645172), (0.3585, 0.20324669754505156), (0.3495, 0.19590411865711213), (0.3455, 0.21608438217639922), (0.3505, 0.19864649319648742), (0.3535, 0.21047155106067658), (0.355, 0.21278256225585937), (0.3465, 0.19956117868423462), (0.3455, 0.20717753148078918), (0.341, 0.2121177637577057), (0.3495, 0.21760942375659942), (0.3525, 0.22134272718429565), (0.353, 0.22087150013446807), (0.3665, 0.218604052901268), (0.346, 0.24256868743896484), (0.3655, 0.21429029738903047), (0.354, 0.23042859518527983), (0.3515, 0.22565902853012085), (0.3575, 0.2336233366727829)]
TEST: 
[(0.025, 0.11568001115322113), (0.2715, 0.08451522350311279), (0.35425, 0.07457317951321601), (0.37425, 0.06988064742088318), (0.39, 0.06977423000335693), (0.3815, 0.07345964890718461), (0.3905, 0.07420769792795182), (0.3905, 0.0765920096039772), (0.39475, 0.07975351035594941), (0.383, 0.08422339767217636), (0.38, 0.09361396685242653), (0.38875, 0.09276524165272712), (0.36875, 0.11220125937461853), (0.35775, 0.1182964859008789), (0.36775, 0.11821249628067017), (0.36525, 0.12791820114851), (0.357, 0.14500766015052796), (0.3735, 0.13651096552610398), (0.36725, 0.1453119980096817), (0.35425, 0.15022755801677704), (0.36625, 0.15250932848453522), (0.37225, 0.15479016441106797), (0.36675, 0.15669416403770448), (0.352, 0.1788954175710678), (0.361, 0.17000047951936723), (0.3635, 0.1684037988781929), (0.3645, 0.1758409236073494), (0.36, 0.19286591225862504), (0.3625, 0.18638719791173936), (0.355, 0.19005073136091233), (0.36025, 0.18653746610879898), (0.3575, 0.19465532100200653), (0.34325, 0.20568003875017166), (0.37575, 0.19851449394226076), (0.35675, 0.1960168296098709), (0.3615, 0.21324511086940764), (0.36225, 0.19958117055892943), (0.348, 0.21254876601696016), (0.3575, 0.21436803084611894), (0.353, 0.1993772889971733), (0.35775, 0.20356355792284012), (0.35825, 0.2102251928448677), (0.3585, 0.21837812024354936), (0.36675, 0.21924779522418975), (0.35875, 0.2154949699640274), (0.36325, 0.21863517343997954), (0.36175, 0.23548604309558868), (0.36025, 0.21771465694904327), (0.36075, 0.23197003990411758), (0.365, 0.22230753833055497), (0.37, 0.22868142247200013)]
DETAILED: 
              precision    recall  f1-score   support

           8       0.53      0.24      0.33       100
           9       0.48      0.49      0.49       100
          10       0.35      0.35      0.35       100
          17       0.42      0.51      0.46       100
          19       0.42      0.31      0.36       100
          21       0.49      0.54      0.51       100
          26       0.21      0.32      0.25       100
          27       0.18      0.22      0.20       100
          32       0.34      0.20      0.25       100
          33       0.47      0.47      0.47       100
          37       0.25      0.19      0.21       100
          38       0.21      0.21      0.21       100
          42       0.19      0.15      0.17       100
          44       0.18      0.13      0.15       100
          45       0.21      0.26      0.23       100
          48       0.57      0.46      0.51       100
          49       0.44      0.53      0.48       100
          50       0.16      0.24      0.19       100
          51       0.28      0.26      0.27       100
          53       0.62      0.62      0.62       100
          59       0.35      0.32      0.33       100
          63       0.43      0.49      0.46       100
          70       0.45      0.37      0.40       100
          73       0.42      0.44      0.43       100
          76       0.56      0.55      0.55       100
          77       0.30      0.13      0.18       100
          79       0.39      0.49      0.44       100
          81       0.27      0.31      0.29       100
          82       0.65      0.67      0.66       100
          84       0.34      0.23      0.27       100
          85       0.55      0.48      0.51       100
          86       0.44      0.48      0.46       100
          87       0.44      0.44      0.44       100
          88       0.22      0.19      0.20       100
          90       0.34      0.30      0.32       100
          91       0.52      0.54      0.53       100
          92       0.28      0.38      0.32       100
          95       0.50      0.52      0.51       100
          96       0.39      0.47      0.43       100
          98       0.23      0.30      0.26       100

    accuracy                           0.37      4000
   macro avg       0.38      0.37      0.37      4000
weighted avg       0.38      0.37      0.37      4000

Competition
DC 0, val_set_size=2000, COIs=[0, 36, 24, 93, 72, 6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99, 1, 69, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([ 0, 36, 24, 93, 72,  6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99,
         1, 69, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.021, 0.11636942088603973)
DC 1, val_set_size=2000, COIs=[20, 56, 29, 75, 11, 97, 39, 2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25, 58, 55, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([20, 56, 29, 75, 11, 97, 39,  2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25,
        58, 55, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.0275, 0.11640495622158051)
DC 2, val_set_size=2000, COIs=[63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70, 32, 92, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70,
        32, 92, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.026, 0.11662145912647247)
D00: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D01: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D02: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D03: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D04: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D05: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D06: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D07: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D08: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D09: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D010: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D011: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D012: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D013: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D014: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D015: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D016: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D017: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D018: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D019: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D020: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D021: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D022: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D023: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO1', '(DO2']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.214, 0.13939608415961266) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2085, 0.13738772463798524) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2215, 0.13582113835215567) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.247, 0.17927399343252182) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.239, 0.17473341812193394) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.17821416395902634) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.252, 0.2262220277786255) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.258, 0.2292762549519539) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.263, 0.22453040020167828) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.246, 0.28435525691509245) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.28962296010553834) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.28246956259012224) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2535, 0.31421481040120125) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.35805536733567717) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.271, 0.3169645260870457) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO1', '(DO0']
DC 2 --> ['(DO4', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.35594364973902703) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.3427839697897434) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2725, 0.32814462769031527) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2565, 0.37150038585066797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.36045402199029924) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2705, 0.34569668626785277) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2545, 0.3613056474030018) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.36281399366259576) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.36248317345976827) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2585, 0.37159205186367034) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2595, 0.3757147751003504) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2745, 0.38252213567495347) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2525, 0.394279827952385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.39636490201950075) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.266, 0.3886403323709965) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO3', '(DO1']
DC 1 --> ['(DO0', '(DO4']
DC 2 --> ['(DO5', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.384207865267992) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2595, 0.3965814482867718) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.4038893460333347) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.411532920986414) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2395, 0.4708445436805487) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.4154665951728821) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.40831105291843417) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2535, 0.4182642904520035) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.259, 0.4435438338518143) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.249, 0.39421516048908234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2505, 0.425472064524889) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.4331565588116646) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.39789602187275885) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2425, 0.4226730733215809) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.40071369749307634) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO5', '(DO0']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO3', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.249, 0.3905569221973419) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.24, 0.4229372771382332) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.255, 0.41381391942501067) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.3952914291024208) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2575, 0.39488445821404455) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.40673647356033327) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2395, 0.3765286412835121) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2415, 0.3970064467191696) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2485, 0.43263175803422926) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.252, 0.38714557948708533) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.249, 0.37885337403416636) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.267, 0.41238060522079467) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.37151946711540224) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2595, 0.3840288952589035) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.4275773882865906) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO1', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2465, 0.37754163587093353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.36326762443780897) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.41118375104665755) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.37230688154697417) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.247, 0.3631187734603882) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.39750655442476274) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.253, 0.366032836407423) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2535, 0.35164704322814944) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.259, 0.3875456431508064) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.255, 0.33163026863336564) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.246, 0.3436571171879768) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.259, 0.3691788805127144) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.3428802158534527) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.3297464409768581) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.3911461799144745) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO1', '(DO5']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.3200793561041355) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2515, 0.3281160180568695) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2665, 0.34436738097667696) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.3319777855277061) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.253, 0.3251419457793236) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.35390010675787925) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.243, 0.32107167744636533) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2555, 0.32683301354944705) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.266, 0.3351826696097851) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.325256324917078) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.253, 0.32088302570581434) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.35940741831064227) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.253, 0.3177112819552422) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.3188911024928093) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2525, 0.3375042434334755) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO3', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.3449138906598091) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2505, 0.32901144187152387) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.245, 0.3484740172624588) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.2999446452260017) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.34083813887834546) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.247, 0.3376344500184059) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.30014830303192136) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2535, 0.335278795838356) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2445, 0.3412100088596344) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.33278782343864444) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2565, 0.31385395607352257) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.33987638652324675) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2445, 0.31233155870437623) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.249, 0.35103599065542224) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.36082790964841843) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO0', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.28816353344917295) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2505, 0.30678489136695863) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2615, 0.328773213326931) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2405, 0.2986738958060741) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2535, 0.33733435297012326) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.259, 0.3036590536236763) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2515, 0.29464098542928696) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.244, 0.3033877473026514) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.267, 0.30540064704418185) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.29567429012060165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.248, 0.29114073207974434) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2535, 0.3028826933503151) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.243, 0.2934592404961586) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2435, 0.3169556147456169) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.3171485747694969) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO5', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.256, 0.26689215111732484) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.253, 0.2961744694709778) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.2800693606734276) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2515, 0.26792013305425644) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2485, 0.28537421894073484) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2565, 0.2863258340358734) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.2990308040976524) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.2998753824532032) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2555, 0.2718093654513359) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2585, 0.2841039626002312) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.244, 0.2957997730374336) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.3020268666148186) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.252, 0.3059673483371735) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2425, 0.319694933116436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.2661362190246582) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO5', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.256, 0.28471740502119064) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.247, 0.3075719975829124) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.261, 0.2765061337351799) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.254, 0.27962376600503924) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2545, 0.28802009600400924) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.2638256653547287) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2505, 0.2965780769586563) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.253, 0.265920966476202) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.27324718809127807) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.28470204120874404) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.28501966093480585) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2615, 0.2766160883307457) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.25769533950090406) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.253, 0.2757972536087036) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.25773456066846845) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.021, 0.11636942088603973), (0.214, 0.13939608415961266), (0.247, 0.17927399343252182), (0.252, 0.2262220277786255), (0.246, 0.28435525691509245), (0.2535, 0.31421481040120125), (0.251, 0.35594364973902703), (0.2565, 0.37150038585066797), (0.2545, 0.3613056474030018), (0.2585, 0.37159205186367034), (0.2525, 0.394279827952385), (0.251, 0.384207865267992), (0.251, 0.411532920986414), (0.241, 0.40831105291843417), (0.249, 0.39421516048908234), (0.241, 0.39789602187275885), (0.249, 0.3905569221973419), (0.244, 0.3952914291024208), (0.2395, 0.3765286412835121), (0.252, 0.38714557948708533), (0.2475, 0.37151946711540224), (0.2465, 0.37754163587093353), (0.235, 0.37230688154697417), (0.253, 0.366032836407423), (0.255, 0.33163026863336564), (0.244, 0.3428802158534527), (0.2455, 0.3200793561041355), (0.2495, 0.3319777855277061), (0.243, 0.32107167744636533), (0.241, 0.325256324917078), (0.253, 0.3177112819552422), (0.245, 0.3449138906598091), (0.2455, 0.2999446452260017), (0.241, 0.30014830303192136), (0.245, 0.33278782343864444), (0.2445, 0.31233155870437623), (0.245, 0.28816353344917295), (0.2405, 0.2986738958060741), (0.2515, 0.29464098542928696), (0.2455, 0.29567429012060165), (0.243, 0.2934592404961586), (0.256, 0.26689215111732484), (0.2515, 0.26792013305425644), (0.245, 0.2990308040976524), (0.2585, 0.2841039626002312), (0.252, 0.3059673483371735), (0.256, 0.28471740502119064), (0.254, 0.27962376600503924), (0.2505, 0.2965780769586563), (0.25, 0.28470204120874404), (0.257, 0.25769533950090406)]
TEST: 
[(0.022, 0.11542808198928833), (0.228, 0.13712789285182952), (0.2585, 0.17487822914123535), (0.26, 0.22097817826271057), (0.26175, 0.2746106003522873), (0.26375, 0.3025626410245895), (0.268, 0.34085119831562044), (0.26325, 0.3534361546039581), (0.2595, 0.34845002806186676), (0.25525, 0.3594417545795441), (0.2535, 0.38085241305828094), (0.25625, 0.37145351922512054), (0.262, 0.3957788883447647), (0.255, 0.39614760160446166), (0.25475, 0.38537072932720184), (0.24775, 0.3926036353111267), (0.255, 0.38326462531089783), (0.25225, 0.3888289406299591), (0.254, 0.37376915419101714), (0.251, 0.38163549470901487), (0.25725, 0.36525223553180697), (0.24875, 0.37951096963882447), (0.24225, 0.36879388225078585), (0.2495, 0.3588755655288696), (0.254, 0.3263905860185623), (0.24525, 0.3387190123796463), (0.2435, 0.3149093911647797), (0.24625, 0.32546456611156466), (0.25125, 0.3191968857049942), (0.25025, 0.3210345788002014), (0.2475, 0.3063545740842819), (0.25175, 0.33711014366149905), (0.25125, 0.2909411482810974), (0.2455, 0.2927738391160965), (0.25075, 0.3289991512298584), (0.24875, 0.30620910584926603), (0.2485, 0.28094683957099914), (0.251, 0.29511586189270017), (0.251, 0.29690338027477264), (0.247, 0.29952940773963926), (0.24675, 0.29174363243579865), (0.25575, 0.26138919401168825), (0.25175, 0.26726080405712127), (0.25425, 0.2958085730075836), (0.255, 0.28724400675296785), (0.25775, 0.3007722177505493), (0.259, 0.2797273191213608), (0.25375, 0.27702967286109925), (0.25475, 0.29434533715248107), (0.2585, 0.28074825334548953), (0.25825, 0.2598725153207779)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.48      0.72      0.57       100
           1       0.30      0.71      0.42       100
           6       0.22      0.58      0.32       100
           8       0.00      0.00      0.00       100
           9       0.00      0.00      0.00       100
          10       0.00      0.00      0.00       100
          12       0.31      0.51      0.39       100
          13       0.22      0.45      0.30       100
          14       0.24      0.37      0.29       100
          17       0.75      0.06      0.11       100
          19       0.00      0.00      0.00       100
          21       0.00      0.00      0.00       100
          23       0.46      0.65      0.54       100
          24       0.40      0.70      0.51       100
          26       0.00      0.00      0.00       100
          33       0.67      0.04      0.08       100
          36       0.23      0.37      0.29       100
          38       0.00      0.00      0.00       100
          42       1.00      0.01      0.02       100
          44       0.00      0.00      0.00       100
          46       0.22      0.47      0.30       100
          52       0.46      0.87      0.60       100
          53       0.80      0.12      0.21       100
          54       0.44      0.59      0.50       100
          59       0.00      0.00      0.00       100
          64       0.15      0.22      0.18       100
          65       0.10      0.34      0.15       100
          69       0.46      0.69      0.55       100
          72       0.12      0.24      0.16       100
          73       0.00      0.00      0.00       100
          74       0.14      0.39      0.21       100
          77       0.00      0.00      0.00       100
          79       1.00      0.03      0.06       100
          87       0.00      0.00      0.00       100
          88       1.00      0.01      0.02       100
          89       0.23      0.51      0.32       100
          90       0.00      0.00      0.00       100
          93       0.14      0.31      0.19       100
          98       0.00      0.00      0.00       100
          99       0.17      0.37      0.23       100

    accuracy                           0.26      4000
   macro avg       0.27      0.26      0.19      4000
weighted avg       0.27      0.26      0.19      4000

Competition_DC_1
VAL: 
[(0.0275, 0.11640495622158051), (0.2085, 0.13738772463798524), (0.239, 0.17473341812193394), (0.258, 0.2292762549519539), (0.2605, 0.28962296010553834), (0.263, 0.35805536733567717), (0.2635, 0.3427839697897434), (0.263, 0.36045402199029924), (0.256, 0.36281399366259576), (0.2595, 0.3757147751003504), (0.2685, 0.39636490201950075), (0.2595, 0.3965814482867718), (0.2395, 0.4708445436805487), (0.2535, 0.4182642904520035), (0.2505, 0.425472064524889), (0.2425, 0.4226730733215809), (0.24, 0.4229372771382332), (0.2575, 0.39488445821404455), (0.2415, 0.3970064467191696), (0.249, 0.37885337403416636), (0.2595, 0.3840288952589035), (0.251, 0.36326762443780897), (0.247, 0.3631187734603882), (0.2535, 0.35164704322814944), (0.246, 0.3436571171879768), (0.25, 0.3297464409768581), (0.2515, 0.3281160180568695), (0.253, 0.3251419457793236), (0.2555, 0.32683301354944705), (0.253, 0.32088302570581434), (0.252, 0.3188911024928093), (0.2505, 0.32901144187152387), (0.2525, 0.34083813887834546), (0.2535, 0.335278795838356), (0.2565, 0.31385395607352257), (0.249, 0.35103599065542224), (0.2505, 0.30678489136695863), (0.2535, 0.33733435297012326), (0.244, 0.3033877473026514), (0.248, 0.29114073207974434), (0.2435, 0.3169556147456169), (0.253, 0.2961744694709778), (0.2485, 0.28537421894073484), (0.2525, 0.2998753824532032), (0.244, 0.2957997730374336), (0.2425, 0.319694933116436), (0.247, 0.3075719975829124), (0.2545, 0.28802009600400924), (0.253, 0.265920966476202), (0.2525, 0.28501966093480585), (0.253, 0.2757972536087036)]
TEST: 
[(0.026, 0.1154832991361618), (0.21275, 0.13598166090250016), (0.2445, 0.17370991337299346), (0.2535, 0.22700007712841033), (0.26325, 0.2849153835773468), (0.265, 0.3544698557853699), (0.25975, 0.33811635529994966), (0.267, 0.3598675744533539), (0.2685, 0.35872767102718356), (0.253, 0.3731823440790176), (0.26075, 0.39281567883491514), (0.2585, 0.3916116859912872), (0.249, 0.46580397987365724), (0.253, 0.40981045937538146), (0.26325, 0.4184316952228546), (0.25825, 0.41552443099021913), (0.242, 0.41867806553840636), (0.249, 0.39402566087245944), (0.24875, 0.39318994998931883), (0.2575, 0.3758527032136917), (0.2545, 0.3762929335832596), (0.25825, 0.3569750165939331), (0.25225, 0.3545649626255035), (0.2495, 0.3418493914604187), (0.24825, 0.33899042177200317), (0.25375, 0.32018236529827115), (0.2485, 0.31953349328041075), (0.249, 0.3255521348714828), (0.2545, 0.3311178476810455), (0.2635, 0.31791083800792697), (0.255, 0.315279781460762), (0.2515, 0.3237407420873642), (0.2435, 0.3300068584680557), (0.25225, 0.32587874364852903), (0.25775, 0.3058299707174301), (0.255, 0.3423615208864212), (0.25825, 0.2943076263666153), (0.25825, 0.3256989387273789), (0.25525, 0.2908982996940613), (0.26075, 0.2844013522863388), (0.25125, 0.3133639417886734), (0.25725, 0.28946692967414855), (0.25525, 0.28236931467056275), (0.26425, 0.29786110651493075), (0.256, 0.28949179923534396), (0.25425, 0.31143713426589964), (0.248, 0.304437531709671), (0.26075, 0.2802961968183518), (0.257, 0.2565288848876953), (0.255, 0.2781299154758453), (0.258, 0.27195362269878387)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.26      0.30      0.28       100
           8       0.00      0.00      0.00       100
           9       1.00      0.02      0.04       100
          10       0.00      0.00      0.00       100
          11       0.17      0.30      0.22       100
          17       0.00      0.00      0.00       100
          19       0.00      0.00      0.00       100
          20       0.51      0.71      0.59       100
          21       0.00      0.00      0.00       100
          22       0.17      0.42      0.24       100
          25       0.14      0.28      0.19       100
          26       0.00      0.00      0.00       100
          29       0.20      0.50      0.29       100
          33       0.67      0.06      0.11       100
          35       0.18      0.25      0.21       100
          38       0.10      0.01      0.02       100
          39       0.36      0.57      0.44       100
          40       0.19      0.48      0.28       100
          42       0.12      0.01      0.02       100
          44       0.00      0.00      0.00       100
          53       0.74      0.25      0.37       100
          55       0.10      0.27      0.15       100
          56       0.29      0.71      0.41       100
          58       0.28      0.65      0.39       100
          59       0.00      0.00      0.00       100
          60       0.56      0.69      0.62       100
          66       0.13      0.37      0.19       100
          67       0.27      0.53      0.36       100
          71       0.50      0.80      0.61       100
          73       0.52      0.11      0.18       100
          75       0.49      0.66      0.56       100
          77       0.00      0.00      0.00       100
          79       0.40      0.02      0.04       100
          80       0.12      0.27      0.16       100
          83       0.31      0.67      0.43       100
          87       0.00      0.00      0.00       100
          88       0.00      0.00      0.00       100
          90       0.25      0.02      0.04       100
          97       0.28      0.39      0.33       100
          98       0.00      0.00      0.00       100

    accuracy                           0.26      4000
   macro avg       0.23      0.26      0.19      4000
weighted avg       0.23      0.26      0.19      4000

Competition_DC_2
VAL: 
[(0.026, 0.11662145912647247), (0.2215, 0.13582113835215567), (0.249, 0.17821416395902634), (0.263, 0.22453040020167828), (0.2625, 0.28246956259012224), (0.271, 0.3169645260870457), (0.2725, 0.32814462769031527), (0.2705, 0.34569668626785277), (0.2625, 0.36248317345976827), (0.2745, 0.38252213567495347), (0.266, 0.3886403323709965), (0.2595, 0.4038893460333347), (0.2645, 0.4154665951728821), (0.259, 0.4435438338518143), (0.253, 0.4331565588116646), (0.253, 0.40071369749307634), (0.255, 0.41381391942501067), (0.264, 0.40673647356033327), (0.2485, 0.43263175803422926), (0.267, 0.41238060522079467), (0.26, 0.4275773882865906), (0.265, 0.41118375104665755), (0.254, 0.39750655442476274), (0.259, 0.3875456431508064), (0.259, 0.3691788805127144), (0.258, 0.3911461799144745), (0.2665, 0.34436738097667696), (0.264, 0.35390010675787925), (0.266, 0.3351826696097851), (0.2625, 0.35940741831064227), (0.2525, 0.3375042434334755), (0.245, 0.3484740172624588), (0.247, 0.3376344500184059), (0.2445, 0.3412100088596344), (0.2605, 0.33987638652324675), (0.26, 0.36082790964841843), (0.2615, 0.328773213326931), (0.259, 0.3036590536236763), (0.267, 0.30540064704418185), (0.2535, 0.3028826933503151), (0.2585, 0.3171485747694969), (0.2605, 0.2800693606734276), (0.2565, 0.2863258340358734), (0.2555, 0.2718093654513359), (0.2645, 0.3020268666148186), (0.26, 0.2661362190246582), (0.261, 0.2765061337351799), (0.2645, 0.2638256653547287), (0.258, 0.27324718809127807), (0.2615, 0.2766160883307457), (0.2575, 0.25773456066846845)]
TEST: 
[(0.025, 0.11568001115322113), (0.23125, 0.13377222967147828), (0.25175, 0.17618851602077484), (0.2745, 0.22095151805877686), (0.27125, 0.2771901915073395), (0.2735, 0.31401518535614015), (0.27125, 0.3237093061208725), (0.27175, 0.3420405431985855), (0.271, 0.36097063827514647), (0.275, 0.3841755056381226), (0.27, 0.3867848100662231), (0.26025, 0.4055904698371887), (0.268, 0.41568959212303164), (0.26675, 0.44051887702941894), (0.2645, 0.4300026590824127), (0.2525, 0.39093274450302123), (0.26, 0.40721351504325864), (0.24975, 0.4057823352813721), (0.26375, 0.4235343897342682), (0.2615, 0.4003398656845093), (0.261, 0.4159945075511932), (0.25675, 0.40385767531394956), (0.26175, 0.38904056572914125), (0.25875, 0.37915701484680175), (0.2505, 0.3551512485742569), (0.256, 0.37654368829727175), (0.25425, 0.3370885360240936), (0.24875, 0.3442557672262192), (0.257, 0.326560910820961), (0.2555, 0.3511294333934784), (0.25225, 0.3279659832715988), (0.2515, 0.34028616273403167), (0.24875, 0.3274430460929871), (0.2525, 0.33215852856636047), (0.25625, 0.33201380383968354), (0.251, 0.352508714556694), (0.25075, 0.32130779683589933), (0.263, 0.29697762382030485), (0.26125, 0.301260253071785), (0.25075, 0.2988525643348694), (0.258, 0.31253264367580413), (0.25775, 0.2778730345964432), (0.257, 0.28112238240242005), (0.2575, 0.2728674297332764), (0.26, 0.29904569959640503), (0.2575, 0.2654263882637024), (0.263, 0.27252947425842283), (0.268, 0.25590955126285553), (0.25575, 0.2715044399499893), (0.26175, 0.27248015797138214), (0.26725, 0.25563910698890685)]
DETAILED: 
              precision    recall  f1-score   support

           8       0.00      0.00      0.00       100
           9       0.75      0.03      0.06       100
          10       0.00      0.00      0.00       100
          17       0.00      0.00      0.00       100
          19       0.33      0.01      0.02       100
          21       0.50      0.03      0.06       100
          26       0.00      0.00      0.00       100
          27       0.16      0.42      0.23       100
          32       0.22      0.35      0.27       100
          33       0.00      0.00      0.00       100
          37       0.17      0.34      0.23       100
          38       0.00      0.00      0.00       100
          42       0.33      0.01      0.02       100
          44       0.00      0.00      0.00       100
          45       0.13      0.30      0.18       100
          48       0.38      0.68      0.49       100
          49       0.34      0.64      0.44       100
          50       0.15      0.38      0.21       100
          51       0.25      0.41      0.31       100
          53       1.00      0.06      0.11       100
          59       1.00      0.02      0.04       100
          63       0.23      0.56      0.33       100
          70       0.31      0.42      0.36       100
          73       0.83      0.10      0.18       100
          76       0.45      0.69      0.54       100
          77       0.00      0.00      0.00       100
          79       0.33      0.01      0.02       100
          81       0.23      0.45      0.30       100
          82       0.39      0.82      0.53       100
          84       0.17      0.36      0.23       100
          85       0.50      0.57      0.53       100
          86       0.29      0.67      0.40       100
          87       0.78      0.07      0.13       100
          88       0.00      0.00      0.00       100
          90       0.00      0.00      0.00       100
          91       0.30      0.56      0.39       100
          92       0.19      0.39      0.25       100
          95       0.35      0.72      0.47       100
          96       0.30      0.62      0.40       100
          98       0.00      0.00      0.00       100

    accuracy                           0.27      4000
   macro avg       0.28      0.27      0.19      4000
weighted avg       0.28      0.27      0.19      4000

Collaboration
DC 0, val_set_size=2000, COIs=[0, 36, 24, 93, 72, 6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99, 1, 69, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([ 0, 36, 24, 93, 72,  6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99,
         1, 69, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.021, 0.11636942088603973)
DC 1, val_set_size=2000, COIs=[20, 56, 29, 75, 11, 97, 39, 2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25, 58, 55, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([20, 56, 29, 75, 11, 97, 39,  2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25,
        58, 55, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.0275, 0.11640495622158051)
DC 2, val_set_size=2000, COIs=[63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70, 32, 92, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70,
        32, 92, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.026, 0.11662145912647247)
D00: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D01: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D02: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D03: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D04: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D05: 1000 samples from classes {8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98}
D06: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D07: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D08: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D09: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D010: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D011: 1000 samples from classes {0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99}
D012: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D013: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D014: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D015: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D016: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D017: 1000 samples from classes {2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97}
D018: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D019: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D020: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D021: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D022: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
D023: 1000 samples from classes {27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO5']
DC 1 --> ['(DO0', '(DO2']
DC 2 --> ['(DO3', '(DO4']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.22, 0.13754756233096121) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2135, 0.13594629448652268) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.22, 0.1368619656562805) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.17683629727363587) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.243, 0.17517612533271312) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2505, 0.18014646819233895) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.247, 0.22790123710036278) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2555, 0.2323345988690853) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.23363830757141113) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.27849779188632967) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.28106806361675263) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.2690845322608948) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.264, 0.35954025813937185) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2575, 0.325802987575531) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2635, 0.3161880518198013) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO3', '(DO0']
DC 2 --> ['(DO4', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.34514185118675234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.257, 0.34110771512240173) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2665, 0.33229723542928696) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.258, 0.33588201507925985) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.3575446818768978) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2635, 0.32901220029592515) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2625, 0.3619445806145668) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.248, 0.3827916401028633) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.3473083771765232) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2585, 0.3479913552105427) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2495, 0.3621383276283741) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.35725149473547935) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.256, 0.3627200526893139) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.253, 0.38088164520263673) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.38205440494418147) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[8, 9, 10, 17, 19, 21, 26, 33, 38, 42, 44, 53, 59, 73, 77, 79, 87, 88, 90, 98], M=tensor([ 0,  1,  2,  6,  8,  9, 10, 11, 12, 13, 14, 17, 19, 20, 21, 22, 23, 24,
        25, 26, 27, 29, 32, 33, 35, 36, 37, 38, 39, 40, 42, 44, 45, 46, 48, 49,
        50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 63, 64, 65, 66, 67, 69, 70, 71,
        72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,
        91, 92, 93, 95, 96, 97, 98, 99], device='cuda:0'), Initial Performance: (0.37866666666666665, 0.0712100871304671)
DC Expert-0, val_set_size=1000, COIs=[0, 1, 6, 12, 13, 14, 23, 24, 36, 46, 52, 54, 64, 65, 69, 72, 74, 89, 93, 99], M=tensor([ 0, 36, 24, 93, 72,  6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99,
         1, 69, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.512, 0.067360602080822)
DC Expert-1, val_set_size=1000, COIs=[2, 11, 20, 22, 25, 29, 35, 39, 40, 55, 56, 58, 60, 66, 67, 71, 75, 80, 83, 97], M=tensor([20, 56, 29, 75, 11, 97, 39,  2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25,
        58, 55, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.506, 0.06909288430213928)
DC Expert-2, val_set_size=1000, COIs=[27, 32, 37, 45, 48, 49, 50, 51, 63, 70, 76, 81, 82, 84, 85, 86, 91, 92, 95, 96], M=tensor([63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70,
        32, 92, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), Initial Performance: (0.529, 0.06760012048482895)
SUPER-DC 0, val_set_size=2000, COIs=[0, 36, 24, 93, 72, 6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99, 1, 69, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([ 0, 36, 24, 93, 72,  6, 89, 52, 14, 12, 65, 74, 13, 64, 46, 54, 23, 99,
         1, 69, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[20, 56, 29, 75, 11, 97, 39, 2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25, 58, 55, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([20, 56, 29, 75, 11, 97, 39,  2, 83, 67, 40, 66, 60, 22, 71, 35, 80, 25,
        58, 55, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70, 32, 92, 10, 98, 77, 90, 33, 17, 44, 8, 19, 53, 42, 26, 38, 88, 87, 9, 73, 21, 59, 79], M=tensor([63, 81, 51, 91, 85, 48, 37, 84, 49, 76, 45, 50, 82, 96, 27, 95, 86, 70,
        32, 92, 10, 98, 77, 90, 33, 17, 44,  8, 19, 53, 42, 26, 38, 88, 87,  9,
        73, 21, 59, 79], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.08648990005254746) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.51, 0.09010096037387848) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.498, 0.09282702147960663) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48966666666666664, 0.053266898343960443) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.264, 0.1717180874943733) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2765, 0.17038080313801765) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2735, 0.1866365749537945) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.0960156661272049) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.49, 0.10041000723838805) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.514, 0.08856773310899735) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.513, 0.05375335880120595) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2685, 0.13007165199518203) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.13914632304012775) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.286, 0.13071834284067155) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.501, 0.09862444591522217) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.473, 0.10935098886489868) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.09571514528989791) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5166666666666667, 0.056092088391383486) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.273, 0.12220056968927383) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2735, 0.13767767107486725) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.291, 0.12127682948112488) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.502, 0.10571370071172714) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.5, 0.10819712418317795) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.10496746182441712) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.517, 0.05928762639562289) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.291, 0.11047793754935264) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.281, 0.11963300397992134) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2855, 0.12508830159902573) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.495, 0.10999164891242981) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.506, 0.10534197479486465) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.484, 0.10711307656764985) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.502, 0.06493278499444326) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.292, 0.10937551200389863) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.289, 0.11288745158910751) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3045, 0.1069495246708393) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.5, 0.1040008715391159) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.487, 0.10954358357191085) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.521, 0.10157073068618774) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.487, 0.07465260991454124) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3025, 0.09849542421102524) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3005, 0.10870459163188935) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3015, 0.10113447779417038) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.1172709242105484) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.494, 0.11449075818061828) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.12099974131584168) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5056666666666667, 0.07501654036343097) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.285, 0.10523478996753692) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3045, 0.10559129828214646) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.311, 0.10663085895776749) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.496, 0.11314096212387086) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.507, 0.12185164496302604) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.496, 0.12012920570373535) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5013333333333333, 0.08476929344733557) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.301, 0.09699627548456193) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.306, 0.10046390715241432) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.313, 0.09877859091758728) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.499, 0.12403759407997131) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.497, 0.12669945478439332) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.519, 0.1210158189535141) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4686666666666667, 0.11310844242572785) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3025, 0.10511052742600441) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.329, 0.1030478265285492) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.328, 0.10144329792261124) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.497, 0.13718751800060272) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.487, 0.12714937442541122) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.13218827414512635) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48433333333333334, 0.09770439779261748) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2885, 0.105904726177454) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.315, 0.10272068470716476) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.10073468774557114) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.495, 0.13606039440631867) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.512, 0.12440492296218872) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.515, 0.12050307071208954) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49533333333333335, 0.09419133362174034) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.301, 0.1074554987847805) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.315, 0.09980508601665497) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3265, 0.09957143211364747) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.5, 0.13149469339847564) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.501, 0.1457128323316574) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.512, 0.12857575857639314) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48933333333333334, 0.10792117210229238) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3125, 0.10429721611738205) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3015, 0.10360908104479313) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3245, 0.09884433913230896) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.484, 0.1389663907289505) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.485, 0.1429079533815384) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.13970146942138673) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49766666666666665, 0.09963559579849243) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2905, 0.10857743865251542) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.10652587503194809) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3135, 0.10045487314462662) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.481, 0.14745572793483736) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.477, 0.1447814931869507) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.1390569145679474) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.475, 0.11965360482533773) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3115, 0.10945455318689347) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.10748265755176545) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.309, 0.10594415271282195) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.471, 0.14184701019525528) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.491, 0.12275938642024994) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.495, 0.14128150963783265) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.485, 0.11278336664040883) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.288, 0.1160361858010292) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3075, 0.10685285025835037) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.323, 0.10233748149871826) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.499, 0.14451257479190827) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.487, 0.14134356421232225) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.13984328305721283) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5016666666666667, 0.11652533008654913) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2815, 0.11166840595006942) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.294, 0.1110940090417862) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.323, 0.10356262946128846) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.14797603249549865) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.485, 0.13774592036008834) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.14448174035549163) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4876666666666667, 0.13451911882559459) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3005, 0.11441361647844314) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3065, 0.11527359461784363) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3165, 0.10704363828897476) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.506, 0.15179165863990784) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.5, 0.14795269703865052) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.508, 0.1419614874124527) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49366666666666664, 0.12238369864225387) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.307, 0.11135540306568145) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.299, 0.11149445949494839) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3125, 0.10759366750717163) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.501, 0.16375872242450715) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.497, 0.16364164245128632) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.514, 0.13974320781230926) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4886666666666667, 0.1319245544175307) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3135, 0.11550725597143173) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2965, 0.1164003849029541) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3085, 0.10760109800100326) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.478, 0.15294959604740144) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.476, 0.17056526404619216) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.521, 0.1533623332977295) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4836666666666667, 0.13056691040595372) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3055, 0.11551632618904113) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.11798053035140038) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.304, 0.11041686493158341) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.5, 0.1584139838218689) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.479, 0.16135735166072845) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.513, 0.1501758519411087) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49166666666666664, 0.1456932233174642) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2945, 0.11655270367860794) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2825, 0.12471646767854691) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.295, 0.11795819008350372) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.15129910838603974) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.485, 0.16973363029956817) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.504, 0.16130685198307038) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4876666666666667, 0.13537483763198058) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3035, 0.11539086747169494) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.11865280359983445) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.31, 0.11370717996358871) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.16328485941886903) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.497, 0.1821454201936722) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.518, 0.15658420717716218) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5003333333333333, 0.138807818988959) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2945, 0.11895864963531494) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.284, 0.12424389538168908) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.11296365767717362) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.489, 0.1663613064289093) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.493, 0.1805477398633957) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.502, 0.16269618678092956) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49766666666666665, 0.13303855601946513) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3115, 0.1188361667394638) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.283, 0.1295003347992897) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3, 0.11838702827692033) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.489, 0.17271713602542876) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.518, 0.16358059084415436) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.16114715015888215) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47933333333333333, 0.15473410491148631) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3135, 0.1202499902844429) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3025, 0.12388654416799545) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.305, 0.11854500710964203) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.48, 0.17913997447490693) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.513, 0.17460842609405516) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.512, 0.16297760605812073) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4766666666666667, 0.16177215421199798) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.306, 0.12387669855356216) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2975, 0.12445749187469482) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3085, 0.12372587078809738) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.1691481190919876) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.494, 0.19967507791519165) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.514, 0.16147469890117644) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48033333333333333, 0.1510108225742976) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3035, 0.11890108007192611) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.285, 0.12914277285337447) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3175, 0.11511567586660384) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.49, 0.1761505424976349) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.509, 0.16503609323501586) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.503, 0.17097214019298554) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47733333333333333, 0.15900347096721332) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2875, 0.12800482296943663) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.299, 0.13187763080000878) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.298, 0.12312961089611053) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.485, 0.17802113342285156) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.495, 0.17516960471868515) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.494, 0.1726538233757019) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.491, 0.15349630151192348) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.1286894217133522) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.307, 0.12644876128435134) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3215, 0.1208007305264473) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.495, 0.16987402546405792) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.509, 0.18306656503677368) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.515, 0.14414422249794007) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.478, 0.1623439939916134) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.311, 0.12195715063810349) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3095, 0.12762689492106438) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.12901787120103836) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.519, 0.17102243292331695) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.488, 0.1930937718153) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.495, 0.1790567843914032) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.483, 0.152976947983106) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.12498976743221282) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.302, 0.13527510160207748) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.303, 0.12314630329608917) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.494, 0.18962915337085723) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.488, 0.19643070840835572) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.488, 0.17371749591827393) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.484, 0.16718500558535257) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2865, 0.13304683232307435) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.302, 0.1390278951525688) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3085, 0.126568521797657) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.497, 0.18988667345046997) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.501, 0.18917556023597717) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.519, 0.17424859356880187) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48133333333333334, 0.16749623998006186) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2725, 0.13349092698097229) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.13482949060201646) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.297, 0.12725613540410996) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.508, 0.1728043568134308) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.498, 0.18417282593250275) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.17876847386360167) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.488, 0.16046324370304743) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2945, 0.13043430519104005) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3085, 0.13173856416344643) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.12429342544078827) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.477, 0.18830975699424743) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.481, 0.1911516990661621) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.512, 0.16567130875587463) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4736666666666667, 0.15470038871467112) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2885, 0.13006898325681687) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.287, 0.1330917713046074) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3155, 0.12623638570308685) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.517, 0.15591903114318847) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.493, 0.20512291538715363) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.507, 0.16856263780593872) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48966666666666664, 0.15071712440252305) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.292, 0.1256819988489151) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.13435154050588607) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.304, 0.12193676209449768) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.513, 0.17451327919960022) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.493, 0.18594511073827744) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.497, 0.17876094186306) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4856666666666667, 0.16119454747935136) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2855, 0.13094776159524918) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2945, 0.13589299130439758) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2925, 0.12826381450891494) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.488, 0.19220037746429444) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.483, 0.19714214503765107) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.514, 0.1765787513256073) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4776666666666667, 0.1663808355530103) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.13516752332448959) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.292, 0.1330181406736374) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.13146318507194518) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.496, 0.1759144172668457) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.497, 0.1826478044986725) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.521, 0.1889096302986145) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48133333333333334, 0.17806888519475858) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2905, 0.13568974101543427) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.292, 0.1390604500770569) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3125, 0.13303936886787415) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.504, 0.1780208854675293) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.493, 0.19102665209770203) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.525, 0.1848683022260666) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4846666666666667, 0.174953265927732) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2945, 0.13834034103155135) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.283, 0.13859611040353775) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.288, 0.1362591621875763) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.021, 0.11636942088603973), (0.22, 0.13754756233096121), (0.2475, 0.17683629727363587), (0.247, 0.22790123710036278), (0.2555, 0.27849779188632967), (0.264, 0.35954025813937185), (0.2595, 0.34514185118675234), (0.258, 0.33588201507925985), (0.2625, 0.3619445806145668), (0.2585, 0.3479913552105427), (0.256, 0.3627200526893139), (0.264, 0.1717180874943733), (0.2685, 0.13007165199518203), (0.273, 0.12220056968927383), (0.291, 0.11047793754935264), (0.292, 0.10937551200389863), (0.3025, 0.09849542421102524), (0.285, 0.10523478996753692), (0.301, 0.09699627548456193), (0.3025, 0.10511052742600441), (0.2885, 0.105904726177454), (0.301, 0.1074554987847805), (0.3125, 0.10429721611738205), (0.2905, 0.10857743865251542), (0.3115, 0.10945455318689347), (0.288, 0.1160361858010292), (0.2815, 0.11166840595006942), (0.3005, 0.11441361647844314), (0.307, 0.11135540306568145), (0.3135, 0.11550725597143173), (0.3055, 0.11551632618904113), (0.2945, 0.11655270367860794), (0.3035, 0.11539086747169494), (0.2945, 0.11895864963531494), (0.3115, 0.1188361667394638), (0.3135, 0.1202499902844429), (0.306, 0.12387669855356216), (0.3035, 0.11890108007192611), (0.2875, 0.12800482296943663), (0.2895, 0.1286894217133522), (0.311, 0.12195715063810349), (0.294, 0.12498976743221282), (0.2865, 0.13304683232307435), (0.2725, 0.13349092698097229), (0.2945, 0.13043430519104005), (0.2885, 0.13006898325681687), (0.292, 0.1256819988489151), (0.2855, 0.13094776159524918), (0.2895, 0.13516752332448959), (0.2905, 0.13568974101543427), (0.2945, 0.13834034103155135)]
TEST: 
[(0.022, 0.11542808198928833), (0.23025, 0.1350580322742462), (0.258, 0.1721940666437149), (0.26875, 0.22401119685173035), (0.265, 0.2719587595462799), (0.2715, 0.34088577270507814), (0.27125, 0.33269723427295683), (0.27375, 0.32256564462184906), (0.26975, 0.34961671113967896), (0.26825, 0.33769538712501523), (0.26325, 0.353266485452652), (0.27375, 0.1666022797226906), (0.27875, 0.12812242442369462), (0.286, 0.1198500275015831), (0.304, 0.10727544116973876), (0.298, 0.10831757670640946), (0.31425, 0.09599323019385338), (0.2895, 0.10308893954753875), (0.3115, 0.09454651707410812), (0.3115, 0.09901977398991585), (0.30125, 0.10276226511597633), (0.312, 0.10529960697889328), (0.30975, 0.10218155676126481), (0.2915, 0.10500222575664521), (0.30825, 0.10598615247011185), (0.29875, 0.11148410141468049), (0.306, 0.11009669196605683), (0.29425, 0.11486982250213623), (0.31075, 0.11104898971319199), (0.32, 0.11499910920858383), (0.31225, 0.1149790575504303), (0.3125, 0.11867131328582764), (0.306, 0.11545756322145462), (0.30375, 0.11766995739936828), (0.31375, 0.11731189322471619), (0.3145, 0.1181103395819664), (0.29175, 0.12442216521501541), (0.30825, 0.11864646255970002), (0.287, 0.12467806720733643), (0.2935, 0.12756191408634185), (0.3145, 0.12189207917451858), (0.306, 0.12305258738994598), (0.29775, 0.1304811789393425), (0.29525, 0.13089764946699142), (0.3105, 0.12912501841783525), (0.29875, 0.12598718041181564), (0.30525, 0.12525646615028382), (0.3085, 0.12940307718515395), (0.28475, 0.13378150570392608), (0.30175, 0.13354092699289322), (0.29975, 0.13546243304014205)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.61      0.48      0.54       100
           1       0.35      0.37      0.36       100
           6       0.23      0.27      0.25       100
           8       0.34      0.22      0.27       100
           9       0.36      0.29      0.32       100
          10       0.33      0.16      0.22       100
          12       0.24      0.46      0.31       100
          13       0.26      0.42      0.32       100
          14       0.24      0.09      0.13       100
          17       0.33      0.21      0.26       100
          19       0.22      0.23      0.23       100
          21       0.48      0.48      0.48       100
          23       0.53      0.62      0.57       100
          24       0.47      0.67      0.55       100
          26       0.26      0.14      0.18       100
          33       0.52      0.15      0.23       100
          36       0.33      0.27      0.30       100
          38       0.15      0.20      0.17       100
          42       0.20      0.17      0.18       100
          44       0.26      0.08      0.12       100
          46       0.20      0.22      0.21       100
          52       0.56      0.45      0.50       100
          53       0.43      0.72      0.54       100
          54       0.39      0.52      0.44       100
          59       0.24      0.39      0.30       100
          64       0.10      0.12      0.11       100
          65       0.15      0.14      0.14       100
          69       0.51      0.57      0.54       100
          72       0.11      0.13      0.12       100
          73       0.35      0.44      0.39       100
          74       0.22      0.31      0.26       100
          77       0.15      0.11      0.13       100
          79       0.33      0.16      0.21       100
          87       0.40      0.40      0.40       100
          88       0.15      0.06      0.09       100
          89       0.28      0.19      0.22       100
          90       0.22      0.20      0.21       100
          93       0.28      0.21      0.24       100
          98       0.21      0.36      0.27       100
          99       0.22      0.31      0.26       100

    accuracy                           0.30      4000
   macro avg       0.30      0.30      0.29      4000
weighted avg       0.30      0.30      0.29      4000

Collaboration_DC_1
VAL: 
[(0.0275, 0.11640495622158051), (0.2135, 0.13594629448652268), (0.243, 0.17517612533271312), (0.2555, 0.2323345988690853), (0.2525, 0.28106806361675263), (0.2575, 0.325802987575531), (0.257, 0.34110771512240173), (0.2635, 0.3575446818768978), (0.248, 0.3827916401028633), (0.2495, 0.3621383276283741), (0.253, 0.38088164520263673), (0.2765, 0.17038080313801765), (0.273, 0.13914632304012775), (0.2735, 0.13767767107486725), (0.281, 0.11963300397992134), (0.289, 0.11288745158910751), (0.3005, 0.10870459163188935), (0.3045, 0.10559129828214646), (0.306, 0.10046390715241432), (0.329, 0.1030478265285492), (0.315, 0.10272068470716476), (0.315, 0.09980508601665497), (0.3015, 0.10360908104479313), (0.2745, 0.10652587503194809), (0.301, 0.10748265755176545), (0.3075, 0.10685285025835037), (0.294, 0.1110940090417862), (0.3065, 0.11527359461784363), (0.299, 0.11149445949494839), (0.2965, 0.1164003849029541), (0.305, 0.11798053035140038), (0.2825, 0.12471646767854691), (0.293, 0.11865280359983445), (0.284, 0.12424389538168908), (0.283, 0.1295003347992897), (0.3025, 0.12388654416799545), (0.2975, 0.12445749187469482), (0.285, 0.12914277285337447), (0.299, 0.13187763080000878), (0.307, 0.12644876128435134), (0.3095, 0.12762689492106438), (0.302, 0.13527510160207748), (0.302, 0.1390278951525688), (0.3165, 0.13482949060201646), (0.3085, 0.13173856416344643), (0.287, 0.1330917713046074), (0.312, 0.13435154050588607), (0.2945, 0.13589299130439758), (0.292, 0.1330181406736374), (0.292, 0.1390604500770569), (0.283, 0.13859611040353775)]
TEST: 
[(0.026, 0.1154832991361618), (0.22275, 0.13456794476509093), (0.247, 0.17304520946741103), (0.26175, 0.22963531816005706), (0.25525, 0.27812759625911715), (0.265, 0.32254131460189817), (0.2665, 0.33583824145793917), (0.2645, 0.35353563380241393), (0.25575, 0.37498036205768587), (0.2605, 0.3560092433691025), (0.26675, 0.37404692089557645), (0.27575, 0.1692203459739685), (0.27625, 0.13853117060661316), (0.279, 0.13606086641550064), (0.281, 0.12165146565437317), (0.29575, 0.1128076170682907), (0.29025, 0.10825631141662598), (0.29725, 0.10604856795072555), (0.31375, 0.09952655810117722), (0.31825, 0.1019566039443016), (0.297, 0.10410091355443), (0.31175, 0.09998535379767418), (0.314, 0.10402970933914185), (0.3065, 0.10693801724910736), (0.2995, 0.10600824576616287), (0.32, 0.10441043445467948), (0.302, 0.11033288782835007), (0.3065, 0.11122531786561013), (0.31225, 0.10999061465263367), (0.30975, 0.1136400379538536), (0.30675, 0.11686309063434601), (0.2975, 0.12067754080891609), (0.3095, 0.11380948948860169), (0.28875, 0.12055308216810226), (0.294, 0.12608089250326157), (0.31125, 0.11946302622556687), (0.3055, 0.12196915739774704), (0.30775, 0.12492803078889847), (0.29375, 0.131566461622715), (0.29575, 0.12424610012769699), (0.30675, 0.12659658896923065), (0.30625, 0.13234245759248733), (0.30675, 0.13663894146680833), (0.30925, 0.13391092437505722), (0.312, 0.12949805521965027), (0.306, 0.128867482483387), (0.311, 0.13251507943868637), (0.28625, 0.1353996849656105), (0.299, 0.13146351736783982), (0.29625, 0.13619972389936447), (0.30175, 0.1351305433511734)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.27      0.20      0.23       100
           8       0.32      0.32      0.32       100
           9       0.28      0.38      0.32       100
          10       0.29      0.27      0.28       100
          11       0.14      0.03      0.05       100
          17       0.52      0.33      0.40       100
          19       0.21      0.33      0.26       100
          20       0.64      0.57      0.60       100
          21       0.33      0.52      0.41       100
          22       0.27      0.21      0.23       100
          25       0.24      0.28      0.26       100
          26       0.23      0.15      0.18       100
          29       0.32      0.36      0.34       100
          33       0.47      0.29      0.36       100
          35       0.13      0.05      0.07       100
          38       0.17      0.21      0.19       100
          39       0.35      0.26      0.30       100
          40       0.35      0.28      0.31       100
          42       0.18      0.17      0.17       100
          44       0.18      0.17      0.17       100
          53       0.41      0.73      0.53       100
          55       0.14      0.13      0.14       100
          56       0.34      0.47      0.39       100
          58       0.41      0.46      0.44       100
          59       0.43      0.26      0.32       100
          60       0.62      0.75      0.68       100
          66       0.19      0.14      0.16       100
          67       0.26      0.42      0.32       100
          71       0.71      0.45      0.55       100
          73       0.30      0.39      0.34       100
          75       0.58      0.42      0.49       100
          77       0.20      0.21      0.21       100
          79       0.31      0.27      0.29       100
          80       0.08      0.06      0.07       100
          83       0.30      0.28      0.29       100
          87       0.28      0.28      0.28       100
          88       0.19      0.11      0.14       100
          90       0.23      0.27      0.25       100
          97       0.22      0.18      0.20       100
          98       0.15      0.41      0.22       100

    accuracy                           0.30      4000
   macro avg       0.31      0.30      0.29      4000
weighted avg       0.31      0.30      0.29      4000

Collaboration_DC_2
VAL: 
[(0.026, 0.11662145912647247), (0.22, 0.1368619656562805), (0.2505, 0.18014646819233895), (0.2645, 0.23363830757141113), (0.2605, 0.2690845322608948), (0.2635, 0.3161880518198013), (0.2665, 0.33229723542928696), (0.2635, 0.32901220029592515), (0.2595, 0.3473083771765232), (0.265, 0.35725149473547935), (0.2645, 0.38205440494418147), (0.2735, 0.1866365749537945), (0.286, 0.13071834284067155), (0.291, 0.12127682948112488), (0.2855, 0.12508830159902573), (0.3045, 0.1069495246708393), (0.3015, 0.10113447779417038), (0.311, 0.10663085895776749), (0.313, 0.09877859091758728), (0.328, 0.10144329792261124), (0.308, 0.10073468774557114), (0.3265, 0.09957143211364747), (0.3245, 0.09884433913230896), (0.3135, 0.10045487314462662), (0.309, 0.10594415271282195), (0.323, 0.10233748149871826), (0.323, 0.10356262946128846), (0.3165, 0.10704363828897476), (0.3125, 0.10759366750717163), (0.3085, 0.10760109800100326), (0.304, 0.11041686493158341), (0.295, 0.11795819008350372), (0.31, 0.11370717996358871), (0.314, 0.11296365767717362), (0.3, 0.11838702827692033), (0.305, 0.11854500710964203), (0.3085, 0.12372587078809738), (0.3175, 0.11511567586660384), (0.298, 0.12312961089611053), (0.3215, 0.1208007305264473), (0.314, 0.12901787120103836), (0.303, 0.12314630329608917), (0.3085, 0.126568521797657), (0.297, 0.12725613540410996), (0.317, 0.12429342544078827), (0.3155, 0.12623638570308685), (0.304, 0.12193676209449768), (0.2925, 0.12826381450891494), (0.2985, 0.13146318507194518), (0.3125, 0.13303936886787415), (0.288, 0.1362591621875763)]
TEST: 
[(0.025, 0.11568001115322113), (0.23025, 0.1342366327047348), (0.25625, 0.17739222007989883), (0.27575, 0.2297095288038254), (0.26925, 0.26670655703544616), (0.26575, 0.3120480726957321), (0.27275, 0.32897703409194945), (0.2625, 0.32764660215377805), (0.27125, 0.3427666646242142), (0.26825, 0.3545177426338196), (0.2675, 0.3815537405014038), (0.2725, 0.1854520691037178), (0.28525, 0.130466967523098), (0.297, 0.12235406732559204), (0.28875, 0.12634137558937072), (0.3025, 0.10748010116815566), (0.30475, 0.10039863449335099), (0.30525, 0.10754251235723496), (0.31275, 0.10063499963283538), (0.314, 0.10392097938060761), (0.3085, 0.10072494333982468), (0.32075, 0.10138307124376297), (0.32625, 0.10007972326874733), (0.312, 0.10181334829330445), (0.31675, 0.10546527343988418), (0.31475, 0.10454169529676437), (0.321, 0.10562206470966339), (0.31025, 0.1097640740275383), (0.31575, 0.10891915315389633), (0.30925, 0.11093940836191177), (0.3285, 0.1087267450094223), (0.31525, 0.11717037504911423), (0.311, 0.11562727457284927), (0.31875, 0.11474949824810028), (0.3145, 0.11903988951444626), (0.31225, 0.11937049174308777), (0.31725, 0.12346678638458251), (0.3195, 0.11744793570041656), (0.29575, 0.12050366538763047), (0.3135, 0.11978803020715713), (0.317, 0.1261662141084671), (0.3215, 0.12325770783424378), (0.30775, 0.12763878625631334), (0.30975, 0.1269691633582115), (0.32975, 0.1263715020418167), (0.3195, 0.128774549305439), (0.305, 0.12563269370794297), (0.285, 0.12987705081701278), (0.2995, 0.13240458387136458), (0.3015, 0.1353227117061615), (0.29675, 0.13765242433547972)]
DETAILED: 
              precision    recall  f1-score   support

           8       0.37      0.33      0.35       100
           9       0.37      0.29      0.32       100
          10       0.46      0.27      0.34       100
          17       0.26      0.24      0.25       100
          19       0.28      0.28      0.28       100
          21       0.43      0.50      0.46       100
          26       0.20      0.09      0.12       100
          27       0.19      0.35      0.25       100
          32       0.12      0.08      0.10       100
          33       0.21      0.06      0.09       100
          37       0.32      0.10      0.15       100
          38       0.20      0.12      0.15       100
          42       0.20      0.14      0.16       100
          44       0.13      0.07      0.09       100
          45       0.15      0.28      0.19       100
          48       0.51      0.40      0.45       100
          49       0.39      0.56      0.46       100
          50       0.14      0.18      0.16       100
          51       0.38      0.25      0.30       100
          53       0.32      0.75      0.45       100
          59       0.31      0.34      0.33       100
          63       0.29      0.48      0.36       100
          70       0.38      0.31      0.34       100
          73       0.39      0.63      0.48       100
          76       0.43      0.55      0.48       100
          77       0.22      0.11      0.15       100
          79       0.43      0.20      0.27       100
          81       0.26      0.23      0.24       100
          82       0.62      0.25      0.36       100
          84       0.14      0.15      0.14       100
          85       0.37      0.41      0.39       100
          86       0.25      0.24      0.25       100
          87       0.34      0.35      0.34       100
          88       0.33      0.13      0.19       100
          90       0.20      0.30      0.24       100
          91       0.46      0.39      0.42       100
          92       0.27      0.24      0.25       100
          95       0.39      0.33      0.36       100
          96       0.28      0.51      0.36       100
          98       0.24      0.38      0.29       100

    accuracy                           0.30      4000
   macro avg       0.31      0.30      0.28      4000
weighted avg       0.31      0.30      0.28      4000

do_assignment: None
seeds: [95]
name: naive-cifar100-feddf95
score_metric: contrloss
aggregation: <function fed_df at 0x7a60f9021e50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=95
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91, 9, 86, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91,
         9, 86, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.025, 0.11661210441589355)
DC 1, val_set_size=2000, COIs=[39, 93, 66, 48, 34, 0, 2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62, 41, 12, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([39, 93, 66, 48, 34,  0,  2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62,
        41, 12, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.025, 0.11657447910308838)
DC 2, val_set_size=2000, COIs=[18, 13, 88, 6, 4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59, 47, 79, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([18, 13, 88,  6,  4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59,
        47, 79, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.0345, 0.11658469200134278)
D00: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D01: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D02: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D03: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D04: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D05: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D06: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D07: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D08: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D09: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D010: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D011: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D012: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D013: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D014: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D015: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D016: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D017: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D018: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D019: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D020: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D021: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D022: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D023: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.2995, 0.08364042228460312) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3095, 0.08201634657382965) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.316, 0.08201313215494156) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.0739146603345871) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3645, 0.07280443269014358) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.07354478538036346) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3895, 0.07053277748823165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3855, 0.07251653423905373) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3955, 0.06929199036955834) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4045, 0.07054091748595238) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4045, 0.07031139296293258) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.402, 0.06997380438446998) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4045, 0.0728291392326355) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4035, 0.07202988198399543) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3895, 0.07335115653276443) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.395, 0.07641686177253723) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3985, 0.07479749894142151) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3875, 0.07556041604280472) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3905, 0.07947719937562943) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3955, 0.07719399720430374) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4015, 0.07825059574842454) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.081802139878273) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.398, 0.08132479721307755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3905, 0.0803280391395092) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.403, 0.08534663993120194) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3955, 0.08380527898669243) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3995, 0.08349217543005943) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4045, 0.09042351484298707) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.392, 0.0891885765492916) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.401, 0.08924299171566963) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.39, 0.09940140679478646) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.409, 0.08806263524293899) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4095, 0.09213249507546425) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.391, 0.10845281049609184) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4025, 0.09560451292991638) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.10163138052821159) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.382, 0.11663922548294067) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.411, 0.10077046963572502) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.398, 0.11140233141183853) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3715, 0.12912609452009202) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.408, 0.10949301025271416) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3925, 0.11948941606283188) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.374, 0.13159463226795196) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3745, 0.13445332515239716) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.394, 0.12859431272745134) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3905, 0.128885536134243) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3725, 0.13665147292613983) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.383, 0.13358487993478774) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.357, 0.14707253152132035) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.13735730016231537) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.13590636253356933) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3755, 0.14882823663949968) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3785, 0.1375231835246086) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.13766176417469977) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3585, 0.15460962837934494) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.397, 0.1423942477107048) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3935, 0.146287089407444) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3615, 0.1529895824790001) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3755, 0.15209560418128967) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.382, 0.15333827906847) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.365, 0.1585643944144249) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.386, 0.15499882835149764) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3935, 0.15233211308717728) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.16306659191846848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3775, 0.16258833128213881) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3815, 0.1750344733595848) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.375, 0.16802862066030502) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3805, 0.16726830369234086) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3885, 0.15836536115407943) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.17120027351379394) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.16146213451027872) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.36, 0.17107941085100173) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.364, 0.17629080069065095) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.38, 0.17055159997940064) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3825, 0.16491007709503175) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3715, 0.1724166848063469) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3645, 0.16944586551189422) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.17352618235349654) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3555, 0.1779253660440445) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3905, 0.17760091096162797) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3785, 0.17844472450017929) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3765, 0.17437378925085067) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.1701675659418106) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.36, 0.17612643480300905) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.374, 0.1763933061361313) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.385, 0.18733427667617797) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.378, 0.18684735038876535) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.37, 0.19013674753904342) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.1880714300274849) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.391, 0.18285865104198457) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.379, 0.18430943328142166) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.18740504759550095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3855, 0.18615579223632814) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3715, 0.1987066445350647) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.375, 0.19136932629346848) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.381, 0.18947600400447845) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.1999964083433151) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.19766877233982086) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3825, 0.18856670087575914) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3595, 0.20841416025161744) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.20655017805099488) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.381, 0.19217507201433182) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.20761215221881865) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3835, 0.19465066522359847) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3705, 0.20677682548761367) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.1985555595755577) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.20152962875366212) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.381, 0.19045810306072236) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3735, 0.20578594160079955) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3905, 0.19162417262792586) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3825, 0.19267515289783477) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.366, 0.21217672300338744) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3905, 0.20573574566841127) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3785, 0.1989546536207199) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.366, 0.21589863002300264) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.387, 0.2132830772995949) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.20148250210285187) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.369, 0.20674948513507843) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3715, 0.21832886171340943) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.21000516843795777) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3665, 0.21881218671798705) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.386, 0.20134688246250151) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.18849462175369264) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3595, 0.21297266000509263) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.38, 0.22197042965888977) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3765, 0.21257275640964507) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.368, 0.22667806804180146) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3765, 0.23435211354494095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.377, 0.20953110730648042) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.367, 0.2267571016550064) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.22377198296785356) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3775, 0.20566613405942916) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.37, 0.2235977360010147) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3825, 0.22945407032966614) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.384, 0.22621862417459487) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3735, 0.23328440499305725) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.379, 0.23944648933410645) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.368, 0.24112453091144562) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.361, 0.23249373865127562) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3885, 0.22686034578084946) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.365, 0.22541932415962218) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3585, 0.22310904479026794) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.24182513785362245) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.383, 0.23027026498317718) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3615, 0.2298804748058319) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.38, 0.23501720869541168) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.377, 0.21208670645952224) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.2294162495136261) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.2368087886571884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.22934970599412918) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.025, 0.11661210441589355), (0.2995, 0.08364042228460312), (0.3695, 0.0739146603345871), (0.3895, 0.07053277748823165), (0.4045, 0.07054091748595238), (0.4045, 0.0728291392326355), (0.395, 0.07641686177253723), (0.3905, 0.07947719937562943), (0.398, 0.081802139878273), (0.403, 0.08534663993120194), (0.4045, 0.09042351484298707), (0.39, 0.09940140679478646), (0.391, 0.10845281049609184), (0.382, 0.11663922548294067), (0.3715, 0.12912609452009202), (0.374, 0.13159463226795196), (0.3905, 0.128885536134243), (0.357, 0.14707253152132035), (0.3755, 0.14882823663949968), (0.3585, 0.15460962837934494), (0.3615, 0.1529895824790001), (0.365, 0.1585643944144249), (0.3725, 0.16306659191846848), (0.375, 0.16802862066030502), (0.3725, 0.17120027351379394), (0.364, 0.17629080069065095), (0.3715, 0.1724166848063469), (0.3555, 0.1779253660440445), (0.3765, 0.17437378925085067), (0.374, 0.1763933061361313), (0.37, 0.19013674753904342), (0.379, 0.18430943328142166), (0.3715, 0.1987066445350647), (0.3725, 0.1999964083433151), (0.3595, 0.20841416025161744), (0.3725, 0.20761215221881865), (0.378, 0.1985555595755577), (0.3735, 0.20578594160079955), (0.366, 0.21217672300338744), (0.366, 0.21589863002300264), (0.369, 0.20674948513507843), (0.3665, 0.21881218671798705), (0.3595, 0.21297266000509263), (0.368, 0.22667806804180146), (0.367, 0.2267571016550064), (0.37, 0.2235977360010147), (0.3735, 0.23328440499305725), (0.361, 0.23249373865127562), (0.3585, 0.22310904479026794), (0.3615, 0.2298804748058319), (0.3695, 0.2294162495136261)]
TEST: 
[(0.0255, 0.11574103558063507), (0.311, 0.08272946220636368), (0.38675, 0.07202922433614731), (0.397, 0.0691679984331131), (0.41075, 0.06792748594284058), (0.415, 0.07040429529547691), (0.41, 0.07363260653614997), (0.41775, 0.07678098067641258), (0.4155, 0.07948572409152985), (0.4145, 0.08222416079044342), (0.4055, 0.08758346244692802), (0.40725, 0.09460931074619293), (0.4015, 0.1048622224032879), (0.38575, 0.11006593936681748), (0.38925, 0.12263095140457153), (0.39325, 0.12511924117803574), (0.39975, 0.12366419631242752), (0.37475, 0.14387345957756043), (0.37075, 0.14655107218027114), (0.37, 0.15090623593330382), (0.38075, 0.14816728621721267), (0.378, 0.1566410019993782), (0.3885, 0.15753126233816148), (0.37275, 0.1603438152074814), (0.37875, 0.16647143709659576), (0.375, 0.1705155643224716), (0.38175, 0.16820613300800324), (0.372, 0.1697371726632118), (0.39375, 0.16833663958311082), (0.3825, 0.17140892750024794), (0.3845, 0.18060972607135772), (0.391, 0.17321150451898576), (0.385, 0.18777203565835954), (0.3815, 0.18788163727521898), (0.382, 0.19677244538068772), (0.3815, 0.19435014331340789), (0.38325, 0.19403344047069548), (0.38775, 0.2046455255150795), (0.381, 0.20490690499544142), (0.37175, 0.20436259442567825), (0.381, 0.20075109148025513), (0.385, 0.20859676402807237), (0.39275, 0.1981160055398941), (0.383, 0.21422088372707368), (0.3905, 0.21211210936307906), (0.3745, 0.21381645905971527), (0.3735, 0.22764125645160674), (0.38725, 0.22319761633872987), (0.38825, 0.20828541433811187), (0.38625, 0.21629569989442826), (0.38075, 0.21960479485988618)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.44      0.51      0.47       100
           3       0.25      0.35      0.29       100
           5       0.25      0.33      0.28       100
           9       0.35      0.44      0.39       100
          10       0.36      0.27      0.31       100
          16       0.35      0.34      0.35       100
          17       0.50      0.60      0.55       100
          19       0.28      0.26      0.27       100
          20       0.71      0.54      0.61       100
          24       0.58      0.61      0.60       100
          26       0.34      0.20      0.25       100
          27       0.23      0.28      0.25       100
          29       0.32      0.33      0.33       100
          30       0.49      0.58      0.53       100
          31       0.27      0.22      0.24       100
          35       0.28      0.33      0.31       100
          40       0.31      0.23      0.26       100
          43       0.32      0.43      0.37       100
          51       0.39      0.25      0.30       100
          54       0.49      0.44      0.47       100
          55       0.11      0.14      0.12       100
          57       0.46      0.36      0.40       100
          58       0.33      0.44      0.38       100
          63       0.34      0.31      0.32       100
          65       0.24      0.19      0.21       100
          67       0.33      0.46      0.38       100
          68       0.83      0.72      0.77       100
          69       0.50      0.59      0.54       100
          72       0.12      0.16      0.14       100
          76       0.75      0.38      0.50       100
          77       0.23      0.13      0.17       100
          80       0.21      0.16      0.18       100
          82       0.78      0.62      0.69       100
          86       0.39      0.42      0.41       100
          87       0.37      0.38      0.38       100
          89       0.39      0.30      0.34       100
          91       0.37      0.53      0.44       100
          92       0.42      0.51      0.46       100
          94       0.73      0.57      0.64       100
          97       0.44      0.32      0.37       100

    accuracy                           0.38      4000
   macro avg       0.40      0.38      0.38      4000
weighted avg       0.40      0.38      0.38      4000

No_Competition_DC_1
VAL: 
[(0.025, 0.11657447910308838), (0.3095, 0.08201634657382965), (0.3645, 0.07280443269014358), (0.3855, 0.07251653423905373), (0.4045, 0.07031139296293258), (0.4035, 0.07202988198399543), (0.3985, 0.07479749894142151), (0.3955, 0.07719399720430374), (0.398, 0.08132479721307755), (0.3955, 0.08380527898669243), (0.392, 0.0891885765492916), (0.409, 0.08806263524293899), (0.4025, 0.09560451292991638), (0.411, 0.10077046963572502), (0.408, 0.10949301025271416), (0.3745, 0.13445332515239716), (0.3725, 0.13665147292613983), (0.381, 0.13735730016231537), (0.3785, 0.1375231835246086), (0.397, 0.1423942477107048), (0.3755, 0.15209560418128967), (0.386, 0.15499882835149764), (0.3775, 0.16258833128213881), (0.3805, 0.16726830369234086), (0.382, 0.16146213451027872), (0.38, 0.17055159997940064), (0.3645, 0.16944586551189422), (0.3905, 0.17760091096162797), (0.382, 0.1701675659418106), (0.385, 0.18733427667617797), (0.382, 0.1880714300274849), (0.382, 0.18740504759550095), (0.375, 0.19136932629346848), (0.389, 0.19766877233982086), (0.364, 0.20655017805099488), (0.3835, 0.19465066522359847), (0.376, 0.20152962875366212), (0.3905, 0.19162417262792586), (0.3905, 0.20573574566841127), (0.387, 0.2132830772995949), (0.3715, 0.21832886171340943), (0.386, 0.20134688246250151), (0.38, 0.22197042965888977), (0.3765, 0.23435211354494095), (0.376, 0.22377198296785356), (0.3825, 0.22945407032966614), (0.379, 0.23944648933410645), (0.3885, 0.22686034578084946), (0.376, 0.24182513785362245), (0.38, 0.23501720869541168), (0.374, 0.2368087886571884)]
TEST: 
[(0.0235, 0.11564568477869033), (0.312, 0.08039234134554862), (0.368, 0.07099179750680923), (0.38775, 0.06967788550257682), (0.39725, 0.06974655348062515), (0.39575, 0.07088904297351838), (0.40225, 0.07257259202003478), (0.39925, 0.07524963158369065), (0.40525, 0.07998397034406662), (0.3965, 0.08327389416098595), (0.3975, 0.0871833048760891), (0.406, 0.08781160497665405), (0.403, 0.09545548170804978), (0.40825, 0.10071950578689576), (0.40625, 0.11134896332025528), (0.37775, 0.13324887531995774), (0.38025, 0.13396614086627961), (0.391, 0.13580700933933257), (0.397, 0.1358050883412361), (0.395, 0.14455043715238572), (0.38825, 0.15080897009372712), (0.38225, 0.15327571880817414), (0.3795, 0.15966589087247848), (0.37625, 0.16869234544038772), (0.39025, 0.15979242932796478), (0.3885, 0.16846793979406358), (0.39275, 0.16926632571220399), (0.3915, 0.17164844155311584), (0.3805, 0.17451505935192108), (0.37025, 0.19080159801244737), (0.38775, 0.18842328161001207), (0.383, 0.18822291111946107), (0.385, 0.18882940632104875), (0.389, 0.19709746557474137), (0.3805, 0.20092072188854218), (0.3815, 0.19238633483648301), (0.3755, 0.20355176258087157), (0.3865, 0.18980754125118254), (0.38, 0.20908959293365478), (0.3885, 0.20889718341827393), (0.37675, 0.21654840540885925), (0.37475, 0.20564803463220596), (0.37325, 0.21970755994319915), (0.3725, 0.2300906102657318), (0.38375, 0.21907213109731674), (0.3735, 0.23512791931629182), (0.37925, 0.23829278254508973), (0.3875, 0.23246790033578874), (0.381, 0.23810252982378005), (0.373, 0.23066798090934754), (0.3815, 0.23554592132568358)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.75      0.44      0.55       100
           1       0.35      0.44      0.39       100
           2       0.31      0.15      0.20       100
           3       0.22      0.24      0.23       100
           5       0.29      0.31      0.30       100
          10       0.34      0.32      0.33       100
          12       0.40      0.47      0.43       100
          16       0.31      0.30      0.30       100
          17       0.53      0.69      0.60       100
          21       0.42      0.59      0.49       100
          24       0.58      0.63      0.61       100
          34       0.25      0.40      0.31       100
          35       0.20      0.13      0.16       100
          38       0.15      0.30      0.20       100
          39       0.44      0.34      0.38       100
          41       0.55      0.47      0.51       100
          45       0.18      0.15      0.16       100
          48       0.70      0.53      0.60       100
          52       0.88      0.86      0.87       100
          60       0.67      0.66      0.66       100
          62       0.40      0.33      0.36       100
          63       0.29      0.26      0.27       100
          65       0.20      0.22      0.21       100
          66       0.23      0.30      0.26       100
          67       0.34      0.36      0.35       100
          68       0.69      0.65      0.67       100
          69       0.68      0.50      0.57       100
          72       0.16      0.16      0.16       100
          73       0.50      0.44      0.47       100
          76       0.69      0.59      0.63       100
          77       0.24      0.21      0.22       100
          80       0.18      0.15      0.16       100
          83       0.37      0.36      0.37       100
          85       0.44      0.39      0.41       100
          92       0.24      0.41      0.30       100
          93       0.18      0.27      0.22       100
          94       0.65      0.62      0.63       100
          97       0.38      0.25      0.30       100
          98       0.20      0.13      0.16       100
          99       0.38      0.24      0.29       100

    accuracy                           0.38      4000
   macro avg       0.40      0.38      0.38      4000
weighted avg       0.40      0.38      0.38      4000

No_Competition_DC_2
VAL: 
[(0.0345, 0.11658469200134278), (0.316, 0.08201313215494156), (0.355, 0.07354478538036346), (0.3955, 0.06929199036955834), (0.402, 0.06997380438446998), (0.3895, 0.07335115653276443), (0.3875, 0.07556041604280472), (0.4015, 0.07825059574842454), (0.3905, 0.0803280391395092), (0.3995, 0.08349217543005943), (0.401, 0.08924299171566963), (0.4095, 0.09213249507546425), (0.387, 0.10163138052821159), (0.398, 0.11140233141183853), (0.3925, 0.11948941606283188), (0.394, 0.12859431272745134), (0.383, 0.13358487993478774), (0.395, 0.13590636253356933), (0.39, 0.13766176417469977), (0.3935, 0.146287089407444), (0.382, 0.15333827906847), (0.3935, 0.15233211308717728), (0.3815, 0.1750344733595848), (0.3885, 0.15836536115407943), (0.36, 0.17107941085100173), (0.3825, 0.16491007709503175), (0.379, 0.17352618235349654), (0.3785, 0.17844472450017929), (0.36, 0.17612643480300905), (0.378, 0.18684735038876535), (0.391, 0.18285865104198457), (0.3855, 0.18615579223632814), (0.381, 0.18947600400447845), (0.3825, 0.18856670087575914), (0.381, 0.19217507201433182), (0.3705, 0.20677682548761367), (0.381, 0.19045810306072236), (0.3825, 0.19267515289783477), (0.3785, 0.1989546536207199), (0.371, 0.20148250210285187), (0.361, 0.21000516843795777), (0.38, 0.18849462175369264), (0.3765, 0.21257275640964507), (0.377, 0.20953110730648042), (0.3775, 0.20566613405942916), (0.384, 0.22621862417459487), (0.368, 0.24112453091144562), (0.365, 0.22541932415962218), (0.383, 0.23027026498317718), (0.377, 0.21208670645952224), (0.3725, 0.22934970599412918)]
TEST: 
[(0.025, 0.11562635576725006), (0.3115, 0.08142845445871354), (0.362, 0.07283945858478547), (0.389, 0.06850037810206414), (0.40325, 0.06801409420371056), (0.40125, 0.07157087105512619), (0.39575, 0.07357381814718246), (0.408, 0.07641030916571617), (0.402, 0.07905487114191055), (0.40725, 0.08050769290328026), (0.39425, 0.08860070526599884), (0.39825, 0.09119916892051697), (0.39675, 0.10005758306384087), (0.3915, 0.10928127178549767), (0.39125, 0.11647147020697593), (0.377, 0.126586573779583), (0.385, 0.1337768729329109), (0.38925, 0.13554388004541398), (0.3855, 0.1374672920703888), (0.38825, 0.14944863718748091), (0.37075, 0.1538260673880577), (0.38825, 0.15356959038972853), (0.371, 0.1737997333407402), (0.3745, 0.16063123893737793), (0.374, 0.1666826302409172), (0.39075, 0.16738386046886444), (0.372, 0.17216360169649125), (0.376, 0.1801537084579468), (0.36975, 0.17839996856451035), (0.38, 0.18283087712526322), (0.39, 0.18362847357988357), (0.3815, 0.18741460293531417), (0.382, 0.1898082031607628), (0.37375, 0.189227865755558), (0.3725, 0.19607319939136506), (0.37975, 0.2038251094818115), (0.36925, 0.18977532005310058), (0.36775, 0.2015711669921875), (0.37175, 0.20070350176095964), (0.364, 0.2037520826458931), (0.357, 0.2133317636847496), (0.36775, 0.19681302803754808), (0.37325, 0.21640203207731246), (0.36725, 0.2233484789133072), (0.371, 0.20857305628061296), (0.369, 0.22456595700979232), (0.37725, 0.23939026653766632), (0.36425, 0.22258364629745483), (0.373, 0.23100124436616898), (0.37175, 0.21015564620494842), (0.3785, 0.22663381266593932)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.49      0.49      0.49       100
           3       0.23      0.42      0.29       100
           4       0.24      0.17      0.20       100
           5       0.34      0.36      0.35       100
           6       0.41      0.32      0.36       100
          10       0.29      0.21      0.24       100
          13       0.47      0.45      0.46       100
          16       0.37      0.25      0.30       100
          17       0.43      0.63      0.51       100
          18       0.27      0.27      0.27       100
          24       0.52      0.68      0.59       100
          28       0.60      0.55      0.57       100
          32       0.32      0.32      0.32       100
          33       0.44      0.39      0.41       100
          35       0.29      0.35      0.32       100
          44       0.17      0.22      0.19       100
          47       0.67      0.49      0.57       100
          49       0.44      0.44      0.44       100
          50       0.15      0.17      0.16       100
          53       0.62      0.66      0.64       100
          56       0.63      0.39      0.48       100
          59       0.38      0.36      0.37       100
          61       0.54      0.49      0.51       100
          63       0.32      0.34      0.33       100
          65       0.18      0.22      0.20       100
          67       0.55      0.40      0.46       100
          68       0.86      0.64      0.74       100
          69       0.66      0.52      0.58       100
          70       0.37      0.36      0.36       100
          72       0.09      0.11      0.10       100
          74       0.17      0.16      0.17       100
          76       0.61      0.51      0.55       100
          77       0.14      0.17      0.15       100
          79       0.35      0.35      0.35       100
          80       0.18      0.22      0.20       100
          88       0.30      0.19      0.23       100
          92       0.36      0.43      0.39       100
          94       0.50      0.60      0.55       100
          95       0.54      0.61      0.57       100
          97       0.33      0.23      0.27       100

    accuracy                           0.38      4000
   macro avg       0.40      0.38      0.38      4000
weighted avg       0.40      0.38      0.38      4000

Competition
DC 0, val_set_size=2000, COIs=[31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91, 9, 86, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91,
         9, 86, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.025, 0.11661210441589355)
DC 1, val_set_size=2000, COIs=[39, 93, 66, 48, 34, 0, 2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62, 41, 12, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([39, 93, 66, 48, 34,  0,  2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62,
        41, 12, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.025, 0.11657447910308838)
DC 2, val_set_size=2000, COIs=[18, 13, 88, 6, 4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59, 47, 79, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([18, 13, 88,  6,  4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59,
        47, 79, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.0345, 0.11658469200134278)
D00: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D01: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D02: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D03: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D04: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D05: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D06: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D07: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D08: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D09: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D010: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D011: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D012: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D013: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D014: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D015: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D016: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D017: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D018: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D019: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D020: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D021: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D022: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D023: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO1', '(DO3']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.22, 0.13269182819128036) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.209, 0.13862044098973275) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2235, 0.13842157109081746) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2525, 0.16798055917024612) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2415, 0.17649103528261184) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.251, 0.18062758001685142) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2575, 0.2209732731580734) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.21805497920513153) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.257, 0.23552828869223594) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.263, 0.27321153327822684) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2695, 0.263950015231967) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.2847494157701731) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.256, 0.32779241812229154) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2695, 0.3056244052797556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.262, 0.32305246141552924) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO3']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO4', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2605, 0.34094382083415986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2755, 0.32011408983170986) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.3336557738482952) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.33739736998081205) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.268, 0.3468747380152345) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.266, 0.3523096878826618) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.34274441182613374) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.258, 0.3573940420895815) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.267, 0.3606431941688061) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.34662794214487075) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.276, 0.36446899342536926) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.3670590571463108) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.36048966693878176) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.36093612107634543) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2545, 0.36984921553730965) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO0', '(DO2']
DC 2 --> ['(DO1', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2445, 0.38809939083456996) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2585, 0.3825460737645626) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2535, 0.3833720346093178) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.3802665272653103) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2625, 0.38963853546977045) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.255, 0.3930540835857391) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.249, 0.3932733156681061) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2645, 0.37638188250362875) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.4094136599004269) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2575, 0.40482678484916684) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2635, 0.4018096249997616) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2505, 0.4225937563180924) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.249, 0.3894625535607338) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.4211920323073864) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2525, 0.4268539460897446) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO3', '(DO2']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO1', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.237, 0.41171978795528413) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.39650962489843367) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.257, 0.3945933136343956) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2485, 0.41041025549173354) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.262, 0.38371198734641077) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2555, 0.39589177817106247) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2425, 0.362107670545578) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.261, 0.36226870465278627) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.256, 0.39056522578001024) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.36921313950419427) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.259, 0.37501582330465316) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.255, 0.34522413593530654) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2385, 0.37905862686038017) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.3663995613157749) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2565, 0.33740407472848893) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO3', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2415, 0.36017031893134116) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.3706979085505009) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.3428642379641533) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2395, 0.38221257776021955) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.269, 0.33993332463502884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.255, 0.34777070593833925) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.225, 0.36686835646629334) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2705, 0.35786380505561827) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2515, 0.33601594530045986) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2335, 0.3357623063325882) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2595, 0.345951908826828) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2535, 0.33723072189092634) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.3361744437813759) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.32861906751990316) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.3247212522029877) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO3', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.231, 0.3531305883526802) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2605, 0.322459882825613) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.33761993393301964) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.237, 0.3337986703515053) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.261, 0.32038820499181747) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.31967194530367854) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2435, 0.356546298801899) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.3100208678841591) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.2937206630110741) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2405, 0.33942326754331587) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.264, 0.2977209896445274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.2900320784449577) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2485, 0.33722026920318604) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2615, 0.29565287750959396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.262, 0.2763058236241341) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO2', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.33145454168319705) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2575, 0.2767262431383133) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.29274955481290815) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2425, 0.31053947645425795) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.27854839274287224) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.263, 0.2790674384236336) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2435, 0.31632262670993805) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.26124482131004334) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.263, 0.2628972557783127) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.341993880212307) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2625, 0.25581029215455053) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.2658051980137825) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.325377870798111) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.27414844927191734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.2667849141359329) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO0', '(DO1']
DC 1 --> ['(DO4', '(DO5']
DC 2 --> ['(DO2', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.3104822887182236) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.2505391250550747) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.289125256896019) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.32318434697389603) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2525, 0.2801330623924732) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.2786313821077347) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2455, 0.3142241969108582) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2675, 0.26079171073436735) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.26575938242673874) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.253, 0.30485341876745226) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.258, 0.25151986515522) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.269, 0.27456161016225816) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2545, 0.2994388320446014) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.2504008010327816) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2565, 0.27304998129606245) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2515, 0.29440091252326966) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2665, 0.251318998336792) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.2574673665761948) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.253, 0.31733259999752045) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.2517363148927689) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.2744475573897362) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.3038789492845535) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.273, 0.23860814267396926) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.2764611104130745) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.246, 0.278176313996315) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2725, 0.2281766458749771) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.273, 0.25209919607639314) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2495, 0.27918509089946747) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.283, 0.23195127171278) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.266, 0.23759619289636613) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO0', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.243, 0.2935365967154503) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.27, 0.2369273086488247) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2685, 0.25944558107852933) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.28186636900901796) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2805, 0.2432816701233387) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2665, 0.2550791214108467) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2615, 0.2663454779982567) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.2347430192232132) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.27634837591648104) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.3002837447524071) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2785, 0.22881875723600387) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.266, 0.2683482516407967) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2545, 0.2646911576986313) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.281, 0.25742805260419843) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.25685298871994017) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.025, 0.11661210441589355), (0.22, 0.13269182819128036), (0.2525, 0.16798055917024612), (0.2575, 0.2209732731580734), (0.263, 0.27321153327822684), (0.256, 0.32779241812229154), (0.2605, 0.34094382083415986), (0.2595, 0.33739736998081205), (0.257, 0.34274441182613374), (0.2555, 0.34662794214487075), (0.2555, 0.36048966693878176), (0.2445, 0.38809939083456996), (0.2455, 0.3802665272653103), (0.249, 0.3932733156681061), (0.2575, 0.40482678484916684), (0.249, 0.3894625535607338), (0.237, 0.41171978795528413), (0.2485, 0.41041025549173354), (0.2425, 0.362107670545578), (0.2475, 0.36921313950419427), (0.2385, 0.37905862686038017), (0.2415, 0.36017031893134116), (0.2395, 0.38221257776021955), (0.225, 0.36686835646629334), (0.2335, 0.3357623063325882), (0.242, 0.3361744437813759), (0.231, 0.3531305883526802), (0.237, 0.3337986703515053), (0.2435, 0.356546298801899), (0.2405, 0.33942326754331587), (0.2485, 0.33722026920318604), (0.241, 0.33145454168319705), (0.2425, 0.31053947645425795), (0.2435, 0.31632262670993805), (0.235, 0.341993880212307), (0.242, 0.325377870798111), (0.2455, 0.3104822887182236), (0.25, 0.32318434697389603), (0.2455, 0.3142241969108582), (0.253, 0.30485341876745226), (0.2545, 0.2994388320446014), (0.2515, 0.29440091252326966), (0.253, 0.31733259999752045), (0.2475, 0.3038789492845535), (0.246, 0.278176313996315), (0.2495, 0.27918509089946747), (0.243, 0.2935365967154503), (0.242, 0.28186636900901796), (0.2615, 0.2663454779982567), (0.251, 0.3002837447524071), (0.2545, 0.2646911576986313)]
TEST: 
[(0.0255, 0.11574103558063507), (0.2185, 0.13145311176776886), (0.244, 0.16690990447998047), (0.2505, 0.2200305950641632), (0.2585, 0.2703091044425964), (0.25725, 0.32620010113716125), (0.26025, 0.3399254899024963), (0.26625, 0.33566945838928225), (0.2585, 0.34232521438598634), (0.26275, 0.34096534276008605), (0.254, 0.3570160264968872), (0.2495, 0.3862573240995407), (0.25625, 0.3720898611545563), (0.25325, 0.3877470579147339), (0.255, 0.3983780233860016), (0.243, 0.3864023332595825), (0.24525, 0.4023173971176148), (0.248, 0.400706200838089), (0.25025, 0.36020971536636354), (0.253, 0.35927340924739837), (0.25025, 0.3725443003177643), (0.24975, 0.35199796116352083), (0.2485, 0.3732718915939331), (0.242, 0.3578195229768753), (0.24775, 0.3301678196191788), (0.2545, 0.33343027436733247), (0.23675, 0.3463087568283081), (0.2475, 0.32570847344398496), (0.25225, 0.3496864856481552), (0.24275, 0.333585533618927), (0.25275, 0.32540645682811736), (0.25125, 0.32114536440372465), (0.24825, 0.30438433873653414), (0.251, 0.30761543071269987), (0.2455, 0.3304919078350067), (0.2515, 0.3159163284301758), (0.25275, 0.3026370997428894), (0.25075, 0.3180148822069168), (0.24775, 0.31079926490783694), (0.2485, 0.29987295806407926), (0.24975, 0.29650921642780304), (0.24725, 0.29462086856365205), (0.24725, 0.3167529363632202), (0.244, 0.3023392946720123), (0.25225, 0.27418854224681855), (0.251, 0.2756056394577026), (0.246, 0.28579731357097626), (0.248, 0.2735421830415726), (0.259, 0.2553710638284683), (0.2625, 0.2897714246511459), (0.25875, 0.2590630224943161)]
DETAILED: 
              precision    recall  f1-score   support

           1       1.00      0.02      0.04       100
           3       0.14      0.01      0.02       100
           5       0.00      0.00      0.00       100
           9       0.24      0.53      0.33       100
          10       1.00      0.01      0.02       100
          16       0.00      0.00      0.00       100
          17       1.00      0.01      0.02       100
          19       0.21      0.25      0.23       100
          20       0.46      0.62      0.53       100
          24       0.50      0.01      0.02       100
          26       0.22      0.41      0.28       100
          27       0.17      0.44      0.24       100
          29       0.33      0.42      0.37       100
          30       0.31      0.66      0.42       100
          31       0.18      0.41      0.25       100
          35       0.00      0.00      0.00       100
          40       0.24      0.39      0.30       100
          43       0.23      0.62      0.33       100
          51       0.19      0.40      0.25       100
          54       0.30      0.58      0.39       100
          55       0.10      0.26      0.15       100
          57       0.33      0.39      0.36       100
          58       0.41      0.57      0.47       100
          63       0.18      0.02      0.04       100
          65       0.00      0.00      0.00       100
          67       0.64      0.07      0.13       100
          68       0.88      0.07      0.13       100
          69       0.50      0.01      0.02       100
          72       0.00      0.00      0.00       100
          76       1.00      0.01      0.02       100
          77       0.00      0.00      0.00       100
          80       0.00      0.00      0.00       100
          82       0.41      0.87      0.56       100
          86       0.22      0.47      0.30       100
          87       0.26      0.47      0.33       100
          89       0.26      0.51      0.35       100
          91       0.22      0.59      0.32       100
          92       0.00      0.00      0.00       100
          94       0.86      0.25      0.39       100
          97       0.00      0.00      0.00       100

    accuracy                           0.26      4000
   macro avg       0.32      0.26      0.19      4000
weighted avg       0.32      0.26      0.19      4000

Competition_DC_1
VAL: 
[(0.025, 0.11657447910308838), (0.209, 0.13862044098973275), (0.2415, 0.17649103528261184), (0.267, 0.21805497920513153), (0.2695, 0.263950015231967), (0.2695, 0.3056244052797556), (0.2755, 0.32011408983170986), (0.268, 0.3468747380152345), (0.258, 0.3573940420895815), (0.276, 0.36446899342536926), (0.266, 0.36093612107634543), (0.2585, 0.3825460737645626), (0.2625, 0.38963853546977045), (0.2645, 0.37638188250362875), (0.2635, 0.4018096249997616), (0.265, 0.4211920323073864), (0.2665, 0.39650962489843367), (0.262, 0.38371198734641077), (0.261, 0.36226870465278627), (0.259, 0.37501582330465316), (0.2665, 0.3663995613157749), (0.267, 0.3706979085505009), (0.269, 0.33993332463502884), (0.2705, 0.35786380505561827), (0.2595, 0.345951908826828), (0.2605, 0.32861906751990316), (0.2605, 0.322459882825613), (0.261, 0.32038820499181747), (0.2665, 0.3100208678841591), (0.264, 0.2977209896445274), (0.2615, 0.29565287750959396), (0.2575, 0.2767262431383133), (0.266, 0.27854839274287224), (0.266, 0.26124482131004334), (0.2625, 0.25581029215455053), (0.2655, 0.27414844927191734), (0.2655, 0.2505391250550747), (0.2525, 0.2801330623924732), (0.2675, 0.26079171073436735), (0.258, 0.25151986515522), (0.277, 0.2504008010327816), (0.2665, 0.251318998336792), (0.267, 0.2517363148927689), (0.273, 0.23860814267396926), (0.2725, 0.2281766458749771), (0.283, 0.23195127171278), (0.27, 0.2369273086488247), (0.2805, 0.2432816701233387), (0.263, 0.2347430192232132), (0.2785, 0.22881875723600387), (0.281, 0.25742805260419843)]
TEST: 
[(0.0235, 0.11564568477869033), (0.223, 0.1343015501499176), (0.25775, 0.17179667097330092), (0.2795, 0.212224973320961), (0.282, 0.25842282927036286), (0.283, 0.29569908666610717), (0.28025, 0.3129441111087799), (0.27725, 0.33460747861862183), (0.278, 0.3417089340686798), (0.27675, 0.34995218861103056), (0.27275, 0.34843199110031126), (0.27425, 0.3709858535528183), (0.27325, 0.37883108854293823), (0.2785, 0.36616578125953675), (0.2735, 0.3921496906280518), (0.271, 0.4091721758842468), (0.27275, 0.38708138358592986), (0.27325, 0.3744817966222763), (0.27125, 0.35154099416732787), (0.2685, 0.36411574137210845), (0.2755, 0.3546337047815323), (0.26925, 0.359463303565979), (0.2795, 0.33098678719997404), (0.2735, 0.34751429176330567), (0.27, 0.3375285851955414), (0.273, 0.3209394692182541), (0.269, 0.313794194817543), (0.265, 0.3106801959276199), (0.27075, 0.3011956392526627), (0.273, 0.2899021188020706), (0.27325, 0.29046738135814665), (0.27125, 0.2696054000854492), (0.279, 0.2694296097755432), (0.279, 0.25075022828578947), (0.26875, 0.2544130643606186), (0.27725, 0.26477833950519564), (0.27625, 0.24218052172660828), (0.27475, 0.2679771100282669), (0.27875, 0.2536270854473114), (0.28075, 0.2403731415271759), (0.2855, 0.24136027479171754), (0.2765, 0.24138500547409059), (0.27825, 0.23972434258460998), (0.27675, 0.23077969455718994), (0.28, 0.22158422672748565), (0.2845, 0.22849918830394744), (0.27975, 0.22929822301864625), (0.28575, 0.23231997203826904), (0.28125, 0.2218277167081833), (0.2935, 0.21429127836227416), (0.29, 0.2426366845369339)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.40      0.68      0.51       100
           1       0.67      0.02      0.04       100
           2       0.28      0.37      0.32       100
           3       0.33      0.04      0.07       100
           5       0.23      0.03      0.05       100
          10       0.62      0.05      0.09       100
          12       0.25      0.47      0.33       100
          16       0.00      0.00      0.00       100
          17       0.67      0.10      0.17       100
          21       0.31      0.69      0.43       100
          24       1.00      0.08      0.15       100
          34       0.21      0.49      0.30       100
          35       0.00      0.00      0.00       100
          38       0.14      0.39      0.21       100
          39       0.37      0.57      0.45       100
          41       0.38      0.59      0.46       100
          45       0.14      0.38      0.20       100
          48       0.44      0.63      0.52       100
          52       0.69      0.85      0.76       100
          60       0.49      0.88      0.63       100
          62       0.39      0.68      0.49       100
          63       0.00      0.00      0.00       100
          65       0.00      0.00      0.00       100
          66       0.21      0.46      0.29       100
          67       0.45      0.05      0.09       100
          68       1.00      0.05      0.10       100
          69       1.00      0.11      0.20       100
          72       0.00      0.00      0.00       100
          73       0.30      0.55      0.39       100
          76       0.00      0.00      0.00       100
          77       0.00      0.00      0.00       100
          80       0.33      0.01      0.02       100
          83       0.29      0.41      0.34       100
          85       0.33      0.62      0.43       100
          92       0.17      0.01      0.02       100
          93       0.19      0.43      0.26       100
          94       0.94      0.15      0.26       100
          97       0.50      0.01      0.02       100
          98       0.12      0.31      0.17       100
          99       0.18      0.44      0.26       100

    accuracy                           0.29      4000
   macro avg       0.35      0.29      0.23      4000
weighted avg       0.35      0.29      0.23      4000

Competition_DC_2
VAL: 
[(0.0345, 0.11658469200134278), (0.2235, 0.13842157109081746), (0.251, 0.18062758001685142), (0.257, 0.23552828869223594), (0.2655, 0.2847494157701731), (0.262, 0.32305246141552924), (0.264, 0.3336557738482952), (0.266, 0.3523096878826618), (0.267, 0.3606431941688061), (0.2645, 0.3670590571463108), (0.2545, 0.36984921553730965), (0.2535, 0.3833720346093178), (0.255, 0.3930540835857391), (0.2585, 0.4094136599004269), (0.2505, 0.4225937563180924), (0.2525, 0.4268539460897446), (0.257, 0.3945933136343956), (0.2555, 0.39589177817106247), (0.256, 0.39056522578001024), (0.255, 0.34522413593530654), (0.2565, 0.33740407472848893), (0.254, 0.3428642379641533), (0.255, 0.34777070593833925), (0.2515, 0.33601594530045986), (0.2535, 0.33723072189092634), (0.2575, 0.3247212522029877), (0.258, 0.33761993393301964), (0.2625, 0.31967194530367854), (0.253, 0.2937206630110741), (0.252, 0.2900320784449577), (0.262, 0.2763058236241341), (0.2625, 0.29274955481290815), (0.263, 0.2790674384236336), (0.263, 0.2628972557783127), (0.265, 0.2658051980137825), (0.2645, 0.2667849141359329), (0.26, 0.289125256896019), (0.26, 0.2786313821077347), (0.2655, 0.26575938242673874), (0.269, 0.27456161016225816), (0.2565, 0.27304998129606245), (0.264, 0.2574673665761948), (0.264, 0.2744475573897362), (0.2655, 0.2764611104130745), (0.273, 0.25209919607639314), (0.266, 0.23759619289636613), (0.2685, 0.25944558107852933), (0.2665, 0.2550791214108467), (0.26, 0.27634837591648104), (0.266, 0.2683482516407967), (0.265, 0.25685298871994017)]
TEST: 
[(0.025, 0.11562635576725006), (0.2205, 0.1358132424354553), (0.258, 0.17736735165119172), (0.26275, 0.23078425562381744), (0.2665, 0.2764681534767151), (0.271, 0.31345299530029297), (0.27075, 0.3248773156404495), (0.26725, 0.34227508771419524), (0.27, 0.3479113093614578), (0.26775, 0.35526830768585205), (0.264, 0.35983342897891996), (0.26425, 0.37495402443408965), (0.257, 0.3800447050333023), (0.26025, 0.39889687991142275), (0.25925, 0.41097073674201967), (0.2505, 0.41379248261451723), (0.2565, 0.38538402938842775), (0.25975, 0.38374060463905335), (0.2635, 0.3819289827346802), (0.26, 0.34041330552101134), (0.2615, 0.33126818323135376), (0.265, 0.3391891679763794), (0.26875, 0.3391628577709198), (0.25325, 0.3311852813959122), (0.248, 0.3333213368654251), (0.26125, 0.3225403505563736), (0.26, 0.33333639800548553), (0.258, 0.3222287849187851), (0.251, 0.2968592798709869), (0.264, 0.2871823682785034), (0.26225, 0.2724351181983948), (0.25575, 0.2891847754716873), (0.25925, 0.2786626386642456), (0.26425, 0.2625223215818405), (0.2645, 0.266844585776329), (0.2625, 0.26306551933288574), (0.25525, 0.281947723031044), (0.25475, 0.27307792568206785), (0.256, 0.2613441791534424), (0.25975, 0.27103738236427305), (0.2575, 0.27270886695384977), (0.2655, 0.25368839120864867), (0.26225, 0.26708323216438296), (0.2635, 0.2727414765357971), (0.2615, 0.2516378540992737), (0.26675, 0.23437462508678436), (0.273, 0.25147918176651), (0.26825, 0.2578854176998138), (0.25875, 0.2737740923166275), (0.25975, 0.26772811901569366), (0.26625, 0.2523640637397766)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00       100
           3       0.12      0.02      0.03       100
           4       0.15      0.33      0.20       100
           5       0.80      0.04      0.08       100
           6       0.31      0.38      0.34       100
          10       0.00      0.00      0.00       100
          13       0.30      0.57      0.39       100
          16       0.00      0.00      0.00       100
          17       0.44      0.08      0.14       100
          18       0.23      0.43      0.30       100
          24       0.72      0.13      0.22       100
          28       0.32      0.69      0.44       100
          32       0.18      0.40      0.24       100
          33       0.24      0.49      0.32       100
          35       0.00      0.00      0.00       100
          44       0.12      0.21      0.15       100
          47       0.45      0.60      0.52       100
          49       0.27      0.55      0.37       100
          50       0.14      0.32      0.20       100
          53       0.47      0.70      0.56       100
          56       0.31      0.58      0.40       100
          59       0.28      0.42      0.33       100
          61       0.30      0.67      0.41       100
          63       0.33      0.02      0.04       100
          65       0.40      0.02      0.04       100
          67       0.90      0.19      0.31       100
          68       1.00      0.07      0.13       100
          69       1.00      0.01      0.02       100
          70       0.31      0.70      0.43       100
          72       0.00      0.00      0.00       100
          74       0.16      0.36      0.22       100
          76       0.65      0.13      0.22       100
          77       0.00      0.00      0.00       100
          79       0.23      0.49      0.31       100
          80       0.00      0.00      0.00       100
          88       0.16      0.32      0.21       100
          92       0.18      0.02      0.04       100
          94       0.75      0.03      0.06       100
          95       0.39      0.68      0.50       100
          97       0.00      0.00      0.00       100

    accuracy                           0.27      4000
   macro avg       0.32      0.27      0.20      4000
weighted avg       0.32      0.27      0.20      4000

Collaboration
DC 0, val_set_size=2000, COIs=[31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91, 9, 86, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91,
         9, 86, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.025, 0.11661210441589355)
DC 1, val_set_size=2000, COIs=[39, 93, 66, 48, 34, 0, 2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62, 41, 12, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([39, 93, 66, 48, 34,  0,  2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62,
        41, 12, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.025, 0.11657447910308838)
DC 2, val_set_size=2000, COIs=[18, 13, 88, 6, 4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59, 47, 79, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([18, 13, 88,  6,  4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59,
        47, 79, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.0345, 0.11658469200134278)
D00: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D01: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D02: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D03: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D04: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D05: 1000 samples from classes {1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97}
D06: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D07: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D08: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D09: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D010: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D011: 1000 samples from classes {9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91}
D012: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D013: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D014: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D015: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D016: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D017: 1000 samples from classes {0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99}
D018: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D019: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D020: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D021: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D022: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
D023: 1000 samples from classes {4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO5']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO2', '(DO0']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.209, 0.13625536447763442) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2305, 0.1351115824431181) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.229, 0.13082764801383018) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.17950479835271835) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2575, 0.17054746913909913) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.1756962062418461) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2555, 0.2298193385452032) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.274, 0.20892276149988173) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.269, 0.22105668456852437) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.2796086165755987) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2855, 0.2572083220630884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2725, 0.2725491283982992) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2615, 0.3199525647610426) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.3128767554908991) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.274, 0.34641893085837366) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO0', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2655, 0.334598257035017) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.3304642647653818) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.278, 0.36381375828385354) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.34843066284060475) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.282, 0.3397960841655731) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.269, 0.36080143667757514) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.26, 0.3434905242323875) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2735, 0.3526268084943295) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.37230826407670975) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.26, 0.35027329009771346) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2815, 0.36207278744876387) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.3871887118220329) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2605, 0.37927059778571126) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.279, 0.3649644255191088) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.3991517504602671) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[1, 3, 5, 10, 16, 17, 24, 35, 63, 65, 67, 68, 69, 72, 76, 77, 80, 92, 94, 97], M=tensor([ 0,  1,  2,  3,  4,  5,  6,  9, 10, 12, 13, 16, 17, 18, 19, 20, 21, 24,
        26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 45, 47,
        48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66,
        67, 68, 69, 70, 72, 73, 74, 76, 77, 79, 80, 82, 83, 85, 86, 87, 88, 89,
        91, 92, 93, 94, 95, 97, 98, 99], device='cuda:0'), Initial Performance: (0.41, 0.0651462857723236)
DC Expert-0, val_set_size=1000, COIs=[9, 19, 20, 26, 27, 29, 30, 31, 40, 43, 51, 54, 55, 57, 58, 82, 86, 87, 89, 91], M=tensor([31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91,
         9, 86, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.521, 0.0713669405579567)
DC Expert-1, val_set_size=1000, COIs=[0, 2, 12, 21, 34, 38, 39, 41, 45, 48, 52, 60, 62, 66, 73, 83, 85, 93, 98, 99], M=tensor([39, 93, 66, 48, 34,  0,  2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62,
        41, 12, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.558, 0.05923165455460548)
DC Expert-2, val_set_size=1000, COIs=[4, 6, 13, 18, 28, 32, 33, 44, 47, 49, 50, 53, 56, 59, 61, 70, 74, 79, 88, 95], M=tensor([18, 13, 88,  6,  4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59,
        47, 79, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), Initial Performance: (0.516, 0.07459347680211068)
SUPER-DC 0, val_set_size=2000, COIs=[31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91, 9, 86, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([31, 19, 54, 43, 57, 55, 26, 29, 30, 58, 51, 89, 87, 40, 82, 27, 20, 91,
         9, 86, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[39, 93, 66, 48, 34, 0, 2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62, 41, 12, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([39, 93, 66, 48, 34,  0,  2, 99, 73, 52, 83, 60, 38, 98, 85, 21, 45, 62,
        41, 12, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[18, 13, 88, 6, 4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59, 47, 79, 97, 68, 65, 94, 69, 16, 63, 3, 17, 1, 76, 10, 80, 35, 24, 5, 72, 92, 67, 77], M=tensor([18, 13, 88,  6,  4, 32, 56, 33, 74, 61, 53, 49, 44, 95, 50, 70, 28, 59,
        47, 79, 97, 68, 65, 94, 69, 16, 63,  3, 17,  1, 76, 10, 80, 35, 24,  5,
        72, 92, 67, 77], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.505, 0.08322442066669464) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.532, 0.08323676732182503) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.544, 0.07999605676531792) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48933333333333334, 0.05266744138797124) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2875, 0.15409028026461602) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.289, 0.15766153267025948) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.292, 0.16585743182897567) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.484, 0.10318037676811219) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.529, 0.08312599244713784) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.532, 0.0975115583240986) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5123333333333333, 0.0529405300617218) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2885, 0.13013828033208846) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.1322941642254591) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2825, 0.133302660882473) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.502, 0.10920872128009797) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.08733610183000565) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.10831550920009612) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5256666666666666, 0.05404091300566991) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.263, 0.13095395529270173) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.302, 0.12705176350474356) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.293, 0.13179956179857255) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.507, 0.10961863315105438) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.516, 0.10736750769615173) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.513, 0.09806356769800186) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5446666666666666, 0.05514366097251574) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2615, 0.13331241250038148) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2825, 0.1314576868712902) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2835, 0.12224992954730987) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.493, 0.11061988532543182) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.544, 0.09541456198692322) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.524, 0.10519703674316407) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5433333333333333, 0.060409511764844256) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2815, 0.10481063491106034) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.31, 0.10483852407336235) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.292, 0.1146598643064499) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.481, 0.1172685171365738) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.525, 0.09835475650429726) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.12552673429250716) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5356666666666666, 0.06376925567785899) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.307, 0.10416794776916503) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.324, 0.10325219196081162) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2955, 0.11431249248981476) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.494, 0.12106762540340424) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.54, 0.10614869862794876) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.11555580067634583) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5243333333333333, 0.07114946021636327) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.10074295037984848) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.32, 0.09795807793736458) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3075, 0.10188793271780014) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.489, 0.13042033779621123) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.522, 0.12307282203435897) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.506, 0.1335123553276062) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.512, 0.08840134078264236) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3085, 0.10083560228347778) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3385, 0.09816928365826606) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.325, 0.10169461959600448) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.509, 0.11889738750457764) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.1058061243891716) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.516, 0.12096912336349487) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5316666666666666, 0.0866379836599032) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.331, 0.09870141422748566) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.333, 0.09653350767493248) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3195, 0.10117067539691925) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.492, 0.12470484536886216) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.545, 0.11799036157131194) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.491, 0.13628411543369293) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5196666666666667, 0.09574687655766805) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3105, 0.10507266658544541) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3375, 0.10038081195950509) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.10297136282920838) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.481, 0.14241471707820894) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.538, 0.11569299376010894) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.521, 0.12645007544755935) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5063333333333333, 0.09493304153283437) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.311, 0.10648210543394089) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.325, 0.09706908217072487) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.315, 0.10204912048578263) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.5, 0.1441380443572998) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.553, 0.11719482326507569) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.475, 0.14423422491550444) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5173333333333333, 0.09837329586346945) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.305, 0.1098507422208786) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.337, 0.09786128735542297) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3205, 0.104395698428154) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.1342760932445526) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.533, 0.1327068061232567) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.513, 0.1294396959543228) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.522, 0.10913049860795339) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2825, 0.1163581553697586) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3185, 0.10529465502500535) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3115, 0.10662471988797188) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.508, 0.13031837594509124) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.525, 0.1312789078950882) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.506, 0.14315476620197295) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.515, 0.10903366124629975) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3135, 0.11312868559360505) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3145, 0.10945042511820793) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3115, 0.11302039223909378) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.506, 0.14112611204385758) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.539, 0.12632701182365416) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.13579799056053163) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5213333333333333, 0.11060297240813573) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3145, 0.11082591152191162) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3335, 0.09877617210149765) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.33, 0.10456527477502822) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.1476306245326996) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.529, 0.1413646255135536) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.151811678647995) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5163333333333333, 0.11444197662671407) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3095, 0.11025847828388215) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.351, 0.10036861994862556) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.319, 0.11061611634492874) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.496, 0.15056247913837434) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.513, 0.1430306513309479) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.14514136588573456) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5043333333333333, 0.12829320124785104) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.312, 0.11630502247810363) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.337, 0.10782941102981568) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3155, 0.11346122360229492) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.504, 0.14802755951881408) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.51, 0.13436061227321625) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.531, 0.15183264017105103) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49933333333333335, 0.11651081921656926) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3105, 0.11747127598524093) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3365, 0.10336412221193314) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.10967406111955642) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.515, 0.1547368768453598) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.534, 0.14285851028561591) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.52, 0.15892432177066804) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.521, 0.10901105986038843) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3005, 0.11562032777070999) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.10837073922157288) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3155, 0.11242468446493149) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.514, 0.15701852822303772) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.532, 0.14539188826084137) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.507, 0.16097774982452392) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5013333333333333, 0.13465947480996449) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3015, 0.12163432955741882) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3415, 0.11079858502745628) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.313, 0.11198552960157394) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.501, 0.14562845838069916) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.529, 0.14432812935113906) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.16534115356206894) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5056666666666667, 0.12569675509134928) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.304, 0.11955364197492599) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.344, 0.10708304139971733) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.307, 0.1131127704679966) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.511, 0.15033144736289977) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.534, 0.1666955544948578) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.507, 0.16571339654922485) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5086666666666667, 0.13024515084425609) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3045, 0.12160820603370666) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3425, 0.10889422118663789) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.11816541382670402) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.498, 0.16121390056610108) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.515, 0.16118154722452163) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.16438580322265625) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5086666666666667, 0.14355599160989124) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.313, 0.12684563195705414) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.329, 0.11460994043946267) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3235, 0.11688688540458679) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.511, 0.14914503908157348) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.532, 0.14962833136320114) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.1613326852321625) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.511, 0.1387316429615021) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3015, 0.12531731629371642) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.322, 0.11569161641597747) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.11989559561014175) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.503, 0.16817693626880645) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.535, 0.1582597543001175) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.484, 0.16825183361768722) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5106666666666667, 0.14144077734152477) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.291, 0.12327807092666626) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.11568180179595947) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3045, 0.12155466091632842) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.518, 0.16784622132778168) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.519, 0.16776477599143982) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.508, 0.1700201541185379) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5176666666666667, 0.13705946707725525) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3005, 0.12481097066402436) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3075, 0.12086677074432373) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.324, 0.1211310161948204) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.482, 0.17627423977851867) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.528, 0.16856681275367738) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.497, 0.1600705280303955) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5043333333333333, 0.13815678743521373) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.293, 0.12626291716098786) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3135, 0.11952361273765565) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.32, 0.11961795133352279) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.477, 0.16447369480133056) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.531, 0.1678607496023178) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.508, 0.18137316823005675) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5206666666666667, 0.13693927097320557) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.309, 0.13055390161275865) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3225, 0.12044576105475426) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.321, 0.1242945430278778) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.495, 0.1668795840740204) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.548, 0.1501126627922058) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.526, 0.1775965962409973) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.517, 0.14769713207085927) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3, 0.1322252581715584) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.34, 0.11569452655315399) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3255, 0.12357553511857987) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.516, 0.1729925843477249) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.544, 0.1629809308052063) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.504, 0.1672335526943207) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5073333333333333, 0.14241977675755818) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3065, 0.134620294213295) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3285, 0.12076096308231354) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.31, 0.12934572541713715) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.485, 0.18484972071647643) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.547, 0.1765184383392334) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.524, 0.1836947673559189) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5076666666666667, 0.16441868698596954) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2945, 0.13747234177589415) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.329, 0.12609819424152374) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2955, 0.12694223541021346) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.504, 0.17818325233459473) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.53, 0.18255207467079163) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.526, 0.1802998687028885) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.523, 0.15167121748129528) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3015, 0.13844583797454835) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.317, 0.12250599041581153) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.301, 0.13525106233358383) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.496, 0.19322998356819152) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.537, 0.1739402984380722) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.535, 0.1748604564666748) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5136666666666667, 0.15060827747980754) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.13441338163614272) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.325, 0.12192540118098259) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3125, 0.1308543370962143) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.501, 0.19391993248462677) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.534, 0.17871343708038331) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.17733637702465058) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5116666666666667, 0.16074370145797728) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2915, 0.13871853548288346) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.324, 0.12238320043683051) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.303, 0.12801223200559617) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.488, 0.20254161810874938) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.539, 0.19595581215620042) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.52, 0.1895600116252899) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.516, 0.16679113737742107) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.297, 0.14608944368362425) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3275, 0.12701613038778306) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2935, 0.13931028068065643) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.52, 0.17752830755710602) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.559, 0.17785138082504273) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.19146840405464172) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.525, 0.14119822200139365) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3025, 0.13790436124801636) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.325, 0.12699947535991668) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.318, 0.13130823981761933) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.518, 0.20018729090690612) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.53, 0.19157273650169374) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.20394138622283936) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.505, 0.15663836375872295) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2925, 0.14151058948040007) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.309, 0.12809604415297507) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.303, 0.13444972568750382) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.515, 0.20299200332164766) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.529, 0.1743220909535885) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.503, 0.19730961632728578) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5196666666666667, 0.14815052731831868) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2995, 0.14068086063861848) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.339, 0.12606540566682817) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.13801208770275117) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.516, 0.20461306071281432) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.522, 0.19437379485368728) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.53, 0.1966981749534607) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5096666666666667, 0.16703982547918955) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.301, 0.14152202343940734) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.34, 0.12359806567430497) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.312, 0.13375645595788957) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.508, 0.1962347240447998) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.55, 0.17725681626796722) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.536, 0.18799494349956514) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5173333333333333, 0.16572065794467927) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.14240099614858628) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3325, 0.12914542800188064) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3145, 0.134959899187088) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.025, 0.11661210441589355), (0.209, 0.13625536447763442), (0.257, 0.17950479835271835), (0.2555, 0.2298193385452032), (0.269, 0.2796086165755987), (0.2615, 0.3199525647610426), (0.2655, 0.334598257035017), (0.269, 0.34843066284060475), (0.26, 0.3434905242323875), (0.26, 0.35027329009771346), (0.2605, 0.37927059778571126), (0.2875, 0.15409028026461602), (0.2885, 0.13013828033208846), (0.263, 0.13095395529270173), (0.2615, 0.13331241250038148), (0.2815, 0.10481063491106034), (0.307, 0.10416794776916503), (0.3155, 0.10074295037984848), (0.3085, 0.10083560228347778), (0.331, 0.09870141422748566), (0.3105, 0.10507266658544541), (0.311, 0.10648210543394089), (0.305, 0.1098507422208786), (0.2825, 0.1163581553697586), (0.3135, 0.11312868559360505), (0.3145, 0.11082591152191162), (0.3095, 0.11025847828388215), (0.312, 0.11630502247810363), (0.3105, 0.11747127598524093), (0.3005, 0.11562032777070999), (0.3015, 0.12163432955741882), (0.304, 0.11955364197492599), (0.3045, 0.12160820603370666), (0.313, 0.12684563195705414), (0.3015, 0.12531731629371642), (0.291, 0.12327807092666626), (0.3005, 0.12481097066402436), (0.293, 0.12626291716098786), (0.309, 0.13055390161275865), (0.3, 0.1322252581715584), (0.3065, 0.134620294213295), (0.2945, 0.13747234177589415), (0.3015, 0.13844583797454835), (0.2895, 0.13441338163614272), (0.2915, 0.13871853548288346), (0.297, 0.14608944368362425), (0.3025, 0.13790436124801636), (0.2925, 0.14151058948040007), (0.2995, 0.14068086063861848), (0.301, 0.14152202343940734), (0.2895, 0.14240099614858628)]
TEST: 
[(0.0255, 0.11574103558063507), (0.21475, 0.13476956152915956), (0.24775, 0.17735628652572633), (0.24675, 0.2263704423904419), (0.26075, 0.2762515383958817), (0.25875, 0.3153589870929718), (0.25875, 0.3287258608341217), (0.2575, 0.34631458020210265), (0.2555, 0.3404453650712967), (0.25075, 0.3474222450256348), (0.25425, 0.3751777019500732), (0.27925, 0.15342275166511535), (0.291, 0.12637206494808198), (0.2695, 0.1278986828327179), (0.267, 0.13272979217767716), (0.30075, 0.10260364505648613), (0.30675, 0.10106032782793045), (0.31875, 0.09736136344075202), (0.305, 0.09810875028371811), (0.321, 0.09807734298706054), (0.321, 0.10270292806625367), (0.30875, 0.10465486818552017), (0.298, 0.10630231326818466), (0.30075, 0.1112188538312912), (0.31025, 0.10872253125905991), (0.3175, 0.10782546025514603), (0.32275, 0.10629906696081161), (0.3165, 0.11033632284402847), (0.31225, 0.11406462723016739), (0.31025, 0.11416365468502045), (0.3145, 0.11753737276792527), (0.30475, 0.11663309115171433), (0.315, 0.11907061105966568), (0.31225, 0.12343515822291375), (0.318, 0.12108326017856598), (0.2935, 0.12356965219974518), (0.30125, 0.1205284925699234), (0.3065, 0.1223711913228035), (0.3175, 0.12595986744761467), (0.30925, 0.12566007667779922), (0.311, 0.13057459479570388), (0.29475, 0.13254444700479506), (0.31425, 0.13022682791948317), (0.309, 0.12748250722885132), (0.301, 0.13220012599229813), (0.30825, 0.14175688928365707), (0.306, 0.1346713200211525), (0.292, 0.1373278630375862), (0.3085, 0.1350981176495552), (0.31025, 0.1374927617907524), (0.2985, 0.14019980508089067)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.39      0.39      0.39       100
           3       0.12      0.20      0.15       100
           5       0.26      0.28      0.27       100
           9       0.38      0.33      0.35       100
          10       0.37      0.29      0.32       100
          16       0.28      0.27      0.28       100
          17       0.30      0.64      0.41       100
          19       0.20      0.13      0.16       100
          20       0.49      0.32      0.39       100
          24       0.41      0.71      0.52       100
          26       0.25      0.13      0.17       100
          27       0.20      0.31      0.24       100
          29       0.26      0.14      0.18       100
          30       0.29      0.80      0.42       100
          31       0.21      0.19      0.20       100
          35       0.24      0.11      0.15       100
          40       0.26      0.28      0.27       100
          43       0.28      0.49      0.35       100
          51       0.28      0.31      0.30       100
          54       0.40      0.53      0.45       100
          55       0.08      0.15      0.10       100
          57       0.46      0.26      0.33       100
          58       0.61      0.34      0.44       100
          63       0.24      0.23      0.23       100
          65       0.19      0.11      0.14       100
          67       0.20      0.09      0.12       100
          68       0.70      0.60      0.65       100
          69       0.56      0.31      0.40       100
          72       0.10      0.08      0.09       100
          76       0.53      0.10      0.17       100
          77       0.13      0.12      0.12       100
          80       0.15      0.17      0.16       100
          82       0.49      0.34      0.40       100
          86       0.38      0.44      0.41       100
          87       0.33      0.36      0.34       100
          89       0.23      0.03      0.05       100
          91       0.37      0.31      0.34       100
          92       0.25      0.26      0.25       100
          94       0.55      0.47      0.51       100
          97       0.28      0.32      0.30       100

    accuracy                           0.30      4000
   macro avg       0.32      0.30      0.29      4000
weighted avg       0.32      0.30      0.29      4000

Collaboration_DC_1
VAL: 
[(0.025, 0.11657447910308838), (0.2305, 0.1351115824431181), (0.2575, 0.17054746913909913), (0.274, 0.20892276149988173), (0.2855, 0.2572083220630884), (0.277, 0.3128767554908991), (0.28, 0.3304642647653818), (0.282, 0.3397960841655731), (0.2735, 0.3526268084943295), (0.2815, 0.36207278744876387), (0.279, 0.3649644255191088), (0.289, 0.15766153267025948), (0.304, 0.1322941642254591), (0.302, 0.12705176350474356), (0.2825, 0.1314576868712902), (0.31, 0.10483852407336235), (0.324, 0.10325219196081162), (0.32, 0.09795807793736458), (0.3385, 0.09816928365826606), (0.333, 0.09653350767493248), (0.3375, 0.10038081195950509), (0.325, 0.09706908217072487), (0.337, 0.09786128735542297), (0.3185, 0.10529465502500535), (0.3145, 0.10945042511820793), (0.3335, 0.09877617210149765), (0.351, 0.10036861994862556), (0.337, 0.10782941102981568), (0.3365, 0.10336412221193314), (0.33, 0.10837073922157288), (0.3415, 0.11079858502745628), (0.344, 0.10708304139971733), (0.3425, 0.10889422118663789), (0.329, 0.11460994043946267), (0.322, 0.11569161641597747), (0.323, 0.11568180179595947), (0.3075, 0.12086677074432373), (0.3135, 0.11952361273765565), (0.3225, 0.12044576105475426), (0.34, 0.11569452655315399), (0.3285, 0.12076096308231354), (0.329, 0.12609819424152374), (0.317, 0.12250599041581153), (0.325, 0.12192540118098259), (0.324, 0.12238320043683051), (0.3275, 0.12701613038778306), (0.325, 0.12699947535991668), (0.309, 0.12809604415297507), (0.339, 0.12606540566682817), (0.34, 0.12359806567430497), (0.3325, 0.12914542800188064)]
TEST: 
[(0.0235, 0.11564568477869033), (0.23675, 0.132055024266243), (0.264, 0.16610128843784333), (0.28225, 0.20393206882476805), (0.287, 0.2505265463590622), (0.28375, 0.30403777086734773), (0.28375, 0.32007274556159976), (0.2785, 0.32692635214328764), (0.2765, 0.3422686085700989), (0.28125, 0.35194106364250183), (0.27825, 0.3530233861207962), (0.29625, 0.14724602657556535), (0.312, 0.12578759092092515), (0.31, 0.12187743693590164), (0.2975, 0.1266214145421982), (0.3215, 0.09925250387191772), (0.33025, 0.10131195175647735), (0.327, 0.09533789384365082), (0.33925, 0.09409050405025482), (0.3415, 0.09217457661032677), (0.34025, 0.09676618510484696), (0.34425, 0.09434778997302055), (0.34375, 0.09622748097777367), (0.3325, 0.10127899587154389), (0.318, 0.10573493725061417), (0.34475, 0.09566132962703705), (0.33925, 0.0990986084640026), (0.33525, 0.10344946676492692), (0.347, 0.10148450145125389), (0.334, 0.10611134278774262), (0.34025, 0.1082309883236885), (0.341, 0.10529559659957886), (0.3315, 0.10837391221523285), (0.325, 0.11235941731929779), (0.335, 0.11215244928002358), (0.32925, 0.11236145758628845), (0.31725, 0.11807587185502053), (0.33125, 0.11337783327698707), (0.33075, 0.11606863820552826), (0.33525, 0.11129648637771607), (0.333, 0.11768418878316879), (0.3295, 0.12224767869710922), (0.3285, 0.11941450434923172), (0.33425, 0.11901426142454147), (0.335, 0.11826866483688354), (0.31675, 0.12463429218530656), (0.33525, 0.12470554286241531), (0.336, 0.12189588689804078), (0.32675, 0.12202645844221115), (0.3255, 0.12286856779456139), (0.32525, 0.12912566474080087)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.41      0.23      0.29       100
           1       0.39      0.57      0.46       100
           2       0.22      0.16      0.19       100
           3       0.16      0.22      0.19       100
           5       0.34      0.28      0.31       100
          10       0.20      0.34      0.25       100
          12       0.26      0.19      0.22       100
          16       0.26      0.26      0.26       100
          17       0.31      0.54      0.39       100
          21       0.35      0.39      0.37       100
          24       0.46      0.66      0.54       100
          34       0.31      0.27      0.29       100
          35       0.17      0.17      0.17       100
          38       0.19      0.19      0.19       100
          39       0.31      0.34      0.32       100
          41       0.71      0.42      0.53       100
          45       0.13      0.12      0.12       100
          48       0.66      0.57      0.61       100
          52       0.80      0.75      0.77       100
          60       0.62      0.74      0.68       100
          62       0.44      0.44      0.44       100
          63       0.30      0.27      0.28       100
          65       0.23      0.21      0.22       100
          66       0.18      0.13      0.15       100
          67       0.25      0.20      0.22       100
          68       0.62      0.47      0.53       100
          69       0.73      0.33      0.46       100
          72       0.12      0.13      0.12       100
          73       0.37      0.40      0.39       100
          76       0.66      0.42      0.51       100
          77       0.19      0.15      0.17       100
          80       0.14      0.23      0.17       100
          83       0.30      0.21      0.25       100
          85       0.40      0.37      0.38       100
          92       0.22      0.29      0.25       100
          93       0.32      0.21      0.25       100
          94       0.56      0.43      0.49       100
          97       0.22      0.34      0.27       100
          98       0.20      0.23      0.21       100
          99       0.16      0.14      0.15       100

    accuracy                           0.33      4000
   macro avg       0.35      0.33      0.33      4000
weighted avg       0.35      0.33      0.33      4000

Collaboration_DC_2
VAL: 
[(0.0345, 0.11658469200134278), (0.229, 0.13082764801383018), (0.2625, 0.1756962062418461), (0.269, 0.22105668456852437), (0.2725, 0.2725491283982992), (0.274, 0.34641893085837366), (0.278, 0.36381375828385354), (0.269, 0.36080143667757514), (0.258, 0.37230826407670975), (0.2655, 0.3871887118220329), (0.258, 0.3991517504602671), (0.292, 0.16585743182897567), (0.2825, 0.133302660882473), (0.293, 0.13179956179857255), (0.2835, 0.12224992954730987), (0.292, 0.1146598643064499), (0.2955, 0.11431249248981476), (0.3075, 0.10188793271780014), (0.325, 0.10169461959600448), (0.3195, 0.10117067539691925), (0.314, 0.10297136282920838), (0.315, 0.10204912048578263), (0.3205, 0.104395698428154), (0.3115, 0.10662471988797188), (0.3115, 0.11302039223909378), (0.33, 0.10456527477502822), (0.319, 0.11061611634492874), (0.3155, 0.11346122360229492), (0.314, 0.10967406111955642), (0.3155, 0.11242468446493149), (0.313, 0.11198552960157394), (0.307, 0.1131127704679966), (0.308, 0.11816541382670402), (0.3235, 0.11688688540458679), (0.317, 0.11989559561014175), (0.3045, 0.12155466091632842), (0.324, 0.1211310161948204), (0.32, 0.11961795133352279), (0.321, 0.1242945430278778), (0.3255, 0.12357553511857987), (0.31, 0.12934572541713715), (0.2955, 0.12694223541021346), (0.301, 0.13525106233358383), (0.3125, 0.1308543370962143), (0.303, 0.12801223200559617), (0.2935, 0.13931028068065643), (0.318, 0.13130823981761933), (0.303, 0.13444972568750382), (0.317, 0.13801208770275117), (0.312, 0.13375645595788957), (0.3145, 0.134959899187088)]
TEST: 
[(0.025, 0.11562635576725006), (0.22275, 0.12998788487911225), (0.25975, 0.1723295926451683), (0.27, 0.21614485049247742), (0.27075, 0.26708644461631775), (0.27225, 0.33623107242584227), (0.274, 0.35022689306735993), (0.271, 0.34996879160404204), (0.2705, 0.36124744117259977), (0.26625, 0.37582500994205476), (0.2615, 0.39093086290359497), (0.281, 0.16375452560186385), (0.29175, 0.1315901633501053), (0.28875, 0.12868470549583436), (0.28775, 0.12015911597013473), (0.293, 0.11495648092031478), (0.306, 0.11239607375860214), (0.31725, 0.10245487314462662), (0.3235, 0.09839517772197723), (0.322, 0.09823123025894165), (0.33375, 0.10074899783730507), (0.319, 0.10119138878583908), (0.316, 0.10586317783594132), (0.3155, 0.10526266878843307), (0.3105, 0.11206632354855538), (0.31875, 0.10430984681844711), (0.321, 0.10803415328264236), (0.31375, 0.11152331411838531), (0.31925, 0.10856254380941391), (0.31825, 0.10903393608331681), (0.31875, 0.10911076039075851), (0.315, 0.10995586907863616), (0.31225, 0.11670025515556336), (0.3115, 0.1162042606472969), (0.31, 0.11851014220714569), (0.3095, 0.1178330379128456), (0.31225, 0.11929830259084702), (0.31425, 0.11767884790897369), (0.31575, 0.12217690050601959), (0.32925, 0.1205584414601326), (0.31525, 0.12463415789604188), (0.3015, 0.12265324085950852), (0.3065, 0.12925378727912903), (0.3095, 0.12561651784181596), (0.307, 0.124135880112648), (0.3, 0.13338764631748198), (0.31825, 0.12816673189401626), (0.317, 0.12820935928821564), (0.31175, 0.13339962303638458), (0.31725, 0.129801251411438), (0.30575, 0.1319389436841011)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.33      0.48      0.39       100
           3       0.22      0.27      0.24       100
           4       0.22      0.21      0.21       100
           5       0.30      0.24      0.27       100
           6       0.26      0.27      0.26       100
          10       0.24      0.22      0.23       100
          13       0.58      0.21      0.31       100
          16       0.29      0.38      0.33       100
          17       0.28      0.60      0.39       100
          18       0.24      0.15      0.19       100
          24       0.38      0.62      0.47       100
          28       0.36      0.37      0.36       100
          32       0.32      0.21      0.25       100
          33       0.35      0.28      0.31       100
          35       0.39      0.23      0.29       100
          44       0.29      0.14      0.19       100
          47       0.53      0.46      0.49       100
          49       0.37      0.39      0.38       100
          50       0.13      0.12      0.12       100
          53       0.52      0.56      0.54       100
          56       0.50      0.29      0.37       100
          59       0.30      0.14      0.19       100
          61       0.30      0.39      0.34       100
          63       0.26      0.28      0.27       100
          65       0.12      0.10      0.11       100
          67       0.22      0.12      0.15       100
          68       0.61      0.66      0.63       100
          69       0.58      0.33      0.42       100
          70       0.37      0.49      0.42       100
          72       0.12      0.14      0.13       100
          74       0.11      0.11      0.11       100
          76       0.50      0.33      0.40       100
          77       0.09      0.06      0.07       100
          79       0.26      0.36      0.31       100
          80       0.09      0.11      0.10       100
          88       0.29      0.33      0.31       100
          92       0.28      0.26      0.27       100
          94       0.53      0.43      0.48       100
          95       0.38      0.49      0.43       100
          97       0.20      0.40      0.26       100

    accuracy                           0.31      4000
   macro avg       0.32      0.31      0.30      4000
weighted avg       0.32      0.31      0.30      4000

do_assignment: None
seeds: [104]
name: naive-cifar100-feddf104
score_metric: contrloss
aggregation: <function fed_df at 0x796ebbc41e50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=104
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[36, 90, 44, 64, 55, 32, 52, 66, 8, 73, 71, 84, 43, 45, 34, 87, 20, 4, 89, 94, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([36, 90, 44, 64, 55, 32, 52, 66,  8, 73, 71, 84, 43, 45, 34, 87, 20,  4,
        89, 94,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0255, 0.11635315287113189)
DC 1, val_set_size=2000, COIs=[80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12, 5, 69, 53, 86, 54, 67, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12,  5, 69, 53, 86,
        54, 67,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0255, 0.1163812974691391)
DC 2, val_set_size=2000, COIs=[27, 40, 28, 33, 79, 92, 6, 63, 88, 17, 97, 19, 23, 56, 75, 77, 3, 76, 78, 1, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([27, 40, 28, 33, 79, 92,  6, 63, 88, 17, 97, 19, 23, 56, 75, 77,  3, 76,
        78,  1,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0245, 0.11634207487106324)
D00: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D01: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D02: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D03: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D04: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D05: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D06: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D07: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D08: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D09: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D010: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D011: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D012: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D013: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D014: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D015: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D016: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D017: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D018: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D019: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D020: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D021: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D022: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D023: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.291, 0.084491739153862) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.08184864684939384) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.302, 0.08480697828531265) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3125, 0.07900555083155632) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.07167856121063232) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3435, 0.07510614442825317) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.35, 0.07467654305696488) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4075, 0.06720047700405121) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3775, 0.07185562300682068) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.377, 0.07375414788722992) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.396, 0.06884421154856682) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.07329840672016144) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.36, 0.07710159441828728) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.408, 0.06963430070877075) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3815, 0.07578396302461624) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3655, 0.08049584037065506) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3995, 0.07440476255118847) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3925, 0.07860507655143738) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3625, 0.08366030687093735) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4055, 0.07633106315135955) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3855, 0.08164998775720596) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3645, 0.08730740123987198) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4095, 0.07636912268400192) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.373, 0.08709306174516677) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3615, 0.0950911215543747) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3955, 0.08113914446532726) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3775, 0.08999192982912063) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.09608972877264023) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4, 0.08879758298397064) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3725, 0.09595705622434617) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3515, 0.10642057764530181) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.413, 0.09042463886737824) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.375, 0.10239652782678604) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3615, 0.11196044760942458) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.408, 0.09976641291379929) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.378, 0.10737376129627228) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3515, 0.12360150527954102) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.401, 0.10512463536858559) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3765, 0.11799503839015961) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.346, 0.1341104552745819) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.396, 0.11982187494635582) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3605, 0.13671149891614914) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.14539407640695573) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3915, 0.11603652016818523) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.377, 0.12826339173316956) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.15099484580755235) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4015, 0.13224676537513733) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.14558574032783508) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.341, 0.15033137306571007) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.398, 0.13317745469510556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3595, 0.14373026776313783) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.15792321914434432) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.13887994688749314) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.14276024198532106) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.346, 0.16457674872875214) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4, 0.12813228008151054) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3565, 0.15128170788288117) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3665, 0.16877504763007165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.14666082639992237) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.1572652036547661) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3365, 0.1778144632577896) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3885, 0.13677832114696503) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.16714174062013626) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3435, 0.17030365067720413) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3855, 0.1459871746301651) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.361, 0.16975603181123733) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3385, 0.18926231926679613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4025, 0.14856745912134647) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.369, 0.17453718185424805) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.345, 0.18004278934001922) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.387, 0.15448821721971034) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.351, 0.18214821845293044) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.333, 0.1944507811665535) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4055, 0.15610846966505051) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.1882742451429367) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.314, 0.19901802867650986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3775, 0.16853897380828858) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.368, 0.18446374440193175) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3475, 0.19408497232198715) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3865, 0.17219672209024428) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.356, 0.1862647405862808) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.333, 0.20676714193820953) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3825, 0.17048060888051986) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.18945495969057083) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3495, 0.19836901104450225) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.1829182613492012) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.19804227137565614) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.2104657192826271) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.1789216821193695) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3535, 0.1870178883075714) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.351, 0.20600975239276886) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3865, 0.17812780436873435) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3695, 0.18966610550880433) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3365, 0.22127014762163164) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.16719713319838048) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3515, 0.20669635033607484) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3555, 0.20293691009283066) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3655, 0.19181818187236785) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3555, 0.20885380673408507) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.20291757106781005) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.18115347945690155) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3635, 0.21331161510944366) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.341, 0.2196347106695175) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.1953593570291996) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.21870806384086608) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3505, 0.22185374653339385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.367, 0.19648930752277374) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3615, 0.20419235515594483) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.34, 0.22878525602817534) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.387, 0.18859039011597634) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3665, 0.22473919081687926) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.34, 0.2341290894150734) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.394, 0.1882849870622158) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.22709478771686553) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.317, 0.24827253836393356) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3805, 0.19873912408947944) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.358, 0.2368955457210541) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.344, 0.2397461898326874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.20321886026859284) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3395, 0.2308296718597412) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3595, 0.22456152692437173) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.20454838648438453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.337, 0.23185903882980347) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3495, 0.23108715504407884) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.391, 0.20316246032714844) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.357, 0.21707486498355866) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3535, 0.2343251532316208) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.387, 0.20477520644664765) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3395, 0.24003990757465363) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.355, 0.2386781764626503) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.20806522560119628) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.349, 0.2404121114015579) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3425, 0.24228618943691255) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3885, 0.2072000296115875) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.23743796837329864) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.2350431228876114) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.4, 0.20177280414104462) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3585, 0.2352358844280243) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3345, 0.24806825280189515) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3815, 0.20218050485849381) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3705, 0.22768232691287996) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.344, 0.23995371919870376) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.20185783940553664) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.355, 0.24414642715454102) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.352, 0.23795903563499451) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.408, 0.20741122609376908) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3495, 0.24651797127723693) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.343, 0.2606536382436752) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.394, 0.21046072298288346) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.2603135672807694) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.0255, 0.11635315287113189), (0.291, 0.084491739153862), (0.3125, 0.07900555083155632), (0.35, 0.07467654305696488), (0.377, 0.07375414788722992), (0.36, 0.07710159441828728), (0.3655, 0.08049584037065506), (0.3625, 0.08366030687093735), (0.3645, 0.08730740123987198), (0.3615, 0.0950911215543747), (0.362, 0.09608972877264023), (0.3515, 0.10642057764530181), (0.3615, 0.11196044760942458), (0.3515, 0.12360150527954102), (0.346, 0.1341104552745819), (0.354, 0.14539407640695573), (0.345, 0.15099484580755235), (0.341, 0.15033137306571007), (0.345, 0.15792321914434432), (0.346, 0.16457674872875214), (0.3665, 0.16877504763007165), (0.3365, 0.1778144632577896), (0.3435, 0.17030365067720413), (0.3385, 0.18926231926679613), (0.345, 0.18004278934001922), (0.333, 0.1944507811665535), (0.314, 0.19901802867650986), (0.3475, 0.19408497232198715), (0.333, 0.20676714193820953), (0.3495, 0.19836901104450225), (0.338, 0.2104657192826271), (0.351, 0.20600975239276886), (0.3365, 0.22127014762163164), (0.3555, 0.20293691009283066), (0.3485, 0.20291757106781005), (0.341, 0.2196347106695175), (0.3505, 0.22185374653339385), (0.34, 0.22878525602817534), (0.34, 0.2341290894150734), (0.317, 0.24827253836393356), (0.344, 0.2397461898326874), (0.3595, 0.22456152692437173), (0.3495, 0.23108715504407884), (0.3535, 0.2343251532316208), (0.355, 0.2386781764626503), (0.3425, 0.24228618943691255), (0.354, 0.2350431228876114), (0.3345, 0.24806825280189515), (0.344, 0.23995371919870376), (0.352, 0.23795903563499451), (0.343, 0.2606536382436752)]
TEST: 
[(0.02125, 0.11544804280996322), (0.3, 0.08327795606851578), (0.33, 0.07675403863191604), (0.3515, 0.0726413286626339), (0.3715, 0.07230251342058182), (0.36575, 0.07442397969961166), (0.35675, 0.07867735016345978), (0.349, 0.08093936643004418), (0.35575, 0.08370710945129395), (0.356, 0.09146571010351182), (0.35, 0.09464977961778641), (0.3525, 0.10275655373930931), (0.362, 0.10633469039201736), (0.34725, 0.11862956994771957), (0.34375, 0.12822254076600076), (0.34325, 0.1411014533340931), (0.3445, 0.14776702773571015), (0.345, 0.14309909331798554), (0.35325, 0.14847042310237885), (0.343, 0.15366062384843826), (0.34925, 0.16131680423021316), (0.3295, 0.17099939531087877), (0.34375, 0.16560469245910645), (0.3385, 0.18238990414142608), (0.35475, 0.17356043910980223), (0.33275, 0.18743646138906478), (0.31575, 0.19613589024543762), (0.3405, 0.19156165516376494), (0.3355, 0.19670596426725387), (0.3325, 0.18888598275184632), (0.33725, 0.19779546904563905), (0.3485, 0.19663558161258698), (0.33225, 0.21006071078777314), (0.342, 0.19664752066135407), (0.34425, 0.19657797276973724), (0.344, 0.20441442602872847), (0.34875, 0.21038494300842286), (0.345, 0.21688138628005982), (0.339, 0.21890706396102905), (0.33475, 0.2329029961824417), (0.33375, 0.22707404816150664), (0.34125, 0.2157403007745743), (0.341, 0.2175253474712372), (0.34625, 0.21968414676189424), (0.348, 0.21770366621017456), (0.346, 0.2229352622628212), (0.3405, 0.22347294849157334), (0.333, 0.2304431121945381), (0.34425, 0.22649926710128784), (0.335, 0.22368600630760194), (0.33325, 0.2398388568162918)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.59      0.58      0.58       100
           2       0.20      0.28      0.23       100
           4       0.18      0.19      0.19       100
           7       0.44      0.37      0.40       100
           8       0.41      0.33      0.37       100
           9       0.42      0.53      0.47       100
          13       0.32      0.46      0.37       100
          15       0.31      0.16      0.21       100
          16       0.26      0.28      0.27       100
          18       0.26      0.22      0.24       100
          20       0.76      0.60      0.67       100
          21       0.41      0.34      0.37       100
          25       0.27      0.35      0.30       100
          29       0.31      0.25      0.28       100
          32       0.21      0.26      0.23       100
          34       0.22      0.23      0.22       100
          36       0.35      0.30      0.32       100
          42       0.20      0.23      0.21       100
          43       0.28      0.23      0.25       100
          44       0.15      0.11      0.13       100
          45       0.17      0.30      0.21       100
          47       0.51      0.42      0.46       100
          50       0.19      0.23      0.21       100
          52       0.60      0.49      0.54       100
          55       0.20      0.22      0.21       100
          64       0.15      0.12      0.13       100
          66       0.32      0.15      0.20       100
          71       0.65      0.81      0.72       100
          73       0.44      0.37      0.40       100
          82       0.67      0.60      0.63       100
          83       0.39      0.29      0.33       100
          84       0.17      0.13      0.15       100
          87       0.34      0.44      0.39       100
          89       0.33      0.31      0.32       100
          90       0.24      0.29      0.26       100
          91       0.40      0.50      0.45       100
          93       0.28      0.27      0.27       100
          94       0.62      0.47      0.53       100
          96       0.41      0.32      0.36       100
          98       0.20      0.30      0.24       100

    accuracy                           0.33      4000
   macro avg       0.35      0.33      0.33      4000
weighted avg       0.35      0.33      0.33      4000

No_Competition_DC_1
VAL: 
[(0.0255, 0.1163812974691391), (0.312, 0.08184864684939384), (0.384, 0.07167856121063232), (0.4075, 0.06720047700405121), (0.396, 0.06884421154856682), (0.408, 0.06963430070877075), (0.3995, 0.07440476255118847), (0.4055, 0.07633106315135955), (0.4095, 0.07636912268400192), (0.3955, 0.08113914446532726), (0.4, 0.08879758298397064), (0.413, 0.09042463886737824), (0.408, 0.09976641291379929), (0.401, 0.10512463536858559), (0.396, 0.11982187494635582), (0.3915, 0.11603652016818523), (0.4015, 0.13224676537513733), (0.398, 0.13317745469510556), (0.39, 0.13887994688749314), (0.4, 0.12813228008151054), (0.389, 0.14666082639992237), (0.3885, 0.13677832114696503), (0.3855, 0.1459871746301651), (0.4025, 0.14856745912134647), (0.387, 0.15448821721971034), (0.4055, 0.15610846966505051), (0.3775, 0.16853897380828858), (0.3865, 0.17219672209024428), (0.3825, 0.17048060888051986), (0.384, 0.1829182613492012), (0.377, 0.1789216821193695), (0.3865, 0.17812780436873435), (0.384, 0.16719713319838048), (0.3655, 0.19181818187236785), (0.373, 0.18115347945690155), (0.391, 0.1953593570291996), (0.367, 0.19648930752277374), (0.387, 0.18859039011597634), (0.394, 0.1882849870622158), (0.3805, 0.19873912408947944), (0.377, 0.20321886026859284), (0.377, 0.20454838648438453), (0.391, 0.20316246032714844), (0.387, 0.20477520644664765), (0.376, 0.20806522560119628), (0.3885, 0.2072000296115875), (0.4, 0.20177280414104462), (0.3815, 0.20218050485849381), (0.389, 0.20185783940553664), (0.408, 0.20741122609376908), (0.394, 0.21046072298288346)]
TEST: 
[(0.02525, 0.11549618828296661), (0.28475, 0.08185698807239533), (0.34975, 0.07159550014138222), (0.385, 0.06777411407232284), (0.38375, 0.06902729853987694), (0.389, 0.07018249329924584), (0.392, 0.07474909627437591), (0.39075, 0.07671788758039474), (0.4025, 0.07820452988147736), (0.3995, 0.08218665289878845), (0.39475, 0.08878381970524787), (0.39225, 0.09153024235367775), (0.3875, 0.10231651076674461), (0.38375, 0.11063652145862579), (0.394, 0.12094689160585403), (0.3805, 0.121427270591259), (0.371, 0.1359748638868332), (0.3685, 0.14101274013519288), (0.366, 0.14725485563278198), (0.37775, 0.13540245932340622), (0.3685, 0.15559806936979295), (0.37575, 0.1434333612918854), (0.3645, 0.1529234192967415), (0.385, 0.15741773110628127), (0.37125, 0.16618956041336058), (0.36625, 0.16591044747829437), (0.3725, 0.17511949515342712), (0.37525, 0.17978415262699127), (0.38325, 0.17797161000967027), (0.372, 0.18698203259706497), (0.3745, 0.18154502058029176), (0.36125, 0.18589617252349855), (0.3775, 0.17491721093654633), (0.371, 0.19415043652057648), (0.365, 0.18645150518417358), (0.379, 0.2020268982052803), (0.36975, 0.20282294976711274), (0.37, 0.19804228603839874), (0.3815, 0.1975776453614235), (0.37325, 0.20505022740364073), (0.3735, 0.2123416645526886), (0.36975, 0.2170696954727173), (0.3685, 0.21570597112178802), (0.3765, 0.2151101078391075), (0.3665, 0.21412527346611024), (0.37425, 0.21812620079517364), (0.3675, 0.21506666773557662), (0.37425, 0.21092776936292648), (0.36375, 0.21475806152820587), (0.37125, 0.22698817420005798), (0.3715, 0.2297021476626396)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.55      0.49      0.52       100
           2       0.18      0.31      0.22       100
           5       0.26      0.20      0.22       100
           7       0.31      0.37      0.33       100
           9       0.46      0.44      0.45       100
          10       0.35      0.26      0.30       100
          12       0.42      0.34      0.38       100
          13       0.33      0.40      0.36       100
          14       0.28      0.27      0.28       100
          15       0.26      0.22      0.24       100
          16       0.32      0.38      0.35       100
          18       0.36      0.21      0.26       100
          21       0.44      0.47      0.45       100
          22       0.39      0.29      0.33       100
          24       0.52      0.64      0.57       100
          25       0.22      0.19      0.20       100
          29       0.36      0.33      0.34       100
          30       0.47      0.48      0.47       100
          38       0.25      0.25      0.25       100
          39       0.36      0.41      0.38       100
          42       0.23      0.18      0.20       100
          47       0.52      0.51      0.51       100
          48       0.76      0.50      0.60       100
          50       0.17      0.20      0.19       100
          53       0.50      0.70      0.59       100
          54       0.39      0.49      0.43       100
          59       0.34      0.38      0.36       100
          60       0.88      0.74      0.80       100
          65       0.14      0.11      0.12       100
          67       0.46      0.36      0.40       100
          69       0.62      0.55      0.58       100
          80       0.16      0.16      0.16       100
          81       0.28      0.30      0.29       100
          82       0.71      0.67      0.69       100
          83       0.29      0.45      0.35       100
          86       0.32      0.37      0.35       100
          91       0.47      0.44      0.46       100
          93       0.31      0.22      0.26       100
          96       0.28      0.38      0.32       100
          98       0.28      0.20      0.23       100

    accuracy                           0.37      4000
   macro avg       0.38      0.37      0.37      4000
weighted avg       0.38      0.37      0.37      4000

No_Competition_DC_2
VAL: 
[(0.0245, 0.11634207487106324), (0.302, 0.08480697828531265), (0.3435, 0.07510614442825317), (0.3775, 0.07185562300682068), (0.38, 0.07329840672016144), (0.3815, 0.07578396302461624), (0.3925, 0.07860507655143738), (0.3855, 0.08164998775720596), (0.373, 0.08709306174516677), (0.3775, 0.08999192982912063), (0.3725, 0.09595705622434617), (0.375, 0.10239652782678604), (0.378, 0.10737376129627228), (0.3765, 0.11799503839015961), (0.3605, 0.13671149891614914), (0.377, 0.12826339173316956), (0.3565, 0.14558574032783508), (0.3595, 0.14373026776313783), (0.361, 0.14276024198532106), (0.3565, 0.15128170788288117), (0.3615, 0.1572652036547661), (0.3615, 0.16714174062013626), (0.361, 0.16975603181123733), (0.369, 0.17453718185424805), (0.351, 0.18214821845293044), (0.3525, 0.1882742451429367), (0.368, 0.18446374440193175), (0.356, 0.1862647405862808), (0.3665, 0.18945495969057083), (0.353, 0.19804227137565614), (0.3535, 0.1870178883075714), (0.3695, 0.18966610550880433), (0.3515, 0.20669635033607484), (0.3555, 0.20885380673408507), (0.3635, 0.21331161510944366), (0.3525, 0.21870806384086608), (0.3615, 0.20419235515594483), (0.3665, 0.22473919081687926), (0.371, 0.22709478771686553), (0.358, 0.2368955457210541), (0.3395, 0.2308296718597412), (0.337, 0.23185903882980347), (0.357, 0.21707486498355866), (0.3395, 0.24003990757465363), (0.349, 0.2404121114015579), (0.3525, 0.23743796837329864), (0.3585, 0.2352358844280243), (0.3705, 0.22768232691287996), (0.355, 0.24414642715454102), (0.3495, 0.24651797127723693), (0.353, 0.2603135672807694)]
TEST: 
[(0.0265, 0.11540408164262772), (0.304, 0.08289329248666763), (0.34275, 0.0742018551826477), (0.37775, 0.07043213292956352), (0.378, 0.07136544108390808), (0.3905, 0.07360331279039382), (0.38225, 0.07695713001489639), (0.38375, 0.07989049965143204), (0.3805, 0.08409525108337403), (0.38, 0.0872612447142601), (0.37625, 0.09482507637143135), (0.37275, 0.10173738461732865), (0.3765, 0.10626373383402825), (0.37875, 0.11447767573595047), (0.34725, 0.13165356117486954), (0.3655, 0.12689560598134994), (0.3455, 0.14642822217941284), (0.35225, 0.14279957205057145), (0.344, 0.1442368342280388), (0.349, 0.15227461749315263), (0.35475, 0.1555526129603386), (0.3585, 0.16677138966321944), (0.353, 0.16992045605182649), (0.3535, 0.17327285856008529), (0.3485, 0.18063594794273377), (0.3555, 0.18413802886009217), (0.363, 0.17872204899787902), (0.34375, 0.1859004896879196), (0.35625, 0.18664503145217895), (0.347, 0.19690264594554902), (0.33775, 0.1877560019493103), (0.367, 0.18805871468782426), (0.3495, 0.20050947004556655), (0.3365, 0.2057976177930832), (0.35625, 0.21151577246189118), (0.34775, 0.2140682178735733), (0.34875, 0.19645787966251374), (0.34575, 0.21934531784057618), (0.345, 0.21758976805210115), (0.343, 0.2267060536146164), (0.3435, 0.2182169860601425), (0.3445, 0.2225891599059105), (0.3555, 0.21501116824150085), (0.3375, 0.2354152483344078), (0.343, 0.22862037336826324), (0.346, 0.23313542586565017), (0.34575, 0.22838302952051162), (0.3445, 0.22542227005958557), (0.34925, 0.23512598359584808), (0.3405, 0.23683768260478974), (0.334, 0.25528743040561674)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.61      0.53      0.57       100
           1       0.44      0.47      0.46       100
           2       0.21      0.20      0.20       100
           3       0.13      0.14      0.13       100
           6       0.24      0.23      0.24       100
           7       0.25      0.38      0.30       100
           9       0.40      0.44      0.42       100
          13       0.37      0.46      0.41       100
          15       0.21      0.11      0.14       100
          16       0.29      0.30      0.29       100
          17       0.51      0.48      0.49       100
          18       0.22      0.17      0.19       100
          19       0.36      0.23      0.28       100
          21       0.45      0.50      0.48       100
          23       0.68      0.53      0.60       100
          25       0.20      0.38      0.26       100
          27       0.31      0.17      0.22       100
          28       0.39      0.54      0.45       100
          29       0.31      0.22      0.26       100
          33       0.31      0.30      0.30       100
          40       0.22      0.31      0.26       100
          42       0.30      0.21      0.25       100
          47       0.50      0.46      0.48       100
          50       0.17      0.24      0.20       100
          56       0.51      0.53      0.52       100
          63       0.35      0.32      0.34       100
          75       0.66      0.51      0.58       100
          76       0.53      0.62      0.57       100
          77       0.12      0.11      0.11       100
          78       0.18      0.15      0.16       100
          79       0.28      0.24      0.26       100
          82       0.69      0.56      0.62       100
          83       0.29      0.30      0.30       100
          88       0.27      0.24      0.25       100
          91       0.35      0.43      0.39       100
          92       0.30      0.36      0.33       100
          93       0.30      0.23      0.26       100
          96       0.30      0.22      0.25       100
          97       0.29      0.37      0.32       100
          98       0.16      0.17      0.17       100

    accuracy                           0.33      4000
   macro avg       0.34      0.33      0.33      4000
weighted avg       0.34      0.33      0.33      4000

Competition
DC 0, val_set_size=2000, COIs=[36, 90, 44, 64, 55, 32, 52, 66, 8, 73, 71, 84, 43, 45, 34, 87, 20, 4, 89, 94, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([36, 90, 44, 64, 55, 32, 52, 66,  8, 73, 71, 84, 43, 45, 34, 87, 20,  4,
        89, 94,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0255, 0.11635315287113189)
DC 1, val_set_size=2000, COIs=[80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12, 5, 69, 53, 86, 54, 67, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12,  5, 69, 53, 86,
        54, 67,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0255, 0.1163812974691391)
DC 2, val_set_size=2000, COIs=[27, 40, 28, 33, 79, 92, 6, 63, 88, 17, 97, 19, 23, 56, 75, 77, 3, 76, 78, 1, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([27, 40, 28, 33, 79, 92,  6, 63, 88, 17, 97, 19, 23, 56, 75, 77,  3, 76,
        78,  1,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0245, 0.11634207487106324)
D00: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D01: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D02: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D03: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D04: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D05: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D06: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D07: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D08: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D09: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D010: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D011: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D012: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D013: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D014: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D015: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D016: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D017: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D018: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D019: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D020: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D021: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D022: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D023: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO3', '(DO0']
DC 2 --> ['(DO5', '(DO1']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.2015, 0.14124413320422172) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2495, 0.13685523000359534) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2185, 0.14063080114126206) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.236, 0.179111756965518) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2785, 0.1757461946159601) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.17301067760586739) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.229370989382267) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2925, 0.2267121412716806) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.217200438529253) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2475, 0.31285255762934683) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.2991894645988941) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.2717050033211708) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.243, 0.3873253252059221) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2975, 0.3596560550108552) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.268, 0.3085551010072231) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO1']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO5', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2435, 0.36498584380745885) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.3955635580569506) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2665, 0.35130796375870704) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2445, 0.4316484837830067) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.299, 0.3823554871082306) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2765, 0.3392691394984722) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2505, 0.37543490731716156) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2945, 0.3672485957592726) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.272, 0.3635945824086666) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.248, 0.3744322711229324) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.291, 0.3750496474206448) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.272, 0.3831251815855503) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2415, 0.41414068785309793) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.295, 0.38876627540588377) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.273, 0.4116355989873409) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO2', '(DO0']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO1', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.3895511109530926) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2865, 0.45914177082479) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.3883693727552891) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2465, 0.4077915594875813) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.294, 0.3909648527726531) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.43349434465169906) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.239, 0.40767621570825574) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2875, 0.43594921577721835) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.264, 0.406418315500021) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.424606371819973) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.283, 0.4276070909202099) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.4164353334903717) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.237, 0.4200220473706722) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.294, 0.44055644181370734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2635, 0.4287929049134254) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO0', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.238, 0.3932724508047104) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2885, 0.4242779934704304) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.272, 0.38343951976299284) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.41074343967437743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2845, 0.41769053849577903) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.3885553339570761) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.24, 0.39088104954361913) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2835, 0.4055671846270561) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2575, 0.38946292212605477) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.239, 0.3800577490925789) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2845, 0.35896043363213537) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.279, 0.3874071967899799) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2315, 0.3579966588318348) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2875, 0.3807564615532756) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.38770563280582426) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO0', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.36152239471673964) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.284, 0.35668346312642096) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.381985375225544) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2305, 0.3488195944428444) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2805, 0.35696204176545143) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.258, 0.3826098202466965) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.234, 0.3380102637708187) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.29, 0.35474332236498596) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.34946673089265823) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2315, 0.3644614401459694) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2835, 0.35541364930570124) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.262, 0.34573341900110244) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.231, 0.3585214794278145) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.32224218398332594) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.265, 0.3417379797697067) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO5', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2405, 0.32027762800455095) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.284, 0.32520760379731656) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.32749824845790865) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.236, 0.3414340166151524) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.288, 0.3419267881512642) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2665, 0.32218621230125427) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.237, 0.34694113171100616) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.285, 0.29861920295655725) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.32752420735359195) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2325, 0.3324029332399368) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.285, 0.29376079949736594) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.2964027807116508) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2265, 0.326534677490592) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.27062677864730356) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.253, 0.30115581917762757) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO1', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.243, 0.31310147494077684) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2875, 0.27341199113428594) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.32021439707279203) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2465, 0.3141744227707386) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2875, 0.2744226095974445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.3035641466975212) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2415, 0.3129668953716755) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2865, 0.30126551395654677) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2615, 0.3012550274729729) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.31856124544143677) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2895, 0.2862660208493471) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2685, 0.3023498349785805) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.306733842253685) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2955, 0.27522863356024024) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2635, 0.2941250303387642) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.2922548055052757) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2885, 0.2762724220752716) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.259, 0.29440037554502485) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.233, 0.29236388599872587) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.288, 0.2831251385211945) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.2991552227139473) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.24, 0.30527580708265306) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2855, 0.3009596798121929) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.268, 0.2963351156115532) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.28967583531141283) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.29, 0.2890057182610035) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.261, 0.292664675116539) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.23, 0.2878394427001476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2945, 0.2998885536789894) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.26, 0.30583331871032715) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO0', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.26885527673363685) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.291, 0.2602178136110306) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2675, 0.2913057435154915) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.26251584059000016) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.292, 0.2833673030436039) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2625, 0.3058628175854683) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.241, 0.27565398362278937) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2945, 0.2593865067064762) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2685, 0.31134763318300246) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2435, 0.2790254282951355) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2855, 0.2783007901608944) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.31600346046686173) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.235, 0.2869941259026527) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2855, 0.26053358834981916) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.266, 0.31866445577144625) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO5', '(DO0']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO3', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2355, 0.2855747596025467) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.295, 0.25296506026387217) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2595, 0.3101835444569588) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2355, 0.29380671203136444) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2895, 0.2775615207254887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.249, 0.31162931233644486) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2335, 0.2883211831152439) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2945, 0.2735396216511726) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.2744625632166863) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2415, 0.2871698104441166) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.295, 0.2717346629798412) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.254, 0.3004831680059433) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.2830501104593277) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.299, 0.2793758324086666) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.259, 0.2876504848599434) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.0255, 0.11635315287113189), (0.2015, 0.14124413320422172), (0.236, 0.179111756965518), (0.244, 0.229370989382267), (0.2475, 0.31285255762934683), (0.243, 0.3873253252059221), (0.2435, 0.36498584380745885), (0.2445, 0.4316484837830067), (0.2505, 0.37543490731716156), (0.248, 0.3744322711229324), (0.2415, 0.41414068785309793), (0.241, 0.3895511109530926), (0.2465, 0.4077915594875813), (0.239, 0.40767621570825574), (0.233, 0.424606371819973), (0.237, 0.4200220473706722), (0.238, 0.3932724508047104), (0.244, 0.41074343967437743), (0.24, 0.39088104954361913), (0.239, 0.3800577490925789), (0.2315, 0.3579966588318348), (0.241, 0.36152239471673964), (0.2305, 0.3488195944428444), (0.234, 0.3380102637708187), (0.2315, 0.3644614401459694), (0.231, 0.3585214794278145), (0.2405, 0.32027762800455095), (0.236, 0.3414340166151524), (0.237, 0.34694113171100616), (0.2325, 0.3324029332399368), (0.2265, 0.326534677490592), (0.243, 0.31310147494077684), (0.2465, 0.3141744227707386), (0.2415, 0.3129668953716755), (0.242, 0.31856124544143677), (0.235, 0.306733842253685), (0.244, 0.2922548055052757), (0.233, 0.29236388599872587), (0.24, 0.30527580708265306), (0.235, 0.28967583531141283), (0.23, 0.2878394427001476), (0.244, 0.26885527673363685), (0.242, 0.26251584059000016), (0.241, 0.27565398362278937), (0.2435, 0.2790254282951355), (0.235, 0.2869941259026527), (0.2355, 0.2855747596025467), (0.2355, 0.29380671203136444), (0.2335, 0.2883211831152439), (0.2415, 0.2871698104441166), (0.242, 0.2830501104593277)]
TEST: 
[(0.02125, 0.11544804280996322), (0.2115, 0.13888888734579086), (0.23175, 0.1767195948958397), (0.243, 0.22580609637498855), (0.24425, 0.30429329097270963), (0.247, 0.3795030519962311), (0.247, 0.3601631795167923), (0.24825, 0.42789508962631223), (0.24775, 0.3672107158899307), (0.2455, 0.3697466288805008), (0.2495, 0.40668471944332124), (0.24425, 0.38180296003818515), (0.2455, 0.4013561545610428), (0.24275, 0.3988431282043457), (0.244, 0.4170570106506348), (0.242, 0.4117679896354675), (0.23775, 0.3897316732406616), (0.2385, 0.4058790078163147), (0.23725, 0.38779403614997865), (0.23675, 0.3720280361175537), (0.23475, 0.3517470943927765), (0.242, 0.35569657766819), (0.23725, 0.34316841065883635), (0.23575, 0.33583703196048736), (0.2345, 0.35763053011894225), (0.22825, 0.3546383864879608), (0.24325, 0.3134962986707687), (0.24325, 0.3337186183929443), (0.23975, 0.3377706079483032), (0.23825, 0.32755644810199736), (0.22275, 0.3195955239534378), (0.241, 0.305181800365448), (0.23525, 0.30888771450519564), (0.2345, 0.3114036864042282), (0.23525, 0.31146413218975066), (0.2395, 0.3011649478673935), (0.24275, 0.2872846380472183), (0.24425, 0.28552813053131104), (0.2435, 0.30091615378856656), (0.2385, 0.28679944241046906), (0.2365, 0.28122139048576356), (0.23975, 0.2669140980243683), (0.2445, 0.26066856980323794), (0.2475, 0.2729711428880692), (0.24025, 0.27862224197387697), (0.23925, 0.2845443885326385), (0.243, 0.2829920675754547), (0.236, 0.29142646300792696), (0.246, 0.28959779953956605), (0.243, 0.289328385591507), (0.2365, 0.2872427932024002)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           2       0.00      0.00      0.00       100
           4       0.16      0.36      0.22       100
           7       1.00      0.01      0.02       100
           8       0.40      0.56      0.47       100
           9       0.40      0.02      0.04       100
          13       0.00      0.00      0.00       100
          15       0.00      0.00      0.00       100
          16       0.00      0.00      0.00       100
          18       0.00      0.00      0.00       100
          20       0.44      0.67      0.53       100
          21       0.00      0.00      0.00       100
          25       0.00      0.00      0.00       100
          29       1.00      0.01      0.02       100
          32       0.14      0.36      0.20       100
          34       0.21      0.33      0.25       100
          36       0.22      0.52      0.31       100
          42       0.00      0.00      0.00       100
          43       0.24      0.46      0.31       100
          44       0.11      0.24      0.15       100
          45       0.12      0.27      0.17       100
          47       0.00      0.00      0.00       100
          50       0.00      0.00      0.00       100
          52       0.34      0.89      0.50       100
          55       0.09      0.23      0.13       100
          64       0.17      0.25      0.20       100
          66       0.24      0.24      0.24       100
          71       0.54      0.81      0.65       100
          73       0.33      0.67      0.44       100
          82       0.86      0.18      0.30       100
          83       0.29      0.04      0.07       100
          84       0.14      0.26      0.18       100
          87       0.23      0.51      0.32       100
          89       0.24      0.45      0.31       100
          90       0.17      0.44      0.25       100
          91       0.00      0.00      0.00       100
          93       0.00      0.00      0.00       100
          94       0.35      0.68      0.46       100
          96       0.00      0.00      0.00       100
          98       0.00      0.00      0.00       100

    accuracy                           0.24      4000
   macro avg       0.21      0.24      0.17      4000
weighted avg       0.21      0.24      0.17      4000

Competition_DC_1
VAL: 
[(0.0255, 0.1163812974691391), (0.2495, 0.13685523000359534), (0.2785, 0.1757461946159601), (0.2925, 0.2267121412716806), (0.293, 0.2991894645988941), (0.2975, 0.3596560550108552), (0.301, 0.3955635580569506), (0.299, 0.3823554871082306), (0.2945, 0.3672485957592726), (0.291, 0.3750496474206448), (0.295, 0.38876627540588377), (0.2865, 0.45914177082479), (0.294, 0.3909648527726531), (0.2875, 0.43594921577721835), (0.283, 0.4276070909202099), (0.294, 0.44055644181370734), (0.2885, 0.4242779934704304), (0.2845, 0.41769053849577903), (0.2835, 0.4055671846270561), (0.2845, 0.35896043363213537), (0.2875, 0.3807564615532756), (0.284, 0.35668346312642096), (0.2805, 0.35696204176545143), (0.29, 0.35474332236498596), (0.2835, 0.35541364930570124), (0.293, 0.32224218398332594), (0.284, 0.32520760379731656), (0.288, 0.3419267881512642), (0.285, 0.29861920295655725), (0.285, 0.29376079949736594), (0.293, 0.27062677864730356), (0.2875, 0.27341199113428594), (0.2875, 0.2744226095974445), (0.2865, 0.30126551395654677), (0.2895, 0.2862660208493471), (0.2955, 0.27522863356024024), (0.2885, 0.2762724220752716), (0.288, 0.2831251385211945), (0.2855, 0.3009596798121929), (0.29, 0.2890057182610035), (0.2945, 0.2998885536789894), (0.291, 0.2602178136110306), (0.292, 0.2833673030436039), (0.2945, 0.2593865067064762), (0.2855, 0.2783007901608944), (0.2855, 0.26053358834981916), (0.295, 0.25296506026387217), (0.2895, 0.2775615207254887), (0.2945, 0.2735396216511726), (0.295, 0.2717346629798412), (0.299, 0.2793758324086666)]
TEST: 
[(0.02525, 0.11549618828296661), (0.24625, 0.13654447674751283), (0.27025, 0.1758717939257622), (0.28025, 0.22758129954338074), (0.282, 0.3041711230278015), (0.2885, 0.368138565659523), (0.2885, 0.4128724836111069), (0.29075, 0.3892303282022476), (0.28225, 0.3777688231468201), (0.28775, 0.38387882661819456), (0.2805, 0.39324738609790805), (0.26975, 0.4814384090900421), (0.28425, 0.39532028818130494), (0.28125, 0.4480146996974945), (0.282, 0.43891456294059755), (0.28575, 0.44711227059364317), (0.2825, 0.43585598373413087), (0.278, 0.4263685324192047), (0.27125, 0.4132267074584961), (0.28025, 0.36009111142158506), (0.2775, 0.3841754589080811), (0.281, 0.3589736496210098), (0.27325, 0.35623944556713105), (0.27375, 0.35361087250709533), (0.2815, 0.3557342745065689), (0.2815, 0.3217112336158752), (0.28125, 0.3207189869880676), (0.276, 0.34142480766773226), (0.2815, 0.3023842228651047), (0.27775, 0.2974078924655914), (0.278, 0.2734921514987946), (0.2775, 0.27557133603096007), (0.2915, 0.2715899608135223), (0.27725, 0.3014012749195099), (0.28325, 0.2877471638917923), (0.28025, 0.27770001208782197), (0.2815, 0.2785239018201828), (0.282, 0.28389418935775756), (0.28, 0.2988953068256378), (0.2765, 0.2922477205991745), (0.28025, 0.29282696843147277), (0.2885, 0.25862939453125), (0.2815, 0.27842021715641024), (0.27825, 0.2598267800807953), (0.2765, 0.2787038031816483), (0.27875, 0.25209282493591306), (0.28625, 0.24925511872768402), (0.2855, 0.27421896350383757), (0.287, 0.2701603416204453), (0.2865, 0.26989653980731965), (0.2825, 0.2770481761693954)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00       100
           2       0.00      0.00      0.00       100
           5       0.23      0.46      0.31       100
           7       0.00      0.00      0.00       100
           9       0.78      0.07      0.13       100
          10       0.22      0.37      0.27       100
          12       0.28      0.51      0.36       100
          13       0.00      0.00      0.00       100
          14       0.24      0.45      0.32       100
          15       0.25      0.01      0.02       100
          16       0.90      0.09      0.16       100
          18       0.00      0.00      0.00       100
          21       1.00      0.01      0.02       100
          22       0.33      0.42      0.37       100
          24       0.30      0.75      0.43       100
          25       0.00      0.00      0.00       100
          29       0.00      0.00      0.00       100
          30       0.48      0.60      0.53       100
          38       0.18      0.44      0.25       100
          39       0.38      0.47      0.42       100
          42       0.00      0.00      0.00       100
          47       0.70      0.07      0.13       100
          48       0.45      0.67      0.54       100
          50       0.00      0.00      0.00       100
          53       0.26      0.79      0.39       100
          54       0.29      0.67      0.40       100
          59       0.21      0.68      0.32       100
          60       0.62      0.89      0.73       100
          65       0.16      0.29      0.21       100
          67       0.28      0.53      0.36       100
          69       0.44      0.72      0.55       100
          80       0.12      0.33      0.18       100
          81       0.26      0.43      0.33       100
          82       1.00      0.01      0.02       100
          83       1.00      0.01      0.02       100
          86       0.23      0.45      0.30       100
          91       0.92      0.11      0.20       100
          93       0.00      0.00      0.00       100
          96       0.00      0.00      0.00       100
          98       0.00      0.00      0.00       100

    accuracy                           0.28      4000
   macro avg       0.31      0.28      0.21      4000
weighted avg       0.31      0.28      0.21      4000

Competition_DC_2
VAL: 
[(0.0245, 0.11634207487106324), (0.2185, 0.14063080114126206), (0.253, 0.17301067760586739), (0.2605, 0.217200438529253), (0.264, 0.2717050033211708), (0.268, 0.3085551010072231), (0.2665, 0.35130796375870704), (0.2765, 0.3392691394984722), (0.272, 0.3635945824086666), (0.272, 0.3831251815855503), (0.273, 0.4116355989873409), (0.2655, 0.3883693727552891), (0.258, 0.43349434465169906), (0.264, 0.406418315500021), (0.258, 0.4164353334903717), (0.2635, 0.4287929049134254), (0.272, 0.38343951976299284), (0.2645, 0.3885553339570761), (0.2575, 0.38946292212605477), (0.279, 0.3874071967899799), (0.265, 0.38770563280582426), (0.2595, 0.381985375225544), (0.258, 0.3826098202466965), (0.254, 0.34946673089265823), (0.262, 0.34573341900110244), (0.265, 0.3417379797697067), (0.253, 0.32749824845790865), (0.2665, 0.32218621230125427), (0.2595, 0.32752420735359195), (0.2625, 0.2964027807116508), (0.253, 0.30115581917762757), (0.2595, 0.32021439707279203), (0.2605, 0.3035641466975212), (0.2615, 0.3012550274729729), (0.2685, 0.3023498349785805), (0.2635, 0.2941250303387642), (0.259, 0.29440037554502485), (0.2585, 0.2991552227139473), (0.268, 0.2963351156115532), (0.261, 0.292664675116539), (0.26, 0.30583331871032715), (0.2675, 0.2913057435154915), (0.2625, 0.3058628175854683), (0.2685, 0.31134763318300246), (0.2605, 0.31600346046686173), (0.266, 0.31866445577144625), (0.2595, 0.3101835444569588), (0.249, 0.31162931233644486), (0.2605, 0.2744625632166863), (0.254, 0.3004831680059433), (0.259, 0.2876504848599434)]
TEST: 
[(0.0265, 0.11540408164262772), (0.21625, 0.13826286536455154), (0.25175, 0.17030568391084672), (0.263, 0.21405084884166717), (0.267, 0.26886495351791384), (0.2665, 0.30544296979904173), (0.2705, 0.3478743932247162), (0.271, 0.3331995316743851), (0.27275, 0.358649737238884), (0.2725, 0.37594563138484954), (0.27275, 0.40429456877708436), (0.271, 0.3830161112546921), (0.25725, 0.43599658012390136), (0.2635, 0.4026748967170715), (0.255, 0.40674976336956026), (0.258, 0.4206792163848877), (0.26325, 0.3798733819723129), (0.25525, 0.38632370376586916), (0.259, 0.3852367960214615), (0.25675, 0.3878340035676956), (0.2625, 0.3756756570339203), (0.2545, 0.37682893466949463), (0.25625, 0.3758586901426315), (0.25425, 0.3419332580566406), (0.26125, 0.33773614990711215), (0.2535, 0.3392991579771042), (0.2615, 0.3205639086961746), (0.25475, 0.3183043313026428), (0.25175, 0.3241081484556198), (0.26325, 0.2923232229948044), (0.2485, 0.29510169982910156), (0.2645, 0.31545290184020996), (0.2585, 0.2983936325311661), (0.255, 0.2930122476816177), (0.25825, 0.2964335765838623), (0.254, 0.2910341184139252), (0.2665, 0.28969471275806424), (0.2545, 0.29418607115745543), (0.25075, 0.29309684681892395), (0.25625, 0.29136620795726775), (0.257, 0.30336598098278045), (0.2605, 0.28573418951034546), (0.25475, 0.30119785153865813), (0.26075, 0.3082853944301605), (0.26175, 0.3157772334814072), (0.2565, 0.31595674669742585), (0.2605, 0.3020868804454803), (0.259, 0.3071153573989868), (0.26225, 0.2696528742313385), (0.253, 0.2889113231897354), (0.2585, 0.2786104781627655)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.38      0.03      0.06       100
           1       0.31      0.70      0.43       100
           2       0.00      0.00      0.00       100
           3       0.15      0.31      0.21       100
           6       0.15      0.46      0.22       100
           7       0.33      0.01      0.02       100
           9       0.33      0.01      0.02       100
          13       1.00      0.01      0.02       100
          15       0.25      0.01      0.02       100
          16       0.00      0.00      0.00       100
          17       0.44      0.69      0.54       100
          18       0.00      0.00      0.00       100
          19       0.21      0.31      0.25       100
          21       0.00      0.00      0.00       100
          23       0.60      0.76      0.67       100
          25       0.00      0.00      0.00       100
          27       0.21      0.34      0.26       100
          28       0.32      0.61      0.42       100
          29       0.00      0.00      0.00       100
          33       0.24      0.56      0.33       100
          40       0.18      0.38      0.25       100
          42       0.00      0.00      0.00       100
          47       0.50      0.01      0.02       100
          50       0.00      0.00      0.00       100
          56       0.28      0.64      0.39       100
          63       0.25      0.49      0.33       100
          75       0.45      0.75      0.56       100
          76       0.38      0.80      0.51       100
          77       0.15      0.28      0.20       100
          78       0.17      0.34      0.23       100
          79       0.29      0.52      0.37       100
          82       0.67      0.02      0.04       100
          83       0.00      0.00      0.00       100
          88       0.19      0.40      0.26       100
          91       0.00      0.00      0.00       100
          92       0.20      0.51      0.29       100
          93       1.00      0.01      0.02       100
          96       1.00      0.01      0.02       100
          97       0.21      0.37      0.26       100
          98       0.00      0.00      0.00       100

    accuracy                           0.26      4000
   macro avg       0.27      0.26      0.18      4000
weighted avg       0.27      0.26      0.18      4000

Collaboration
DC 0, val_set_size=2000, COIs=[36, 90, 44, 64, 55, 32, 52, 66, 8, 73, 71, 84, 43, 45, 34, 87, 20, 4, 89, 94, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([36, 90, 44, 64, 55, 32, 52, 66,  8, 73, 71, 84, 43, 45, 34, 87, 20,  4,
        89, 94,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0255, 0.11635315287113189)
DC 1, val_set_size=2000, COIs=[80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12, 5, 69, 53, 86, 54, 67, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12,  5, 69, 53, 86,
        54, 67,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0255, 0.1163812974691391)
DC 2, val_set_size=2000, COIs=[27, 40, 28, 33, 79, 92, 6, 63, 88, 17, 97, 19, 23, 56, 75, 77, 3, 76, 78, 1, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([27, 40, 28, 33, 79, 92,  6, 63, 88, 17, 97, 19, 23, 56, 75, 77,  3, 76,
        78,  1,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.0245, 0.11634207487106324)
D00: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D01: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D02: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D03: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D04: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D05: 1000 samples from classes {0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98}
D06: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D07: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D08: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D09: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D010: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D011: 1000 samples from classes {4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94}
D012: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D013: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D014: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D015: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D016: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D017: 1000 samples from classes {5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86}
D018: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D019: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D020: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D021: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D022: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
D023: 1000 samples from classes {1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO2']
DC 1 --> ['(DO4', '(DO5']
DC 2 --> ['(DO0', '(DO1']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.1995, 0.14250404837727546) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.254, 0.13570773777365686) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.215, 0.14347445401549339) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.225, 0.17866674886643888) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2905, 0.1741541069149971) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.18216418340802193) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2395, 0.22812835544347762) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2935, 0.2231589137762785) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.257, 0.23195096601545812) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2435, 0.3198331189155579) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.296, 0.28562465962022543) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.273, 0.3106925796866417) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.238, 0.36310096162557604) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2945, 0.3695896264081821) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.268, 0.34243845027685166) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO4', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.243, 0.38475264793634417) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2915, 0.3522641851827502) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2665, 0.3503003849387169) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.242, 0.359465590685606) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.296, 0.38746142720058563) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2645, 0.37890497291088104) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.3827433533966541) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.295, 0.41894165879115464) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2655, 0.3658933582901955) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.244, 0.36117097902297973) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2905, 0.39712453600019215) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.267, 0.40693069669604304) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.239, 0.3823912570476532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2915, 0.38058441007882354) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2585, 0.411374171346426) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[0, 2, 7, 9, 13, 15, 16, 18, 21, 25, 29, 42, 47, 50, 82, 83, 91, 93, 96, 98], M=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 13, 14, 15, 16, 17, 18,
        19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 32, 33, 34, 36, 38, 39, 40,
        42, 43, 44, 45, 47, 48, 50, 52, 53, 54, 55, 56, 59, 60, 63, 64, 65, 66,
        67, 69, 71, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89,
        90, 91, 92, 93, 94, 96, 97, 98], device='cuda:0'), Initial Performance: (0.3913333333333333, 0.0682864624261856)
DC Expert-0, val_set_size=1000, COIs=[4, 8, 20, 32, 34, 36, 43, 44, 45, 52, 55, 64, 66, 71, 73, 84, 87, 89, 90, 94], M=tensor([36, 90, 44, 64, 55, 32, 52, 66,  8, 73, 71, 84, 43, 45, 34, 87, 20,  4,
        89, 94,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.478, 0.07371786987781524)
DC Expert-1, val_set_size=1000, COIs=[5, 10, 12, 14, 22, 24, 30, 38, 39, 48, 53, 54, 59, 60, 65, 67, 69, 80, 81, 86], M=tensor([80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12,  5, 69, 53, 86,
        54, 67,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.583, 0.052465935006737706)
DC Expert-2, val_set_size=1000, COIs=[1, 3, 6, 17, 19, 23, 27, 28, 33, 40, 56, 63, 75, 76, 77, 78, 79, 88, 92, 97], M=tensor([27, 40, 28, 33, 79, 92,  6, 63, 88, 17, 97, 19, 23, 56, 75, 77,  3, 76,
        78,  1,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), Initial Performance: (0.517, 0.07212217003107071)
SUPER-DC 0, val_set_size=2000, COIs=[36, 90, 44, 64, 55, 32, 52, 66, 8, 73, 71, 84, 43, 45, 34, 87, 20, 4, 89, 94, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([36, 90, 44, 64, 55, 32, 52, 66,  8, 73, 71, 84, 43, 45, 34, 87, 20,  4,
        89, 94,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12, 5, 69, 53, 86, 54, 67, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([80, 14, 81, 22, 30, 60, 48, 38, 10, 39, 65, 59, 24, 12,  5, 69, 53, 86,
        54, 67,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[27, 40, 28, 33, 79, 92, 6, 63, 88, 17, 97, 19, 23, 56, 75, 77, 3, 76, 78, 1, 2, 25, 93, 98, 29, 50, 42, 0, 91, 16, 21, 13, 18, 15, 9, 83, 96, 47, 82, 7], M=tensor([27, 40, 28, 33, 79, 92,  6, 63, 88, 17, 97, 19, 23, 56, 75, 77,  3, 76,
        78,  1,  2, 25, 93, 98, 29, 50, 42,  0, 91, 16, 21, 13, 18, 15,  9, 83,
        96, 47, 82,  7], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.482, 0.09529244938492774) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.57, 0.07298976355791092) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.496, 0.09250729250907898) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.46266666666666667, 0.05748398328820865) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2355, 0.2055981173813343) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2785, 0.18754596295952797) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2605, 0.17473138755559922) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.468, 0.1023488684296608) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.559, 0.08492520099878312) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.09857550382614136) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.487, 0.0588433598279953) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.245, 0.1471187520623207) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3055, 0.1472313774973154) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.274, 0.14557588082551956) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.461, 0.11701870477199554) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.582, 0.07579302704334259) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.514, 0.10946490144729615) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49766666666666665, 0.06112120225032171) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.256, 0.1369932941198349) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2895, 0.137876234292984) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2745, 0.13847722744941712) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.467, 0.12685182285308838) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.579, 0.09626908615231514) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.528, 0.09586700850725174) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49033333333333334, 0.0638318982521693) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.251, 0.1356406309902668) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2935, 0.13541350734233856) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2925, 0.1259216719865799) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.473, 0.11780588054656982) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.566, 0.09156461066007614) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.508, 0.10635035562515259) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49133333333333334, 0.06794240222374598) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.271, 0.12050136747956276) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.307, 0.10706505592167377) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.301, 0.11619293731451034) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.474, 0.13088794887065888) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.566, 0.09206916999816894) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.506, 0.12034104692935943) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49866666666666665, 0.07443076860904693) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2705, 0.1160085627734661) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.316, 0.10806401120126248) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3065, 0.11475553891062737) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.46, 0.13588841438293456) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.57, 0.09631183874607087) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.517, 0.10772775220870971) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.492, 0.08252390774091085) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.272, 0.11287842482328415) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.32, 0.10235288408398628) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.326, 0.1006708619594574) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.46, 0.14129129338264465) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.572, 0.10577030944824219) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.5, 0.12535666781663896) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47733333333333333, 0.09611941895882288) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2995, 0.1052548959851265) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3205, 0.1021951001137495) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.323, 0.10412345951795578) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.462, 0.13671513628959656) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.58, 0.1038670450747013) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.508, 0.11170401412248611) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49266666666666664, 0.09655021814505259) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2885, 0.1032880156636238) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.321, 0.10195443458855152) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.318, 0.0994925121665001) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.456, 0.1514054423570633) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.564, 0.11325526458024979) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.12575803565979005) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49733333333333335, 0.10536588948965073) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2955, 0.10916080957651138) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3235, 0.10137389242649078) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3125, 0.10811030262708664) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.445, 0.1475852792263031) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.569, 0.10269226157665252) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.49, 0.13719660174846648) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48133333333333334, 0.12284583906332652) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.303, 0.1054950289130211) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3045, 0.10506650626659393) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.305, 0.11012195515632629) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.461, 0.1429649389386177) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.571, 0.11413949865102768) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.494, 0.13590139186382294) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.469, 0.1297874150077502) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2935, 0.11431733074784278) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.314, 0.10685486708581447) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.307, 0.11348593699932098) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.453, 0.1655931898355484) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.575, 0.10999002967774868) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.494, 0.13371708023548126) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.485, 0.12295686721801757) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3035, 0.1145729883313179) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3265, 0.10682209038734436) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3045, 0.11869784563779831) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.447, 0.17329463744163512) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.585, 0.12259104809165002) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.487, 0.14432360112667084) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48533333333333334, 0.1197013148466746) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3, 0.11430355966091156) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.337, 0.10554116100817919) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.303, 0.11168428432941437) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.451, 0.16113756942749025) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.576, 0.11379950059950351) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.493, 0.14397901910543442) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48333333333333334, 0.1251693328221639) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.299, 0.11810135847330093) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3135, 0.10970544588565827) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.295, 0.11782360780239105) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.46, 0.14367526078224183) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.54, 0.13046685618162154) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.15788573169708253) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.477, 0.13497015817960104) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.303, 0.11819045931100845) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3275, 0.10536131876707076) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3115, 0.11659401255846023) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.476, 0.1548589656352997) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.573, 0.11740545180439949) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.495, 0.16859959757328033) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.474, 0.1305300262371699) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2835, 0.11586962017416955) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3395, 0.10300986307859421) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.11797687077522277) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.491, 0.1658431943655014) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.576, 0.12078697374463081) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.512, 0.14806037449836731) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.45166666666666666, 0.13357432536284128) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2775, 0.12588822683691978) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.11138868993520737) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2855, 0.11816111373901367) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.459, 0.1571560000181198) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.591, 0.12295171928405761) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.14263860714435578) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4766666666666667, 0.12790758268038432) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.293, 0.11491313546895982) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.325, 0.11029220679402352) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.314, 0.11158628463745117) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.458, 0.1672907086610794) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.583, 0.12612702536582948) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.515, 0.15070635080337524) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4756666666666667, 0.13915869796276092) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.12829121845960617) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.1132708645761013) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.297, 0.12478659510612487) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.452, 0.16990934205055236) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.579, 0.12847234582901002) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.482, 0.1581992896795273) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.491, 0.14000230729579927) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2885, 0.12217155903577805) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3205, 0.1120641210079193) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2875, 0.12212382060289383) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.459, 0.1784645619392395) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.577, 0.12173221123218536) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.508, 0.15419770777225494) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4706666666666667, 0.16304594016075133) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2855, 0.12522156202793122) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3115, 0.12103703245520592) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3185, 0.12367651635408401) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.48, 0.17091417449712754) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.57, 0.12688566601276396) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.493, 0.16304859715700148) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47933333333333333, 0.15616216540336608) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2925, 0.12867466634511948) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3005, 0.12127307051420212) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2775, 0.12734784495830537) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.475, 0.17016802847385407) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.574, 0.1350366980433464) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.16633652377128602) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47833333333333333, 0.14216379966338474) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2805, 0.12706831759214401) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.314, 0.11602777105569839) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.298, 0.12665105760097503) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.472, 0.17156834471225738) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.568, 0.12991901040077208) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.503, 0.16542564749717711) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.479, 0.16345082708199818) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.297, 0.12440079855918884) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.313, 0.11662444591522217) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.1265532564520836) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.468, 0.1702804478406906) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.573, 0.12965755254030228) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.489, 0.17077123188972473) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.472, 0.14398190565903982) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.297, 0.1209442774951458) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3335, 0.11285697638988496) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3105, 0.12737749910354615) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.478, 0.16687378764152527) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.562, 0.13842232447862626) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.17962364888191223) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.482, 0.14596319377422332) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.302, 0.12336036133766175) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.302, 0.12150298309326171) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3005, 0.12734161907434463) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.473, 0.18234613853693007) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.585, 0.13045093655586243) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.499, 0.18087548363208772) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4713333333333333, 0.15687708707650502) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.278, 0.12750488969683646) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.321, 0.11806308954954148) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.13545333248376845) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.472, 0.19619151318073272) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.579, 0.13771067082881927) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.506, 0.17750694584846496) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48033333333333333, 0.16308405256271363) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.12797709593176843) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3, 0.12039885559678078) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.12692674148082733) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.469, 0.19469250357151033) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.567, 0.14088284355401992) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.509, 0.17543383169174195) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4796666666666667, 0.16340285070737204) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.27, 0.13416814923286438) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2865, 0.13014865273237228) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.301, 0.13255062264204026) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.486, 0.18670273160934447) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.571, 0.14410104924440384) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.503, 0.18340001726150512) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47433333333333333, 0.17069947111606598) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.291, 0.13281335031986236) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2885, 0.12813355684280395) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2965, 0.13424476182460784) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.473, 0.19374082839488982) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.577, 0.1325664923787117) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.17641553354263306) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47333333333333333, 0.15852827461560567) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2805, 0.13330346170067786) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.319, 0.12508996561914681) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.315, 0.12823168748617172) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.452, 0.19477567714452743) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.575, 0.15083777350187302) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.503, 0.1906568683385849) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.475, 0.17686437276999156) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2735, 0.13436212241649628) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.1335299938619137) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3065, 0.14014750242233276) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.469, 0.20980548667907714) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.587, 0.14114066874980927) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.18736124682426453) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48, 0.16247581889232) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2735, 0.13989526116847992) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3015, 0.13027017448842526) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.302, 0.13820288771390915) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.445, 0.21477699941396713) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.57, 0.14831192046403885) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.495, 0.1953421950340271) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4786666666666667, 0.17390959409872692) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2875, 0.13841173791885375) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.31, 0.128621647387743) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3125, 0.13084529462456704) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.456, 0.20225241315364836) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.566, 0.139826023876667) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.501, 0.18600175631046295) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.466, 0.1766742437283198) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2895, 0.13278175908327103) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3075, 0.13131863540410996) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3035, 0.13441597270965577) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.469, 0.20934775578975678) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.567, 0.14123717772960662) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.497, 0.1996988123655319) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4876666666666667, 0.15538573265075684) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2805, 0.14171937036514282) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.315, 0.13168346616625787) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.303, 0.13488249492645263) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.436, 0.2131832971572876) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.578, 0.142053724527359) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.51, 0.18501477217674256) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4703333333333333, 0.18987097267309824) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2825, 0.1388945976495743) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.31, 0.12981266787648202) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.283, 0.14386160147190094) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.464, 0.19112648385763167) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.589, 0.14433295050263406) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.505, 0.18671282505989076) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.469, 0.1690292237997055) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.2805, 0.14242929953336717) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3065, 0.12708864867687225) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.297, 0.1412202823162079) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.454, 0.2180122220516205) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.578, 0.1542776671051979) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.503, 0.1929753279685974) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.481, 0.17583062088489532) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.14070618230104445) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.3145, 0.1293789545893669) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.13799137258529662) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.0255, 0.11635315287113189), (0.1995, 0.14250404837727546), (0.225, 0.17866674886643888), (0.2395, 0.22812835544347762), (0.2435, 0.3198331189155579), (0.238, 0.36310096162557604), (0.243, 0.38475264793634417), (0.242, 0.359465590685606), (0.244, 0.3827433533966541), (0.244, 0.36117097902297973), (0.239, 0.3823912570476532), (0.2355, 0.2055981173813343), (0.245, 0.1471187520623207), (0.256, 0.1369932941198349), (0.251, 0.1356406309902668), (0.271, 0.12050136747956276), (0.2705, 0.1160085627734661), (0.272, 0.11287842482328415), (0.2995, 0.1052548959851265), (0.2885, 0.1032880156636238), (0.2955, 0.10916080957651138), (0.303, 0.1054950289130211), (0.2935, 0.11431733074784278), (0.3035, 0.1145729883313179), (0.3, 0.11430355966091156), (0.299, 0.11810135847330093), (0.303, 0.11819045931100845), (0.2835, 0.11586962017416955), (0.2775, 0.12588822683691978), (0.293, 0.11491313546895982), (0.275, 0.12829121845960617), (0.2885, 0.12217155903577805), (0.2855, 0.12522156202793122), (0.2925, 0.12867466634511948), (0.2805, 0.12706831759214401), (0.297, 0.12440079855918884), (0.297, 0.1209442774951458), (0.302, 0.12336036133766175), (0.278, 0.12750488969683646), (0.2895, 0.12797709593176843), (0.27, 0.13416814923286438), (0.291, 0.13281335031986236), (0.2805, 0.13330346170067786), (0.2735, 0.13436212241649628), (0.2735, 0.13989526116847992), (0.2875, 0.13841173791885375), (0.2895, 0.13278175908327103), (0.2805, 0.14171937036514282), (0.2825, 0.1388945976495743), (0.2805, 0.14242929953336717), (0.294, 0.14070618230104445)]
TEST: 
[(0.02125, 0.11544804280996322), (0.2145, 0.1397663295865059), (0.238, 0.17471699225902557), (0.24925, 0.22432150536775589), (0.24125, 0.3140524219274521), (0.24625, 0.3561435933113098), (0.24775, 0.3739503070116043), (0.24825, 0.35147336292266845), (0.24325, 0.3775367860794067), (0.2465, 0.35334183073043823), (0.2435, 0.36952728736400603), (0.24475, 0.1978753354549408), (0.2555, 0.141276780128479), (0.261, 0.13284917271137237), (0.2525, 0.13060888665914536), (0.27225, 0.11765493530035019), (0.26325, 0.11393409383296967), (0.2705, 0.1096401983499527), (0.30525, 0.10354331409931183), (0.29925, 0.10022136640548707), (0.302, 0.10754200702905654), (0.30325, 0.1026755501627922), (0.29275, 0.11071016579866409), (0.321, 0.10977773842215538), (0.29025, 0.1106755890250206), (0.30525, 0.11495908361673356), (0.29875, 0.11363168084621429), (0.29775, 0.11230008786916733), (0.2835, 0.12087922585010528), (0.2935, 0.11300212007761001), (0.281, 0.12355452972650528), (0.29225, 0.118956001162529), (0.29525, 0.12169266164302826), (0.29675, 0.1258468726873398), (0.29575, 0.12485003459453582), (0.30375, 0.12256108498573304), (0.29975, 0.11940204548835755), (0.2965, 0.12323145723342896), (0.29225, 0.12349934285879136), (0.2925, 0.12532928556203843), (0.284, 0.12909875869750975), (0.29775, 0.12972883254289627), (0.30325, 0.130028870344162), (0.28775, 0.13273488116264343), (0.28475, 0.13565457463264466), (0.28225, 0.136353120803833), (0.3015, 0.13271612054109574), (0.30075, 0.13738158684968949), (0.29275, 0.13494938069581985), (0.27425, 0.13844713693857194), (0.3025, 0.13569934475421905)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.62      0.56      0.59       100
           2       0.16      0.27      0.20       100
           4       0.17      0.11      0.13       100
           7       0.33      0.23      0.27       100
           8       0.25      0.57      0.34       100
           9       0.51      0.45      0.48       100
          13       0.25      0.47      0.32       100
          15       0.22      0.12      0.15       100
          16       0.24      0.41      0.30       100
          18       0.41      0.17      0.24       100
          20       0.55      0.64      0.59       100
          21       0.53      0.35      0.42       100
          25       0.25      0.17      0.20       100
          29       0.26      0.23      0.24       100
          32       0.19      0.19      0.19       100
          34       0.18      0.30      0.22       100
          36       0.40      0.23      0.29       100
          42       0.19      0.17      0.18       100
          43       0.43      0.24      0.31       100
          44       0.18      0.25      0.21       100
          45       0.11      0.27      0.16       100
          47       0.38      0.41      0.39       100
          50       0.17      0.23      0.19       100
          52       0.43      0.56      0.49       100
          55       0.25      0.06      0.10       100
          64       0.24      0.14      0.18       100
          66       0.28      0.35      0.31       100
          71       0.69      0.80      0.74       100
          73       0.51      0.45      0.48       100
          82       0.67      0.58      0.62       100
          83       0.42      0.10      0.16       100
          84       0.18      0.14      0.16       100
          87       0.50      0.18      0.26       100
          89       0.30      0.18      0.23       100
          90       0.21      0.15      0.18       100
          91       0.28      0.42      0.34       100
          93       0.13      0.16      0.15       100
          94       0.76      0.37      0.50       100
          96       0.33      0.22      0.27       100
          98       0.20      0.20      0.20       100

    accuracy                           0.30      4000
   macro avg       0.33      0.30      0.30      4000
weighted avg       0.33      0.30      0.30      4000

Collaboration_DC_1
VAL: 
[(0.0255, 0.1163812974691391), (0.254, 0.13570773777365686), (0.2905, 0.1741541069149971), (0.2935, 0.2231589137762785), (0.296, 0.28562465962022543), (0.2945, 0.3695896264081821), (0.2915, 0.3522641851827502), (0.296, 0.38746142720058563), (0.295, 0.41894165879115464), (0.2905, 0.39712453600019215), (0.2915, 0.38058441007882354), (0.2785, 0.18754596295952797), (0.3055, 0.1472313774973154), (0.2895, 0.137876234292984), (0.2935, 0.13541350734233856), (0.307, 0.10706505592167377), (0.316, 0.10806401120126248), (0.32, 0.10235288408398628), (0.3205, 0.1021951001137495), (0.321, 0.10195443458855152), (0.3235, 0.10137389242649078), (0.3045, 0.10506650626659393), (0.314, 0.10685486708581447), (0.3265, 0.10682209038734436), (0.337, 0.10554116100817919), (0.3135, 0.10970544588565827), (0.3275, 0.10536131876707076), (0.3395, 0.10300986307859421), (0.323, 0.11138868993520737), (0.325, 0.11029220679402352), (0.3165, 0.1132708645761013), (0.3205, 0.1120641210079193), (0.3115, 0.12103703245520592), (0.3005, 0.12127307051420212), (0.314, 0.11602777105569839), (0.313, 0.11662444591522217), (0.3335, 0.11285697638988496), (0.302, 0.12150298309326171), (0.321, 0.11806308954954148), (0.3, 0.12039885559678078), (0.2865, 0.13014865273237228), (0.2885, 0.12813355684280395), (0.319, 0.12508996561914681), (0.312, 0.1335299938619137), (0.3015, 0.13027017448842526), (0.31, 0.128621647387743), (0.3075, 0.13131863540410996), (0.315, 0.13168346616625787), (0.31, 0.12981266787648202), (0.3065, 0.12708864867687225), (0.3145, 0.1293789545893669)]
TEST: 
[(0.02525, 0.11549618828296661), (0.2415, 0.13498873233795167), (0.27275, 0.17425446557998658), (0.2825, 0.2225978115797043), (0.28375, 0.2840835298299789), (0.2805, 0.37261486911773684), (0.28675, 0.3535348227024078), (0.279, 0.3923811281919479), (0.283, 0.4206191577911377), (0.27975, 0.4017773680686951), (0.281, 0.3906874883174896), (0.25975, 0.1895625501871109), (0.28225, 0.1492175484895706), (0.2755, 0.14091529673337935), (0.276, 0.13645269459486006), (0.302, 0.1077735197544098), (0.2935, 0.1094623801112175), (0.30125, 0.10451980447769164), (0.3075, 0.10294171750545501), (0.3075, 0.10365690034627914), (0.30575, 0.10478274816274644), (0.3065, 0.10771805697679519), (0.314, 0.10843935018777848), (0.32275, 0.10581808406114578), (0.33, 0.10579013073444367), (0.3155, 0.11105840021371842), (0.31925, 0.1067848590016365), (0.33275, 0.10428575783967972), (0.31175, 0.11308684104681015), (0.30775, 0.11146526265144348), (0.3205, 0.11367084592580795), (0.32175, 0.11276793158054352), (0.31575, 0.11994895142316818), (0.3095, 0.11824146747589111), (0.31525, 0.11674732905626296), (0.323, 0.11607183295488357), (0.3035, 0.11612471503019332), (0.29625, 0.12285062563419342), (0.32025, 0.11973671215772629), (0.3015, 0.12331670665740967), (0.2955, 0.1325931752920151), (0.31, 0.12845355314016343), (0.31975, 0.12668646019697188), (0.31925, 0.13443081498146056), (0.30925, 0.13373153209686278), (0.3135, 0.12956701785326005), (0.31975, 0.13143726658821106), (0.3045, 0.13316015642881393), (0.305, 0.13299601924419402), (0.31375, 0.12838761180639266), (0.31, 0.1327248255610466)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.34      0.17      0.23       100
           2       0.14      0.21      0.17       100
           5       0.26      0.21      0.23       100
           7       0.25      0.25      0.25       100
           9       0.37      0.32      0.34       100
          10       0.21      0.27      0.23       100
          12       0.32      0.34      0.33       100
          13       0.25      0.40      0.31       100
          14       0.28      0.16      0.20       100
          15       0.31      0.17      0.22       100
          16       0.28      0.22      0.24       100
          18       0.25      0.33      0.28       100
          21       0.40      0.49      0.44       100
          22       0.38      0.26      0.31       100
          24       0.47      0.45      0.46       100
          25       0.30      0.19      0.23       100
          29       0.36      0.38      0.37       100
          30       0.57      0.39      0.46       100
          38       0.10      0.05      0.07       100
          39       0.46      0.22      0.30       100
          42       0.14      0.19      0.16       100
          47       0.36      0.39      0.37       100
          48       0.44      0.51      0.47       100
          50       0.17      0.24      0.20       100
          53       0.39      0.74      0.51       100
          54       0.28      0.48      0.35       100
          59       0.23      0.32      0.27       100
          60       0.72      0.84      0.78       100
          65       0.17      0.15      0.16       100
          67       0.41      0.32      0.36       100
          69       0.43      0.57      0.49       100
          80       0.09      0.07      0.08       100
          81       0.31      0.18      0.23       100
          82       0.58      0.33      0.42       100
          83       0.25      0.11      0.15       100
          86       0.33      0.32      0.32       100
          91       0.29      0.48      0.37       100
          93       0.24      0.24      0.24       100
          96       0.30      0.29      0.29       100
          98       0.21      0.15      0.17       100

    accuracy                           0.31      4000
   macro avg       0.32      0.31      0.30      4000
weighted avg       0.32      0.31      0.30      4000

Collaboration_DC_2
VAL: 
[(0.0245, 0.11634207487106324), (0.215, 0.14347445401549339), (0.252, 0.18216418340802193), (0.257, 0.23195096601545812), (0.273, 0.3106925796866417), (0.268, 0.34243845027685166), (0.2665, 0.3503003849387169), (0.2645, 0.37890497291088104), (0.2655, 0.3658933582901955), (0.267, 0.40693069669604304), (0.2585, 0.411374171346426), (0.2605, 0.17473138755559922), (0.274, 0.14557588082551956), (0.2745, 0.13847722744941712), (0.2925, 0.1259216719865799), (0.301, 0.11619293731451034), (0.3065, 0.11475553891062737), (0.326, 0.1006708619594574), (0.323, 0.10412345951795578), (0.318, 0.0994925121665001), (0.3125, 0.10811030262708664), (0.305, 0.11012195515632629), (0.307, 0.11348593699932098), (0.3045, 0.11869784563779831), (0.303, 0.11168428432941437), (0.295, 0.11782360780239105), (0.3115, 0.11659401255846023), (0.2985, 0.11797687077522277), (0.2855, 0.11816111373901367), (0.314, 0.11158628463745117), (0.297, 0.12478659510612487), (0.2875, 0.12212382060289383), (0.3185, 0.12367651635408401), (0.2775, 0.12734784495830537), (0.298, 0.12665105760097503), (0.308, 0.1265532564520836), (0.3105, 0.12737749910354615), (0.3005, 0.12734161907434463), (0.2985, 0.13545333248376845), (0.308, 0.12692674148082733), (0.301, 0.13255062264204026), (0.2965, 0.13424476182460784), (0.315, 0.12823168748617172), (0.3065, 0.14014750242233276), (0.302, 0.13820288771390915), (0.3125, 0.13084529462456704), (0.3035, 0.13441597270965577), (0.303, 0.13488249492645263), (0.283, 0.14386160147190094), (0.297, 0.1412202823162079), (0.2985, 0.13799137258529662)]
TEST: 
[(0.0265, 0.11540408164262772), (0.22025, 0.14138810914754868), (0.25075, 0.17935872280597687), (0.263, 0.2276571819782257), (0.26825, 0.3023426102399826), (0.27225, 0.33617787158489226), (0.271, 0.3464322115182877), (0.27075, 0.3731597831249237), (0.271, 0.3592612317800522), (0.2705, 0.39709512364864347), (0.2645, 0.4062535195350647), (0.2695, 0.17426073306798934), (0.276, 0.1435806338787079), (0.27125, 0.13640189415216447), (0.29725, 0.12637732577323912), (0.29525, 0.1151840054988861), (0.298, 0.11204904133081436), (0.3205, 0.09885268813371659), (0.31775, 0.1003908980190754), (0.31675, 0.09605774557590485), (0.32275, 0.10517394483089447), (0.29725, 0.10716133946180344), (0.30275, 0.11323535573482514), (0.303, 0.11704249912500382), (0.3135, 0.11005598092079162), (0.29025, 0.11405602711439132), (0.30475, 0.11178877139091492), (0.298, 0.11340390479564667), (0.2935, 0.11458363789319992), (0.312, 0.1078479066491127), (0.3015, 0.1193114914894104), (0.30125, 0.11753687256574631), (0.312, 0.11994077432155609), (0.29175, 0.12215153080224991), (0.28925, 0.12352257913351058), (0.31325, 0.1216668911576271), (0.2975, 0.1232661782503128), (0.3095, 0.12176035940647126), (0.30875, 0.12953533947467805), (0.307, 0.12188542199134826), (0.29875, 0.1266667701601982), (0.2895, 0.12920422607660292), (0.31525, 0.12442041754722595), (0.2975, 0.13501163589954376), (0.28925, 0.13535701155662536), (0.311, 0.12496758806705476), (0.3095, 0.12807861053943634), (0.3185, 0.12774541628360747), (0.28525, 0.13685896342992782), (0.295, 0.13398536038398742), (0.3005, 0.13351029294729233)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.44      0.21      0.28       100
           1       0.36      0.48      0.41       100
           2       0.19      0.20      0.19       100
           3       0.20      0.17      0.18       100
           6       0.20      0.27      0.23       100
           7       0.27      0.30      0.28       100
           9       0.35      0.37      0.36       100
          13       0.33      0.51      0.40       100
          15       0.19      0.12      0.15       100
          16       0.28      0.32      0.30       100
          17       0.50      0.63      0.56       100
          18       0.19      0.31      0.23       100
          19       0.26      0.17      0.20       100
          21       0.42      0.36      0.39       100
          23       0.60      0.56      0.58       100
          25       0.24      0.12      0.16       100
          27       0.30      0.29      0.29       100
          28       0.54      0.37      0.44       100
          29       0.25      0.28      0.26       100
          33       0.32      0.39      0.35       100
          40       0.25      0.17      0.20       100
          42       0.19      0.35      0.25       100
          47       0.36      0.48      0.41       100
          50       0.13      0.19      0.16       100
          56       0.33      0.32      0.32       100
          63       0.34      0.44      0.38       100
          75       0.45      0.54      0.49       100
          76       0.62      0.56      0.59       100
          77       0.19      0.13      0.15       100
          78       0.30      0.17      0.22       100
          79       0.32      0.12      0.17       100
          82       0.58      0.34      0.43       100
          83       0.20      0.04      0.07       100
          88       0.17      0.12      0.14       100
          91       0.26      0.36      0.30       100
          92       0.24      0.54      0.33       100
          93       0.28      0.25      0.26       100
          96       0.23      0.15      0.18       100
          97       0.22      0.17      0.19       100
          98       0.19      0.15      0.17       100

    accuracy                           0.30      4000
   macro avg       0.31      0.30      0.29      4000
weighted avg       0.31      0.30      0.29      4000

do_assignment: None
seeds: [145]
name: naive-cifar100-feddf145
score_metric: contrloss
aggregation: <function fed_df at 0x7c734d65ae50>
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 10
batch_size: 32
dataset: cifar100
num_classes_in_dataset: 100
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 2000
partitioning: ((20, 6), [(20, 6), (20, 6), (20, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=145
Partitioning data
No_Competition
DC 0, val_set_size=2000, COIs=[97, 48, 69, 8, 42, 43, 39, 62, 28, 49, 1, 79, 76, 6, 27, 9, 31, 4, 34, 53, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([97, 48, 69,  8, 42, 43, 39, 62, 28, 49,  1, 79, 76,  6, 27,  9, 31,  4,
        34, 53, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.025, 0.11637692308425904)
DC 1, val_set_size=2000, COIs=[67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90, 2, 18, 44, 30, 85, 24, 89, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90,  2, 18, 44, 30, 85,
        24, 89, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.0285, 0.11684445023536683)
DC 2, val_set_size=2000, COIs=[29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70, 20, 52, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70,
        20, 52, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.025, 0.11637047803401947)
D00: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D01: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D02: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D03: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D04: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D05: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D06: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D07: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D08: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D09: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D010: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D011: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D012: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D013: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D014: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D015: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D016: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D017: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D018: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D019: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D020: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D021: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D022: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D023: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
FL ROUND 1
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.3, 0.0823607651591301) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2625, 0.08543058878183366) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.319, 0.07959043988585472) --> New model used: True
FL ROUND 2
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3485, 0.07350572538375855) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.08047096493840218) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3905, 0.07002512681484223) --> New model used: True
FL ROUND 3
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3865, 0.06920615586638451) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3355, 0.07648416707664728) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4215, 0.06624932512640953) --> New model used: True
FL ROUND 4
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3885, 0.07006565856933594) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3295, 0.08013388013839721) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4205, 0.06638672140240669) --> New model used: True
FL ROUND 5
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.391, 0.07076251152157784) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.323, 0.08545805021375417) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.06746244126558304) --> New model used: True
FL ROUND 6
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.393, 0.07367354980111122) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.327, 0.09017448663711548) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.06880825689435005) --> New model used: True
FL ROUND 7
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4075, 0.07651882898807526) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.322, 0.09171110028028488) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4235, 0.06976037520170211) --> New model used: True
FL ROUND 8
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.0802728660106659) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3225, 0.0969473537504673) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4165, 0.07399530482292176) --> New model used: True
FL ROUND 9
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.395, 0.08503337007761001) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3295, 0.10395346869528294) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4195, 0.0769789312183857) --> New model used: True
FL ROUND 10
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3985, 0.09037243127822876) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3305, 0.10500100679695606) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.07822612127661704) --> New model used: True
FL ROUND 11
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3935, 0.09487825953960419) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3215, 0.11333060623705388) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.08563040739297867) --> New model used: True
FL ROUND 12
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3655, 0.11437488520145417) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.333, 0.12225823067873716) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4225, 0.09480781546235084) --> New model used: True
FL ROUND 13
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.382, 0.11567729645967484) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3145, 0.12733187480270863) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.0978297035396099) --> New model used: True
FL ROUND 14
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3905, 0.1163041439652443) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3135, 0.14189933579415082) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3955, 0.12018565374612808) --> New model used: True
FL ROUND 15
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.1385107060074806) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.15984395156800746) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4075, 0.11298611366748809) --> New model used: True
FL ROUND 16
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3725, 0.1324278346300125) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.15446773805096745) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4185, 0.11910756275057793) --> New model used: True
FL ROUND 17
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.376, 0.13223914778232573) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.319, 0.15888788056373596) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4, 0.1254558355808258) --> New model used: True
FL ROUND 18
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.14131734895706177) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3175, 0.18011247800290583) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.384, 0.1525318244099617) --> New model used: True
FL ROUND 19
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3685, 0.15767463833093642) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3035, 0.17473773302137852) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4055, 0.13695913726091385) --> New model used: True
FL ROUND 20
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3695, 0.15823966246843338) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.291, 0.1934114496409893) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.389, 0.14696310794353484) --> New model used: True
FL ROUND 21
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3785, 0.16343804347515106) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.316, 0.17392288175970316) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4085, 0.14518183010816574) --> New model used: True
FL ROUND 22
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.372, 0.15975519722700118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.17828138250112532) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3995, 0.1522122699022293) --> New model used: True
FL ROUND 23
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.368, 0.16754066145420074) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3065, 0.19228179074823856) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3885, 0.16010743510723113) --> New model used: True
FL ROUND 24
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3855, 0.16976991552114487) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.285, 0.20573783145844937) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.391, 0.15534955960512162) --> New model used: True
FL ROUND 25
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.382, 0.17874280977249146) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.1905995259359479) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.398, 0.17384004390239716) --> New model used: True
FL ROUND 26
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.376, 0.18246756470203399) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2935, 0.1938469368070364) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4055, 0.1579597328901291) --> New model used: True
FL ROUND 27
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3775, 0.18217136257886887) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3095, 0.19899388429522513) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.17423603534698487) --> New model used: True
FL ROUND 28
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3805, 0.174257275223732) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.297, 0.2051363346874714) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3865, 0.18215013384819032) --> New model used: True
FL ROUND 29
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.361, 0.17480064553022384) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3065, 0.2095097936987877) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3925, 0.16750063973665238) --> New model used: True
FL ROUND 30
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3755, 0.18179587310552597) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3125, 0.21360191106796264) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.18096669125556947) --> New model used: True
FL ROUND 31
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.37, 0.1878995567560196) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.22222481782734393) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3895, 0.18402249866724013) --> New model used: True
FL ROUND 32
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.347, 0.18470937061309814) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.305, 0.21144683128315955) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4095, 0.1807955300807953) --> New model used: True
FL ROUND 33
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3855, 0.19431840133666992) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.23353605853021145) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3965, 0.18215879464149476) --> New model used: True
FL ROUND 34
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.352, 0.19781934273242952) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.312, 0.2267703079096973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.1933765608072281) --> New model used: True
FL ROUND 35
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3615, 0.20207160949707031) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2895, 0.23227894258499146) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.1837411172389984) --> New model used: True
FL ROUND 36
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3575, 0.19144817912578582) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.2217983788251877) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3985, 0.188357483625412) --> New model used: True
FL ROUND 37
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3655, 0.19543443167209626) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3115, 0.21584716978669166) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3985, 0.19538503682613373) --> New model used: True
FL ROUND 38
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.365, 0.20681799578666688) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2915, 0.23822809132933617) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3865, 0.21474194234609603) --> New model used: True
FL ROUND 39
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3795, 0.20355310130119325) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.313, 0.23223944960534573) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3865, 0.19646559983491899) --> New model used: True
FL ROUND 40
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.361, 0.20964790993928908) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3165, 0.23501370258629323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3865, 0.19918439984321595) --> New model used: True
FL ROUND 41
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.21581750130653382) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3015, 0.25077084615826606) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.389, 0.20850533613562583) --> New model used: True
FL ROUND 42
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.21267766797542573) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.291, 0.25991169223189353) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.200112651437521) --> New model used: True
FL ROUND 43
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3595, 0.216555779337883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3085, 0.24564932537078857) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.394, 0.1942971009016037) --> New model used: True
FL ROUND 44
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.364, 0.21699061393737792) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.306, 0.2451301762163639) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3985, 0.20707674366235734) --> New model used: True
FL ROUND 45
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.37, 0.2192678579688072) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.314, 0.24080487996339797) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.208553078353405) --> New model used: True
FL ROUND 46
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3755, 0.23630684781074524) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.307, 0.2512239540219307) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.20533084213733674) --> New model used: True
FL ROUND 47
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.364, 0.23905653405189514) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3005, 0.26197882974147796) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3875, 0.2249939466714859) --> New model used: True
FL ROUND 48
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.367, 0.23642169904708862) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.3055, 0.26160948699712755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3935, 0.21485923337936402) --> New model used: True
FL ROUND 49
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.3735, 0.2158826552629471) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.316, 0.24497224777936935) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.4115, 0.21693010979890823) --> New model used: True
FL ROUND 50
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.371, 0.22463644778728484) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.308, 0.24201523652672768) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.22455951875448227) --> New model used: True
Results: No_Competition
No_Competition_DC_0
VAL: 
[(0.025, 0.11637692308425904), (0.3, 0.0823607651591301), (0.3485, 0.07350572538375855), (0.3865, 0.06920615586638451), (0.3885, 0.07006565856933594), (0.391, 0.07076251152157784), (0.393, 0.07367354980111122), (0.4075, 0.07651882898807526), (0.398, 0.0802728660106659), (0.395, 0.08503337007761001), (0.3985, 0.09037243127822876), (0.3935, 0.09487825953960419), (0.3655, 0.11437488520145417), (0.382, 0.11567729645967484), (0.3905, 0.1163041439652443), (0.3695, 0.1385107060074806), (0.3725, 0.1324278346300125), (0.376, 0.13223914778232573), (0.362, 0.14131734895706177), (0.3685, 0.15767463833093642), (0.3695, 0.15823966246843338), (0.3785, 0.16343804347515106), (0.372, 0.15975519722700118), (0.368, 0.16754066145420074), (0.3855, 0.16976991552114487), (0.382, 0.17874280977249146), (0.376, 0.18246756470203399), (0.3775, 0.18217136257886887), (0.3805, 0.174257275223732), (0.361, 0.17480064553022384), (0.3755, 0.18179587310552597), (0.37, 0.1878995567560196), (0.347, 0.18470937061309814), (0.3855, 0.19431840133666992), (0.352, 0.19781934273242952), (0.3615, 0.20207160949707031), (0.3575, 0.19144817912578582), (0.3655, 0.19543443167209626), (0.365, 0.20681799578666688), (0.3795, 0.20355310130119325), (0.361, 0.20964790993928908), (0.354, 0.21581750130653382), (0.362, 0.21267766797542573), (0.3595, 0.216555779337883), (0.364, 0.21699061393737792), (0.37, 0.2192678579688072), (0.3755, 0.23630684781074524), (0.364, 0.23905653405189514), (0.367, 0.23642169904708862), (0.3735, 0.2158826552629471), (0.371, 0.22463644778728484)]
TEST: 
[(0.02475, 0.11546592718362808), (0.314, 0.08107728695869446), (0.356, 0.07214161503314973), (0.398, 0.06779683685302734), (0.394, 0.0687542944252491), (0.39525, 0.06980948227643967), (0.39825, 0.07258443385362626), (0.4095, 0.07460131376981735), (0.40025, 0.07846478924155235), (0.4025, 0.08276558113098144), (0.397, 0.08736623880267143), (0.3955, 0.09303065130114556), (0.3755, 0.11015421015024185), (0.37875, 0.11018655645847321), (0.39875, 0.10920623135566711), (0.36225, 0.13322221344709395), (0.37675, 0.12745835208892822), (0.385, 0.1284706448316574), (0.37825, 0.13714942276477815), (0.37325, 0.15253272068500517), (0.3705, 0.1527293717265129), (0.3695, 0.16320340436697006), (0.36925, 0.15775412952899934), (0.37075, 0.16339329421520232), (0.3815, 0.16717607599496842), (0.3695, 0.1701093270778656), (0.37475, 0.17622570997476578), (0.36975, 0.18316628676652907), (0.38, 0.17339052098989488), (0.36425, 0.17247975188493728), (0.37575, 0.17672204512357712), (0.37925, 0.182374111533165), (0.369, 0.1811453902721405), (0.364, 0.19024457597732544), (0.364, 0.1932166002392769), (0.36425, 0.19393452686071397), (0.3635, 0.18218039107322692), (0.37125, 0.19152707892656326), (0.3765, 0.20004387736320495), (0.37375, 0.19838162845373153), (0.36825, 0.20543273317813873), (0.3635, 0.20634353756904603), (0.36025, 0.2037806929945946), (0.373, 0.2088214756846428), (0.36375, 0.21289494204521178), (0.36075, 0.21228862833976744), (0.37825, 0.22168281751871108), (0.3735, 0.22690926098823547), (0.3705, 0.23337568414211274), (0.37775, 0.20468354451656343), (0.37425, 0.21628327625989913)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.68      0.57      0.62       100
           1       0.47      0.40      0.43       100
           3       0.21      0.30      0.25       100
           4       0.25      0.19      0.22       100
           6       0.28      0.30      0.29       100
           7       0.43      0.31      0.36       100
           8       0.42      0.30      0.35       100
           9       0.59      0.44      0.51       100
          14       0.29      0.20      0.24       100
          23       0.47      0.55      0.51       100
          27       0.34      0.28      0.31       100
          28       0.62      0.50      0.55       100
          31       0.40      0.36      0.38       100
          33       0.37      0.41      0.39       100
          34       0.19      0.15      0.17       100
          35       0.25      0.32      0.28       100
          39       0.47      0.44      0.46       100
          41       0.62      0.56      0.59       100
          42       0.28      0.25      0.27       100
          43       0.30      0.09      0.14       100
          48       0.54      0.50      0.52       100
          49       0.46      0.46      0.46       100
          53       0.48      0.45      0.47       100
          54       0.29      0.48      0.36       100
          56       0.53      0.38      0.44       100
          60       0.70      0.77      0.73       100
          62       0.33      0.47      0.39       100
          65       0.22      0.27      0.24       100
          66       0.21      0.31      0.25       100
          69       0.58      0.51      0.54       100
          72       0.17      0.18      0.18       100
          76       0.65      0.60      0.62       100
          79       0.32      0.29      0.31       100
          83       0.35      0.30      0.32       100
          84       0.23      0.29      0.26       100
          87       0.32      0.47      0.38       100
          92       0.17      0.20      0.19       100
          93       0.21      0.31      0.25       100
          96       0.48      0.48      0.48       100
          97       0.39      0.33      0.36       100

    accuracy                           0.37      4000
   macro avg       0.39      0.37      0.38      4000
weighted avg       0.39      0.37      0.38      4000

No_Competition_DC_1
VAL: 
[(0.0285, 0.11684445023536683), (0.2625, 0.08543058878183366), (0.301, 0.08047096493840218), (0.3355, 0.07648416707664728), (0.3295, 0.08013388013839721), (0.323, 0.08545805021375417), (0.327, 0.09017448663711548), (0.322, 0.09171110028028488), (0.3225, 0.0969473537504673), (0.3295, 0.10395346869528294), (0.3305, 0.10500100679695606), (0.3215, 0.11333060623705388), (0.333, 0.12225823067873716), (0.3145, 0.12733187480270863), (0.3135, 0.14189933579415082), (0.3165, 0.15984395156800746), (0.312, 0.15446773805096745), (0.319, 0.15888788056373596), (0.3175, 0.18011247800290583), (0.3035, 0.17473773302137852), (0.291, 0.1934114496409893), (0.316, 0.17392288175970316), (0.312, 0.17828138250112532), (0.3065, 0.19228179074823856), (0.285, 0.20573783145844937), (0.304, 0.1905995259359479), (0.2935, 0.1938469368070364), (0.3095, 0.19899388429522513), (0.297, 0.2051363346874714), (0.3065, 0.2095097936987877), (0.3125, 0.21360191106796264), (0.304, 0.22222481782734393), (0.305, 0.21144683128315955), (0.304, 0.23353605853021145), (0.312, 0.2267703079096973), (0.2895, 0.23227894258499146), (0.304, 0.2217983788251877), (0.3115, 0.21584716978669166), (0.2915, 0.23822809132933617), (0.313, 0.23223944960534573), (0.3165, 0.23501370258629323), (0.3015, 0.25077084615826606), (0.291, 0.25991169223189353), (0.3085, 0.24564932537078857), (0.306, 0.2451301762163639), (0.314, 0.24080487996339797), (0.307, 0.2512239540219307), (0.3005, 0.26197882974147796), (0.3055, 0.26160948699712755), (0.316, 0.24497224777936935), (0.308, 0.24201523652672768)]
TEST: 
[(0.029, 0.11603228694200515), (0.26375, 0.08392066782712937), (0.31075, 0.07807755503058433), (0.3385, 0.0750626861155033), (0.3335, 0.07781035748124122), (0.32925, 0.08292011690139771), (0.3355, 0.08746522998809815), (0.3295, 0.0893229615688324), (0.33325, 0.09495107629895211), (0.3225, 0.10177459686994553), (0.3185, 0.10399209779500962), (0.32625, 0.11106787842512131), (0.3235, 0.11821491926908494), (0.32375, 0.1225012674331665), (0.322, 0.13802636843919755), (0.3125, 0.15696040672063827), (0.32075, 0.14592962962388992), (0.311, 0.1540199793577194), (0.3095, 0.1766867269873619), (0.30775, 0.16828655099868775), (0.2965, 0.1849988529086113), (0.312, 0.1707121486067772), (0.3055, 0.17588528430461883), (0.31425, 0.18788467371463777), (0.297, 0.20200711369514465), (0.31875, 0.18871689295768737), (0.3165, 0.19244594955444336), (0.32075, 0.19394278663396836), (0.31425, 0.20001272428035735), (0.29975, 0.21010470402240752), (0.31125, 0.21225725227594375), (0.3155, 0.21845696073770524), (0.30925, 0.21143266570568084), (0.30025, 0.2308641185760498), (0.3125, 0.22410890352725982), (0.301, 0.22645622515678407), (0.3125, 0.22288933330774308), (0.31475, 0.2176279057264328), (0.29975, 0.24343733346462249), (0.316, 0.22939289420843123), (0.3145, 0.23648284006118775), (0.309, 0.24639899361133574), (0.30775, 0.2533489017486572), (0.32025, 0.24395285952091217), (0.29925, 0.2451122773885727), (0.31425, 0.24244968616962434), (0.3115, 0.24766233479976654), (0.3105, 0.2554905463457108), (0.31175, 0.25377352321147917), (0.311, 0.24820002508163452), (0.30525, 0.24317135989665986)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.54      0.29      0.38       100
           2       0.21      0.25      0.23       100
           3       0.20      0.18      0.19       100
           7       0.36      0.38      0.37       100
          14       0.23      0.21      0.22       100
          18       0.27      0.28      0.28       100
          23       0.57      0.67      0.61       100
          24       0.50      0.61      0.55       100
          30       0.45      0.36      0.40       100
          32       0.28      0.23      0.25       100
          33       0.45      0.34      0.39       100
          35       0.18      0.17      0.18       100
          38       0.20      0.24      0.22       100
          41       0.70      0.44      0.54       100
          44       0.22      0.17      0.19       100
          45       0.14      0.23      0.18       100
          47       0.41      0.47      0.44       100
          51       0.28      0.33      0.30       100
          54       0.52      0.39      0.45       100
          56       0.47      0.37      0.42       100
          59       0.29      0.30      0.30       100
          60       0.80      0.72      0.76       100
          65       0.16      0.18      0.17       100
          66       0.21      0.29      0.24       100
          67       0.40      0.19      0.26       100
          72       0.10      0.17      0.13       100
          73       0.34      0.32      0.33       100
          74       0.22      0.25      0.23       100
          78       0.28      0.24      0.26       100
          80       0.12      0.15      0.13       100
          83       0.24      0.13      0.17       100
          84       0.20      0.26      0.22       100
          85       0.45      0.49      0.47       100
          87       0.41      0.43      0.42       100
          89       0.38      0.41      0.40       100
          90       0.26      0.25      0.26       100
          92       0.32      0.22      0.26       100
          93       0.20      0.21      0.21       100
          96       0.25      0.21      0.23       100
          98       0.15      0.18      0.16       100

    accuracy                           0.31      4000
   macro avg       0.32      0.31      0.31      4000
weighted avg       0.32      0.31      0.31      4000

No_Competition_DC_2
VAL: 
[(0.025, 0.11637047803401947), (0.319, 0.07959043988585472), (0.3905, 0.07002512681484223), (0.4215, 0.06624932512640953), (0.4205, 0.06638672140240669), (0.424, 0.06746244126558304), (0.422, 0.06880825689435005), (0.4235, 0.06976037520170211), (0.4165, 0.07399530482292176), (0.4195, 0.0769789312183857), (0.423, 0.07822612127661704), (0.418, 0.08563040739297867), (0.4225, 0.09480781546235084), (0.415, 0.0978297035396099), (0.3955, 0.12018565374612808), (0.4075, 0.11298611366748809), (0.4185, 0.11910756275057793), (0.4, 0.1254558355808258), (0.384, 0.1525318244099617), (0.4055, 0.13695913726091385), (0.389, 0.14696310794353484), (0.4085, 0.14518183010816574), (0.3995, 0.1522122699022293), (0.3885, 0.16010743510723113), (0.391, 0.15534955960512162), (0.398, 0.17384004390239716), (0.4055, 0.1579597328901291), (0.379, 0.17423603534698487), (0.3865, 0.18215013384819032), (0.3925, 0.16750063973665238), (0.39, 0.18096669125556947), (0.3895, 0.18402249866724013), (0.4095, 0.1807955300807953), (0.3965, 0.18215879464149476), (0.387, 0.1933765608072281), (0.395, 0.1837411172389984), (0.3985, 0.188357483625412), (0.3985, 0.19538503682613373), (0.3865, 0.21474194234609603), (0.3865, 0.19646559983491899), (0.3865, 0.19918439984321595), (0.389, 0.20850533613562583), (0.404, 0.200112651437521), (0.394, 0.1942971009016037), (0.3985, 0.20707674366235734), (0.397, 0.208553078353405), (0.403, 0.20533084213733674), (0.3875, 0.2249939466714859), (0.3935, 0.21485923337936402), (0.4115, 0.21693010979890823), (0.404, 0.22455951875448227)]
TEST: 
[(0.025, 0.11544345253705979), (0.32175, 0.07941186809539795), (0.3685, 0.07031229373812675), (0.403, 0.06695407658815383), (0.4055, 0.06666015133261681), (0.40925, 0.06791718977689742), (0.41275, 0.0692356576025486), (0.41225, 0.07007172617316246), (0.4195, 0.07366185930371284), (0.40975, 0.07744685626029968), (0.4195, 0.07876184543967248), (0.41475, 0.08620486488938331), (0.412, 0.09354234755039215), (0.41825, 0.09760739430785179), (0.39075, 0.12071257340908051), (0.3995, 0.1141322589814663), (0.4125, 0.12089840665459634), (0.404, 0.12597140163183213), (0.38775, 0.15223058193922043), (0.40525, 0.13828286343812943), (0.38975, 0.15030903530120848), (0.39875, 0.149600633084774), (0.39525, 0.15458497583866118), (0.38825, 0.16431110173463823), (0.3915, 0.1557841911315918), (0.4005, 0.1741053593158722), (0.39075, 0.15978347074985505), (0.39, 0.16949609291553497), (0.39675, 0.18114700931310654), (0.3975, 0.1710084863305092), (0.386, 0.18260297417640686), (0.3865, 0.18792297142744063), (0.39975, 0.18261851954460145), (0.3885, 0.18786999547481537), (0.3855, 0.19226899510622025), (0.38925, 0.1858566358089447), (0.40025, 0.1879426304101944), (0.39075, 0.1968249441385269), (0.392, 0.21196080076694487), (0.38275, 0.19717557913064956), (0.37525, 0.20155658042430877), (0.38825, 0.20509630757570266), (0.3905, 0.2030131493806839), (0.394, 0.1968210598230362), (0.4, 0.20616918534040451), (0.39375, 0.20753299695253372), (0.39475, 0.20460395604372025), (0.397, 0.22415231972932814), (0.3955, 0.21409869056940078), (0.38675, 0.22139318531751634), (0.398, 0.22420857763290405)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.64      0.58      0.61       100
           3       0.27      0.30      0.28       100
           7       0.33      0.42      0.37       100
          11       0.20      0.16      0.18       100
          14       0.46      0.23      0.31       100
          17       0.49      0.45      0.47       100
          20       0.69      0.57      0.62       100
          23       0.56      0.58      0.57       100
          26       0.26      0.27      0.26       100
          29       0.40      0.28      0.33       100
          33       0.42      0.38      0.40       100
          35       0.29      0.26      0.27       100
          37       0.25      0.31      0.27       100
          41       0.64      0.54      0.59       100
          46       0.28      0.30      0.29       100
          50       0.25      0.18      0.21       100
          52       0.63      0.67      0.65       100
          54       0.34      0.42      0.38       100
          56       0.41      0.56      0.48       100
          60       0.71      0.79      0.75       100
          61       0.59      0.61      0.60       100
          63       0.43      0.46      0.44       100
          65       0.20      0.16      0.18       100
          66       0.31      0.24      0.27       100
          68       0.59      0.67      0.63       100
          70       0.27      0.33      0.30       100
          72       0.18      0.19      0.18       100
          77       0.22      0.19      0.20       100
          82       0.61      0.69      0.65       100
          83       0.30      0.27      0.28       100
          84       0.25      0.20      0.22       100
          86       0.43      0.46      0.44       100
          87       0.34      0.42      0.38       100
          91       0.44      0.53      0.48       100
          92       0.20      0.24      0.22       100
          93       0.30      0.27      0.29       100
          94       0.53      0.58      0.56       100
          95       0.60      0.51      0.55       100
          96       0.39      0.41      0.40       100
          99       0.24      0.24      0.24       100

    accuracy                           0.40      4000
   macro avg       0.40      0.40      0.39      4000
weighted avg       0.40      0.40      0.39      4000

Competition
DC 0, val_set_size=2000, COIs=[97, 48, 69, 8, 42, 43, 39, 62, 28, 49, 1, 79, 76, 6, 27, 9, 31, 4, 34, 53, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([97, 48, 69,  8, 42, 43, 39, 62, 28, 49,  1, 79, 76,  6, 27,  9, 31,  4,
        34, 53, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.025, 0.11637692308425904)
DC 1, val_set_size=2000, COIs=[67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90, 2, 18, 44, 30, 85, 24, 89, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90,  2, 18, 44, 30, 85,
        24, 89, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.0285, 0.11684445023536683)
DC 2, val_set_size=2000, COIs=[29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70, 20, 52, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70,
        20, 52, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.025, 0.11637047803401947)
D00: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D01: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D02: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D03: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D04: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D05: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D06: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D07: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D08: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D09: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D010: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D011: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D012: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D013: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D014: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D015: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D016: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D017: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D018: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D019: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D020: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D021: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D022: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D023: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO1', '(DO0']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.235, 0.13646255910396576) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.193, 0.1438817813694477) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.24, 0.14013344839215278) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.265, 0.17189570766687393) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2155, 0.1964953014552593) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2805, 0.18400845822691916) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.268, 0.2169084646999836) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2125, 0.2560572375059128) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2945, 0.23679691341519354) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.26714032801985743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.218, 0.35100569695234296) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2925, 0.30195116309821607) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.279, 0.3116454122811556) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.225, 0.4333983327448368) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.293, 0.3305833096951246) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO1', '(DO0']
DC 2 --> ['(DO2', '(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.279, 0.36128022530674936) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.22, 0.38793309094011785) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3, 0.35259693352878096) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2785, 0.35528980785608294) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.224, 0.35954430177807806) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.33720189756155017) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.274, 0.3634169872701168) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.226, 0.37335166200995445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2935, 0.3510033143609762) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.272, 0.38715057584643364) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2225, 0.37561649245023726) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.296, 0.36033891608566043) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.3845009683668613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.216, 0.4188616375625134) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2985, 0.3996308395564556) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2715, 0.41073952603340147) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.215, 0.4270609440207481) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.293, 0.3867718013674021) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2765, 0.38425979849696157) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.213, 0.4180722508430481) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.38628539292514324) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2635, 0.4458310455083847) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2175, 0.4269939658045769) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.291, 0.3813492998480797) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2695, 0.385557568103075) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2025, 0.4486020265221596) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2905, 0.39398507660627363) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.274, 0.4032967520952225) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.215, 0.4061214776933193) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.292, 0.4024509579539299) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO4', '(DO5']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO1', '(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.27, 0.4450267319381237) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2045, 0.41726283687353133) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.296, 0.41091943909227846) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.263, 0.41211589889228345) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2125, 0.41580246269702914) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.286, 0.46015576204657554) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.254, 0.39486653250455855) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.216, 0.3839368011951447) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2885, 0.3632778430879116) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2635, 0.38039471197128294) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2105, 0.4176867270469666) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2895, 0.39972613739967344) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2665, 0.3806304811835289) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.218, 0.37931018191576005) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.288, 0.3530941485762596) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO2', '(DO5']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO0', '(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2745, 0.36094196304678916) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.202, 0.40410060811042786) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2745, 0.337207077473402) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.3723496068716049) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.212, 0.38468977284431455) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.291, 0.31897277882695196) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2675, 0.35639621755480766) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2085, 0.38761450558900834) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2895, 0.31769467920064925) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2695, 0.3425093652904034) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.212, 0.3598610353767872) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.281, 0.304906700193882) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2715, 0.3392107605934143) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.204, 0.3688090083003044) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.281, 0.2925912225246429) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO3', '(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.278, 0.33127733558416367) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.215, 0.3407663398385048) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2735, 0.29786265051364896) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.263, 0.3541696026623249) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2135, 0.34854109114408494) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2835, 0.2640274191200733) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.27, 0.30929268074035643) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2015, 0.3504893842339516) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.283, 0.2581105837523937) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2605, 0.3106349483728409) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2115, 0.3354788272976875) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2865, 0.25779887636005877) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.267, 0.30968081095814703) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.202, 0.3360335075855255) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.287, 0.243881698936224) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO1', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.257, 0.312161654651165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2045, 0.34469924521446227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.288, 0.2521726489663124) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2675, 0.30235798269510267) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2075, 0.32522883582115175) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2925, 0.25124630627036093) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.261, 0.2830125495791435) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2145, 0.3234491937458515) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.26197176331281663) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2615, 0.2839728572964668) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.206, 0.3207395525574684) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.294, 0.2712062961757183) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.264, 0.29483000421524047) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.224, 0.32332483541965484) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2935, 0.2475722286105156) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO1', '(DO5']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2715, 0.27737851375341416) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.204, 0.3433258147239685) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2865, 0.2691237331926823) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2665, 0.2711680626869202) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.209, 0.33094253700971604) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2905, 0.2625396746993065) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2685, 0.27672640639543533) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.218, 0.3175289470553398) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.285, 0.26332329577207564) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.2840370298027992) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2175, 0.3336211709976196) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.287, 0.2695363045334816) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2655, 0.2928835654258728) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2145, 0.33462529951333997) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.292, 0.25651232835650445) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO1', '(DO5']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO3', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2735, 0.2667523117661476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2105, 0.323912348985672) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2975, 0.25043291983008387) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.271, 0.24801586443185805) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.208, 0.31995175302028656) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2915, 0.2518656887114048) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2805, 0.27102916687726974) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.222, 0.31183395683765414) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2865, 0.2491802466213703) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.264, 0.2750229218006134) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2055, 0.3486752260923386) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.301, 0.24669591081142425) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.23612956243753433) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2155, 0.32091534686088563) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.294, 0.24596705876290798) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO1', '(DO5']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO4', '(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.259, 0.26430063575506213) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2075, 0.33784891855716703) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2995, 0.23282830303907395) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.272, 0.2710549719929695) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.218, 0.32876098960638045) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.29, 0.24694163116812706) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.278, 0.24541437339782715) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2035, 0.30515779757499695) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.304, 0.21846081510186197) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2775, 0.2366647009253502) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2085, 0.3277924470305443) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3015, 0.24060065686702728) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.266, 0.25864433544874194) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2095, 0.35006488156318666) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.301, 0.2472796681523323) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.025, 0.11637692308425904), (0.235, 0.13646255910396576), (0.265, 0.17189570766687393), (0.268, 0.2169084646999836), (0.276, 0.26714032801985743), (0.279, 0.3116454122811556), (0.279, 0.36128022530674936), (0.2785, 0.35528980785608294), (0.274, 0.3634169872701168), (0.272, 0.38715057584643364), (0.276, 0.3845009683668613), (0.2715, 0.41073952603340147), (0.2765, 0.38425979849696157), (0.2635, 0.4458310455083847), (0.2695, 0.385557568103075), (0.274, 0.4032967520952225), (0.27, 0.4450267319381237), (0.263, 0.41211589889228345), (0.254, 0.39486653250455855), (0.2635, 0.38039471197128294), (0.2665, 0.3806304811835289), (0.2745, 0.36094196304678916), (0.275, 0.3723496068716049), (0.2675, 0.35639621755480766), (0.2695, 0.3425093652904034), (0.2715, 0.3392107605934143), (0.278, 0.33127733558416367), (0.263, 0.3541696026623249), (0.27, 0.30929268074035643), (0.2605, 0.3106349483728409), (0.267, 0.30968081095814703), (0.257, 0.312161654651165), (0.2675, 0.30235798269510267), (0.261, 0.2830125495791435), (0.2615, 0.2839728572964668), (0.264, 0.29483000421524047), (0.2715, 0.27737851375341416), (0.2665, 0.2711680626869202), (0.2685, 0.27672640639543533), (0.2595, 0.2840370298027992), (0.2655, 0.2928835654258728), (0.2735, 0.2667523117661476), (0.271, 0.24801586443185805), (0.2805, 0.27102916687726974), (0.264, 0.2750229218006134), (0.276, 0.23612956243753433), (0.259, 0.26430063575506213), (0.272, 0.2710549719929695), (0.278, 0.24541437339782715), (0.2775, 0.2366647009253502), (0.266, 0.25864433544874194)]
TEST: 
[(0.02475, 0.11546592718362808), (0.22875, 0.13392281126976013), (0.26175, 0.16805527663230896), (0.2725, 0.21223795926570893), (0.2795, 0.2618409206867218), (0.27425, 0.30652483654022217), (0.2795, 0.3548354196548462), (0.2785, 0.346992377281189), (0.27275, 0.35429292953014374), (0.27775, 0.3752575206756592), (0.274, 0.3730240747928619), (0.2745, 0.39937816953659055), (0.2755, 0.3738892929553986), (0.265, 0.42832590913772584), (0.27425, 0.3743641109466553), (0.271, 0.39244889783859255), (0.273, 0.4339277379512787), (0.271, 0.40180539894104006), (0.26125, 0.3818334400653839), (0.26275, 0.36972708070278165), (0.2655, 0.37061808133125307), (0.26975, 0.3542742565870285), (0.26875, 0.3647453181743622), (0.26625, 0.3441948702335358), (0.2615, 0.3314293150901794), (0.26275, 0.32839433014392855), (0.273, 0.3231146389245987), (0.2655, 0.34335739755630496), (0.2735, 0.30074304950237274), (0.268, 0.3078539932966232), (0.27075, 0.30345554876327513), (0.26125, 0.3067946380376816), (0.27225, 0.2958484448194504), (0.265, 0.275679435133934), (0.2685, 0.2735390293598175), (0.269, 0.2869514412879944), (0.275, 0.27405125975608824), (0.26525, 0.2648835381269455), (0.26775, 0.27065356051921846), (0.27075, 0.27842318654060366), (0.26825, 0.2853920660018921), (0.271, 0.2628588413000107), (0.2795, 0.24131809508800506), (0.27475, 0.2617100795507431), (0.2735, 0.2645449718236923), (0.28375, 0.22839333832263947), (0.27925, 0.25174569034576416), (0.28075, 0.261950896024704), (0.286, 0.24052491569519044), (0.27425, 0.2327061766386032), (0.27425, 0.25406328320503235)]
DETAILED: 
              precision    recall  f1-score   support

           0       1.00      0.03      0.06       100
           1       0.32      0.63      0.42       100
           3       0.00      0.00      0.00       100
           4       0.12      0.23      0.16       100
           6       0.24      0.47      0.32       100
           7       0.00      0.00      0.00       100
           8       0.29      0.53      0.38       100
           9       0.27      0.59      0.37       100
          14       1.00      0.01      0.02       100
          23       1.00      0.01      0.02       100
          27       0.20      0.37      0.26       100
          28       0.38      0.57      0.46       100
          31       0.25      0.50      0.33       100
          33       0.50      0.01      0.02       100
          34       0.25      0.42      0.31       100
          35       0.00      0.00      0.00       100
          39       0.30      0.52      0.38       100
          41       0.82      0.14      0.24       100
          42       0.14      0.32      0.19       100
          43       0.25      0.40      0.31       100
          48       0.39      0.59      0.47       100
          49       0.26      0.69      0.38       100
          53       0.34      0.70      0.46       100
          54       0.00      0.00      0.00       100
          56       1.00      0.01      0.02       100
          60       0.88      0.22      0.35       100
          62       0.28      0.76      0.41       100
          65       0.00      0.00      0.00       100
          66       0.33      0.01      0.02       100
          69       0.34      0.66      0.45       100
          72       0.00      0.00      0.00       100
          76       0.38      0.63      0.48       100
          79       0.23      0.46      0.31       100
          83       0.43      0.03      0.06       100
          84       0.00      0.00      0.00       100
          87       0.00      0.00      0.00       100
          92       0.00      0.00      0.00       100
          93       0.67      0.04      0.08       100
          96       0.33      0.01      0.02       100
          97       0.19      0.41      0.26       100

    accuracy                           0.27      4000
   macro avg       0.33      0.27      0.20      4000
weighted avg       0.33      0.27      0.20      4000

Competition_DC_1
VAL: 
[(0.0285, 0.11684445023536683), (0.193, 0.1438817813694477), (0.2155, 0.1964953014552593), (0.2125, 0.2560572375059128), (0.218, 0.35100569695234296), (0.225, 0.4333983327448368), (0.22, 0.38793309094011785), (0.224, 0.35954430177807806), (0.226, 0.37335166200995445), (0.2225, 0.37561649245023726), (0.216, 0.4188616375625134), (0.215, 0.4270609440207481), (0.213, 0.4180722508430481), (0.2175, 0.4269939658045769), (0.2025, 0.4486020265221596), (0.215, 0.4061214776933193), (0.2045, 0.41726283687353133), (0.2125, 0.41580246269702914), (0.216, 0.3839368011951447), (0.2105, 0.4176867270469666), (0.218, 0.37931018191576005), (0.202, 0.40410060811042786), (0.212, 0.38468977284431455), (0.2085, 0.38761450558900834), (0.212, 0.3598610353767872), (0.204, 0.3688090083003044), (0.215, 0.3407663398385048), (0.2135, 0.34854109114408494), (0.2015, 0.3504893842339516), (0.2115, 0.3354788272976875), (0.202, 0.3360335075855255), (0.2045, 0.34469924521446227), (0.2075, 0.32522883582115175), (0.2145, 0.3234491937458515), (0.206, 0.3207395525574684), (0.224, 0.32332483541965484), (0.204, 0.3433258147239685), (0.209, 0.33094253700971604), (0.218, 0.3175289470553398), (0.2175, 0.3336211709976196), (0.2145, 0.33462529951333997), (0.2105, 0.323912348985672), (0.208, 0.31995175302028656), (0.222, 0.31183395683765414), (0.2055, 0.3486752260923386), (0.2155, 0.32091534686088563), (0.2075, 0.33784891855716703), (0.218, 0.32876098960638045), (0.2035, 0.30515779757499695), (0.2085, 0.3277924470305443), (0.2095, 0.35006488156318666)]
TEST: 
[(0.029, 0.11603228694200515), (0.19425, 0.14265226924419402), (0.2095, 0.19390611326694487), (0.21225, 0.24950142300128936), (0.2215, 0.3393291345834732), (0.2265, 0.41605182719230654), (0.21825, 0.37589412808418277), (0.2165, 0.3515729539394379), (0.21925, 0.36310257291793824), (0.21925, 0.3697680175304413), (0.2095, 0.4066902079582214), (0.207, 0.4186436223983765), (0.215, 0.4074503457546234), (0.214, 0.4207613954544067), (0.21275, 0.4382778129577637), (0.21075, 0.39608364820480346), (0.208, 0.40762322664260864), (0.20725, 0.40581098437309265), (0.214, 0.37148428630828856), (0.2, 0.4051414391994476), (0.2055, 0.37069649815559386), (0.2075, 0.3918355963230133), (0.21175, 0.37112932682037353), (0.2065, 0.3729368586540222), (0.21175, 0.34373171877861025), (0.202, 0.3603932332992554), (0.21425, 0.33252625024318694), (0.213, 0.3399019513130188), (0.2065, 0.34411545789241793), (0.20925, 0.3258977173566818), (0.2045, 0.32509677231311795), (0.20575, 0.33895638525485994), (0.20775, 0.31632176899909975), (0.2065, 0.31338019835948944), (0.209, 0.30917843329906464), (0.21725, 0.3147936650514603), (0.2125, 0.33403572368621826), (0.213, 0.31675659835338593), (0.212, 0.30508829295635226), (0.214, 0.3184783831834793), (0.22, 0.31965047883987424), (0.21925, 0.31158780443668366), (0.21, 0.3097573293447495), (0.22175, 0.2984627636671066), (0.21125, 0.3323892900943756), (0.20975, 0.30991242718696593), (0.215, 0.32166397106647493), (0.2165, 0.31327139043807983), (0.218, 0.2943530341386795), (0.21825, 0.31427115046978), (0.22175, 0.3357926067113876)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.79      0.11      0.19       100
           2       0.21      0.35      0.27       100
           3       0.00      0.00      0.00       100
           7       0.67      0.06      0.11       100
          14       1.00      0.01      0.02       100
          18       0.18      0.46      0.26       100
          23       0.67      0.08      0.14       100
          24       0.41      0.72      0.53       100
          30       0.30      0.53      0.38       100
          32       0.19      0.35      0.24       100
          33       0.00      0.00      0.00       100
          35       0.00      0.00      0.00       100
          38       0.13      0.37      0.19       100
          41       1.00      0.03      0.06       100
          44       0.19      0.24      0.21       100
          45       0.10      0.30      0.15       100
          47       0.35      0.74      0.47       100
          51       0.22      0.47      0.30       100
          54       0.00      0.00      0.00       100
          56       0.00      0.00      0.00       100
          59       0.21      0.49      0.30       100
          60       1.00      0.14      0.25       100
          65       0.00      0.00      0.00       100
          66       0.00      0.00      0.00       100
          67       0.25      0.32      0.28       100
          72       0.00      0.00      0.00       100
          73       0.28      0.39      0.33       100
          74       0.18      0.37      0.24       100
          78       0.16      0.40      0.23       100
          80       0.09      0.18      0.12       100
          83       0.00      0.00      0.00       100
          84       0.00      0.00      0.00       100
          85       0.32      0.54      0.40       100
          87       0.00      0.00      0.00       100
          89       0.25      0.44      0.32       100
          90       0.23      0.42      0.30       100
          92       0.00      0.00      0.00       100
          93       0.00      0.00      0.00       100
          96       0.00      0.00      0.00       100
          98       0.21      0.36      0.26       100

    accuracy                           0.22      4000
   macro avg       0.24      0.22      0.16      4000
weighted avg       0.24      0.22      0.16      4000

Competition_DC_2
VAL: 
[(0.025, 0.11637047803401947), (0.24, 0.14013344839215278), (0.2805, 0.18400845822691916), (0.2945, 0.23679691341519354), (0.2925, 0.30195116309821607), (0.293, 0.3305833096951246), (0.3, 0.35259693352878096), (0.2985, 0.33720189756155017), (0.2935, 0.3510033143609762), (0.296, 0.36033891608566043), (0.2985, 0.3996308395564556), (0.293, 0.3867718013674021), (0.2915, 0.38628539292514324), (0.291, 0.3813492998480797), (0.2905, 0.39398507660627363), (0.292, 0.4024509579539299), (0.296, 0.41091943909227846), (0.286, 0.46015576204657554), (0.2885, 0.3632778430879116), (0.2895, 0.39972613739967344), (0.288, 0.3530941485762596), (0.2745, 0.337207077473402), (0.291, 0.31897277882695196), (0.2895, 0.31769467920064925), (0.281, 0.304906700193882), (0.281, 0.2925912225246429), (0.2735, 0.29786265051364896), (0.2835, 0.2640274191200733), (0.283, 0.2581105837523937), (0.2865, 0.25779887636005877), (0.287, 0.243881698936224), (0.288, 0.2521726489663124), (0.2925, 0.25124630627036093), (0.2915, 0.26197176331281663), (0.294, 0.2712062961757183), (0.2935, 0.2475722286105156), (0.2865, 0.2691237331926823), (0.2905, 0.2625396746993065), (0.285, 0.26332329577207564), (0.287, 0.2695363045334816), (0.292, 0.25651232835650445), (0.2975, 0.25043291983008387), (0.2915, 0.2518656887114048), (0.2865, 0.2491802466213703), (0.301, 0.24669591081142425), (0.294, 0.24596705876290798), (0.2995, 0.23282830303907395), (0.29, 0.24694163116812706), (0.304, 0.21846081510186197), (0.3015, 0.24060065686702728), (0.301, 0.2472796681523323)]
TEST: 
[(0.025, 0.11544345253705979), (0.24775, 0.13861423110961915), (0.27625, 0.18343008744716643), (0.28, 0.23506767535209655), (0.28875, 0.29818867599964144), (0.2895, 0.32688425755500794), (0.29625, 0.3491660008430481), (0.294, 0.3358581105470657), (0.2895, 0.3474338722229004), (0.29325, 0.3537992333173752), (0.2935, 0.39476948177814486), (0.291, 0.3797877466678619), (0.28375, 0.3835422582626343), (0.2925, 0.37797126722335816), (0.295, 0.38981323504447934), (0.28525, 0.3963869379758835), (0.29475, 0.40415494537353513), (0.28625, 0.4541899688243866), (0.29325, 0.3587131694555283), (0.2835, 0.3927868185043335), (0.2845, 0.34863383102416995), (0.27825, 0.33162746322155), (0.29125, 0.315565895318985), (0.29075, 0.3171741186380386), (0.2865, 0.3026546938419342), (0.2825, 0.291487837433815), (0.27925, 0.2973485368490219), (0.28375, 0.2628336983919144), (0.28225, 0.25620242035388946), (0.285, 0.2571150369644165), (0.289, 0.24301604974269866), (0.287, 0.2538253242969513), (0.2965, 0.2471312164068222), (0.294, 0.25714032888412475), (0.288, 0.2646378139257431), (0.29075, 0.24740822064876555), (0.29075, 0.2674160190820694), (0.29025, 0.2570450476408005), (0.292, 0.2569726722240448), (0.28425, 0.26652451276779177), (0.2885, 0.2583042995929718), (0.298, 0.24659888052940368), (0.29525, 0.25046407407522203), (0.2975, 0.2494123340845108), (0.29375, 0.2442604819536209), (0.294, 0.24323623353242874), (0.294, 0.23160053515434265), (0.294, 0.24723973906040192), (0.301, 0.21995452976226806), (0.292, 0.23650013768672942), (0.289, 0.24853316330909728)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.50      0.02      0.04       100
           3       0.00      0.00      0.00       100
           7       0.00      0.00      0.00       100
          11       0.18      0.34      0.23       100
          14       0.00      0.00      0.00       100
          17       0.33      0.63      0.43       100
          20       0.56      0.65      0.60       100
          23       0.69      0.11      0.19       100
          26       0.23      0.40      0.29       100
          29       0.25      0.29      0.27       100
          33       1.00      0.03      0.06       100
          35       0.00      0.00      0.00       100
          37       0.22      0.43      0.29       100
          41       0.89      0.16      0.27       100
          46       0.14      0.40      0.21       100
          50       0.18      0.40      0.25       100
          52       0.43      0.89      0.58       100
          54       0.00      0.00      0.00       100
          56       1.00      0.02      0.04       100
          60       0.79      0.11      0.19       100
          61       0.38      0.62      0.47       100
          63       0.27      0.51      0.35       100
          65       0.17      0.01      0.02       100
          66       0.00      0.00      0.00       100
          68       0.50      0.75      0.60       100
          70       0.20      0.63      0.30       100
          72       0.00      0.00      0.00       100
          77       0.14      0.27      0.19       100
          82       0.39      0.81      0.53       100
          83       0.50      0.01      0.02       100
          84       0.00      0.00      0.00       100
          86       0.27      0.54      0.36       100
          87       0.00      0.00      0.00       100
          91       0.33      0.64      0.44       100
          92       0.00      0.00      0.00       100
          93       0.50      0.06      0.11       100
          94       0.43      0.67      0.52       100
          95       0.35      0.74      0.47       100
          96       0.00      0.00      0.00       100
          99       0.18      0.42      0.26       100

    accuracy                           0.29      4000
   macro avg       0.30      0.29      0.21      4000
weighted avg       0.30      0.29      0.21      4000

Collaboration
DC 0, val_set_size=2000, COIs=[97, 48, 69, 8, 42, 43, 39, 62, 28, 49, 1, 79, 76, 6, 27, 9, 31, 4, 34, 53, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([97, 48, 69,  8, 42, 43, 39, 62, 28, 49,  1, 79, 76,  6, 27,  9, 31,  4,
        34, 53, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.025, 0.11637692308425904)
DC 1, val_set_size=2000, COIs=[67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90, 2, 18, 44, 30, 85, 24, 89, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90,  2, 18, 44, 30, 85,
        24, 89, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.0285, 0.11684445023536683)
DC 2, val_set_size=2000, COIs=[29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70, 20, 52, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70,
        20, 52, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.025, 0.11637047803401947)
D00: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D01: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D02: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D03: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D04: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D05: 1000 samples from classes {0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96}
D06: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D07: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D08: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D09: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D010: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D011: 1000 samples from classes {1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97}
D012: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D013: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D014: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D015: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D016: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D017: 1000 samples from classes {2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98}
D018: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D019: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D020: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D021: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D022: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
D023: 1000 samples from classes {11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO5', '(DO4']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO0', '(DO2']
TRAINING: DC 0
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

New Model (Val Acc, Val Loss) = (0.2355, 0.13705004966259002) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.1985, 0.14567000111937523) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2385, 0.13897214257717133) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2595, 0.17670645502209664) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2125, 0.1960242808163166) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2825, 0.17651037204265593) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2745, 0.2186212230026722) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.217, 0.26838243025541303) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.284, 0.22688274544477463) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2775, 0.27114493784308435) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.224, 0.3678848887085915) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2955, 0.2832941095083952) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.274, 0.31195108592510223) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.228, 0.3995761286914349) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3005, 0.3405042081475258) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO1', '(DO0']
DC 2 --> ['(DO5', '(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2755, 0.35171471667289733) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2225, 0.3856946724355221) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.2965, 0.38966808015108106) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.274, 0.3509212189167738) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2265, 0.4040875232219696) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.302, 0.41606841498613356) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.2685, 0.3840856594741344) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2165, 0.387835705190897) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.292, 0.36270577543973925) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.264, 0.403664115101099) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.219, 0.41843263429403305) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.298, 0.3655254957377911) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.274, 0.45353206706047056) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.2255, 0.40183169914782046) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.3, 0.39564908677339555) --> New model used: True
CREATING ALLIANCE...
DC Alliance, val_set_size=3000, COIs=[0, 3, 7, 14, 23, 33, 35, 41, 54, 56, 60, 65, 66, 72, 83, 84, 87, 92, 93, 96], M=tensor([ 0,  1,  2,  3,  4,  6,  7,  8,  9, 11, 14, 17, 18, 20, 23, 24, 26, 27,
        28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47,
        48, 49, 50, 51, 52, 53, 54, 56, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69,
        70, 72, 73, 74, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 89, 90, 91,
        92, 93, 94, 95, 96, 97, 98, 99], device='cuda:0'), Initial Performance: (0.37566666666666665, 0.07115047478675843)
DC Expert-0, val_set_size=1000, COIs=[1, 4, 6, 8, 9, 27, 28, 31, 34, 39, 42, 43, 48, 49, 53, 62, 69, 76, 79, 97], M=tensor([97, 48, 69,  8, 42, 43, 39, 62, 28, 49,  1, 79, 76,  6, 27,  9, 31,  4,
        34, 53, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.548, 0.0655372389703989)
DC Expert-1, val_set_size=1000, COIs=[2, 18, 24, 30, 32, 38, 44, 45, 47, 51, 59, 67, 73, 74, 78, 80, 85, 89, 90, 98], M=tensor([67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90,  2, 18, 44, 30, 85,
        24, 89, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.451, 0.07846047034859657)
DC Expert-2, val_set_size=1000, COIs=[11, 17, 20, 26, 29, 37, 46, 50, 52, 61, 63, 68, 70, 77, 82, 86, 91, 94, 95, 99], M=tensor([29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70,
        20, 52, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), Initial Performance: (0.6, 0.05642045271396637)
SUPER-DC 0, val_set_size=2000, COIs=[97, 48, 69, 8, 42, 43, 39, 62, 28, 49, 1, 79, 76, 6, 27, 9, 31, 4, 34, 53, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([97, 48, 69,  8, 42, 43, 39, 62, 28, 49,  1, 79, 76,  6, 27,  9, 31,  4,
        34, 53, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=2000, COIs=[67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90, 2, 18, 44, 30, 85, 24, 89, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([67, 47, 51, 80, 59, 74, 32, 73, 78, 98, 38, 45, 90,  2, 18, 44, 30, 85,
        24, 89, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=2000, COIs=[29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70, 20, 52, 96, 54, 66, 0, 56, 3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35, 14, 33, 23, 7], M=tensor([29, 94, 37, 91, 82, 11, 61, 63, 99, 86, 17, 95, 26, 77, 50, 46, 68, 70,
        20, 52, 96, 54, 66,  0, 56,  3, 84, 41, 93, 83, 87, 92, 60, 72, 65, 35,
        14, 33, 23,  7], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
FL ROUND 11
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.521, 0.08634113700687886) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.421, 0.10054253351688384) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.589, 0.0705530261900276) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49, 0.053023059959212936) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.287, 0.1868454243540764) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2385, 0.1748322422504425) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.308, 0.15162878228724003) --> New model used: True
FL ROUND 12
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.552, 0.08636701861768961) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.425, 0.11204993438720703) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.599, 0.07529523305222392) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5186666666666667, 0.053740302190184594) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.1478297944813967) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2355, 0.15338004991412163) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3365, 0.13719230760633946) --> New model used: True
FL ROUND 13
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.529, 0.09488842391967774) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.421, 0.12487336313724517) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.57, 0.08604410629696213) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5283333333333333, 0.055404993551472825) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.298, 0.14647282871603967) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.237, 0.1599159089922905) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3245, 0.12103746837377548) --> New model used: True
FL ROUND 14
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.517, 0.1002826099358499) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.422, 0.13483114647865296) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.58, 0.08125630202842876) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.519, 0.05902964067707459) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.291, 0.13260380443930625) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2215, 0.1705506677031517) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.329, 0.12282763284444809) --> New model used: True
FL ROUND 15
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.513, 0.09932889080932364) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.422, 0.1431725436449051) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.569, 0.09539401501230896) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.52, 0.0617400771677494) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3065, 0.11500516128540039) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2395, 0.12940869793295862) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.341, 0.10850519765913487) --> New model used: True
FL ROUND 16
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.52, 0.10490405656397343) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.41, 0.14537162482738494) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.546, 0.10838749913513311) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5093333333333333, 0.06904183181126912) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.305, 0.1151618540585041) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.249, 0.12581431818008423) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3455, 0.0948589592576027) --> New model used: True
FL ROUND 17
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.521, 0.1108541860034602) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.407, 0.1673277518749237) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.575, 0.09659449195861816) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5016666666666667, 0.07833998942871888) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3225, 0.10480677339434624) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.277, 0.112770283639431) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.349, 0.09477083569765091) --> New model used: True
FL ROUND 18
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.519, 0.11373647063411772) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.427, 0.1417144147157669) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.582, 0.10879068955779075) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5193333333333333, 0.0786301981707414) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.318, 0.10692963859438896) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.281, 0.10323803451657296) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3425, 0.09978493961691856) --> New model used: True
FL ROUND 19
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.532, 0.11464614505087957) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.413, 0.1638307178914547) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.595, 0.09863847175613046) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5026666666666667, 0.08884774555265904) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.10058419331908226) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.263, 0.11214711138606072) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3255, 0.1012710379064083) --> New model used: True
FL ROUND 20
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.523, 0.12184529475867749) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.438, 0.15692076361179352) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.579, 0.09915474506071768) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49333333333333335, 0.10844783327976863) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3285, 0.0969183090031147) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2785, 0.1131934554874897) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.362, 0.09069159251451492) --> New model used: True
FL ROUND 21
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.52, 0.12616692350059747) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.418, 0.16572234988212586) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.594, 0.10049067982052656) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.501, 0.09689821140964826) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3115, 0.11005553179979324) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.262, 0.11359753495454789) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.34, 0.10077474257349968) --> New model used: True
FL ROUND 22
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.511, 0.12916854970529676) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.417, 0.1633801305294037) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.587, 0.11058121295273304) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5036666666666667, 0.10546309153238932) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.327, 0.10280932134389878) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2745, 0.11149610424041748) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3555, 0.09792085038125516) --> New model used: True
FL ROUND 23
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.529, 0.12438080855365842) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.402, 0.17007460820674897) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.569, 0.11911566743254662) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5043333333333333, 0.11495614617069562) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.10328964564204215) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.264, 0.11433393716812133) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3455, 0.09944004441797734) --> New model used: True
FL ROUND 24
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.53, 0.1312472778838128) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.425, 0.17825217044353486) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.582, 0.12492897263355553) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.499, 0.12001500097910563) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.328, 0.10427358657121659) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2805, 0.11814213943481446) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3575, 0.10308784103393555) --> New model used: True
FL ROUND 25
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.545, 0.12915469360302814) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.427, 0.16761332964897155) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.586, 0.11386065518623217) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4876666666666667, 0.11743346734841664) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3265, 0.10465319961309433) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2805, 0.11593296653032303) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3275, 0.10408166408538819) --> New model used: True
FL ROUND 26
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.542, 0.12133851703542314) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.421, 0.17854148972034453) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.58, 0.12127311137318611) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49166666666666664, 0.1240021205842495) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.34, 0.10386421424150467) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.272, 0.11916795359551907) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3425, 0.10339245951175689) --> New model used: True
FL ROUND 27
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.53, 0.1390581115782261) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.427, 0.16844237005710602) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.586, 0.12738409514026716) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48833333333333334, 0.13247465418775875) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3225, 0.11333042562007904) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.291, 0.11789532010257243) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.10412343925237656) --> New model used: True
FL ROUND 28
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.531, 0.14144020479917527) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.431, 0.1694416155219078) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.569, 0.12357161568652372) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.488, 0.13548099162677923) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3265, 0.11456640517711639) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2775, 0.12075822766125202) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.342, 0.10874800264835358) --> New model used: True
FL ROUND 29
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.535, 0.1353585109077394) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.412, 0.1761733246445656) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.569, 0.13231454300880433) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48833333333333334, 0.13020382686456045) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3175, 0.115427001953125) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2925, 0.11878262370824814) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.349, 0.1071575877070427) --> New model used: True
FL ROUND 30
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.505, 0.15710661375522614) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.418, 0.20111648643016816) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.571, 0.12382733758795075) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.485, 0.13325629632174968) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.329, 0.11217819958925247) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.276, 0.1240098040997982) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.1076714004278183) --> New model used: True
FL ROUND 31
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.533, 0.14969426796899643) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.418, 0.189732209444046) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.575, 0.12420832195878029) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.495, 0.13507377055287362) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3145, 0.11280911546945573) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2965, 0.12400566929578781) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.345, 0.10787575852870941) --> New model used: True
FL ROUND 32
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.542, 0.1334612590111792) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.414, 0.1813508837223053) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.565, 0.14065489476919174) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.5026666666666667, 0.13913020234306653) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.328, 0.11485831928253173) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2875, 0.12215146473050117) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3215, 0.11971566081047058) --> New model used: True
FL ROUND 33
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.532, 0.13802686244249343) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.431, 0.17656297206878663) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.582, 0.1317958823442459) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.491, 0.14921655929088593) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.328, 0.11377626395225525) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.281, 0.12189487880468368) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.35, 0.11261362087726592) --> New model used: True
FL ROUND 34
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.509, 0.15393748161101029) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.413, 0.1909658977985382) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.586, 0.13393543713632972) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.47533333333333333, 0.15853498083353043) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3185, 0.11760103762149811) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.274, 0.12995578836649657) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3315, 0.11506626522541046) --> New model used: True
FL ROUND 35
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.516, 0.15793313993141056) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.431, 0.20445156300067902) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.59, 0.1274254499990493) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.495, 0.1416067588031292) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3345, 0.12070422470569611) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.292, 0.1270097560286522) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.359, 0.10934456747770309) --> New model used: True
FL ROUND 36
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.521, 0.16139155055364246) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.407, 0.20269331407546998) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.59, 0.137758530646556) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.484, 0.15479976044098537) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3365, 0.12138768088817596) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2855, 0.12986273935437204) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3715, 0.10677421885728836) --> New model used: True
FL ROUND 37
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.53, 0.16872511899396794) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.409, 0.20238516759872435) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.587, 0.15249201125814216) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48433333333333334, 0.1510945799748103) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3155, 0.12544974797964095) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2685, 0.13471385484933854) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.351, 0.11364424461126328) --> New model used: True
FL ROUND 38
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.524, 0.14825301550977746) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.412, 0.2013034887313843) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.587, 0.1506541150890698) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49166666666666664, 0.14497450780371826) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3265, 0.12141532361507416) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.13411114060878754) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.339, 0.12514340779185296) --> New model used: True
FL ROUND 39
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.522, 0.1556237436850788) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.404, 0.22345978760719298) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.598, 0.1347487277239561) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4856666666666667, 0.15640449118614197) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.322, 0.1217591148018837) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2875, 0.13124904212355615) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.349, 0.11508903485536576) --> New model used: True
FL ROUND 40
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.514, 0.15604411149024963) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.42, 0.20875380623340606) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.584, 0.13766629675775766) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.494, 0.15887668242057165) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3195, 0.12222437471151353) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.283, 0.13281975543498992) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3475, 0.11966486406326295) --> New model used: True
FL ROUND 41
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.533, 0.18092810169336734) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.409, 0.22138055014610292) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.578, 0.13857855097256833) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48633333333333334, 0.1517842397093773) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.13387877810001372) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.28, 0.13523098129034042) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.353, 0.12120220983028412) --> New model used: True
FL ROUND 42
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.529, 0.15592924118041993) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.426, 0.21005824959278108) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.575, 0.15015388364640966) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4846666666666667, 0.1634699347813924) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.312, 0.13008959674835205) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.267, 0.14028599238395692) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3405, 0.12165333354473114) --> New model used: True
FL ROUND 43
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.529, 0.17075128507614135) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.423, 0.19089152526855468) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.589, 0.15236085660941898) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.498, 0.15479506591955822) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3195, 0.1281985812187195) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2645, 0.13667462982237338) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.342, 0.12387854212522506) --> New model used: True
FL ROUND 44
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.532, 0.15475464689731597) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.402, 0.20430545151233673) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.598, 0.1441883567625191) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.49266666666666664, 0.1678553345600764) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.324, 0.12712281721830368) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.261, 0.13887459215521814) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.33, 0.11802143585681915) --> New model used: True
FL ROUND 45
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.527, 0.15695280605554582) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.429, 0.2028967080116272) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.592, 0.14870041679614224) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4876666666666667, 0.19162036995093026) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3215, 0.12889885276556015) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.144099440574646) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3525, 0.12521302831172942) --> New model used: True
FL ROUND 46
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.51, 0.16714443457126618) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.402, 0.2171314413547516) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.587, 0.15037436792174594) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.497, 0.17915903294086458) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3185, 0.12332549422979355) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2655, 0.14510114687681197) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3475, 0.1260079762339592) --> New model used: True
FL ROUND 47
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.537, 0.1556704197851941) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.401, 0.23021939659118654) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.558, 0.17191224464774132) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.48733333333333334, 0.17388245123624801) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.323, 0.12966176491975784) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.268, 0.14653782759606837) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3495, 0.12418242174386979) --> New model used: True
FL ROUND 48
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.537, 0.15216947746276854) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.403, 0.25475354623794555) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.578, 0.16738455319404602) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.487, 0.18425644387801488) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.32, 0.12797868067026139) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.2615, 0.14643713915348053) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3345, 0.13061974239349367) --> New model used: True
FL ROUND 49
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.532, 0.1622044314146042) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.433, 0.22779656648635865) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.596, 0.17015744531154633) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.4866666666666667, 0.17287872052192688) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.3175, 0.13289325737953187) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.266, 0.1453232729434967) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.3425, 0.1270923773050308) --> New model used: True
FL ROUND 50
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.521, 0.182498282790184) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.415, 0.22936450576782227) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.592, 0.16403094840049742) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.494, 0.1664102844595909) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.308, 0.13125060775876046) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.264, 0.14652425515651704) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.339, 0.12946908295154572) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.025, 0.11637692308425904), (0.2355, 0.13705004966259002), (0.2595, 0.17670645502209664), (0.2745, 0.2186212230026722), (0.2775, 0.27114493784308435), (0.274, 0.31195108592510223), (0.2755, 0.35171471667289733), (0.274, 0.3509212189167738), (0.2685, 0.3840856594741344), (0.264, 0.403664115101099), (0.274, 0.45353206706047056), (0.287, 0.1868454243540764), (0.275, 0.1478297944813967), (0.298, 0.14647282871603967), (0.291, 0.13260380443930625), (0.3065, 0.11500516128540039), (0.305, 0.1151618540585041), (0.3225, 0.10480677339434624), (0.318, 0.10692963859438896), (0.3155, 0.10058419331908226), (0.3285, 0.0969183090031147), (0.3115, 0.11005553179979324), (0.327, 0.10280932134389878), (0.3155, 0.10328964564204215), (0.328, 0.10427358657121659), (0.3265, 0.10465319961309433), (0.34, 0.10386421424150467), (0.3225, 0.11333042562007904), (0.3265, 0.11456640517711639), (0.3175, 0.115427001953125), (0.329, 0.11217819958925247), (0.3145, 0.11280911546945573), (0.328, 0.11485831928253173), (0.328, 0.11377626395225525), (0.3185, 0.11760103762149811), (0.3345, 0.12070422470569611), (0.3365, 0.12138768088817596), (0.3155, 0.12544974797964095), (0.3265, 0.12141532361507416), (0.322, 0.1217591148018837), (0.3195, 0.12222437471151353), (0.294, 0.13387877810001372), (0.312, 0.13008959674835205), (0.3195, 0.1281985812187195), (0.324, 0.12712281721830368), (0.3215, 0.12889885276556015), (0.3185, 0.12332549422979355), (0.323, 0.12966176491975784), (0.32, 0.12797868067026139), (0.3175, 0.13289325737953187), (0.308, 0.13125060775876046)]
TEST: 
[(0.02475, 0.11546592718362808), (0.24625, 0.13425422495603562), (0.2685, 0.17297466289997102), (0.2845, 0.21346812582015992), (0.282, 0.26616925156116483), (0.284, 0.3075293883085251), (0.28275, 0.34699194860458377), (0.282, 0.34570257520675657), (0.27825, 0.3790682841539383), (0.27375, 0.39798440170288085), (0.274, 0.451372394323349), (0.29375, 0.1840089282989502), (0.279, 0.14493239319324494), (0.3, 0.14585656386613846), (0.29475, 0.13472865849733354), (0.31425, 0.11680749154090882), (0.3115, 0.11618949872255326), (0.32325, 0.10587972432374954), (0.3235, 0.10840656453371048), (0.32375, 0.10189035940170288), (0.326, 0.09947338455915451), (0.31925, 0.10941210135817528), (0.3185, 0.10302717918157578), (0.31225, 0.1047273935675621), (0.32825, 0.10578046023845672), (0.3185, 0.10557718867063523), (0.3285, 0.10564149791002274), (0.31575, 0.11371149456501008), (0.31825, 0.11370902049541473), (0.3145, 0.11360327452421189), (0.31325, 0.11378956282138825), (0.31975, 0.11422268563508987), (0.32275, 0.115754061460495), (0.31475, 0.11370689916610718), (0.30975, 0.11794154703617096), (0.3265, 0.12023841053247451), (0.32775, 0.12133804816007614), (0.31225, 0.12516838681697845), (0.3215, 0.12186490470170974), (0.316, 0.12322861498594284), (0.32325, 0.12302748215198517), (0.30775, 0.13358877032995223), (0.30425, 0.1305598440170288), (0.31225, 0.12694054782390596), (0.3175, 0.12794643974304198), (0.31775, 0.13059656649827958), (0.31025, 0.12449972879886627), (0.3265, 0.128086599111557), (0.318, 0.12711768859624864), (0.311, 0.13296288192272188), (0.32025, 0.12821506029367447)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.78      0.39      0.52       100
           1       0.35      0.46      0.39       100
           3       0.29      0.08      0.12       100
           4       0.19      0.35      0.24       100
           6       0.26      0.33      0.29       100
           7       0.40      0.29      0.34       100
           8       0.43      0.26      0.33       100
           9       0.34      0.56      0.43       100
          14       0.17      0.17      0.17       100
          23       0.24      0.11      0.15       100
          27       0.22      0.26      0.24       100
          28       0.42      0.52      0.46       100
          31       0.28      0.39      0.33       100
          33       0.31      0.36      0.33       100
          34       0.19      0.18      0.19       100
          35       0.26      0.21      0.23       100
          39       0.32      0.47      0.38       100
          41       0.67      0.40      0.50       100
          42       0.29      0.23      0.26       100
          43       0.28      0.35      0.31       100
          48       0.49      0.50      0.49       100
          49       0.27      0.70      0.39       100
          53       0.40      0.59      0.48       100
          54       0.48      0.16      0.24       100
          56       0.60      0.39      0.47       100
          60       0.69      0.18      0.29       100
          62       0.22      0.27      0.24       100
          65       0.21      0.25      0.23       100
          66       0.22      0.29      0.25       100
          69       0.46      0.57      0.51       100
          72       0.12      0.05      0.07       100
          76       0.45      0.57      0.50       100
          79       0.27      0.47      0.34       100
          83       0.34      0.19      0.24       100
          84       0.15      0.15      0.15       100
          87       0.44      0.34      0.38       100
          92       0.29      0.23      0.26       100
          93       0.16      0.09      0.12       100
          96       0.52      0.34      0.41       100
          97       0.29      0.11      0.16       100

    accuracy                           0.32      4000
   macro avg       0.34      0.32      0.31      4000
weighted avg       0.34      0.32      0.31      4000

Collaboration_DC_1
VAL: 
[(0.0285, 0.11684445023536683), (0.1985, 0.14567000111937523), (0.2125, 0.1960242808163166), (0.217, 0.26838243025541303), (0.224, 0.3678848887085915), (0.228, 0.3995761286914349), (0.2225, 0.3856946724355221), (0.2265, 0.4040875232219696), (0.2165, 0.387835705190897), (0.219, 0.41843263429403305), (0.2255, 0.40183169914782046), (0.2385, 0.1748322422504425), (0.2355, 0.15338004991412163), (0.237, 0.1599159089922905), (0.2215, 0.1705506677031517), (0.2395, 0.12940869793295862), (0.249, 0.12581431818008423), (0.277, 0.112770283639431), (0.281, 0.10323803451657296), (0.263, 0.11214711138606072), (0.2785, 0.1131934554874897), (0.262, 0.11359753495454789), (0.2745, 0.11149610424041748), (0.264, 0.11433393716812133), (0.2805, 0.11814213943481446), (0.2805, 0.11593296653032303), (0.272, 0.11916795359551907), (0.291, 0.11789532010257243), (0.2775, 0.12075822766125202), (0.2925, 0.11878262370824814), (0.276, 0.1240098040997982), (0.2965, 0.12400566929578781), (0.2875, 0.12215146473050117), (0.281, 0.12189487880468368), (0.274, 0.12995578836649657), (0.292, 0.1270097560286522), (0.2855, 0.12986273935437204), (0.2685, 0.13471385484933854), (0.265, 0.13411114060878754), (0.2875, 0.13124904212355615), (0.283, 0.13281975543498992), (0.28, 0.13523098129034042), (0.267, 0.14028599238395692), (0.2645, 0.13667462982237338), (0.261, 0.13887459215521814), (0.266, 0.144099440574646), (0.2655, 0.14510114687681197), (0.268, 0.14653782759606837), (0.2615, 0.14643713915348053), (0.266, 0.1453232729434967), (0.264, 0.14652425515651704)]
TEST: 
[(0.029, 0.11603228694200515), (0.2015, 0.14342096573114396), (0.20725, 0.19224756515026092), (0.2225, 0.26171292388439177), (0.22325, 0.35750285744667054), (0.2205, 0.3886465504169464), (0.221, 0.37496927845478056), (0.22125, 0.39590018963813783), (0.221, 0.38031812036037443), (0.22025, 0.407045618057251), (0.22475, 0.39169922852516176), (0.23025, 0.16945298516750334), (0.25175, 0.15020240938663482), (0.236, 0.15479735112190246), (0.225, 0.16485808438062668), (0.2545, 0.12600262731313705), (0.247, 0.12420823925733566), (0.27, 0.10871123284101486), (0.27575, 0.10113453125953674), (0.25775, 0.11117163813114166), (0.27175, 0.11120570766925812), (0.25825, 0.112390445291996), (0.27275, 0.11032485932111741), (0.269, 0.1126993641257286), (0.2755, 0.11749012929201126), (0.27075, 0.1131023987531662), (0.2655, 0.1189957755804062), (0.27575, 0.11768659555912017), (0.27425, 0.1206539933681488), (0.28275, 0.1178428674340248), (0.27875, 0.12326135468482971), (0.27375, 0.12401407742500305), (0.268, 0.12359355026483536), (0.27625, 0.12119045650959015), (0.2695, 0.12854615420103074), (0.28725, 0.1266625691652298), (0.276, 0.1278860358595848), (0.2595, 0.1332257451415062), (0.27175, 0.1338664589524269), (0.275, 0.13304934298992158), (0.27325, 0.13417771571874618), (0.27425, 0.13454281789064407), (0.26325, 0.13867623299360277), (0.2605, 0.1344490008354187), (0.25975, 0.1361345088481903), (0.271, 0.13802707123756408), (0.25675, 0.13965603137016297), (0.26125, 0.1454775094985962), (0.26425, 0.14611962056159974), (0.2685, 0.14364156675338746), (0.26575, 0.14289397871494294)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.67      0.46      0.54       100
           2       0.20      0.19      0.20       100
           3       0.15      0.08      0.11       100
           7       0.22      0.18      0.20       100
          14       0.29      0.14      0.19       100
          18       0.24      0.34      0.28       100
          23       0.36      0.40      0.38       100
          24       0.47      0.64      0.54       100
          30       0.32      0.37      0.35       100
          32       0.28      0.31      0.30       100
          33       0.27      0.31      0.29       100
          35       0.14      0.13      0.13       100
          38       0.24      0.15      0.18       100
          41       0.44      0.36      0.40       100
          44       0.22      0.07      0.11       100
          45       0.11      0.27      0.16       100
          47       0.48      0.30      0.37       100
          51       0.25      0.21      0.23       100
          54       0.56      0.14      0.22       100
          56       0.33      0.37      0.35       100
          59       0.32      0.35      0.33       100
          60       0.67      0.57      0.62       100
          65       0.14      0.24      0.17       100
          66       0.18      0.29      0.22       100
          67       0.24      0.37      0.29       100
          72       0.03      0.02      0.02       100
          73       0.29      0.42      0.34       100
          74       0.16      0.17      0.16       100
          78       0.13      0.06      0.08       100
          80       0.02      0.01      0.01       100
          83       0.44      0.19      0.27       100
          84       0.15      0.24      0.19       100
          85       0.51      0.44      0.47       100
          87       0.40      0.34      0.37       100
          89       0.28      0.37      0.32       100
          90       0.22      0.29      0.25       100
          92       0.36      0.14      0.20       100
          93       0.13      0.11      0.12       100
          96       0.30      0.37      0.33       100
          98       0.14      0.22      0.17       100

    accuracy                           0.27      4000
   macro avg       0.28      0.27      0.26      4000
weighted avg       0.28      0.27      0.26      4000

Collaboration_DC_2
VAL: 
[(0.025, 0.11637047803401947), (0.2385, 0.13897214257717133), (0.2825, 0.17651037204265593), (0.284, 0.22688274544477463), (0.2955, 0.2832941095083952), (0.3005, 0.3405042081475258), (0.2965, 0.38966808015108106), (0.302, 0.41606841498613356), (0.292, 0.36270577543973925), (0.298, 0.3655254957377911), (0.3, 0.39564908677339555), (0.308, 0.15162878228724003), (0.3365, 0.13719230760633946), (0.3245, 0.12103746837377548), (0.329, 0.12282763284444809), (0.341, 0.10850519765913487), (0.3455, 0.0948589592576027), (0.349, 0.09477083569765091), (0.3425, 0.09978493961691856), (0.3255, 0.1012710379064083), (0.362, 0.09069159251451492), (0.34, 0.10077474257349968), (0.3555, 0.09792085038125516), (0.3455, 0.09944004441797734), (0.3575, 0.10308784103393555), (0.3275, 0.10408166408538819), (0.3425, 0.10339245951175689), (0.354, 0.10412343925237656), (0.342, 0.10874800264835358), (0.349, 0.1071575877070427), (0.354, 0.1076714004278183), (0.345, 0.10787575852870941), (0.3215, 0.11971566081047058), (0.35, 0.11261362087726592), (0.3315, 0.11506626522541046), (0.359, 0.10934456747770309), (0.3715, 0.10677421885728836), (0.351, 0.11364424461126328), (0.339, 0.12514340779185296), (0.349, 0.11508903485536576), (0.3475, 0.11966486406326295), (0.353, 0.12120220983028412), (0.3405, 0.12165333354473114), (0.342, 0.12387854212522506), (0.33, 0.11802143585681915), (0.3525, 0.12521302831172942), (0.3475, 0.1260079762339592), (0.3495, 0.12418242174386979), (0.3345, 0.13061974239349367), (0.3425, 0.1270923773050308), (0.339, 0.12946908295154572)]
TEST: 
[(0.025, 0.11544345253705979), (0.2395, 0.13823920208215715), (0.27675, 0.17512738859653473), (0.28125, 0.22271550393104553), (0.288, 0.2795979654788971), (0.28725, 0.3376588840484619), (0.29625, 0.3849456037282944), (0.2975, 0.4110357627868652), (0.295, 0.35725490319728853), (0.291, 0.362119450211525), (0.29175, 0.3952874184846878), (0.316, 0.15607069212198257), (0.32975, 0.1418918091058731), (0.32275, 0.12138705724477768), (0.32575, 0.123647993683815), (0.343, 0.10934317761659622), (0.345, 0.0964987068772316), (0.33925, 0.09520505648851395), (0.33525, 0.09969112971425056), (0.32075, 0.10128354698419571), (0.3425, 0.09377096989750862), (0.34175, 0.10229258060455322), (0.336, 0.09884483766555786), (0.33125, 0.10092266836762429), (0.3425, 0.10353942069411277), (0.32675, 0.10643303138017654), (0.345, 0.10112275034189225), (0.34575, 0.10324531003832817), (0.32775, 0.11062950670719146), (0.349, 0.10461698710918427), (0.34625, 0.10658329689502716), (0.3295, 0.10802375388145447), (0.322, 0.11928467541933059), (0.3415, 0.11281858104467392), (0.327, 0.11615858232975006), (0.3415, 0.11136694478988647), (0.3595, 0.106723304271698), (0.33175, 0.11360874408483505), (0.33675, 0.12402750378847122), (0.3325, 0.1116630340218544), (0.32075, 0.1209656794667244), (0.34325, 0.11958773010969162), (0.33175, 0.12058457750082016), (0.33825, 0.12429975712299347), (0.32825, 0.11769563630223275), (0.35275, 0.12388601511716843), (0.3375, 0.12686379653215407), (0.32975, 0.1248673397898674), (0.31675, 0.1316301736831665), (0.32825, 0.12628640741109848), (0.328, 0.13111505967378617)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.65      0.43      0.52       100
           3       0.30      0.10      0.15       100
           7       0.29      0.46      0.36       100
          11       0.24      0.21      0.22       100
          14       0.25      0.27      0.26       100
          17       0.32      0.64      0.43       100
          20       0.65      0.31      0.42       100
          23       0.57      0.31      0.40       100
          26       0.17      0.13      0.15       100
          29       0.25      0.26      0.25       100
          33       0.35      0.32      0.33       100
          35       0.16      0.30      0.21       100
          37       0.31      0.33      0.32       100
          41       0.65      0.49      0.56       100
          46       0.15      0.16      0.16       100
          50       0.21      0.10      0.14       100
          52       0.43      0.92      0.59       100
          54       0.31      0.11      0.16       100
          56       0.28      0.23      0.25       100
          60       0.68      0.36      0.47       100
          61       0.52      0.60      0.56       100
          63       0.33      0.49      0.40       100
          65       0.16      0.28      0.20       100
          66       0.24      0.32      0.28       100
          68       0.53      0.69      0.60       100
          70       0.28      0.54      0.37       100
          72       0.14      0.10      0.12       100
          77       0.23      0.19      0.21       100
          82       0.45      0.74      0.56       100
          83       0.32      0.24      0.28       100
          84       0.11      0.18      0.14       100
          86       0.34      0.21      0.26       100
          87       0.44      0.32      0.37       100
          91       0.49      0.34      0.40       100
          92       0.17      0.09      0.12       100
          93       0.24      0.21      0.22       100
          94       0.61      0.42      0.50       100
          95       0.58      0.45      0.51       100
          96       0.28      0.09      0.14       100
          99       0.18      0.18      0.18       100

    accuracy                           0.33      4000
   macro avg       0.35      0.33      0.32      4000
weighted avg       0.35      0.33      0.32      4000

