do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [145]
name: no-alliance-145
score_metric: contrloss
aggregation: <function fed_avg at 0x797c805cdc10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=145
Partitioning data
[[0, 3, 6, 8], [1, 4, 6, 8], [9, 7, 6, 8], [5, 2, 6, 8]]
[(array([33451, 31000,  3114, 10896,  4060, 19644, 44165, 35381, 42784,
       22062, 10061, 13549, 40945, 48788,  6015,  4859, 39301,  7347,
       13833, 47754, 38289,  3570, 44806, 46113, 42829, 15677,  1338,
       12492, 41880,  4273, 13712, 17232, 13627, 44568, 19945, 26018,
       25242, 36106, 19444, 10954, 21869, 26049,  3713, 35996, 38185,
       30267, 36163, 49050, 33058, 48847, 19778, 26060, 27080, 18572,
       24534, 32538, 42931, 12461, 28068, 31207, 15870, 32062, 42630,
       18211, 22129, 34856, 34017, 32376, 16380, 45186, 16763, 12659,
        4498, 24700, 20267, 44970, 24887, 26724,  6512, 15309, 37328,
       47473, 38618, 37022, 45344, 40635, 32078, 41055, 35269, 22674,
        3204, 26846,  6687,  4079, 46872, 34168, 31837, 47243, 30517,
       32885, 20015, 27415, 39236, 27280, 11812, 38222,   115, 36007,
        4940, 44668, 11734, 38433,  2720, 28080,  4619, 16596, 30029,
       16316, 19015, 17486, 48359, 34434, 11768,  9868, 48986, 16553,
       32479, 13401,  7504,  2365, 25027, 32233, 10326, 32967,  8249,
       38796, 13861, 44443, 18953, 42992, 30913, 14341, 44875, 45347,
       15735, 12911, 14482, 45102,  8858, 22844, 43777, 29295, 12178,
       24349, 33419,  9755, 37729, 25082, 17357, 17866, 26632, 11263,
        5994,  7655, 12149,  1147,  5194, 38545,  7320, 21881, 25362,
       43992, 12499, 34944, 14116,  7970, 12257, 24111, 35973, 38566,
       13714, 29860, 20217, 43757, 40211, 44580, 27768,  7299, 46367,
       33603,   564,  3441, 34917, 38466, 24837, 30895,   695, 17093,
       17994, 23581, 49717, 36052, 20398,  9257,  1522, 36737, 24735,
       30233, 20924, 22684, 25995, 47520,  3136, 14732, 41490, 48147,
        5589,  5828,  9671, 43951, 36149, 30963,  3582, 13368, 25178,
       20813, 16305, 41164, 41851, 21144,  7925, 47324,   405, 39329,
       48924,  4869,  1493, 31284,   392,  9096, 43542, 23746,  9287,
       48958, 15102, 40022, 20454,  5858, 46987,   527,  7603, 39211,
       37554, 16438,  3351, 22761, 37429, 41601, 27304, 17024,  4266,
       30346, 20784, 35960, 10085, 32844, 30318, 30305, 38093,  5507,
       41853,  5741, 29074, 38304, 39761,  1109, 25215, 23465, 12391,
       24733, 23974,  2738,  3365, 15247,  4025,  9034, 39289,  5586,
       33717, 37236, 19808, 47654, 21147, 42233, 13593, 33098, 48742,
       15421, 36249, 20252,  9830,  1951, 37194, 39543, 46349, 33582,
       20017,  7469,  3818,  5006, 18366, 14171, 35007, 45948, 43836,
       38982, 28237, 33633, 41053, 19577,  4558, 28996, 20496, 18188,
        8943,  8105, 22523, 18348, 41159, 46423,  6872, 13064, 21913,
       13581, 30839, 43110,  6205,  8101, 48088, 28513,  7049, 17530,
       38761, 20648, 16301,  5086, 11476, 39045, 14018, 34984, 24276,
        3756, 46977, 10463, 48019, 45117, 23695, 26485,  9394,  5578,
       48062, 25997, 38127, 17567, 47788, 27585, 15383, 47830, 31242,
       10565, 25062, 21373, 26556, 42633, 20625, 17920, 16714,  4472,
        1573,   878,  6716, 14536,    80,  9052, 46529, 27906,  1919,
       41521, 23700, 13688, 49386,  3749, 49233, 14924, 21873, 34647,
        3514, 36990, 21652, 37513, 32145, 35130, 27796, 12155, 38699,
       22008, 20338,  9817, 47997, 48402, 36511, 47603, 28835, 10439,
       43065,  6089, 15298, 36828, 41910, 19871, 49582, 37514, 34581,
       11434, 49574, 30159,  3524, 31632, 41578, 35923, 34822,   287,
        6724,  9345,  3110, 26064, 12904, 32264, 37105, 29109, 29096,
       44093, 48132, 17478, 26482, 34741, 31415, 46855, 35759, 41285,
       23278,  8067, 21832,  7224, 39235, 26036, 18442, 15614, 41845,
       11738, 39921, 13053, 10948, 23541, 11894, 38100, 17251, 33874,
       29865, 28050, 48849, 27673,  7343, 45217, 35323,  2353, 36269,
       42738,  3125, 42445,  3222,  8381, 37912, 23295,  2425, 43013,
       11294, 40651, 45590, 48346, 44456, 36946, 28725, 20965, 24384,
       26160, 14706,  3758, 46341, 42189,  8069, 40039,  1327, 45135,
        5398,  5964, 32918, 15338, 45169, 27346,  3780, 37378, 11392,
       13753, 40261, 21580, 14476, 17680, 46073, 22550, 17788, 31601,
       36903, 14906, 47161, 11651, 17566, 38268, 24354,  3122, 44654,
        9941, 23582, 48400, 26618,  2571, 43264,  6262, 41870,  9928,
       14342,  6506, 34552, 18103, 21497, 16387, 19275, 37013,  4217,
       12396, 35332,  3724, 14212, 14131,  2288, 11675, 20788, 42791,
       20972, 23708,  8991,  2112, 32694,  3892, 12168, 22926, 37481,
        8891, 22085, 35340, 23780,  1192,  3593, 10938, 12508,  8436,
       28690, 13120, 29767, 30323, 46307, 11917, 48977, 14608, 23135,
       46320, 20586, 35831, 25622, 48888, 23008, 15534, 31353, 24478,
        6938,  6949,  7825, 19139, 40202, 27648, 26228, 46473, 23208,
       36310, 21097, 15497, 48619, 39343, 41491,  3111, 25469,   899,
       41668, 45730, 49771, 15351, 26211, 10026,  2684, 36712, 40514,
       18736, 48532, 43392, 33696, 49406, 34965,   143, 19239, 34035,
       28277, 15650, 44012, 41603,  4391, 41309,  1563, 32869, 22889,
       22640, 42447, 31112, 36943, 24728, 49265, 15185,   248, 23086,
       42108, 45862,  5000,  5019, 15402, 17936, 37579, 40153, 19959,
       24000, 29560, 13683, 17172,   368,  5474,  9575, 26566, 30626,
       40357,  8612, 39913, 46322,  4082, 27450, 23516, 38751, 38508,
        2503, 43298, 32793, 25841,  8206,   819, 20934, 16539, 17857,
       49952,  9924, 31447, 18282, 21037, 22044, 12003, 30982, 21588,
        4421, 21192, 13635, 15424, 41549, 43074, 35900, 18992, 20141,
        8309, 11044, 48446,   117, 42099, 40264, 31616, 30625, 25442,
       15651, 25291, 39148, 46612, 10600, 15971, 39068, 35123, 34264,
       18126, 37956,  9681, 23596, 49829, 47115, 34804, 34643, 16822,
       17204, 33069, 46550, 22082, 15231, 37748, 13077, 36625, 24401,
        5095, 16927, 37822,  5921,  7064,  9762, 25255, 44428, 12632,
       10733, 21970, 49962,  5644,  8926, 10234, 31594, 13633, 39336,
       16430,  6250, 10442, 32464,  1914, 41732,  4092, 17442, 23845,
       36146, 46888, 31386, 42050, 46821, 15328,  4233, 16512, 16473,
       45318,  2923, 29746, 12418, 24811, 40109,  3885, 35013, 48943,
        2070, 26109, 25002, 29477, 12452, 40983, 21427, 28153, 33240,
       30299, 20954, 30438,  2462, 11591, 37042,  2586,  5675,  4397,
       28239, 46142, 14693, 13425, 36526, 12481, 17007, 30273, 27271,
        8006, 39502,  8123,  5716, 12457,   627, 21414, 11993,  8512,
       16435,   485, 47708, 10682, 11948, 13109, 31919, 49782, 48213,
       38453,  1250, 30406, 32915, 12605, 11244, 46714, 13729, 37147,
       14132, 11005, 33942, 35687, 22776, 28807, 41041, 18397, 13241,
       41689,  1383, 31526,  5487, 15635,  3614, 27820, 28125,  6989,
       14642, 18645, 16156, 40008,  1141, 39578, 23440,  8854, 43109,
        8574, 49291,  1967, 29340, 21579,  7443, 10680,   888,  8483,
       25902, 44458,  3215,  6219, 37994, 29147, 49182, 39069, 47770,
        6176, 26055,  8668,  2285, 14753, 35300, 30357, 10956,   546,
       32987,  5078,   909, 37308,  6679, 49778, 19502, 31832, 43872,
       23893, 49011, 23219, 40939,  2754, 21246,  9282, 42699, 27661,
       32619,  3319, 27854, 33356, 38008, 14934, 17277, 34910,  5910,
       44213, 47877, 11577,  2732,  8406, 22888,  1439, 16049, 41335,
       12194, 39577, 29493, 25111, 33407, 18141, 34892, 25047, 38105,
       43725,  5999,  7496, 45372,  9708, 37653, 27561, 32281, 31697,
       40429, 37587, 12801, 30807,  1080, 31778, 35661, 32567, 29027,
       15746, 37002,  6627,  6862, 28687, 21565, 16125, 34577, 25557,
       35268,  3800, 37642, 15795, 36940, 40641, 12442, 29175, 49827,
        2065, 31121, 19132, 24451, 33888, 48253, 12861, 37318, 28599,
       16856, 40483, 17210, 39032,  3169,  4723, 33869, 45442,  7013,
       29850, 20913,  6192, 47960, 48838,  8358, 18349, 45375, 28103,
        9960]), [0, 3, 6, 8]), (array([28634, 35692,  6709, 46330, 11418, 23607, 40152, 35799, 48975,
       35004, 29536, 28526, 13032, 37570, 20336, 45210,  4591,  9809,
       49561, 38257, 24191, 23213, 34263, 16736, 35174, 48631, 10153,
       47314, 29828, 36158, 11308, 42294, 18223,  3366, 30902, 49503,
        3822, 41061, 10271, 36033, 25071, 30676, 19310, 39405, 26905,
       18932, 21154,  1251, 49491, 25799, 24807, 49871, 33650, 41248,
       29121, 41607, 29963,  8350, 43711, 28615, 42281, 42637, 17539,
       17397, 47616, 13317, 24484,   644, 49376, 49269,  2455, 25105,
       22900, 23357,  4696, 40145, 29120, 27073, 21540, 22349, 22013,
       33870, 35357, 41654, 15299,  8825,  9251, 47190,  4379, 25827,
       18066, 19565, 32235, 28183,  3735,   119, 14717, 41739, 46421,
       12676,  2727, 11242, 45979, 30208,  2390,  7709, 42287, 22659,
       40178,  4624,  4909, 36101, 32736, 38047,  9095,  4557, 32099,
         493, 30804, 19574,  2180,  7352, 43145,   617, 39800, 32603,
       10714, 42684, 26241, 43935, 43207, 14101, 36991, 38157, 10248,
       45810, 17534, 21398, 49314, 30492, 46649, 36376, 41257, 18063,
       41576, 29791, 23601, 49416,  5356, 19500, 48806, 36872, 13402,
       39352, 37989, 39192, 49354, 12169, 25975, 20494, 40013, 47839,
       19667, 28127, 12049, 46359, 23058, 42410, 31147, 30716,  6922,
       28018, 34349, 34519, 29049, 48528, 30712, 44472, 21528, 10895,
       44965, 13763, 44359, 24263, 43006, 15710, 37401,  6623, 47606,
       38682, 16128, 31985, 17111, 13774, 14040, 39892, 24446,  9480,
       33392, 47151, 30019, 36679, 18134, 44097, 35805, 36334, 17772,
       32472, 28105, 48675, 30049, 14851, 16638, 47026,  4840, 27404,
       44309, 24280, 17312, 39240, 28896, 46966, 49795, 22867, 12628,
       46547, 42318, 28326, 35041, 40220, 10868, 26180, 37702, 10522,
       28248, 47836, 19046, 49933, 12412, 27814, 21877,  9565, 25479,
       10420, 33808, 44411, 12424, 17474, 26362,  9442, 39177, 43897,
       43934, 11794, 30474,   145, 41766, 18834, 11567, 27044,  5256,
       49645, 36345, 49740, 48787, 19197, 40228, 38247,  1953,  3217,
       24548, 30950, 45881, 15170, 33357, 22886, 20147, 33272, 27303,
       40375, 34574, 10766, 20730, 32533, 48429,  1471, 14036, 38386,
       41302, 22247, 33610,  2951,  2363, 15917, 44752, 18434, 32754,
       46947, 25414, 14816, 37775, 38660, 32930, 44978, 32882,  8068,
       14555, 42863,    89, 37996, 15723, 48591, 19045,  4413, 43758,
       24067, 41625, 20406, 25781, 33037, 27028, 40530, 36896, 33073,
       35839, 25474, 28658,  1385, 41270, 21048, 37757, 47573,  2216,
       11765, 46884, 38805, 34752, 48513, 22981, 22480, 14535,  8792,
       14974, 47149, 18455, 11565, 49352, 33222, 21949,  2673,  6378,
       38701, 28084, 22385, 14303,  6387, 43420, 49719, 37853, 47141,
       30794, 17083,  3695, 25292, 44002,  9010, 37563, 13828, 23994,
       21260, 37903, 27214, 47399, 49981, 31588, 43304, 47155,  6612,
        3908,   660, 43923, 36420,  5212, 25672, 39451,  2515, 18114,
        9141, 21607, 10015, 19704, 43928, 19170, 30044, 22606, 42303,
         581, 43202,  2442,  6940, 33002, 48689, 17770, 10629, 40835,
       45022, 22098, 43502, 15024, 24524, 44900, 13134, 17634, 48478,
       23187, 19980, 39932, 28160, 38296,  3688, 32066, 13391, 27342,
       31735, 28797, 20828, 11190,  5462, 40642, 27014, 22202,  2691,
       29353, 16669, 26118, 28561, 31699, 22173, 22284, 22902,  9550,
       27964, 37886, 40798, 46526, 46452, 48016, 27074, 49727, 30153,
       47922, 45555,  6331, 13995, 44994, 49027, 32119,  8405, 48228,
       18912, 14893, 29922,  9776, 10913, 32450, 17800,  8289, 30156,
        4400, 28803,  8404, 10888, 36665, 46077, 33743, 41777, 14382,
       11033, 48985, 16640, 15530,  4317, 16481, 49355, 33862, 37639,
       24498, 29172, 20142, 29265,  4642, 10536, 17260, 22360,  1659,
       38395,  1663, 18821,  3866,  7984, 32063, 19092,  6391, 40823,
       33623, 26383, 39120, 27660, 25000, 16508, 30724, 11875, 20310,
       28188, 26569, 16177, 20802, 32709, 49404, 11719, 49279, 10108,
       36525, 12666, 42261, 36060, 10327, 13403, 45140, 31128, 26748,
       30496, 15994, 15479, 10985, 11657,  3712,  4440, 48771, 17751,
       37654,  8617, 30302, 39994, 32338, 28943, 22113, 28644, 31405,
       45900, 49655, 30953,  6095, 22947,   234,  9278, 17925, 37351,
        4201, 38723, 43624, 13600, 38572, 29339, 46263, 18330, 30222,
       28194, 36984, 41652, 29408, 36816, 28481, 10400, 26872, 19707,
        1584, 41466, 45978,  6745, 13719, 40627,  1628, 21956, 38959,
        4415,  4810, 22347, 44959, 17766, 22157,  5785, 45266,  3942,
       44492, 31315, 17776, 13101,  6804, 29680, 32359, 46342, 18610,
        3683, 16429,  9855, 34942, 10345, 43482, 33528, 37625, 45944,
       11394,  3991, 47874, 15986,  7720, 48325,  6722, 32739, 32759,
       45420, 30193,  2631, 29360, 24424, 39889,  6454, 42013, 44258,
       12356, 21846, 33244,  1956,  8002, 24552,   807, 19834, 33420,
       38310, 28171, 47567, 40364, 40031,  8008, 16287, 23066,   862,
       33953, 39057,  3912,  9045, 11468, 29506, 12221, 34754, 45823,
       20800,  8466, 26771, 36666, 33405, 39823, 43611, 23412, 31270,
       15179,  2625, 39007, 21560, 33543,  1761, 28325, 42778, 30130,
       34644, 49853, 49516,  9877, 34031, 39680, 47240, 20268,    19,
        7232, 15064, 27803, 35672, 14973, 24510, 15810, 11310, 18494,
       33366, 29889, 15612, 17663,  4911, 48947, 26921, 14660,  9406,
       43210, 23686, 33411, 32768, 44675, 34823, 42888,  6749, 16442,
        9850,   680, 12879, 28722, 14951,  9785, 15039, 29973, 41707,
       32904, 44597, 28506, 26727, 31629, 48463, 16687,   770, 36296,
        7143,  4118, 18338, 22910,  3742, 31118,  4637, 43166, 13162,
        1176, 39678, 44602,  6046, 39639, 34694, 45355, 39637, 44393,
       30741, 26240, 45208,  8027, 47214, 11808,  2441, 40988, 37384,
       16246, 13591, 40137,  6508, 36949, 26603, 13958, 25386, 43755,
       46431, 38724, 44570, 36739, 16902, 41954, 41980, 33752, 44981,
       38133,  5906,  5378, 36151, 29479, 11148, 47206, 24878, 33410,
        4446, 27842, 16395, 13404, 18132, 31530, 19706, 27339, 10876,
        1862, 39707, 18996,  6775, 38301, 47894, 14158, 27857, 45095,
       33029, 41256, 12600, 10102, 30844, 47758, 40691, 26220, 37064,
       31372,  6474, 49330, 44057, 49257, 40491,  9387,  3268, 31695,
       25258, 45017, 22661, 47119, 46008,  6403, 24692, 40276,  8879,
       37420, 48602, 25238, 29919, 37798, 49025, 41678, 22712, 22649,
        1138,  2212, 34359, 22214, 47723, 22255, 12762,  7118,  4652,
        5297,  4313, 20239,  4848,  2803,  8222, 26463, 26898, 19238,
       49220, 46313, 15727, 38443, 11593, 49712, 45486, 10848,  5245,
       39334, 43705, 42032, 29782, 27982, 20000, 46015, 10763, 31628,
       20650, 34208, 42325, 38120, 14684, 43334, 15323, 23149, 29877,
       10346, 27976, 39383, 49446, 40136, 41207, 49960, 49144, 13695,
       17922, 27955,  2749, 34135,  6825,  8214, 45161, 15405, 13868,
       35540, 48805,   291,  9786,  4191,  9490, 15759, 16381, 11965,
       47342, 15429, 49042,  5463,  2663, 27536, 43409, 43161, 23342,
        8250,  3330, 31313, 32259, 42265, 40019, 35619, 36267, 18548,
       40805, 42749, 32611, 38237, 18822,  1617, 36579, 19622, 23617,
       38362, 40493, 32077, 29734, 18112, 36966, 48286,  4611, 43855,
       36295, 12207, 30388, 37832, 45839, 39106, 41957, 27243, 26703,
       48757, 42004,  5630, 22862, 41753, 17482,  1151,  4794, 29674,
        2666, 35177,  2109,  3740, 36933,  6139, 16792, 47000, 41543,
       23723, 43678, 14123, 28295, 35912, 45892, 15046, 29078, 43662,
       23538,  1702, 26156,  7507,  7108, 30123, 23438, 45853, 42024,
       48541, 22635, 41676,  1357, 17628,  4602, 22455,  6964,  2454,
       41555]), [1, 4, 6, 8]), (array([36549,  8103, 41592, 46390, 14059, 48503, 20804, 21775, 41996,
       19199, 15690, 30525,  6571,  9911, 39672,  4364, 11549, 22475,
       15337,  3097, 26585, 46221, 23358, 27589, 12315, 25476,  3006,
       47437, 35558, 39434, 46775, 24343, 48084, 40168, 28949, 36683,
         615, 20693, 49193, 20825, 13711,  7657,  7583, 28379, 46599,
        3755, 26150, 43566, 35749, 12483, 31324, 23819, 27569, 14545,
       31983,   438, 38651,  1893, 46848, 17905,  6111, 38328,  1504,
        4665, 34049,  6480, 37201, 37536, 18019, 20683, 16592,  7854,
       12008, 14467, 40257, 38414, 39830,  1049, 21470, 46296, 39440,
       21159,   672, 25094,  9382, 11173,  5591,  6129, 49880, 39884,
       11332,  1868, 24712, 41994, 19670, 49123,  2526,  3941, 43560,
       47175, 37058, 27675, 47641, 14326, 15616,  8295, 47086,  4768,
        2585, 41477, 14077, 24149, 12588, 20851, 19111, 40540, 38226,
       44678, 29186, 48292, 23383, 21520, 19076, 19432, 24987,  8728,
       35203, 32724, 10806,  3677, 22663, 44182, 39174, 26347, 16205,
       22497, 32273, 19976,  1263, 47036, 17747,  5650, 15159, 30785,
        3014, 18937, 29580, 24211, 35807, 41357, 35197,  5009, 33729,
       49670, 45146, 41329, 32133, 37780, 33856, 25648,   881, 22233,
       27287, 49370,  8323, 28709, 38789, 48654, 43882,  8561, 23141,
       46788, 43008, 46059, 18749, 12279, 41044,  2818, 40052, 40237,
       15993, 21289, 43901, 14083, 17163, 33547, 44871, 45844, 21010,
       34774, 15637,  6275, 27959,  4416, 13138,  2985, 20187, 17108,
       30161, 24494, 37040, 36059, 42101, 27058, 41316,  5532, 20613,
       14205, 37126, 11504, 24718, 39418, 43218, 15252, 19687, 46346,
       17109, 17709,  1706,  9715, 43696, 31905, 17435, 42154, 49087,
       40387,  7494, 42385, 29651, 15221, 28871,  1437, 38277, 39115,
       41181, 40229, 46744, 16488, 43141, 38280, 29349, 44901, 28141,
       23633, 43558, 48353, 21516, 45008, 15976,  2961, 49310, 14254,
       23310, 49162,   163, 20063, 15881,  9462, 33605, 21113, 43833,
       15539,  1927, 42951, 27824, 23773, 30066, 20971, 24090,  9581,
       23356, 47321, 46694, 47027, 32660,  7604, 20618, 36303,  4883,
       42386, 15999, 46283, 37679, 40426, 37159, 45630, 39354, 12365,
       11877, 25868, 36256, 30348,  9454, 49640,  4241, 10368, 28296,
       40837, 16175,  6748, 19089, 33374, 15324, 19691, 32073, 37884,
       36452, 21086, 30845, 24536, 25572, 10879, 13975, 47387, 18144,
        6119, 18443, 36936, 33936,   289, 23331, 29904, 29618, 23506,
       23330, 18975, 20879, 44278, 31073, 49058, 27913, 18620, 39246,
       34357, 32382, 16600, 17377, 26861,  9805,  7546, 20773, 28249,
       39168, 46455, 38616, 34440,  6600, 37474, 37660, 14556,  2370,
       45493, 31082, 22647, 45725, 10162, 14044, 38611,  3544, 35341,
       22965, 36977, 19037, 37573, 44115, 47856, 41755, 38196, 17499,
       18041, 26788, 25164, 46025, 17462, 22198, 31653, 35589,  6695,
        7385, 12925, 15290,  9294,  9891, 11781,  9329, 41397, 42366,
       17119,  3844, 31777, 29644, 35284, 14897, 26699, 45459, 32354,
       35290, 21268,  7544, 20010, 28812, 45939, 39307, 13482,  6301,
         916, 34518, 32973, 18022, 37019,   237,  6864, 23501,  4871,
       14641, 17436,  1908, 31461, 18997, 18350, 15047, 49695, 30163,
       16251, 26692, 13347, 32148, 22625, 33678, 34228, 12527, 13124,
       31597, 16633, 26086, 40579, 42895, 37211, 14983, 35812,  8481,
       25896, 18079,   152,  6051, 22612, 20166, 29669, 21493,  6792,
       49183, 36815,  2563,  6278, 47607,  4485, 31718, 38920,  5771,
       32545, 13514, 17649, 44471, 27672,  1197, 33341,  4928,  3251,
       44353, 49037,  5168,  5542, 19306, 23484, 25227, 21660, 28156,
       49903, 25957,  7482, 18313, 35056,  7314, 42368, 23456, 37100,
       13703, 12547, 36205,  7611, 22130, 49809, 31746, 20738, 22934,
       19600, 42485,  4386, 30670,  3157, 14236, 13796,  5796, 24897,
       20098,  7638, 27087,  3841,  2369, 21941, 11363, 36176, 32170,
         921, 39808, 40313,  2972, 46306,  6179, 29931, 19421,  3341,
       22266, 28061, 36716, 27610,  5723,  1453, 24989, 46850, 13441,
       24156, 10920, 20628, 38769, 39759,  4430, 44077, 18265,  7968,
       24215,  6521,  9245,  8983, 15631,  8171,  1819, 12550, 22290,
       38401,  2596, 49119,  1760, 32240, 17272, 11233,  1393, 19984,
       27872, 45836, 44244, 31794,  9482, 31421, 16675, 36061, 48079,
       31984, 38409, 46385, 13365, 12004, 12739,  8950, 44314, 40016,
       31142, 46193, 10256, 45863,  8253, 19588, 49776,  2179, 38397,
       21030,  3563,  7481, 12996,  5679, 42079, 33309,  6395, 37356,
       46273, 37682, 13261, 40927, 27611, 26340, 46492, 36913, 44068,
       45474,  1259, 16440,  2831, 23054, 38088, 21091,  2638, 23745,
       40436, 39969, 34304, 20986, 28042, 29456, 33551, 22525, 27367,
       13634, 27473, 11638, 14828,  2100, 34157, 47545, 29538,  5599,
        1550, 29958,  9468, 31133, 42527,  1246,  4605,  5177, 16606,
       39922, 46885,  4077, 14163, 49854, 19482,  6107, 42391, 15026,
       26701,  6962,  4786, 27577, 25612, 42358,  4606, 27503, 25935,
        5878, 46915, 42614, 25340, 36348, 25569, 41684, 47135, 37796,
       41399, 13619, 15048, 41350, 28859,  8633, 26122, 38391,  9974,
       26247, 18033, 22670, 40675, 34953,  7092, 24950, 40438, 26619,
        7315, 40047,  7001,  1741, 32856, 15458,   210, 19189, 26413,
          23, 16242, 31312, 31181, 33018,   721, 24213, 44011,   242,
       20254, 18050, 21146,  9339,  7267, 48526, 40568, 24895, 29935,
       14395, 39000,  9219, 43661, 29279, 12850, 43155, 30644, 12302,
       28776, 32339, 21438,  7348, 16639, 22068, 23030,  3403, 45385,
       32385, 38845,  2517, 20008, 38721, 45582,  1766, 25723,  4946,
       38137, 38227, 35098, 49459, 16059, 49804,  1837,  6768, 43538,
       32812, 20561, 35342, 30916, 46670, 22600, 36229, 49043, 44444,
       13582,  9818, 44927, 49551, 35468, 37258, 19124,  3881, 21755,
       26195, 16388, 42963,  5648, 30660,  2987, 30071, 43400, 17894,
       15698, 27515, 15268, 37329, 45536, 36985,  7834, 45743,  7131,
       15803, 44504, 36911,  4272, 11616,  4300, 39462, 48273, 43014,
       36650, 29030, 11091, 17531,  7366, 47094, 26643, 41165,  7996,
       30262,  3860, 47791, 22504, 30948, 18312, 16700, 46413, 19557,
       48311, 44528, 17933, 39367, 31981,  7440,   602, 20508, 47646,
       25749, 42897, 28115,  6238, 48158, 14821,  4955, 20081, 21298,
        3829,  6995,  5253, 29934, 20530, 13565, 13015, 40919, 40881,
       21671, 24716, 23031, 35419, 43159, 25669, 39419, 34893, 40292,
       11906, 48965, 46152,  5251, 44304, 35388, 48475, 23294,  8007,
        7886,  4761, 45024, 38881, 23281, 43540, 15022, 24286, 11207,
       32861, 27978,  2426, 49631, 39838, 26639, 35169, 14369,  8273,
       28855, 23045,  8952, 47713,  3165, 10245, 43381,  9378, 31625,
       19713, 23148, 21931, 49378, 38255, 13606, 48320, 38045, 20946,
        6337, 46012, 27858, 34212, 13504, 41264, 36443, 27699, 34171,
        5891, 27783,  9192, 36013, 13674, 42870, 21664, 48107,  7716,
       12585, 11149, 39089, 31917, 22518, 47596,   566, 41835, 40409,
       31795, 22462, 35496,  1964, 40470, 18461, 17405,  8852, 37643,
        9975, 26967,  9883, 47827, 35321, 33418, 35181, 47245, 25508,
       46028, 30018, 10120, 36097, 25042,  4283, 35295,  8672,  3384,
       48737, 15491, 20290, 32183, 15241,  2640, 46938, 16090, 22696,
        8468, 45309, 32783, 18444, 44681, 28928, 42933, 18892, 49542,
       46892, 19725, 32897, 44691,  6182, 48120, 24578, 45138, 45994,
       13131, 21596, 27507, 49372, 23728, 16942, 18795, 12951,    92,
       14464, 11461, 33104, 40575,  2176,  8211, 38382, 27252, 22616,
       47609, 24039, 26579, 20100, 20089, 38158, 19472, 34636, 49677,
       38214]), [9, 7, 6, 8]), (array([ 2260,   993, 34763, 37911, 39124, 49703, 43397, 20416,   681,
       21586, 13411, 43886, 11735, 13679, 44223,  1486, 10033, 41400,
       33552, 47211, 20083, 13103, 41587, 15403,  9174, 18552, 43330,
       18732,  3810, 36540, 22470, 45331,  6094, 44268, 15987, 25722,
       25670, 26479, 32255, 45260, 24899, 33136,  7072, 45991,  2657,
       10175,  5049, 12832, 35748, 27188, 20076, 20809, 35438, 45903,
       36952, 42537, 49548, 31303, 42538,  3008, 21889, 49399, 24663,
       39694, 31243, 25065, 38272,  5755, 49327, 43466,  4960, 18685,
       46017, 10897, 14405, 33423, 24226, 37790, 43063, 44312, 10709,
       30684, 32137, 37537, 42924,  5431, 30543, 26738, 39869,  5372,
       37740, 36587,  4319, 15849, 12567, 11953,  2532, 30356, 11495,
       37046, 15686, 12205, 11153, 24440, 19965, 10484,  9295, 23162,
        9729, 20477, 39539,  1691, 22478, 35640, 12480, 48566,  9450,
       30567, 14221, 48139, 41210, 17811, 22742, 49826, 48754, 49288,
        7518, 12716,  7873, 49859,  7699, 14477, 20922, 23233, 19786,
       34252,  2305, 48991, 49431, 23207, 26313, 30723, 19900,  9987,
        4572, 22830, 38997, 49579, 27701, 19437, 24287, 27844, 11314,
       27671,  8223, 24532,  4362,  5208, 39920, 27225,  9815, 38975,
        4153, 28090,  2242,  5780, 34634, 19735, 42686,  5905,  6982,
       13551, 46621, 21465, 15591, 20394,  7545, 31949, 36629, 31155,
       37151,  8730, 34941, 48181,  5962, 30028, 21352,  9337,   624,
       18038, 40014, 40721, 34303, 26028,  1134, 13930, 36246, 13345,
       38437, 19319, 12532, 16610, 39647,  3660,  7773, 19515, 11396,
       31190, 15861, 44740, 43796, 33707, 26728, 46720,  3616, 12135,
        5960,  3966, 35879, 29321, 25658, 18856, 25127, 46246, 21286,
        3249, 17293,  6778, 35410,   534, 39973, 48645, 21598, 21726,
       37486, 24714, 34074, 33313, 27462, 30105, 31781, 39446, 24952,
       44899, 12728, 11347, 15985,  8851, 16934, 47621, 42648, 22966,
       20556, 40288,  8186, 42986, 38741, 38287, 33771,  1812, 46642,
       49956, 21711, 29637, 42345, 36510, 13433, 35126, 17826, 12536,
       13777, 27546, 26630,  7807, 36293, 33829, 37305, 28343, 19337,
       24739, 46516, 25587, 10433, 20695,  1129, 37353, 47133, 31026,
        6976, 27430, 41208, 31999, 18485, 17239, 43122, 22726, 41856,
       33030, 46895, 13655,  8333, 12589, 29243,  4431, 13415,  7878,
       39887, 32850, 19163, 35157, 46214,  4471, 33580,  9446, 31721,
       38007, 41970, 26279, 15326, 44736,  6542, 49549,  7853,  2550,
       45692,  2983, 42602, 48014, 14017,  5707, 16232,  9144, 46527,
       44344,  3569, 35880, 24789,  3118,  7194, 33621,  6744, 17682,
       33932, 22797, 11766, 41890, 24313, 13757,  6488, 14768,   538,
        8264, 18204,  8534,  2377, 43118, 49099,  1500, 38451, 38806,
       26572, 30524, 22734, 26837, 18756, 20880, 19110, 24097, 23347,
        9989, 21316, 12978, 14915, 12669, 35320, 26234,  2291, 47508,
       15940,  7480, 33513,  6477, 31813, 12703, 25939, 40917, 47255,
       14655, 25830, 37578, 39935, 16352,  4375,  1067, 43450,  2091,
       16202, 22667, 35022, 49536, 34149, 31307, 32711, 44186, 43127,
       24643, 36095, 49020,  9350, 37821, 16327,   963, 36184,  7397,
       25650, 41273, 25149,  3423, 24272, 28708, 47903,  6317, 18564,
       21399, 42094, 10440, 36496, 21650, 25195, 36137, 17363, 36355,
       35477,  5781, 37840, 26956, 49274,  7477, 40716, 30493, 21145,
       32693, 37754, 28095, 23209, 24919, 15584, 23181, 17267, 12533,
       15977, 41498,  2033, 42891,  9933, 30791,  2626, 26629, 27764,
       22045,  2467,  8065, 39928, 37361, 26265,  1800,    54, 41599,
       22275, 16102, 39976, 16176, 29942, 42993, 35360, 24399, 44622,
       42532, 33929, 16165, 36242,  6106, 12161, 44652, 39983, 35255,
        1288,  1139, 12607, 31275, 41538,  5012, 31035, 42746, 31020,
       19153,  7929, 43421, 48885,  3476, 24717,  4824, 29253, 37284,
        1402,  4678, 13410, 44405, 11412, 15343, 23517, 45149, 28692,
       49364,  8431, 31342, 23230, 41320,  7137,  1194, 23764, 24176,
       25829, 13389,   347, 19000, 42509, 16008, 29802, 10203, 45453,
       21311, 33704,  3098, 37574,  9360, 48167, 22986, 25638, 33848,
       33536, 32606, 43258, 30526, 24864, 36188, 24743, 21264, 25519,
       11733, 43353, 49061, 27332, 36561, 20930, 21763, 34861, 14830,
       49200, 39200,  3976, 37077, 49747, 18653, 40519, 42291, 12014,
        4875, 40640, 23413, 24282, 27385, 30976, 19456, 18826, 48542,
       31685, 11303, 11429, 16097, 31598, 20830, 27464, 38738, 38747,
       30001,  8009,  4126, 40544, 15211, 31639, 31413, 37970, 49834,
       18571,  7795, 17697, 26139,     0, 20921, 15812, 21747, 12906,
       44163, 15038, 14345,  7073, 39630, 26387, 31808, 41476, 43459,
       26841, 46921, 23762, 46317, 36023,  2915, 15833, 42436, 34242,
       48203, 41064, 34045,  7226, 41621, 25681, 19184,  3290, 17555,
       21200, 19608,  6582,  1085,   755, 33466, 25526, 40437, 44999,
       28088, 46404, 33785, 33402, 13806,   361,  3986, 14027, 18965,
       28588, 33060, 10053,  1131, 28956, 48658, 37759,  1447, 43958,
       12034, 12218, 32949, 48072, 41326, 48047, 12031, 27229,  8589,
       25139, 44772, 12374, 30212, 42484, 36638, 24423, 19420, 34139,
       43986, 23734, 44014, 48022, 45958, 29327, 25815, 18542,  1248,
       41140, 34539,  6252, 20641, 15107, 30666, 27928, 25985, 11358,
       23375, 44128, 47653, 25795, 46066, 17075,  8108, 33924, 14133,
       35506,  5508, 23323, 18794, 33436, 31577, 23927,  7422,  1917,
       49928, 25493,  5122, 16489,  1637,  7393,  7321, 13184, 34716,
        8699, 42562, 44274,  3804, 31798, 32035, 49698, 40850,  7070,
       28490, 33541, 23876, 15489,    22,  2810, 48373, 35600, 37992,
       37933,  7967, 13916, 32628, 24397, 33381, 23563,  6246, 45759,
       33759, 22282, 49361, 16108, 17962,  7281,  1763, 46540, 21699,
       24085, 49090, 15619, 35150, 22010,  5255, 14208, 13350,  8270,
       33398, 12189, 13967, 41647, 25721, 13810, 11866, 20228,  7112,
       17854, 47999, 36636, 24608, 37681, 20467, 46048, 18295, 48125,
       12125, 44807, 40415, 18274, 31565, 18045,  5916, 32038,  3121,
       20395, 26214, 16558, 10572,  9722, 49625, 38239, 11117, 23858,
       41251, 29537, 20749, 35678,  5452, 48920, 16342, 47112, 15617,
       48872, 13738,  2658, 18367, 31187,   793, 38458, 10068, 34404,
       49108, 34834, 26563, 36555, 25780, 36024, 32092, 32356, 21953,
       10354, 17824, 49216, 10228,  9545, 36708, 12258, 10708, 12866,
        5222, 42822, 10395,  4514, 32370, 31553,  3333,  7646, 43913,
       16676,  5667,  4380,  3324, 43191,  2974, 34419,  3269, 11042,
       24631,  5280, 15835, 29762, 35035,  9154, 43035, 33158, 20082,
       21434,  6788,  6631, 27669, 38524,  9913,  1853, 29371, 10688,
       46668, 27640,  1404, 23509,  8401, 31489, 18327, 31066, 13545,
       27747,  1014, 34310, 27902,  8717, 43175, 32234, 16042, 44438,
        9306, 46707, 16830, 19618, 47842, 48124, 18675, 22589, 46292,
       47236, 36744,  5977,  1325, 26927,  1775, 20118, 37808, 35489,
       32275, 25072, 43985, 14281, 34295, 21140, 11200, 14211, 11674,
        2344, 36804, 49180,  4807, 35621, 43101,  6667, 34021,  4863,
        2471, 37060, 18530,   736, 34527, 21884,  5687, 24688, 48794,
       44362,  7046,  4974, 40547, 31052,  6977, 42449, 11798, 30770,
       32501,  1673, 42842, 26239, 14804, 42847, 48374, 10817, 23707,
       13388, 26306, 35808, 20500, 45461,  1468, 18853,  2847, 15688,
       42763, 11141, 43807, 47992,  6632, 18728, 27864, 22000, 49746,
       38374, 41447,  5777, 39015, 27490, 33815, 39098,  3353, 12426,
       47814, 15334, 23838, 19131, 29530, 22471, 40139, 20052, 27386,
       47506,  3950, 46764, 14260, 46854, 18104, 17246, 18501, 31196,
       27988]), [5, 2, 6, 8])]
Competition
DC 0, val_set_size=1000, COIs=[0, 3, 6, 8], M=tensor([0, 3, 6, 8], device='cuda:0'), Initial Performance: (0.259, 0.04455009317398071)
DC 1, val_set_size=1000, COIs=[1, 4, 6, 8], M=tensor([1, 4, 6, 8], device='cuda:0'), Initial Performance: (0.222, 0.04445403122901916)
DC 2, val_set_size=1000, COIs=[9, 7, 6, 8], M=tensor([9, 7, 6, 8], device='cuda:0'), Initial Performance: (0.222, 0.04478240823745728)
DC 3, val_set_size=1000, COIs=[5, 2, 6, 8], M=tensor([5, 2, 6, 8], device='cuda:0'), Initial Performance: (0.331, 0.04419129967689514)
D00: 1000 samples from classes {8, 6}
D01: 1000 samples from classes {8, 6}
D02: 1000 samples from classes {8, 6}
D03: 1000 samples from classes {8, 6}
D04: 1000 samples from classes {8, 6}
D05: 1000 samples from classes {8, 6}
D06: 1000 samples from classes {0, 3}
D07: 1000 samples from classes {0, 3}
D08: 1000 samples from classes {0, 3}
D09: 1000 samples from classes {0, 3}
D010: 1000 samples from classes {0, 3}
D011: 1000 samples from classes {0, 3}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {9, 7}
D019: 1000 samples from classes {9, 7}
D020: 1000 samples from classes {9, 7}
D021: 1000 samples from classes {9, 7}
D022: 1000 samples from classes {9, 7}
D023: 1000 samples from classes {9, 7}
D024: 1000 samples from classes {2, 5}
D025: 1000 samples from classes {2, 5}
D026: 1000 samples from classes {2, 5}
D027: 1000 samples from classes {2, 5}
D028: 1000 samples from classes {2, 5}
D029: 1000 samples from classes {2, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO1']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.06674954384565353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06847400909662246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.08236931937932968) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09157281869649887) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.445, 0.06579716289043426) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.08552074305713177) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.08714920091629029) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.307, 0.12057992538809777) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.08181295055150986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.10531247013807296) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.10465574797987938) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.16387051099538802) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.45, 0.11633282428979874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.385, 0.123072186216712) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.1520442082285881) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.16512881445884706) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.14339558928459883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.415, 0.1408000374212861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.1840640387907624) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.20105758833885193) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO3']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.441, 0.16513536177948118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.414, 0.16343627533689142) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.2134882289879024) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2007472256422043) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.16899262822791933) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.1821278923470527) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.2760615418329835) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.20005470670759679) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.19086242514848709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.17739180225878953) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.30164285308122635) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.2237542724609375) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.2003168370425701) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.19552786941453815) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.34175975008215753) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.2044250428378582) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.454, 0.20181888235360385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.18588235468231143) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.350616532756947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.258475079447031) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.20314836711622775) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.20381194679252804) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.3502320797201246) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.2532069249004126) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.22967228427156805) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.458, 0.19652947641164065) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.398354775568936) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.25697868639230725) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.22887394103966655) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.481, 0.20223571470938623) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.36694822133192795) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.26074466176331046) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.22317315349448472) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.465, 0.22175838453043253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.3242565594082698) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.2838718731701374) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.2215735500343144) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.477, 0.23038359759002924) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.37038345385319554) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.417, 0.29139304313063624) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.23555392096191644) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.22532344469800591) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.3983980042770272) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.28364160495996477) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.2281944486182183) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.26203676298540085) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.3575886360381264) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.2500907669514418) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.23408992271497844) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.2544789350125939) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.35097223456506615) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.3150567192733288) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.2357151872292161) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.2721301699643955) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.3450698308038991) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.2801826041340828) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.22841387190110982) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.2527237041322514) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.362713147010887) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.2935568973124027) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.2333081962969154) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.477, 0.28547438004706055) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.3427078392263502) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.28669420623779296) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.22225835842918604) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.23957571780122816) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.3506223554044263) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.422, 0.31239167219400404) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.20171175234019756) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.477, 0.2697113108183257) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.38468196071917193) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.2880002975165844) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.2235451211128384) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.2606926901685074) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.35519314402434976) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.307943444699049) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.22108109498769044) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.477, 0.26834578534401954) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.36793676527336355) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.29085979479551316) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.22848778546601534) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.481, 0.2863999698460102) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.35774049184739126) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.418, 0.3075387791395187) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.21808726634737105) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.477, 0.25802304023783657) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.31981823696848005) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.3083328303396702) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.2351549148019403) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.30139585367031396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3889235372582916) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.42, 0.2803436834216118) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.22350290149194188) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.29668230764591136) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.3369788053849479) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.3132623278796673) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.22961218738253228) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.26053879896271975) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.30372872978192755) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.32207940369844434) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO1', '(DO2']
DC 2 --> ['(DO3']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.24245599308470264) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.25789243026357145) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.29898794146970614) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.42, 0.26905355644226076) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.238316007100977) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.2642186013190076) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.36374686462146927) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.2977497322261333) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.449, 0.2427648955527693) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.481, 0.29641188693558795) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.2973495364882983) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.3355833460688591) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.26760310388484504) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.26505220235232263) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.3257206975184381) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.3091136003434658) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.25777921454701574) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.22014222188666463) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.3117095185769722) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.29542170715332033) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.45, 0.2632093209384475) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.2909797939795535) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.302960604725522) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.418, 0.28979805921018126) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.24263956667669118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.482, 0.22986642432166263) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.34647090795647817) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.3319586725831032) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.2586773467748426) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.253104943764396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.34961660420172846) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.417, 0.2730318145751953) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.2813755847197026) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.2230870005940087) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.32251350538543194) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.3327617424428463) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.2917609363935189) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.22415035047661513) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.32530813200911507) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.42, 0.3171914424300194) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO1']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.454, 0.27240411105100065) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.23576857937872409) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.3290393456596357) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.3288188182413578) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.2721108095364179) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.21916498777549714) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.30194721156544985) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.32013435012102126) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.2814439936403651) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.24127840478532017) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.2857380020227283) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.2944237052202225) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.28385167244810144) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.2194213560919743) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.29024134629545734) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.2800550048351288) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.301411773186177) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.2619611202967353) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.3259134705925244) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.424, 0.2494185294508934) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO5']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.25125886540533976) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.25101818910473955) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.34811829933384436) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.28753864073753355) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.27588024063943883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.24231220842618495) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.321979376161471) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.409, 0.2866790091097355) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.2686960128117353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.22688323221495374) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.3218891738431412) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.31233899617195127) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.26007966944482175) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.2032165769133717) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.3052460718565853) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.422, 0.2800768764913082) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.27760994831915015) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.481, 0.21832690579490735) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.31018312722293195) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.2828874658048153) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.259, 0.04455009317398071), (0.269, 0.06674954384565353), (0.445, 0.06579716289043426), (0.451, 0.08181295055150986), (0.45, 0.11633282428979874), (0.447, 0.14339558928459883), (0.441, 0.16513536177948118), (0.452, 0.16899262822791933), (0.453, 0.19086242514848709), (0.448, 0.2003168370425701), (0.454, 0.20181888235360385), (0.461, 0.20314836711622775), (0.459, 0.22967228427156805), (0.459, 0.22887394103966655), (0.457, 0.22317315349448472), (0.471, 0.2215735500343144), (0.461, 0.23555392096191644), (0.462, 0.2281944486182183), (0.465, 0.23408992271497844), (0.461, 0.2357151872292161), (0.458, 0.22841387190110982), (0.467, 0.2333081962969154), (0.466, 0.22225835842918604), (0.459, 0.20171175234019756), (0.46, 0.2235451211128384), (0.463, 0.22108109498769044), (0.464, 0.22848778546601534), (0.457, 0.21808726634737105), (0.462, 0.2351549148019403), (0.458, 0.22350290149194188), (0.457, 0.22961218738253228), (0.456, 0.24245599308470264), (0.459, 0.238316007100977), (0.449, 0.2427648955527693), (0.457, 0.26760310388484504), (0.455, 0.25777921454701574), (0.45, 0.2632093209384475), (0.458, 0.24263956667669118), (0.456, 0.2586773467748426), (0.46, 0.2813755847197026), (0.458, 0.2917609363935189), (0.454, 0.27240411105100065), (0.46, 0.2721108095364179), (0.455, 0.2814439936403651), (0.457, 0.28385167244810144), (0.457, 0.301411773186177), (0.451, 0.25125886540533976), (0.457, 0.27588024063943883), (0.452, 0.2686960128117353), (0.46, 0.26007966944482175), (0.462, 0.27760994831915015)]
TEST: 
[(0.26825, 0.04347268170118332), (0.26725, 0.06421797105669975), (0.4395, 0.06346339270472527), (0.4565, 0.07869957354664803), (0.4475, 0.11173120200634003), (0.457, 0.1374445474743843), (0.4525, 0.15964710181951522), (0.458, 0.1622095382809639), (0.46325, 0.18300087195634843), (0.46175, 0.19399894100427628), (0.45875, 0.1961120474934578), (0.463, 0.19762386679649352), (0.4635, 0.2218723863363266), (0.46475, 0.22008768916130067), (0.46425, 0.21760210692882537), (0.46825, 0.21381520307064056), (0.46625, 0.2259929324388504), (0.467, 0.2204107118844986), (0.46825, 0.2256022847890854), (0.4675, 0.22724632847309112), (0.46725, 0.2223141884803772), (0.465, 0.22239289128780365), (0.4675, 0.21282221555709838), (0.46125, 0.19562257361412047), (0.463, 0.21487575405836104), (0.468, 0.21570199704170226), (0.46625, 0.22183632135391235), (0.465, 0.21401798284053802), (0.467, 0.23057602059841156), (0.46825, 0.22056816303730012), (0.465, 0.2238146390914917), (0.467, 0.23631687712669372), (0.466, 0.2320278980731964), (0.461, 0.23697450470924378), (0.46525, 0.2644848874807358), (0.46325, 0.25402417695522306), (0.45975, 0.2594813474416733), (0.465, 0.23455090343952179), (0.46175, 0.25513279139995576), (0.4645, 0.2757367709875107), (0.4655, 0.2831926050186157), (0.463, 0.26530113315582277), (0.4635, 0.26638300287723543), (0.4625, 0.2733460671901703), (0.463, 0.2756869839429855), (0.46325, 0.2872099657058716), (0.459, 0.24593732309341432), (0.462, 0.2748783963918686), (0.464, 0.26121630215644837), (0.46625, 0.2541531649827957), (0.4665, 0.2691489964723587)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.54      0.90      0.67      1000
           3       0.41      0.97      0.58      1000
           6       0.00      0.00      0.00      1000
           8       0.00      0.00      0.00      1000

    accuracy                           0.47      4000
   macro avg       0.24      0.47      0.31      4000
weighted avg       0.24      0.47      0.31      4000

Competition_DC_1
VAL: 
[(0.222, 0.04445403122901916), (0.25, 0.06847400909662246), (0.256, 0.08552074305713177), (0.33, 0.10531247013807296), (0.385, 0.123072186216712), (0.415, 0.1408000374212861), (0.414, 0.16343627533689142), (0.442, 0.1821278923470527), (0.462, 0.17739180225878953), (0.471, 0.19552786941453815), (0.463, 0.18588235468231143), (0.467, 0.20381194679252804), (0.458, 0.19652947641164065), (0.481, 0.20223571470938623), (0.465, 0.22175838453043253), (0.477, 0.23038359759002924), (0.463, 0.22532344469800591), (0.459, 0.26203676298540085), (0.486, 0.2544789350125939), (0.467, 0.2721301699643955), (0.475, 0.2527237041322514), (0.477, 0.28547438004706055), (0.474, 0.23957571780122816), (0.477, 0.2697113108183257), (0.474, 0.2606926901685074), (0.477, 0.26834578534401954), (0.481, 0.2863999698460102), (0.477, 0.25802304023783657), (0.469, 0.30139585367031396), (0.472, 0.29668230764591136), (0.48, 0.26053879896271975), (0.479, 0.25789243026357145), (0.474, 0.2642186013190076), (0.481, 0.29641188693558795), (0.471, 0.26505220235232263), (0.479, 0.22014222188666463), (0.479, 0.2909797939795535), (0.482, 0.22986642432166263), (0.485, 0.253104943764396), (0.484, 0.2230870005940087), (0.485, 0.22415035047661513), (0.484, 0.23576857937872409), (0.476, 0.21916498777549714), (0.48, 0.24127840478532017), (0.484, 0.2194213560919743), (0.483, 0.2619611202967353), (0.484, 0.25101818910473955), (0.48, 0.24231220842618495), (0.483, 0.22688323221495374), (0.485, 0.2032165769133717), (0.481, 0.21832690579490735)]
TEST: 
[(0.218, 0.04341068071126938), (0.25, 0.06590802815556526), (0.252, 0.08200803625583648), (0.3275, 0.10096185243129731), (0.392, 0.1177846822142601), (0.42225, 0.1349279899597168), (0.42825, 0.15713550812005997), (0.44525, 0.175697412610054), (0.46675, 0.1712297403216362), (0.47075, 0.19024631458520888), (0.4625, 0.18031305837631226), (0.472, 0.19747530728578566), (0.46025, 0.18930306655168533), (0.48075, 0.19672309201955795), (0.4565, 0.2136664024591446), (0.47475, 0.22424467623233796), (0.45925, 0.21570668041706084), (0.4565, 0.2503879601955414), (0.48275, 0.2531699403524399), (0.46275, 0.2604221730232239), (0.471, 0.24602564764022827), (0.47825, 0.27708215272426606), (0.474, 0.23525769209861755), (0.47875, 0.2656977746486664), (0.478, 0.25712952673435213), (0.4795, 0.2600780161619186), (0.4815, 0.2843819305896759), (0.47475, 0.25069663214683535), (0.46975, 0.29097693860530854), (0.472, 0.28855548095703126), (0.48075, 0.25464666104316713), (0.4815, 0.25037656497955324), (0.47375, 0.25517997896671296), (0.48225, 0.2866550005674362), (0.47125, 0.2536519380807877), (0.47875, 0.21311330258846284), (0.4825, 0.2850216830968857), (0.481, 0.22127950859069825), (0.481, 0.24569425821304322), (0.481, 0.21821145462989808), (0.484, 0.21717850852012635), (0.48475, 0.2257467690706253), (0.478, 0.20891218256950378), (0.4835, 0.23139478385448456), (0.4825, 0.2111913474202156), (0.48425, 0.2524998686313629), (0.48325, 0.24212807583808899), (0.48225, 0.23294099009037017), (0.484, 0.22034783720970152), (0.48475, 0.19653196173906326), (0.47975, 0.21063942945003508)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.70      0.92      0.80      1000
           4       0.37      1.00      0.54      1000
           6       0.00      0.00      0.00      1000
           8       0.00      0.00      0.00      1000

    accuracy                           0.48      4000
   macro avg       0.27      0.48      0.33      4000
weighted avg       0.27      0.48      0.33      4000

Competition_DC_2
VAL: 
[(0.222, 0.04478240823745728), (0.442, 0.08236931937932968), (0.397, 0.08714920091629029), (0.462, 0.10465574797987938), (0.472, 0.1520442082285881), (0.475, 0.1840640387907624), (0.472, 0.2134882289879024), (0.468, 0.2760615418329835), (0.469, 0.30164285308122635), (0.479, 0.34175975008215753), (0.474, 0.350616532756947), (0.476, 0.3502320797201246), (0.475, 0.398354775568936), (0.481, 0.36694822133192795), (0.471, 0.3242565594082698), (0.481, 0.37038345385319554), (0.481, 0.3983980042770272), (0.476, 0.3575886360381264), (0.479, 0.35097223456506615), (0.479, 0.3450698308038991), (0.478, 0.362713147010887), (0.48, 0.3427078392263502), (0.475, 0.3506223554044263), (0.478, 0.38468196071917193), (0.479, 0.35519314402434976), (0.481, 0.36793676527336355), (0.479, 0.35774049184739126), (0.481, 0.31981823696848005), (0.474, 0.3889235372582916), (0.48, 0.3369788053849479), (0.479, 0.30372872978192755), (0.478, 0.29898794146970614), (0.484, 0.36374686462146927), (0.479, 0.2973495364882983), (0.481, 0.3257206975184381), (0.48, 0.3117095185769722), (0.479, 0.302960604725522), (0.48, 0.34647090795647817), (0.479, 0.34961660420172846), (0.477, 0.32251350538543194), (0.477, 0.32530813200911507), (0.481, 0.3290393456596357), (0.481, 0.30194721156544985), (0.48, 0.2857380020227283), (0.481, 0.29024134629545734), (0.477, 0.3259134705925244), (0.48, 0.34811829933384436), (0.477, 0.321979376161471), (0.473, 0.3218891738431412), (0.479, 0.3052460718565853), (0.477, 0.31018312722293195)]
TEST: 
[(0.23925, 0.04380939894914627), (0.4305, 0.07938512188196183), (0.4055, 0.0837293835580349), (0.463, 0.1001882000863552), (0.472, 0.1443197346329689), (0.477, 0.17273092937469484), (0.4735, 0.20166757559776305), (0.474, 0.25924735152721406), (0.47175, 0.28262863409519196), (0.47725, 0.32329213321208955), (0.47725, 0.3337722965478897), (0.47775, 0.33196855628490446), (0.47775, 0.3768606812953949), (0.476, 0.35096474111080167), (0.477, 0.3071118451356888), (0.4785, 0.3526632809638977), (0.48025, 0.3784528886079788), (0.47875, 0.3395159616470337), (0.478, 0.33045032060146334), (0.47975, 0.3298676890134811), (0.4785, 0.3474553271532059), (0.4795, 0.32711876595020295), (0.479, 0.3354535572528839), (0.47775, 0.3619641044139862), (0.48, 0.34089178705215456), (0.47925, 0.35078774559497833), (0.47925, 0.34254520070552824), (0.4805, 0.3057555449008942), (0.4775, 0.36904461896419527), (0.4795, 0.3225266844034195), (0.48, 0.28920022535324097), (0.47925, 0.28075439846515654), (0.47425, 0.34409443151950836), (0.47825, 0.27965405774116514), (0.47825, 0.3108972347974777), (0.479, 0.29357099986076357), (0.478, 0.2896999340057373), (0.47575, 0.32775276696681976), (0.479, 0.3267621203660965), (0.479, 0.30745562756061556), (0.48075, 0.30696696281433106), (0.48, 0.31392814886569975), (0.4785, 0.2824258905649185), (0.47925, 0.26795816886425017), (0.48125, 0.27229437255859373), (0.4815, 0.30683312118053435), (0.476, 0.32952615928649903), (0.481, 0.30040061151981357), (0.48125, 0.30135976779460905), (0.4815, 0.288005978345871), (0.4795, 0.28971176528930664)]
DETAILED: 
              precision    recall  f1-score   support

           6       0.00      0.00      0.00      1000
           7       0.58      0.95      0.72      1000
           8       0.00      0.00      0.00      1000
           9       0.41      0.97      0.58      1000

    accuracy                           0.48      4000
   macro avg       0.25      0.48      0.32      4000
weighted avg       0.25      0.48      0.32      4000

Competition_DC_3
VAL: 
[(0.331, 0.04419129967689514), (0.257, 0.09157281869649887), (0.307, 0.12057992538809777), (0.398, 0.16387051099538802), (0.394, 0.16512881445884706), (0.405, 0.20105758833885193), (0.408, 0.2007472256422043), (0.413, 0.20005470670759679), (0.419, 0.2237542724609375), (0.413, 0.2044250428378582), (0.404, 0.258475079447031), (0.401, 0.2532069249004126), (0.416, 0.25697868639230725), (0.41, 0.26074466176331046), (0.425, 0.2838718731701374), (0.417, 0.29139304313063624), (0.408, 0.28364160495996477), (0.414, 0.2500907669514418), (0.405, 0.3150567192733288), (0.413, 0.2801826041340828), (0.413, 0.2935568973124027), (0.416, 0.28669420623779296), (0.422, 0.31239167219400404), (0.407, 0.2880002975165844), (0.414, 0.307943444699049), (0.414, 0.29085979479551316), (0.418, 0.3075387791395187), (0.415, 0.3083328303396702), (0.42, 0.2803436834216118), (0.411, 0.3132623278796673), (0.41, 0.32207940369844434), (0.42, 0.26905355644226076), (0.407, 0.2977497322261333), (0.413, 0.3355833460688591), (0.406, 0.3091136003434658), (0.408, 0.29542170715332033), (0.418, 0.28979805921018126), (0.4, 0.3319586725831032), (0.417, 0.2730318145751953), (0.413, 0.3327617424428463), (0.42, 0.3171914424300194), (0.415, 0.3288188182413578), (0.408, 0.32013435012102126), (0.416, 0.2944237052202225), (0.423, 0.2800550048351288), (0.424, 0.2494185294508934), (0.416, 0.28753864073753355), (0.409, 0.2866790091097355), (0.415, 0.31233899617195127), (0.422, 0.2800768764913082), (0.423, 0.2828874658048153)]
TEST: 
[(0.3205, 0.04325020852684975), (0.2555, 0.08785385882854461), (0.29475, 0.11601749670505523), (0.39875, 0.15733955764770508), (0.3965, 0.1590762510895729), (0.403, 0.1939288192987442), (0.40575, 0.19325960141420365), (0.40525, 0.19291542088985444), (0.412, 0.21559283995628356), (0.412, 0.19505343741178513), (0.411, 0.25018797516822816), (0.40375, 0.24398999881744385), (0.41475, 0.246450012922287), (0.40525, 0.2523914009332657), (0.41775, 0.27191101837158205), (0.41475, 0.2816670107841492), (0.41375, 0.2690245109796524), (0.41925, 0.23708668649196624), (0.41825, 0.3007057189941406), (0.41475, 0.2698317668437958), (0.415, 0.28220335924625395), (0.41775, 0.2730367382764816), (0.421, 0.3023562175035477), (0.40925, 0.2763366940021515), (0.412, 0.2951354792118073), (0.41425, 0.2800241689682007), (0.4165, 0.2975489447116852), (0.41525, 0.29494871890544894), (0.4175, 0.26721937119960787), (0.41675, 0.29902020025253295), (0.421, 0.3152046949863434), (0.41825, 0.25959569978713987), (0.41875, 0.28611071038246155), (0.41925, 0.32258445930480956), (0.41975, 0.30062777721881867), (0.4175, 0.2899896849393845), (0.41725, 0.28817776024341585), (0.407, 0.31807116425037385), (0.4155, 0.2675793021917343), (0.41825, 0.32380161428451537), (0.41475, 0.3034878709316254), (0.4185, 0.31645298600196836), (0.415, 0.3102433650493622), (0.41775, 0.2882400131225586), (0.41675, 0.2718219348192215), (0.41775, 0.24316312515735627), (0.41825, 0.27991255712509155), (0.416, 0.27859258544445037), (0.4135, 0.29605001711845397), (0.417, 0.27181203293800354), (0.41675, 0.2789647134542465)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.34      0.86      0.49      1000
           5       0.55      0.81      0.66      1000
           6       0.00      0.00      0.00      1000
           8       0.00      0.00      0.00      1000

    accuracy                           0.42      4000
   macro avg       0.22      0.42      0.29      4000
weighted avg       0.22      0.42      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [104]
name: no-alliance-104
score_metric: contrloss
aggregation: <function fed_avg at 0x702219175c10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=104
Partitioning data
[[5, 9, 0, 3], [4, 1, 0, 3], [6, 2, 0, 3], [7, 8, 0, 3]]
[(array([29961, 11298,  5755, 34458, 42818,  8742, 20483, 38411, 20171,
         767, 11024, 14802, 11949, 32795, 30095, 15575, 40417, 30072,
       43979, 47688, 40721, 25662, 30618, 14004,  7435, 10429, 35189,
       38600,  2071, 35606,  6017, 37958, 20615, 33853, 13316, 35905,
       16623, 30098, 27844, 37451,  3495, 35602, 35712, 15606, 41318,
       16358, 43338, 22445, 24648, 18626, 23392, 11269, 35053, 38437,
        5853, 35652, 23623, 47167,  7926, 34183, 21561, 45581, 48303,
        8929, 37874, 26503, 46901, 23146,  9284, 17778, 18845, 38369,
       23385, 48701, 26747, 47024, 49638,  8851, 21610, 47555, 24819,
       11740, 21586,  9383,  7601,  7417, 41942,  1717, 43495, 11928,
       16282,  9286, 35534,  9594,  2410, 32468, 24981, 46606, 12239,
       15685,  3229,  7090, 25262,  5606, 40490,  3471, 45698, 35501,
       15849, 32919, 43387, 33138, 48454, 40561, 32926, 46684, 34008,
       44371, 27679, 43759, 30260, 26095, 43878, 34048, 43852, 47810,
       49472, 28460, 45728, 29896,  4002, 48730, 48139, 15130, 14677,
       45184,  1911, 25981, 33818, 39002, 30063, 28845,  3794, 37680,
       13624, 18913, 35510, 44435, 46074, 11405, 48750, 24196, 31145,
       19150, 28652, 30757, 21543, 27345, 16626, 41376, 40044, 12177,
       20122, 22679,  2877, 21910, 23234, 47988, 18167, 23724, 44904,
       36035, 46925,  3653, 35993, 46267, 22999, 37471, 47920, 44094,
       48537, 49927, 41082,  5736, 31791, 45206, 44106, 30085, 45503,
       13960,  8243,   607, 47054, 37366, 18764, 18319, 20207, 10703,
       29110, 23614, 17332, 42745,   988, 41062, 25248, 36132, 38202,
       49920, 42323, 47258, 31031, 19735, 49848,  2361, 48632, 11725,
       42533, 36653, 24295, 28736, 40765, 22390, 34763, 27359, 49052,
       22061, 34941, 27885, 20460, 26589,  5970, 30134, 31222, 40140,
        4494, 49428, 17773, 27956, 45583, 36952, 15775, 49431,  5288,
       19201, 46187, 44150, 47485, 36439,  1662, 18230, 33778, 15785,
       10375, 20172, 37050, 41580, 26822, 25010, 49779, 22366,  3032,
       18860,  2758, 32595, 23798,  8826, 13906, 38331,  8364, 32489,
       10622, 29458, 24537, 31605, 13078, 37547, 41169, 15142, 49369,
       22415, 42627,  6178, 18749, 23877, 29431, 21273, 40508, 13669,
       12793,  9425, 28907, 44714,  5801, 38378, 35057,  6150, 49360,
        2208, 38820, 49793, 38827, 41976, 18213,  5591, 37094, 19824,
       40040,  1893,  7441, 44009, 14008, 42591, 44088,   360,  5244,
       33432, 29115,   664, 27237, 33784, 19505, 12152, 39510, 34943,
       39025, 22959, 43997, 29551, 12680, 33224, 32266, 36031, 40931,
       24609, 42159,  3479, 19766, 28713, 24731, 40215,  6643,  5621,
        2302, 47511, 26289, 34502,  1078, 27837, 32635, 49112, 43695,
       13364,  7556, 23651,  4289, 45876, 17709, 30392, 43839, 40735,
       26393, 16533, 19934, 21537,  6357,  2477, 39041,  4749, 34049,
       48544, 13767, 28850, 39280, 39801, 14458, 24987, 13074, 26113,
       39731, 16818, 10231, 44410, 35203, 47426, 34147, 29672, 38314,
       23225,  3915, 28915,  9466, 16614, 46269, 23770, 32745, 17544,
       40722,  3061, 28899, 30178, 10751, 42704, 24683, 39557, 30175,
       29418, 47076,  7698, 15096, 21237, 15586, 29651, 24603, 36329,
       45735, 23129, 41054, 38561, 15901, 10523, 45463,  1572,  8716,
       24960, 34129, 44947,   512, 26447, 12116,   406, 39870, 32838,
       21688,  4675, 22815, 10161, 24042,   763, 18430, 46195, 47025,
       25965, 26880, 30503, 39572, 15566, 23359,  5881, 19474, 30771,
       32902, 47469,  2609, 23091, 41683, 31453, 13421, 47985,  5622,
        9947, 45791, 44996, 48751,  1115, 13491, 32701, 26852, 44332,
        7652, 27287, 40915, 34640, 26600,  1906, 47709, 40037, 36401,
       45048, 34485, 27899,  6201, 22346, 39894, 33928, 15252, 40693,
       35746,  5234,  6268, 16708, 29529, 19380, 21506, 32575, 48345,
       30758, 37242,  2880,  9990, 28931, 24581, 25624, 35750, 17196,
       47916, 19390, 33960, 24404, 45666, 31726, 11913, 44771, 43324,
       30114,  5719, 10197,  2341, 28972, 19286, 16238, 25920, 10911,
       45297, 27487,   185, 37867, 12634, 18893, 12764, 34783, 16825,
       43989, 23002, 39170, 23796, 16219, 35796, 29307,  7788,  3842,
       10847,  8741,  3292, 46576,   527, 31728, 21878, 27047, 45067,
        9819,  7731, 17082, 37962, 49359, 24356, 43289, 18763, 34568,
       29578, 38041,  6954, 11325, 32471, 24257, 24587, 35731,  3329,
       38044, 29927, 48986, 28724, 41762, 21469, 28272, 42711, 44645,
       10650, 16391, 43657, 46523, 10130,  8857, 20686, 28467, 38043,
        3789,  3689, 28570, 41855, 40703, 22557, 37358, 41146, 17352,
        5414, 13419, 30378, 22299,  1243, 12916, 41222, 48324, 31471,
       27301, 13260, 13966, 15258, 12922, 28833,  6833, 28515, 47651,
       13660, 26644, 10499, 10159, 18768,  5770, 41435, 38963, 39329,
       10275,  6242, 32905, 11372, 47525, 40802, 34259, 47906, 22167,
         557,  3277,  8079,  8018, 34722, 20454, 25212,  8759, 21439,
       41258,  1306, 13368, 41797, 42992, 26704, 13174, 35369, 34234,
         695, 44864, 44624, 27460, 15827, 41274, 24395, 48852,  2053,
       31600, 25748, 15483, 48147,  9193, 29053, 16048, 15304, 10954,
        1319, 32631,  2145, 11122, 15848, 39392,  9697, 17432, 41038,
       21092,    77, 28092, 49633, 10545, 41040,   555, 41745, 18358,
       41907, 49219, 24072, 19492, 49837, 35089, 31412, 36464, 17257,
       19294, 21288, 36955, 40432, 24365, 12944, 37068, 16225, 10782,
       31863, 30968,  4721,  2451, 35658, 21143, 31391, 38478, 49308,
       31976, 22970, 15542, 22449, 47925, 37049, 16552, 27855, 38828,
       44296, 33128, 11768, 21275, 22628, 42309, 21087, 16196, 17947,
       19664, 14867, 11975, 35039, 12430, 39574, 39763, 13401, 36606,
       40851, 23528, 34109, 20287, 30271, 48671, 35690, 19974, 37102,
       23851, 24764,   940,  7389, 12145, 38040, 41021, 42382, 15210,
       15444, 38442, 40378, 34204, 10293,  6496, 39211, 11205,  4558,
       18371, 21373, 23715, 34536, 47995, 45949, 21687, 47284, 38048,
       40651,  5493,  9904, 43551, 19922, 13329, 45552, 40587,  8380,
       36948, 36930, 25380, 33049, 21728, 23389,  9183,  1963,  4166,
       38095,  8356, 47146,  1405, 45885,  7426, 26964, 46094, 43813,
       34011, 37966, 20139, 30347, 49322, 16680,  6482, 44399, 42122,
       45631, 34967, 34517, 20630, 48200, 40249, 11628, 42800, 32172,
       15595, 28020, 27796,  7997, 31148, 40127, 24077, 43598, 46817,
       27734,   416, 46286, 15383, 41330, 25019, 21366, 37863, 33841,
       32180,  2337, 44530,  5329, 31698, 43402,  5478, 44084,   377,
       20050, 48306, 42357, 10135, 20639, 25947,  3340, 32520, 41016,
       32219, 18769, 12500, 20204, 32379, 34155, 21783, 20770, 45908,
       11067,  1895,    91, 10653, 33894,  4294, 43784, 35832, 30461,
        6061, 43336,     9, 25882, 34742, 16714, 11366, 48039, 20271,
       41372, 22611, 28373, 16817, 39151, 37217, 16405, 20529, 37408,
       17930, 18127, 26716, 16209,  4435, 27424, 10869,  5209, 14032,
       35715, 34997, 41705, 24733, 23270, 41364, 29423, 32304, 32725,
       12988, 32917, 27683, 45250, 48629, 24434, 15127, 42233,  5007,
       49190, 36951, 12971, 24522,  3190, 32621, 23590, 41159, 45508,
        3763, 21835, 44868, 24364,  5423, 15417, 13205, 24335,  1150,
       44029, 15640, 32293, 28731, 34317, 49982, 33525, 14064, 19617,
       20410, 17451, 41057, 47478, 22302, 17644, 39341,  6837, 39543,
       35931, 22759, 42752, 37910, 42482, 16872, 40690, 39384, 39158,
       48097, 21672,  2197, 37415,  8876, 38012,  9120,  4481, 29273,
       34822, 26564, 49384, 43704, 30753, 25492,  6732, 31567, 46882,
       44093,  5260,  8105, 12973, 34061, 33582, 32914,  1265, 20784,
       25235, 30817, 46013, 42134, 35935, 34055, 34649,  7493, 20631,
       41853]), [5, 9, 0, 3]), (array([39470, 41148, 10032, 35811, 15140, 44629, 29163, 13803,  6022,
       23743,    89, 12313, 43632, 11116, 43746, 13453, 41246,  7634,
       34410, 41688,  6298, 28577, 40718, 30970, 29138, 41337, 48860,
       28097, 20906, 10904, 41852, 23134, 14600, 15152, 27105, 15790,
         310, 27579, 45274,  9502, 47951, 40529, 16967, 45441,  9685,
       12591, 28414, 21676, 18300, 49154,  3587, 23252, 22491, 33099,
       14060, 11125, 24933, 12106, 36533,  4527, 13636, 10925, 36646,
        6879, 35791, 29787, 25636, 40848,  3219, 48513, 25216, 18450,
        2418,  5902, 22902,  4750, 24290,   520, 15434, 12084, 46803,
        7459,  9780,  4328, 11860, 43049, 25474,  6605,  1889, 21419,
       20450,  1904, 22246, 30153, 13303, 11626,  2691, 42418, 45100,
       45645, 40277, 28151, 41788,  3533,  1303, 42907,  6415,  4725,
       32280, 32491, 31306, 43044,  7818, 47622, 32634, 34306, 47922,
        2155, 39125, 38755, 31061, 24804, 49981,  1866, 25423,  7078,
        1851, 44095,  7295, 21946, 47312,  2999, 32446, 46528, 43386,
        7382,  3577,  7099, 10890, 44974, 31860,  7811, 28904, 36225,
       27847, 24900, 37075, 48309, 23354, 49480, 45595,  5029, 11020,
       40299,  4227, 19471,  3664, 30942, 30354, 10982, 41836, 38730,
       24967, 33827, 12590, 14105, 35429, 49690, 39803, 16672,   856,
       16961, 15909, 36755, 13145, 26943, 43627, 29770, 31256, 13646,
       34418, 26512, 44846, 18818, 16294, 33296, 43682, 22468,  9872,
       43188, 40088, 35304, 22606, 35023, 38776, 11226, 12826,  3089,
       29086,  7154, 11573, 15482, 45764, 40862, 46220, 14474, 34671,
       47796, 41648, 37639, 29830, 36466,  1315,  1333, 17038, 39209,
       47305, 27247, 20243, 18476, 23445, 33735, 33290, 22053, 14576,
       16514,   904, 13134, 49390, 22825, 11769,  3776, 39576, 40067,
        9441, 11279, 18821, 15643,  3695,  3112, 19458,  7536, 11423,
       25099,  5119,  4381, 48338,  6110, 49950, 27914, 44726, 44936,
       42318, 38841,  7607, 28045, 36844, 41445, 20162, 48845, 17244,
       37264,  2394, 19382, 28127, 31854, 10644, 34064, 15710, 15748,
       21151, 17752, 24446, 48656, 17694, 21407, 36546, 35011, 44671,
       38278, 40967, 49151, 27024, 45535, 44138, 24110, 33063, 25544,
        3452, 12373, 46866,  8189, 26491, 35453, 48584, 18438,  7884,
       34911, 43854, 42221, 11532,  8001, 34128,  4591, 31352, 38610,
       20988, 34246, 11558, 35605, 10795, 34530,  6226, 12977, 22818,
       22829, 19079, 14358,  4492, 11075,  1380, 12494, 43217, 22675,
       18654, 41627, 33951, 35357, 41217, 32988, 26008, 20307, 11571,
       36473,  4101,  2428, 12210, 11920, 34063, 22697, 37551, 26292,
       43297, 19939, 24678, 22490, 48574, 39455, 25331, 48806,  4200,
       46135,   834, 26375, 29536, 42085, 12231, 10049, 40075,  9263,
       14740, 17856, 47035, 24288, 25280,  7349, 34913, 26065, 45001,
       44526, 17861, 35051, 18890, 45569, 41742, 25975, 18927,  3414,
       10110, 25558, 41519, 18206, 12741, 39638, 23895, 38919, 24762,
       21422,  6429,  9407, 44499, 27794,  1578, 49121, 48915, 19500,
       17031, 23500, 34637, 11407, 21848, 46173, 37480,  6786, 42121,
        9717, 18702, 19031, 47568,  2849, 35644, 11971, 28532, 35086,
       26789,  2067, 44824,  8869, 22379, 43206, 10035, 40423, 30245,
       33230, 13402,  4858, 40520, 26080, 27402, 37260,  7706, 34162,
       33188, 47540, 15740, 42443,  9411, 16471, 42060, 11403, 42684,
       38630, 41849, 21871, 21118, 42113, 44674, 24975,  4696, 35567,
       26267, 41209, 25974,   840, 48441, 23020, 44960, 41579, 40518,
       20746, 33808, 43893, 46177, 22636, 14201, 16745, 43865, 24979,
       33196, 41092, 49438, 26405, 19226, 46993,  5627, 23709, 31101,
       33271, 14659,  9534, 25278, 24240, 43668,  6411, 39353,  9524,
       35827, 13468, 26509, 48269,  2511, 24003, 48263, 15069, 45483,
       25488,  2752,  3840,  9400, 37136, 17838,  3574, 39995, 20105,
        4651, 30695, 43700, 19066, 14778, 31693, 14909, 20682,  8392,
       35326, 45800, 41220,  2662, 32884, 20831, 42073, 16010, 18298,
       44884, 41677,  8882, 26592, 45737, 35446, 35279,  4165,  5341,
       24793, 44468, 30895, 40343, 14848, 15439, 13031,  9714, 11080,
       48778, 20537, 19639, 32592, 16567, 46380, 49387, 48119, 34831,
       25489, 18979, 13860, 13181, 25605, 49375,  7490,  8849, 35758,
       29732, 17383, 41774, 41806, 39236, 18302, 36119, 48136, 13223,
       45143,  2598, 26003, 32661,  6328, 20976,  3548, 30971,  4524,
        2365,  2171,  6190, 46409, 29468,  9934,  6512,  3337, 18900,
       19216, 31947,  3113, 15949, 21458,  3066,  6085, 47326, 36106,
       16100,  7674, 25337,  2027, 43002, 36786, 48856, 49660, 48470,
        3024, 27986, 11424, 45089,  1338, 28231, 12420,  1142, 11224,
       18231, 31648, 21564, 28897, 28035, 17884, 15900, 32091, 15509,
       34280, 38547, 35403, 39179, 46480,  2066, 20275,  9292, 24367,
       39196, 16733, 34746, 17764, 23478, 46137, 39589,  9119, 26840,
        4311, 25422, 30730, 32247,  3900,  1711, 20790,  7660,  5566,
       45546,  5477, 11671, 32460, 29497, 37729,  4030,  6354, 35286,
        8249, 43998, 32131,  1759, 37177, 36386, 23056, 38566, 32058,
       47865, 14407, 31481, 17763, 32016,  9920, 31048, 35281, 13902,
       38345, 32670, 13861, 18659, 18849, 38205, 44067, 33336, 31654,
       10526, 46987,  5924, 43664, 49276,  5858,   405, 35079, 32398,
       12344,  4571, 34828, 26027, 14329,  1424, 43771,  3644, 32968,
       49863, 43368,   417, 11078, 23972, 39121,  4192, 36389, 21338,
       27609,  5549, 18315,  6904, 33514, 34123, 42621,  2459, 16423,
       41800, 38419, 42272, 30101, 26083, 35529,  6929,  6124, 20566,
       27147, 44742, 35059, 23924, 14138, 33864, 38656, 36038, 27278,
        7002, 18704, 26913,  3852, 30605, 11937, 38344, 16370,  1477,
        9895, 41651, 30152, 29561, 13189,  9300, 21736,  9343, 32702,
        1002, 25307, 16536, 12598, 21909, 20522, 26064, 37924, 36970,
       24348,  1271, 43487,  6162, 41725, 25008, 26613, 26372,  6309,
       38880, 38894, 19221, 13941, 46659, 33914, 43554,  3795, 18111,
       11564, 27652, 22517, 34835, 32732, 11131, 17322, 41734, 44313,
       28091, 32098, 19871, 18169, 23120, 49368, 49336, 40788,  7584,
       37546, 37998,  1363, 22933, 49713, 30838, 22406, 38423, 30022,
       12289, 10383,  4177, 38127, 25256, 47526, 12475, 18222, 49481,
       24135, 36366, 36054, 14225, 36448, 18180,  4799, 30960, 34159,
       36027,  3785, 26336, 35083, 24174, 20215,  5741, 32963, 13857,
       34581, 49015, 23935, 29810, 18662, 31198, 33511, 15706,  2042,
        8330, 35583, 20151, 21739, 28991, 12681,  7835, 17904, 47892,
       34116, 18862, 41815, 40668, 32449,  1057, 19780, 41134, 33355,
       42056, 25055, 10784, 35428, 28529, 15341, 16628, 33642, 22223,
       10770, 34170, 16583, 15397, 24871,  4437, 47819,  8781, 45921,
       13986, 38761,  1803, 40954, 27761, 46085, 33714, 12105,  9166,
       11477,  3447, 24100, 16335, 14108, 13925, 20593, 20121, 31632,
       34345, 10611, 38781, 15739, 30756, 27698, 15753, 42153, 41271,
       19060, 33168, 43685, 38976, 36809, 36979, 40399, 22453,  8722,
       24384, 45251, 37176, 12966, 21326, 21851,  9700, 38879, 39300,
       33601,  8122, 22209,  8760, 10418, 34016, 32325, 36743, 15877,
         479, 43730, 20159, 47553,  2383, 34461, 14373,  2594, 33236,
        6466,  6669, 10244,  4669, 38066, 18072, 24551,  5425,  3110,
       36800,  8686,  1196, 35629, 33955, 24022,   691, 32844, 28523,
       24751, 49298, 31141,  7467, 32021, 28149,  4672, 48045, 33031,
       36216, 15345, 45439,  3043, 28513, 37204,  5797,  4310, 23995,
        9914, 16231,  2223, 29950, 49430, 49944, 22826,  6914,  5106,
       41102, 31261,   774,  9371, 37765, 13811, 33972, 24598, 22161,
       21287]), [4, 1, 0, 3]), (array([20177, 36044,   854,  9901, 32063, 44492, 31408, 40153, 32074,
       45741, 19281, 35219, 30128, 44023,  3504, 18722, 41297, 27687,
        3991, 33948, 11638, 31660, 35136, 46791, 34020, 14476, 30938,
       31356,  5723, 46377, 15986, 37887,  3341, 13188,  3098, 34142,
       45925,  8256, 42584, 23821, 24556,  8633, 10013, 30813,  3716,
       38061, 49082, 25815, 10961,  1191, 49708, 16478, 42881, 16760,
       10738, 10111, 30001, 29976, 46951,  1176, 15823, 37917,  5728,
        9275, 30910, 23606, 45585, 45863, 26469, 41969, 17235,  9625,
       42196, 30685, 22337, 29506, 17427, 30610, 10786, 24879, 26685,
       29108, 13018, 39203, 21941, 23945, 45978, 21612, 49041, 38665,
       13495, 47396, 13804, 39457, 38738, 14043, 46979, 38461,  1959,
       33733, 26243,   937, 49343, 42525, 29141, 17574, 13120, 31342,
        7092, 49412, 23087,  7881, 47552, 16475, 35830, 28254, 15513,
       11204, 43977,  4167, 11181, 32904, 23193,   355, 45878, 35117,
       49200, 27022, 47649, 44259, 44405, 37992, 16455, 32345, 40565,
       34612, 48886, 39122, 23208, 44028, 16945, 22256, 43268, 13866,
       10889, 35747, 40142, 20527, 42167, 39161, 18909, 20766, 40206,
       30682, 27177, 37021, 47821,  6941, 14345, 23542, 34092, 44417,
       41350, 30976,   409,  6517, 37033, 47499, 43869,  6035, 23144,
       28986, 38993,  1837, 14838,  2831, 16539, 24928,  4095, 27229,
        7084, 10600,  5402,  9727, 40710, 43155, 14236,  5381, 48723,
       29233, 25999, 10108, 25829, 20501, 42836, 18655, 28533,  6935,
       34740, 37609, 18868, 49771,  3943, 38707, 43481, 42975, 47150,
       47361, 49212, 32454, 46602, 18085, 38372, 18050, 14679, 34058,
       11903, 18618, 43666,  3311,  7252, 17204, 42762, 28668, 29931,
        9237, 35435,  9681, 34542, 18254, 34563, 31465, 34686, 35370,
       18420,  8063, 45524, 20254, 37219,  7927,  8603, 26619, 17536,
       11110, 22631, 14163,  8069,  8212, 32493, 45155, 42694,  8750,
        2291, 28399, 34488, 39540, 13639, 29612, 38352,  9798, 28397,
       13985,  9463, 33300,  7850, 36441, 22602, 47450, 12584,   271,
        1614, 14918, 37838, 14559,  5773, 10391, 48427, 14423, 25366,
        9168, 17954, 38546,  5506, 29135,  9900,  5535, 28705,  2080,
       22974, 37374, 25263, 13477, 12940, 29912, 22672, 18184, 25312,
       12473, 45023,  5550,  3455, 30021, 36006,  1492, 44193,  8676,
        6073, 38991,  6230, 25757, 44910, 13321, 49020, 13036, 43106,
        6836, 46652, 42014, 18485, 31401, 46877, 49870,  2567, 21486,
        1139, 24617, 24500, 29658,  7714, 12720,  5746,  7136, 34373,
         281, 24789, 11471, 40221, 36598, 44256, 41619,  4679, 37560,
       29988, 30108,  1523, 13934,  8037, 22280,    47,  9217, 31802,
       19130, 17695, 37586,  4014,  4109, 22104, 45550, 13071, 16028,
        6901, 15310,   288,  5012,   108, 14314, 24980,  7890, 27764,
       17980,  1067, 31704,   673, 40860, 36994, 43503,  6927, 33449,
       15895,  4719, 44730, 48270,  5118, 21590,  2812, 18027, 27049,
       46365, 40087, 46405,  6816, 42450, 25276, 33350, 20671, 34149,
       22441, 36920, 31063,  8776, 19944, 28890, 48498, 40687, 24699,
       42271,  5150,  2744, 35126, 30102, 32387, 42193,   648, 20797,
       35656,  4418, 41894, 36268,  3416,  6041, 31884,  7576, 43484,
       38556, 25295, 11478, 45451, 34698, 10398, 20323, 14195, 22603,
       37781, 18328,   800, 39389, 14793, 42472, 46299, 15109, 10217,
       38072, 44652, 22331,   483, 47674,  6317, 10833, 10725, 17325,
       40908, 30694, 42098, 41362, 22843,  1777, 33977, 47360, 33412,
       42185,  3982, 38981,  4012, 22486, 29058, 26893, 45864,  1354,
       24927,  1677, 36385, 43823, 30003, 19899, 30224, 11961, 35360,
        3372, 38705, 41662,   335, 32730, 37208, 13770, 26110,  7688,
       12945, 42803,   864, 49357, 23343, 39426, 21076,    24, 43116,
       45021, 37515,  5281, 11221, 48833, 37178, 44668, 24721, 42399,
       40680,  7328, 39628, 26776, 49869,  4490,  2504, 28363,  2574,
        4941, 27444,   165, 32403, 39650, 24169, 22924, 44464, 11989,
       20834, 12534, 15893, 39281, 16704, 46324, 11642,  8813, 23805,
       26656, 28866, 47201, 19127,  9542,  6374, 19444,  8365, 20523,
       19662, 25976, 32483, 45085, 44141, 32455, 40649,  3332, 22648,
       25668, 30487, 39716, 42755,  1466, 25419, 20039,  1999,  5657,
       37558, 22418,  2460, 41900, 19541,   989, 22329, 24476, 40479,
       25616, 33328, 18591, 20097, 28625, 32025, 48145, 34460, 25930,
        9983, 42678,  7444,  6400, 18706, 11297, 23847, 15622, 17026,
       44794, 13030, 21023, 33148, 15707, 46763, 26842, 12178, 30820,
        1381, 23684, 19277, 48209, 45176,   905,  1185, 31435, 37355,
       38479, 30324, 35418, 36993, 38235, 29881, 26824,  8774, 42952,
       21308, 42945, 35782, 31201, 38551,  3214, 32873, 14615, 21248,
       31700,  8155,  6054,  2932, 32194, 33380, 21297, 28619, 34131,
        4229, 32316, 16596, 48318, 12660, 27440, 37276, 48891, 21881,
       45827, 16521, 41352, 29257, 12133, 11944, 41664, 29944,  1664,
       34758, 22408,  8558, 42573, 34711, 32617,  3609,  5346, 39904,
        1926, 42010, 45373, 47328,   189,  9533, 44568, 17355, 17520,
       25191,  5428, 23024, 24445, 35121,  6711,  3204, 35996, 37279,
        2675, 26045, 33087, 34601, 22858, 13011, 13483, 37712, 20539,
        8098, 32002, 37674, 40648,  8810, 34919, 10893,  8912, 29884,
       44476, 26258,  9801,  8118, 16375, 32444, 20169,  5028, 38847,
       25789, 22026, 27686, 31521, 36558, 10989, 33654, 33076, 25807,
       46625,  8190, 48643, 33926,   129, 39883, 20449, 48635, 21869,
       48024, 28650, 38580, 22967, 46935, 17154,  9616, 42630,  1755,
       31105, 49490, 10215, 14056, 21467, 25820, 47691,   199, 10365,
        6687, 28487, 28921, 37189, 35269, 11451, 33416, 46873, 29940,
        5494,  3515, 25318,  7290, 36781, 29324, 30571, 11271, 21898,
       43626, 23295, 17530, 49970,  9163, 16014, 37135, 26688, 35794,
       48722, 22866, 48062,  9489, 40246, 35549, 41285, 17441, 34545,
        3905, 20017, 34756, 25922, 46204, 25378, 39482,  3490,  5800,
       17051, 25062, 48331, 26711, 21882, 26689, 47413, 46561, 14089,
       44381, 47643, 30896,  3016, 31606, 22914, 16070, 30687, 38626,
       40226, 22011, 47075, 25627,   174, 31886, 43254, 22555, 39166,
       42757, 15247, 33560, 23145, 48326,  6602, 11458, 26609, 43716,
       34361, 32312, 46950, 26210,  7734, 37209, 24707, 20153,  9034,
       25825,  7909, 11199, 39420, 21655, 28067, 33801,  4720, 19895,
       39979, 36074, 45454, 22922, 33952, 41910, 37105, 23483, 48144,
       37180,  3178,  9052, 38552, 33301, 32358, 43034, 30305, 25771,
       41013, 49901,  5183, 11236, 49054, 39822, 18393,  1487, 45479,
       36063, 16339,  9357, 23963, 43922, 17698, 20917,  3910, 30862,
       44550, 48377,   801, 17195, 36124,  9307,  6658, 40369, 43084,
        7765, 12147,  2738, 43500,  2770, 45152,  5720, 24991, 44543,
       46595, 12558, 44234, 32607, 21703, 40326, 27433,    26,  3218,
       45670, 44127, 32006, 20131, 46596, 31295, 35245, 44267,  9345,
       25673, 24839, 26097,   494, 43233, 36518, 48999,  8101,  3807,
       47313, 32264,  6402, 37697, 43283, 34579, 11294, 40097, 44110,
        1127,  3371,  6199, 44376,  9332, 37967, 40584,  5221, 34984,
       37675, 23461, 14558,   776, 13584, 15937, 24611, 26260, 12783,
       32583, 30986, 46712, 17413, 23314, 10070, 22007, 49146, 17835,
        6590, 42647,  6945, 18497, 22994, 46157, 21351, 12647, 17661,
       27753,  5870, 27759, 36230, 19518, 26907, 23278,  5389,  6674,
       12707, 11864, 42021,  7205, 24949, 24749, 37038, 20691, 41540,
       46229, 33915, 36969, 45850, 27263, 46232, 29050, 19381, 36714,
       20338, 29960, 10949, 15226,  1109, 39609,  2505, 26862, 40459,
       14942]), [6, 2, 0, 3]), (array([37228, 15148, 28398, 26615,  5771, 18916, 23880, 46397, 40005,
       45365, 28815, 21340, 47665, 13445, 10874, 45459, 24820, 37482,
          85, 10073, 12844, 24159, 19244, 20094, 24892, 47575, 35767,
       39667, 30939,  3071, 25568,  6734, 22723, 38575, 49335,  3988,
       48066, 14742, 16265, 31212,  5403, 38358, 43617, 46797,  4248,
       42366, 22806, 33191, 16706,  6655, 29535, 31321, 44353, 43150,
        2405,  9590, 40374,  1386, 19997, 29886, 11581, 14531, 11391,
       39081, 13935,  5325,  5976, 39164, 44081,  1583, 37489, 16995,
       27657,  5220, 33821, 40977, 43929, 28015, 15364, 22126, 26056,
       29614, 17720, 12271, 20618, 34904, 12633,  5307, 35120, 17913,
       26916, 49377, 42959, 41239, 26702, 41437,  1902, 10754, 21751,
       39304,  3817, 38104, 27451, 17398, 45292, 16344,  2969, 46154,
        2397, 46465, 38195,  1489, 42555, 43848, 41805, 37293,  3835,
       25283,  3637, 35008, 19159,  1483, 39400, 13799, 35462, 15067,
       32640, 37971,   662, 44578, 46954, 29789, 12794, 33330, 47924,
       44892, 13026, 10156,  6157, 20429, 29859,  9939, 49456,   727,
       25865, 42018, 16096, 46167, 27929,  7141, 33388, 32490, 48307,
       38367, 43556, 45682, 46027, 41959, 46266, 34165, 21060, 48767,
        7185, 11321, 25791, 16879, 27985, 19206, 28206, 21314, 29910,
       46549, 36718, 47657, 15332, 44935, 12397,  7197, 10496, 41347,
       30484, 39351, 45657,   652, 41526, 48732,  4427, 49048, 14066,
        7420, 27217, 29962, 19779, 34733, 13566, 15400, 15842,  1549,
       46672, 15932, 47817, 24930, 43036, 48166, 34584,  6827, 16031,
       44123, 25724, 40243, 44366, 42268, 26314, 18417, 49116, 26692,
       11553,  6335, 44086, 18620, 10818, 39246, 36270, 16654, 16138,
       12941,  3824, 22782, 24371, 33935,  8887,  3782, 47673, 12535,
       16996, 43305, 31389, 42453, 37159, 31529, 20223,  7829, 31653,
        4736,  6713, 41517,  6307, 37948, 37862, 48055, 45327, 18104,
       30433, 15964, 49445, 22440, 26951, 47527, 40491, 32243,  8972,
       13054,  9972, 14010, 32574, 48076,   766, 29414, 19195, 42794,
       19147, 46248, 22462, 20400, 13131, 19175, 38469, 39621,  5348,
       37703, 21312,  5701, 20455, 26337,  4891, 36733, 15721, 21377,
       17116, 25823, 37207, 44199, 20405, 35864,  8241, 40702, 33093,
        7334, 49571, 19851, 33562, 40252, 21830, 22920, 25207, 37583,
       14123, 29868, 29199, 40493, 48959, 13674, 30018,  1770, 32543,
         193, 39802, 44877, 11993, 20652, 18593, 11652,  8401, 41616,
       27864, 43149, 23436, 21253,  1383, 47854,  7118, 44205,  4287,
       44382, 24841, 39298, 49393, 36799,  7455, 26603, 22262, 47206,
       10682,  1250, 12115, 48690, 34152, 31104, 36534, 47791, 19733,
       26246, 34910, 23973, 30510, 12543, 34248, 36013, 48311,  3497,
       19416, 44304, 20089,  1794,  6995, 28384,  9418, 10475,  6418,
       27932, 32915, 26977,  8073, 40939, 16479, 42617, 43899, 24444,
       21936, 46552, 23766, 31170, 23839, 47978, 38525, 35139, 38949,
         442, 12041, 22584, 44458,  3159, 29762, 26714, 48678, 23258,
       35115,  2160, 28435,  5185, 40429,  7104,  3670, 15198, 24386,
       37416, 11977, 40598, 20540, 18171, 33287, 45619, 40458, 19479,
       13672, 46151, 20237, 18853,  3144, 40408, 41988, 19472, 48871,
       30808, 26094, 30411, 29371, 42050, 42842, 18623, 30380, 31423,
       32567, 40778,  6397, 31966, 10095, 34419, 30328, 32463, 33335,
       17345, 28728, 31121, 13355, 19887, 19141,  8526, 35562, 20006,
       19768, 16084, 24350,   880,  1702,  1357, 38276, 30844, 26988,
       49905, 16118, 25258, 21172, 25794, 48602, 23153, 10606, 45890,
        3584, 23215, 13406,  3353,  6508, 42676, 42388, 43278, 12233,
       10272,  2923, 45240, 41570, 34560,  1967, 10445, 23583, 11842,
       11501, 11010, 38315,  8852, 13967,  2395, 17013, 37335, 28874,
       23282, 15043, 45776, 28461, 42182,  1144, 48212, 33000, 27934,
        2169, 16632, 29060, 18508, 37083, 26427, 41588, 29487, 22962,
       44749, 11721, 29295, 33933, 21983, 38433,  1270, 15466, 22190,
       32251,  8549,  3665, 24700, 45615, 15288, 38112,  9171, 41300,
       35594,  9221, 46458, 18525, 27940, 19370,  7747, 32505, 39149,
       11178, 25144,  1329,   497, 19004,  3180, 31459, 21105,  2718,
       17088, 38335, 40644, 48105, 49091, 17620, 26300, 41960, 36363,
       28033, 22881,  3263, 15436, 48280,  7360,  9296,  3466, 21777,
       13850, 10855, 40589,  5984, 30829, 26879, 23157, 43965, 11475,
        7826,  2780, 10266, 15678,  8200, 43750, 24639,  8478, 20434,
       22200, 30667, 17237, 47379, 34022, 32190, 13751, 48508, 14450,
       43659, 38198, 28449, 31674,   663, 29274, 23419, 41574, 34537,
        7808, 42934,  4368, 26488, 13238, 24939, 36149,  8437,  9512,
        2258, 42298, 10353,  6665, 14708, 12785,  1044, 49656, 37265,
       12818, 45113, 22799, 23182, 19527, 11431, 16460,  7504, 38003,
       43184, 27323, 10607, 45936, 48246, 12079, 21227,  8072, 30811,
       22973, 27004, 35991, 30652, 23610, 46831, 24179, 16316, 10513,
       46303, 16554, 12083, 16230, 19814, 48850,  8695,  4314, 48729,
        5484, 11919, 46807,  1816,  5010,  2659, 22276, 47220, 29941,
       25160, 14080, 30002, 36446, 17133, 43219, 41818, 33438,  9004,
        2714, 46655, 33242, 37165, 36623, 30830, 11682, 13106,  8355,
       24881, 30158, 16146,  9375, 29977, 49723, 37181, 13668, 31358,
       43814, 11691, 30078,  4079, 35685,  6629, 42597, 24642, 24291,
       39530, 12836, 19084,   436, 45621, 24325, 47143, 19812,  3441,
       27981, 33732, 49734, 18192, 23738, 19804, 16002,  7792, 39380,
       26723, 29336, 40504, 30304, 34964, 27562, 24600, 46964, 38326,
       18614, 21413, 18100, 10323,  8444, 14509, 36909, 12181, 11029,
       47317, 26761, 21892, 42722,  2345,  6640, 16015, 34125, 14591,
       25070,  7203, 33453, 32139,  2559, 36870, 11188, 49983, 42825,
       25934,  9760, 37163, 25547, 26160, 43735, 43773, 48058, 18798,
       49500, 25971, 31649, 31603, 24330, 42084,  1098, 37817,  8599,
       16869, 15057, 34437, 37453, 38364, 14574, 13061,  3514, 11976,
         949, 31254, 39761, 11257, 11293, 20034, 28738, 14527, 37700,
        6060,  8164, 25745,   550, 27298, 23122, 14861, 18387, 46653,
        2164,  8161, 28838, 28028, 19787,  9737,  1554, 20961, 43742,
       12050,    21, 11938, 16366, 23417, 22778,  5848, 44926,  4288,
       49819, 29692,  9667, 26883,  9982, 44456, 17414, 18865, 34623,
       36418, 29839, 22364, 28433, 33964, 30634, 46841, 23027, 31788,
       23239, 37973, 23465, 44802,  3756,  7107, 27156, 32686, 19838,
         101, 42789,  9421, 28808, 27742, 44142, 29460,  1685, 13064,
       21103, 40511, 39689, 48540, 30624,  9222,  3003,  3376, 17144,
       31094, 14340, 28819, 38171, 39533,  4402, 10011, 25077, 39608,
        8067, 30386, 36273, 43800, 20877, 42997, 38699, 49599, 14148,
       35449,  4392, 17430,  9399, 41511,  8246,  8635, 11231, 25443,
       26906, 13463, 18771,  8422, 25006, 48849, 47973, 32524, 28813,
       35595,  3818, 11578, 25986,  2518, 30779, 28983, 25688,    74,
       33211, 13914, 49499, 28029, 23998, 30573,  6186, 18422, 46962,
       13313,  1778,  8381, 20601, 27467,  6980,  6394, 26302, 41269,
       17909, 31825, 33925, 17098, 21962, 25219, 45091, 22664, 12821,
       32541, 10958,  4791, 32982, 33393, 28034, 44562, 45053, 46347,
       11743, 30839,  5062,  1461, 22638, 29979, 42411, 44409, 34952,
        5915, 31676, 20007,  2359,  3758, 25782, 12631, 32113,  1427,
        6883, 15856, 38932, 37156, 16814,  4083, 25706, 15083, 43105,
       31366, 18954, 39789, 23763, 11040, 34590, 28984,  6343,  8822,
        9943, 18389, 47539, 38951, 43533, 27212, 31161, 15080, 38377,
       21510, 32350, 37773, 20849, 38897, 11409, 37304,  6724, 46198,
       39171]), [7, 8, 0, 3])]
Competition
DC 0, val_set_size=1000, COIs=[5, 9, 0, 3], M=tensor([5, 9, 0, 3], device='cuda:0'), Initial Performance: (0.25, 0.045359280467033386)
DC 1, val_set_size=1000, COIs=[4, 1, 0, 3], M=tensor([4, 1, 0, 3], device='cuda:0'), Initial Performance: (0.274, 0.04425249779224396)
DC 2, val_set_size=1000, COIs=[6, 2, 0, 3], M=tensor([6, 2, 0, 3], device='cuda:0'), Initial Performance: (0.254, 0.04541688692569733)
DC 3, val_set_size=1000, COIs=[7, 8, 0, 3], M=tensor([7, 8, 0, 3], device='cuda:0'), Initial Performance: (0.255, 0.044730136394500734)
D00: 1000 samples from classes {0, 3}
D01: 1000 samples from classes {0, 3}
D02: 1000 samples from classes {0, 3}
D03: 1000 samples from classes {0, 3}
D04: 1000 samples from classes {0, 3}
D05: 1000 samples from classes {0, 3}
D06: 1000 samples from classes {9, 5}
D07: 1000 samples from classes {9, 5}
D08: 1000 samples from classes {9, 5}
D09: 1000 samples from classes {9, 5}
D010: 1000 samples from classes {9, 5}
D011: 1000 samples from classes {9, 5}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {2, 6}
D019: 1000 samples from classes {2, 6}
D020: 1000 samples from classes {2, 6}
D021: 1000 samples from classes {2, 6}
D022: 1000 samples from classes {2, 6}
D023: 1000 samples from classes {2, 6}
D024: 1000 samples from classes {8, 7}
D025: 1000 samples from classes {8, 7}
D026: 1000 samples from classes {8, 7}
D027: 1000 samples from classes {8, 7}
D028: 1000 samples from classes {8, 7}
D029: 1000 samples from classes {8, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.321, 0.059396352589130404) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.0762955961972475) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.08767266011238098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.0864397220313549) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.06267942577600479) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.09520264573395253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.333, 0.12743896949291228) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.09229815077781678) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.07932132732868194) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.11468889297544957) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.15861733889579774) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1226446467190981) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.10350485199689866) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13766499603539706) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.17524761104583741) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1584977867305279) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.425, 0.12845924731343986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.15613945835083723) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.18912637266516685) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.19356412656605243) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.14465451043099165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.17646908596530556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.19739552516490222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.22081087283417583) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.1671894628610462) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1882399409338832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.18544992347434164) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.23137943388335408) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.1650114922504872) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.1933495959304273) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.22099166601337492) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2418985731303692) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.1857669574674219) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.21793028167169542) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.268153380241245) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.272056638228707) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.432, 0.21375495231803507) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.23726580816414208) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.27721165637299416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.26264312120061367) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO0', '(DO5']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO2']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.20949073391780257) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.25432156485319135) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.2396635031234473) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2808012938024476) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.22547791650891305) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.2636961328503676) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.2724379265718162) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3032933616912924) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.24664778713788837) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.23856809167936444) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.27247855254635217) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.2994434492669534) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.24218969002738594) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.2663930945089087) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.28818044400401416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.286316742566647) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.2545489683926571) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.27523242542520165) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.26712499689869584) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.2658526935414411) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO1', '(DO5']
DC 2 --> ['(DO0']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.25536014250153677) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.2802089845049195) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.292853259623982) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.27952661926392464) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.240414518839214) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.27999744079541417) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.29041451392695306) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.2902290935216006) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.2598798098852858) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.3134991130447015) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.3571604142184369) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3120273104561493) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.26400224880222234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.2717725380277261) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.283987540628761) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.27329927616054195) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.2548468398549594) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.31709203173872086) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.36070619109563995) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.28632162483688445) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO4', '(DO5']
DC 1 --> ['(DO3', '(DO0']
DC 2 --> ['(DO1']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.24242846134398133) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.482, 0.30854865290410816) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.3097490460774861) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.28258093482651747) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.25089226608444004) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.481, 0.29905504907807334) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.38722543814918026) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.484, 0.25344942781468854) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.2527057195436209) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.2640578730031848) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.2817087382534519) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.2575883714859374) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.24558119488833471) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.31345517207682133) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.2758447939902544) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.482, 0.31076124103739855) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.28567605319991707) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.33312892883084716) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.2698938334658742) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.2886119961525546) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.25597331893222874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.37879101936350346) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.3418706964030862) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.26787211088649926) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.27421971919503996) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.3744777779346332) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.31820874811708927) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2642190252293367) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.2728473133193329) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.36995479045645335) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.280814008615911) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.2625627075319644) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.2534296447979286) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.3472655354923336) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.26641515155136586) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.2735277586245211) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.29562560623663015) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.32374313069577326) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2937084967046976) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.3019257637154515) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO0']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.26562970924121326) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.35090585362212734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.1956791996024549) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2687006743195816) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.27660058403899895) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.3784230642819311) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.2827615527492017) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.21221535264956765) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.2592827626548242) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.3470377747651655) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.24165907953307034) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.2529665553804953) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.2707529143504798) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.3480768374208128) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.21031852715462446) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.272259936927876) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.29032360180886463) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.3801043282516766) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.24168630594573914) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2812941573478747) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.2812375042250496) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.482, 0.3830059910914861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.1978177965786308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.2391706108476501) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.3117202263462241) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.482, 0.38777401443303094) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.20810848355665804) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.26957819189113796) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.3324030765683856) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.35838856975885575) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.2243955690152943) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.2922203628961579) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.3105005085553275) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.3179247340755537) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.23681803619861602) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.2483695798830595) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.3106359928010497) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.3345537886098027) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.21623031908646226) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.27752380998393344) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO3', '(DO2']
DC 1 --> ['(DO0', '(DO4']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.2883279337710701) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.35202397891506554) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.23955498591437935) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.28464368150441444) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.31191109148669055) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.396263865039713) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.23714901887252926) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.2508951301224879) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.3156064470821293) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.36143901946221013) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.22369963229447604) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.2472529394242738) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.3305572049271432) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.35670473837218014) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.23264287599548697) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.482, 0.25587972114875446) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.3063162078531459) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.35767252327653115) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.24291951238363982) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.483, 0.24521212844213006) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO1']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.31048744490137326) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.34991633078700396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.2399674469307065) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.483, 0.25429370529399603) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.3240206514080055) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.3289065404093126) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.21040930455178022) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.22245906669145915) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.3401853759749793) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.3802961665549519) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.252687506493181) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.21867942726239561) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.3275069180824794) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.3363175278804847) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.22708685258030892) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.2059855689557735) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.3050954312514514) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.32900684575829653) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.20996605513989924) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.22710634471592495) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.25, 0.045359280467033386), (0.321, 0.059396352589130404), (0.405, 0.06267942577600479), (0.434, 0.07932132732868194), (0.459, 0.10350485199689866), (0.425, 0.12845924731343986), (0.434, 0.14465451043099165), (0.452, 0.1671894628610462), (0.465, 0.1650114922504872), (0.456, 0.1857669574674219), (0.432, 0.21375495231803507), (0.457, 0.20949073391780257), (0.451, 0.22547791650891305), (0.468, 0.24664778713788837), (0.466, 0.24218969002738594), (0.458, 0.2545489683926571), (0.464, 0.25536014250153677), (0.465, 0.240414518839214), (0.469, 0.2598798098852858), (0.471, 0.26400224880222234), (0.47, 0.2548468398549594), (0.458, 0.24242846134398133), (0.461, 0.25089226608444004), (0.457, 0.2527057195436209), (0.46, 0.24558119488833471), (0.466, 0.28567605319991707), (0.463, 0.25597331893222874), (0.467, 0.27421971919503996), (0.464, 0.2728473133193329), (0.461, 0.2534296447979286), (0.469, 0.29562560623663015), (0.468, 0.26562970924121326), (0.462, 0.27660058403899895), (0.466, 0.2592827626548242), (0.464, 0.2707529143504798), (0.466, 0.29032360180886463), (0.461, 0.2812375042250496), (0.469, 0.3117202263462241), (0.453, 0.3324030765683856), (0.462, 0.3105005085553275), (0.468, 0.3106359928010497), (0.471, 0.2883279337710701), (0.468, 0.31191109148669055), (0.472, 0.3156064470821293), (0.471, 0.3305572049271432), (0.474, 0.3063162078531459), (0.474, 0.31048744490137326), (0.472, 0.3240206514080055), (0.466, 0.3401853759749793), (0.471, 0.3275069180824794), (0.47, 0.3050954312514514)]
TEST: 
[(0.25, 0.044425843864679335), (0.315, 0.05723254606127739), (0.4005, 0.06019690066576004), (0.43425, 0.07564766851067543), (0.455, 0.09838825124502182), (0.426, 0.12078447866439819), (0.4355, 0.13630374038219453), (0.44775, 0.15698001247644425), (0.46175, 0.15570431262254714), (0.45275, 0.1750297738313675), (0.43225, 0.19910654628276825), (0.45725, 0.1980536785721779), (0.4555, 0.21256149238348007), (0.46675, 0.2322468901872635), (0.467, 0.22817421531677246), (0.45225, 0.2388617113828659), (0.46525, 0.2399423031806946), (0.46875, 0.22862118577957152), (0.469, 0.24449598813056947), (0.47225, 0.2481428462266922), (0.46825, 0.2419218178987503), (0.459, 0.22607713115215303), (0.46275, 0.23405520939826965), (0.4625, 0.23944497549533844), (0.46275, 0.22955696046352386), (0.47075, 0.26823985266685485), (0.46525, 0.24117199337482453), (0.463, 0.2566637020111084), (0.46475, 0.258041360616684), (0.4635, 0.23838451236486435), (0.47175, 0.2791782460212707), (0.47075, 0.2468298937678337), (0.462, 0.25815567302703857), (0.4655, 0.24042991191148758), (0.46675, 0.2506446447372436), (0.46825, 0.27065918111801146), (0.46225, 0.2634758301377296), (0.47325, 0.29095294630527496), (0.458, 0.31070909929275514), (0.46225, 0.2899567254781723), (0.469, 0.2904368636608124), (0.47375, 0.2717693513631821), (0.47125, 0.2944602928161621), (0.47425, 0.2960546463727951), (0.46925, 0.31037044143676756), (0.47475, 0.2852003724575043), (0.478, 0.29178089535236357), (0.47475, 0.3047503012418747), (0.47125, 0.31716600716114046), (0.4745, 0.30781429302692415), (0.46975, 0.28443818151950834)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           3       0.00      0.00      0.00      1000
           5       0.38      0.99      0.55      1000
           9       0.64      0.89      0.74      1000

    accuracy                           0.47      4000
   macro avg       0.25      0.47      0.32      4000
weighted avg       0.25      0.47      0.32      4000

Competition_DC_1
VAL: 
[(0.274, 0.04425249779224396), (0.25, 0.0762955961972475), (0.251, 0.09520264573395253), (0.301, 0.11468889297544957), (0.395, 0.13766499603539706), (0.39, 0.15613945835083723), (0.431, 0.17646908596530556), (0.459, 0.1882399409338832), (0.462, 0.1933495959304273), (0.466, 0.21793028167169542), (0.462, 0.23726580816414208), (0.478, 0.25432156485319135), (0.478, 0.2636961328503676), (0.483, 0.23856809167936444), (0.48, 0.2663930945089087), (0.484, 0.27523242542520165), (0.474, 0.2802089845049195), (0.485, 0.27999744079541417), (0.467, 0.3134991130447015), (0.48, 0.2717725380277261), (0.483, 0.31709203173872086), (0.482, 0.30854865290410816), (0.481, 0.29905504907807334), (0.484, 0.2640578730031848), (0.486, 0.31345517207682133), (0.479, 0.33312892883084716), (0.475, 0.37879101936350346), (0.48, 0.3744777779346332), (0.487, 0.36995479045645335), (0.48, 0.3472655354923336), (0.491, 0.32374313069577326), (0.486, 0.35090585362212734), (0.486, 0.3784230642819311), (0.487, 0.3470377747651655), (0.484, 0.3480768374208128), (0.483, 0.3801043282516766), (0.482, 0.3830059910914861), (0.482, 0.38777401443303094), (0.483, 0.35838856975885575), (0.488, 0.3179247340755537), (0.486, 0.3345537886098027), (0.485, 0.35202397891506554), (0.485, 0.396263865039713), (0.486, 0.36143901946221013), (0.489, 0.35670473837218014), (0.487, 0.35767252327653115), (0.487, 0.34991633078700396), (0.49, 0.3289065404093126), (0.485, 0.3802961665549519), (0.489, 0.3363175278804847), (0.487, 0.32900684575829653)]
TEST: 
[(0.27025, 0.043206459641456606), (0.25, 0.07365380853414535), (0.25025, 0.09170785903930664), (0.305, 0.110004026055336), (0.39225, 0.13228247344493865), (0.3865, 0.14950004374980927), (0.42625, 0.1695541403889656), (0.4605, 0.1806327423453331), (0.45975, 0.18581818532943725), (0.4595, 0.20804015803337098), (0.4505, 0.22729678165912628), (0.4745, 0.24393758952617645), (0.4755, 0.25209505641460417), (0.47775, 0.2265661278963089), (0.474, 0.25195156466960905), (0.4815, 0.25983945977687833), (0.46975, 0.2669775083065033), (0.4835, 0.2635976676940918), (0.466, 0.2985556516647339), (0.4795, 0.2567606574296951), (0.48025, 0.3004980937242508), (0.4785, 0.29213671100139615), (0.478, 0.2837879866361618), (0.47875, 0.25150376188755036), (0.48175, 0.29663720333576205), (0.4765, 0.3182211322784424), (0.4735, 0.3630507315397263), (0.472, 0.3567114963531494), (0.482, 0.3517723191976547), (0.47725, 0.3298676538467407), (0.4865, 0.3061584253311157), (0.48125, 0.3333540201187134), (0.48075, 0.35966566944122313), (0.4815, 0.33078952169418335), (0.4795, 0.33179561626911164), (0.4755, 0.3633701273202896), (0.4765, 0.3685516158342361), (0.47575, 0.36970440232753754), (0.4785, 0.34264273273944856), (0.4855, 0.3050385220050812), (0.48075, 0.3199338674545288), (0.48125, 0.33650758814811704), (0.48, 0.3806164000034332), (0.48075, 0.34554241943359376), (0.48225, 0.3398533527851105), (0.48, 0.3411225242614746), (0.484, 0.3339912532567978), (0.486, 0.31546685898303983), (0.4805, 0.3649639657735825), (0.48525, 0.32374721133708956), (0.48175, 0.31655285251140597)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.80      0.93      0.86      1000
           3       0.00      0.00      0.00      1000
           4       0.35      0.99      0.52      1000

    accuracy                           0.48      4000
   macro avg       0.29      0.48      0.34      4000
weighted avg       0.29      0.48      0.34      4000

Competition_DC_2
VAL: 
[(0.254, 0.04541688692569733), (0.317, 0.08767266011238098), (0.333, 0.12743896949291228), (0.39, 0.15861733889579774), (0.423, 0.17524761104583741), (0.423, 0.18912637266516685), (0.413, 0.19739552516490222), (0.415, 0.18544992347434164), (0.428, 0.22099166601337492), (0.418, 0.268153380241245), (0.431, 0.27721165637299416), (0.435, 0.2396635031234473), (0.435, 0.2724379265718162), (0.421, 0.27247855254635217), (0.408, 0.28818044400401416), (0.438, 0.26712499689869584), (0.428, 0.292853259623982), (0.424, 0.29041451392695306), (0.427, 0.3571604142184369), (0.436, 0.283987540628761), (0.437, 0.36070619109563995), (0.43, 0.3097490460774861), (0.431, 0.38722543814918026), (0.428, 0.2817087382534519), (0.435, 0.2758447939902544), (0.436, 0.2698938334658742), (0.435, 0.3418706964030862), (0.425, 0.31820874811708927), (0.433, 0.280814008615911), (0.434, 0.26641515155136586), (0.437, 0.2937084967046976), (0.432, 0.1956791996024549), (0.431, 0.2827615527492017), (0.433, 0.24165907953307034), (0.43, 0.21031852715462446), (0.432, 0.24168630594573914), (0.435, 0.1978177965786308), (0.435, 0.20810848355665804), (0.438, 0.2243955690152943), (0.431, 0.23681803619861602), (0.439, 0.21623031908646226), (0.431, 0.23955498591437935), (0.437, 0.23714901887252926), (0.428, 0.22369963229447604), (0.431, 0.23264287599548697), (0.43, 0.24291951238363982), (0.438, 0.2399674469307065), (0.439, 0.21040930455178022), (0.435, 0.252687506493181), (0.432, 0.22708685258030892), (0.435, 0.20996605513989924)]
TEST: 
[(0.25275, 0.04452108883857727), (0.3225, 0.08423771148920059), (0.348, 0.12218378227949142), (0.39725, 0.15200227892398835), (0.41825, 0.16702659863233565), (0.416, 0.18177365124225617), (0.3995, 0.19000737953186037), (0.40175, 0.17578768062591552), (0.42075, 0.21132051086425782), (0.41375, 0.25228228414058684), (0.42475, 0.2621416676044464), (0.431, 0.23158015811443328), (0.437, 0.25965184473991393), (0.4175, 0.25419505310058593), (0.409, 0.26901572227478027), (0.43225, 0.25547254157066346), (0.42675, 0.28048447334766385), (0.426, 0.2776934715509415), (0.42875, 0.3514852195978165), (0.43625, 0.2721639250516891), (0.4335, 0.3306039216518402), (0.42875, 0.29801232504844666), (0.43275, 0.37751456689834595), (0.4305, 0.2752658530473709), (0.434, 0.269784338593483), (0.44175, 0.25717438519001007), (0.43575, 0.32615468502044676), (0.43325, 0.3180568752288818), (0.4355, 0.27863226854801176), (0.43725, 0.25918814885616304), (0.44025, 0.27907044446468354), (0.435, 0.18835749298334123), (0.43825, 0.2740577749609947), (0.4375, 0.23252055776119232), (0.438, 0.20344099187850953), (0.4405, 0.2333231600522995), (0.443, 0.19754177743196488), (0.44225, 0.20372809052467347), (0.44225, 0.2178340198993683), (0.435, 0.22757388627529143), (0.44225, 0.2098586111664772), (0.44, 0.23472529149055482), (0.43825, 0.2267504105567932), (0.44025, 0.21620180189609528), (0.43725, 0.22112442046403885), (0.44025, 0.2315396317243576), (0.442, 0.22645822495222093), (0.44375, 0.2028986439704895), (0.44, 0.24485007071495057), (0.43925, 0.2334115093946457), (0.44325, 0.2087548919916153)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           2       0.38      0.85      0.53      1000
           3       0.00      0.00      0.00      1000
           6       0.52      0.93      0.66      1000

    accuracy                           0.44      4000
   macro avg       0.23      0.44      0.30      4000
weighted avg       0.23      0.44      0.30      4000

Competition_DC_3
VAL: 
[(0.255, 0.044730136394500734), (0.45, 0.0864397220313549), (0.465, 0.09229815077781678), (0.477, 0.1226446467190981), (0.477, 0.1584977867305279), (0.481, 0.19356412656605243), (0.477, 0.22081087283417583), (0.479, 0.23137943388335408), (0.48, 0.2418985731303692), (0.479, 0.272056638228707), (0.48, 0.26264312120061367), (0.48, 0.2808012938024476), (0.479, 0.3032933616912924), (0.476, 0.2994434492669534), (0.479, 0.286316742566647), (0.477, 0.2658526935414411), (0.479, 0.27952661926392464), (0.478, 0.2902290935216006), (0.475, 0.3120273104561493), (0.478, 0.27329927616054195), (0.478, 0.28632162483688445), (0.481, 0.28258093482651747), (0.484, 0.25344942781468854), (0.481, 0.2575883714859374), (0.482, 0.31076124103739855), (0.481, 0.2886119961525546), (0.48, 0.26787211088649926), (0.48, 0.2642190252293367), (0.478, 0.2625627075319644), (0.477, 0.2735277586245211), (0.476, 0.3019257637154515), (0.48, 0.2687006743195816), (0.479, 0.21221535264956765), (0.478, 0.2529665553804953), (0.479, 0.272259936927876), (0.48, 0.2812941573478747), (0.479, 0.2391706108476501), (0.481, 0.26957819189113796), (0.473, 0.2922203628961579), (0.479, 0.2483695798830595), (0.48, 0.27752380998393344), (0.479, 0.28464368150441444), (0.479, 0.2508951301224879), (0.481, 0.2472529394242738), (0.482, 0.25587972114875446), (0.483, 0.24521212844213006), (0.483, 0.25429370529399603), (0.48, 0.22245906669145915), (0.478, 0.21867942726239561), (0.479, 0.2059855689557735), (0.479, 0.22710634471592495)]
TEST: 
[(0.2535, 0.04368370050191879), (0.4385, 0.08277494287490844), (0.46125, 0.08807962861657143), (0.47575, 0.11717280966043472), (0.48025, 0.15088662540912628), (0.47625, 0.1845778265595436), (0.48075, 0.20741392266750336), (0.48275, 0.2172055230140686), (0.48175, 0.22952548241615295), (0.481, 0.25865569376945496), (0.47825, 0.2520137227773666), (0.48325, 0.2687718592882156), (0.483, 0.28931310200691224), (0.483, 0.28404978823661803), (0.48375, 0.27364561080932615), (0.4805, 0.2562945487499237), (0.481, 0.264515194773674), (0.4825, 0.2740654250383377), (0.481, 0.2945468761920929), (0.48125, 0.26065332102775574), (0.4835, 0.274174113035202), (0.48175, 0.26803772020339967), (0.48375, 0.23871183121204376), (0.48375, 0.24228389072418213), (0.48475, 0.2915094966888428), (0.481, 0.2728931636810303), (0.48225, 0.25269329130649565), (0.4815, 0.25103021097183226), (0.4835, 0.2451578757762909), (0.48425, 0.2577028657197952), (0.484, 0.28738762164115905), (0.484, 0.2539455517530441), (0.4785, 0.19910027050971985), (0.48375, 0.23962729728221893), (0.48425, 0.25852335953712463), (0.48275, 0.26659481060504914), (0.4835, 0.22609029161930083), (0.48575, 0.25665949428081514), (0.47775, 0.27555504524707797), (0.48375, 0.23546278786659242), (0.48225, 0.26495311641693114), (0.4805, 0.27052506113052366), (0.48375, 0.239398313164711), (0.48125, 0.23339163196086884), (0.4855, 0.24465066385269166), (0.484, 0.23426245164871215), (0.4835, 0.24235641407966613), (0.48325, 0.20904621243476867), (0.4835, 0.20877022337913514), (0.48175, 0.19704684096574784), (0.48325, 0.2181271710395813)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           3       0.00      0.00      0.00      1000
           7       0.51      0.95      0.67      1000
           8       0.46      0.98      0.62      1000

    accuracy                           0.48      4000
   macro avg       0.24      0.48      0.32      4000
weighted avg       0.24      0.48      0.32      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [95]
name: no-alliance-95
score_metric: contrloss
aggregation: <function fed_avg at 0x712d0f864c10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=95
Partitioning data
[[2, 3, 8, 9], [0, 1, 8, 9], [4, 6, 8, 9], [5, 7, 8, 9]]
[(array([46451,  2561, 21623, 18088, 24694, 25512, 35140, 16671, 31785,
       37613, 18952, 26960, 37745, 26051,  8019, 41312, 44181, 19057,
       28834, 28377, 33205,  4293, 43484, 25415, 24165, 33699, 13141,
       33849, 36141,  7008, 21765,  2993,  4008, 26765, 29673, 35157,
       11882, 33204, 43422, 29048,  1670, 35466,  4426, 17883, 12140,
       30849, 25626, 31704, 18477, 27191,  4109, 30690, 37972, 19039,
       25982, 12992, 22618, 17188,  9629, 18496, 12782,   288,  6628,
        8234,  4888, 31677, 48501, 20285, 47632, 47846, 31809,  5686,
       28934, 37247, 21859, 47563, 43545,  9386, 41362,  2702, 48404,
        3673, 22307, 29779, 11385,  5384, 39594, 13071, 41763, 19950,
       12871, 18586, 30680, 34608, 22864, 37280, 27484,  6230,  3068,
       20864, 12346, 16906, 44340, 36883, 24199, 36726, 32257, 19694,
        2193, 44053,  3331, 14926, 46907, 32106, 39339,  6583, 16747,
       11089, 43476, 43503, 15589, 44542, 33621, 45359, 31428, 28443,
        6836,  9038, 19359,  2393,  2693, 19811, 30239, 18888, 37811,
       23642, 43961,  8871,  2033, 40716, 30466, 36450, 42328, 48784,
       24773, 38434, 33535,  8485, 20490, 21168, 27131, 21446, 18101,
        6944, 11486, 30572,  3786, 11171,  8165, 47727,  3253, 19163,
        9827, 21819, 38565, 37015,  7363, 34130, 40965, 42605, 22931,
         849, 42894, 22275,  7178,  3814, 30110, 45533, 33851,  2535,
       13300, 46457,  7009, 39675, 17329,  3338, 30096, 26265, 20563,
        2093, 36469, 34219, 28281, 27973, 32956, 26730, 41604,  8219,
       32457, 12287, 21746, 40849,  9740, 36622, 18138,  5219,  3791,
         586, 46887, 10767,  1676, 17273, 20570,  6069, 37198, 29354,
       46837,    18,  5012, 30454,  4604, 14472, 16593, 37848, 40583,
        7410, 49700,  6305,  2041, 47273,  9896, 13985, 45131, 26001,
       18234, 40115, 36860, 18485, 27499, 46516, 28158, 49725,  5151,
       43738, 39051,  9678, 40480,   924, 17267,  1050, 30013,  2521,
       36431, 26797,  1553, 41117, 44322, 49819, 11960, 29478,   861,
       31676, 34334, 38449,  5945, 30754,  6135,    21, 35525, 18942,
       32215, 13204,  8104, 22273, 42504, 38068, 21103, 13167, 46496,
       44635, 29607,  6847, 36990,  1496, 26485, 26126, 22914, 26675,
       41446, 46561, 46109, 47075, 32813,  6492, 14032, 34042, 17661,
       42133,    91,   806, 48957, 40852, 37410,  4628, 40086, 34379,
       20593, 17831,  6208, 42646,  8255,  7065, 43017, 36772, 28808,
       31460, 37399, 41116, 32371, 20704, 43773,  9564,   774,  9152,
       47410, 14941, 31380, 17047,  5733, 22278, 20425, 48614,  4010,
       47067,  7584, 37095,  1070, 41323, 32914,  7075, 28423, 49982,
       27525, 46290,  7872, 16904, 40226, 47439, 41993, 11359, 39095,
       44267,  9305, 47217, 20770, 25649,  3081, 19580,  7128, 10575,
        4982, 30476,  6556, 30571, 39552, 49233,  5445, 24022, 15154,
       27090, 10135, 25705, 12639, 16131, 20639, 27628,  7213, 33964,
        7180,  6782, 20330, 25218, 15836, 15388,  9107, 42354,  2930,
       48798, 45850, 48561, 17155, 41511, 28712, 24903, 35215, 25492,
       30122,  2562, 39543, 34649, 32520,  5423,    26, 10137, 49443,
       23417, 20163, 49563,  5601, 45053, 42783, 17545, 41493,  2903,
       17912,  1965, 16932, 39325, 14264, 29082, 49463, 42192,  7086,
        9930, 35307, 26598, 34710, 25953, 44973, 18259, 30270, 38066,
       24775, 11564, 48262, 16635, 24949,  6185, 38444, 43438, 13093,
        5534, 21712, 22226, 44530, 19577, 12241,  2525,    33, 38679,
       41053, 15411, 12155, 12076,   494, 21373, 13813,  8288, 44276,
       19727, 42445, 15967, 32379, 35293, 16754, 47014, 17430,  2011,
       46121, 47970, 27250, 23763, 42122, 40985, 33642, 30960, 33915,
       16308, 31974, 44868,  2581,  6433, 48494,   342, 12758, 31204,
        1864,  2565, 37204, 43257,  7389, 25443, 38149, 14232, 49835,
       14419, 37562, 18233, 46424, 15974, 15618, 22682, 38835, 23736,
       10848, 47380,  4292, 19768, 28782, 14369, 29647, 10876, 15237,
       43408, 14804, 13633, 21165,  8120,  8057,  2732, 16820, 16676,
       47430, 27728, 49852, 38754, 22696, 38666, 20650, 32887,  3516,
       47635, 49446, 10245, 31340, 38239, 25374,  1087, 16304,  5807,
       35115, 29371, 30745, 11676,  1957, 11149, 14464, 15835, 30990,
       25096, 47894, 40389, 43344, 34387, 34743,  2325, 19131, 42050,
       48552, 24085, 17418, 11180, 47259, 47202, 16473, 32421, 47094,
        5073,  8919, 26925, 27958, 22492,  7508, 34093, 14969, 35361,
       32644,  8429, 19979,  8892, 31183, 35671, 34641, 44373,  6909,
       33005, 29674, 35730,  6948,  4380,  9188, 38443, 36803, 38162,
       40780, 15046, 23405, 18892, 39114, 36560, 11693, 31332, 36405,
        2229, 28435, 49679, 24608, 13641, 36670, 32051, 42515, 44458,
        5949, 10445, 16457,  6397, 43100, 14497, 47971, 21919,  3980,
       42135, 14790, 44681, 10088, 26188,   410, 34471,  9651, 42371,
       32876, 45045, 41753, 23960,  2817,  2018, 11886, 31732,  9743,
        5297, 42203, 16257, 32760, 37953, 30438,  2534,  3832, 12418,
        2798, 23672, 21451, 14010,  3380, 40990, 20461, 46737,  6773,
       25093, 34814, 33869, 11309, 44236, 47291, 30164, 37072, 27842,
       17653, 48393, 42236, 24878,  9684, 42389,   192,  3359, 20620,
       35995, 26981, 12125, 11345, 29903,  8471, 21172, 21812, 13565,
       39627, 44649, 38881,  6830, 45718, 21457, 42693, 26105, 38817,
       18122,  1688, 24185, 36985, 11266, 33992, 30018,  5732, 29476,
       37641,  7876, 34296, 19081, 26579, 17962, 45830, 12951,  8031,
       35540, 11042,  2743, 19648,  2918, 23534, 14832, 40500, 29316,
       46386, 15239, 38436, 20228, 22920,  9527,  2298, 26149, 18660,
        8526, 48562,  3310, 32676, 15780, 40136, 36742,  3753, 47181,
       20596, 31097,  2754, 39278, 26510,  7149, 48611, 24787, 49897,
       23258, 34893, 13874, 31490, 24343,   369, 15752, 33563, 29352,
       35262,  7698, 21690, 26489, 28709, 37139, 37730, 39934, 34943,
        4321, 39856, 24188, 38494, 49997, 44996, 43039, 26384, 44188,
       15652,  1829,  5861, 30056,  9869, 18361, 11677, 49502, 45740,
       12385, 38928, 28065, 17974, 15817, 43910, 32605, 30796, 19625,
       23770, 44871, 33486,  1633, 37780,   706, 10559, 34587, 15857,
        9097, 18789, 38740, 40381, 22614,  2020, 39879, 23397,  9382,
       27791, 34903, 16910, 23569, 26610, 43376,   798, 26875, 33432,
       37499, 20799,  8650, 27237,  2192, 41363, 42638, 48435, 31911,
        5938, 21658,  3179, 27040,  5662, 13669, 16078, 27168, 10118,
       16174, 10309, 13199, 42025, 19674, 18423, 33248, 47700, 17716,
       25009, 12700,  2935, 29334, 24603, 34502, 18778, 40229, 11828,
       37444, 26942,  5591,  1075, 17941, 11388, 39583, 15142, 12718,
       14599, 39984, 39713, 41495,  2845, 34078,  1353,  8743, 48713,
       12948,  9026, 29721, 34811, 23726,  8245, 27420, 38405, 23299,
       41245, 47765, 34586,  7414, 25604,  2761, 15750, 24331, 37314,
       49057, 29820, 14179, 24214,  3381, 26769, 40819, 25897,   668,
       31742, 11111,  6984, 13000, 18860,  8807, 28560, 43341, 18785,
       17968, 21645, 17470, 17139, 35085,  4215, 43619, 11931,  7708,
       46039, 25091, 31039, 21766, 49634,  2495,  9432, 44829, 18337,
       21209, 39876, 15150, 21121, 49601, 28907, 49417, 41267,  4735,
       19353,  6571, 31106, 29186, 26021,  3672,  5310, 47159, 26511,
        3312, 42632,  1987, 17163, 47419, 44155, 38370,  9213, 29148,
       13665, 38644,   768, 38561, 28591, 22196,   188,  1796, 41282,
       44669, 24114, 24182, 44469, 13343,  1262, 19640, 11485,  1652,
       27570, 49072,  3316, 16454,  5275, 31862, 20430, 13919, 23243,
       41503, 21632,  6644, 47084, 30404, 16735, 33593, 12775,  1201,
       32221, 31324, 11678, 22995, 47636, 29043, 25433, 23291, 27695,
       34290]), [2, 3, 8, 9]), (array([32290, 12297, 10399, 24605, 13194, 11963, 30605, 31086, 32220,
       45004, 33249,  1831, 19630, 17456, 47253, 36924, 32190, 39601,
       28515, 33732, 16297,   116, 26412, 14898, 37324, 12294, 20454,
       40649,  9287, 10776, 32703, 43996, 28080, 39396, 20265, 27128,
       40343, 19066, 47220, 49799, 18745,  7444, 34964,  3304,  4746,
       40906, 48257, 12820, 38185, 47404, 22112, 31991,  9696, 28179,
       14486, 10867,  8086, 33919, 40328, 16316, 11509,  1715, 24802,
       28921, 17178, 32857,  4906, 36007, 25615, 40401, 22167, 37841,
       10989, 34279,   983, 48054, 15707, 13382,  5642, 22190,  1147,
       25616, 40418, 20398,  9657,  4411, 40542, 41324, 17395, 47689,
       46046, 22118, 21903, 12139,  8733, 37758, 30837, 45349,  5472,
       26662,  4141, 34123, 21467,  9157, 21000,  6240, 34009, 35758,
       24447, 26495, 19942,  5924, 38581, 17080, 38748, 28653,  9004,
       27431, 24639, 44946, 48059, 41609,  7731,  3842, 20574, 25016,
        6479, 44925, 22724, 44742, 28791, 11643, 18368, 49050, 43564,
        4192, 40890, 49735,  6425, 41266, 35763, 38791, 38326, 24641,
       12037,  4348, 28650, 40830,  1144, 48430, 38683, 11958, 23822,
       40319, 27749, 10787, 20257,  6165,  7801, 29917, 30888, 45088,
       14056, 40746,  1811, 23924, 15949, 24349,  5174, 40245, 42073,
       30609,  4271, 29893, 46137, 46343, 27155, 17432, 11937,  3066,
       35524, 15620, 34537, 13420,   822,  1514, 17303,  6682, 34855,
       37962, 47267, 27375, 49245, 15594,  9373, 41179, 16392, 37265,
       34125, 20816, 22449, 25644, 28216, 13467, 48355, 23854,  8759,
       44975, 28408, 35042,  3918, 44031,  7592,  7965, 24488, 16160,
       10637, 42175, 39012, 34173, 25820, 32263, 44319,  4060,  6008,
       26846, 33999, 11229, 42945, 44875, 45484, 39875,  1338, 41258,
        2435,  9390, 22123,  1270, 16011, 37881, 36415, 30595, 10990,
       31412, 24549,  9619, 40190,   115, 18854, 26311, 33207, 38037,
       25852,  3992,  6515, 42954,  7289,  7019, 27509,  4360, 30103,
       25915,  7102, 19382,  2994, 17534,  2597, 27404, 35070, 38958,
       46993, 18413, 25910, 26637, 18716, 23463, 48322, 33097,  2569,
       14247, 40283, 39828, 29536, 13722, 13852, 23391, 23248, 43847,
       42282, 14048, 17479, 21080,   275, 44818, 41241,  4496,  3851,
       23425, 31251, 11037, 41568, 31079, 17553, 43655,  7442, 17701,
       16813,  2582, 17539, 24068, 23901, 43015, 24762, 46956, 42976,
       11831, 19351, 12150,  6922,  9668, 10276, 18160, 28245, 26467,
       49589, 30486, 42143, 22983, 31338, 44185, 35344, 28822, 20731,
         354, 33939, 36086,   815,  8232, 46140, 31102, 30166,  6895,
       47362, 46213,  3212,   676, 49411, 34720, 31445,  2084, 34249,
        2318, 29765,  3327,  8197, 23198, 26008, 14851, 38333, 47824,
       28098, 25463, 24269, 45016, 26471, 41254, 42758, 28175, 42318,
       28192, 45268, 15867,  8350,  2180,   330, 13156, 34301, 25105,
       30119, 21593, 21540,  9043, 46173, 12348, 32319, 21992, 24725,
       42697, 33237, 38691, 21003, 28220, 35335, 39405, 11776, 24044,
       12303, 37145,  4464, 29791,  2185, 12523, 16774,  2067, 28810,
        6411,  9889, 23263, 40000, 17205,  9535, 12516, 49435, 47035,
       38086, 40595, 27814,  1377, 24041,  4332, 15779, 31523, 38515,
       22859,  2841, 32226,  8566,  5822,  6356, 40024, 13509, 34721,
       43453, 48791, 46649, 39195, 18583, 46238, 16850, 42834, 39404,
       42307,  8221, 43711, 46731, 31539, 18639, 39805, 29546, 19181,
       32886, 41398, 33252,  4385, 28089,  1574, 19450, 47745, 13088,
       20856, 35531,  5572, 42294, 43859, 32994, 22659, 46240, 10535,
       18890, 35134,  7778, 33569,  4185,  8189, 36542, 43132,  9318,
        3907, 28383, 29143, 29555,  9380, 47453, 29675, 17802, 29252,
       20897, 46461, 12047,  8735, 40394, 23789, 18961,  7352, 15710,
       21270, 44824, 37512, 49354, 42305, 41403, 48168, 39915, 25580,
       42870, 10475,  2464, 17624, 46854,  4848, 40671,  9324, 36267,
       15648, 24039, 19720,   139, 35388, 26863,  7941, 26328, 11141,
       19309, 23031,  6044, 42280,  6379,  2857, 41889, 19413,  4758,
       14891, 24905, 48877,   602,  3039, 40169, 49851, 38322, 46582,
       34094,  9648, 40191, 35416,  8348,  1673, 36579, 48774, 12442,
       25501, 26170,  5360, 21842, 12024, 35345, 11598, 23319, 23267,
       32574, 10171,  4812, 10572, 30035, 26977, 24131, 45062, 39999,
       11577, 19887, 37104, 42521, 31313, 32178, 43913, 31950, 21267,
       39658, 45907, 11529,  7045,  8952, 26369, 31121, 48225, 21596,
        9055, 44888, 18378, 49976, 43400, 24140, 13562, 22849,  9109,
        4304, 36882, 20491, 21830, 21042, 10962, 18840,  3430, 45461,
       45243, 46944,  8544, 13582,  8982,   460, 15011, 16942, 29786,
       36628, 27241, 11674, 29055, 30472, 39590, 42810, 40192,  3518,
       25002,  1671, 32896, 27056,  9326, 25176, 17168, 44205, 20530,
       11010, 32322, 43409, 49967,  4575, 28918, 42162, 13388, 15914,
        5794, 48821,  3898, 21157, 39207, 37659, 20182, 10620, 36892,
       40926,  6004, 15007, 11576,  5948,  3469, 30294,  1391, 21377,
       35864, 21484, 38809, 29287, 46128,  6160,  5787, 47886, 25888,
       34522, 38421, 21036, 27261, 28115, 23399, 35938, 34712, 46048,
       49789, 11265, 14872,  9469, 31644,  9645, 13810, 41727, 46615,
       36874, 26131, 15346, 22064, 37858, 21295,  3170, 43858, 18327,
       20509, 25669, 36636,  5339,  6250, 13015, 23307, 39605, 10483,
       31498, 47628, 49757, 25124, 19547, 21741, 17696,   567, 41042,
       17516,  6192, 49378, 16167,  3067,  7737, 17601, 17327, 41867,
       48286,  7468, 37524, 26856, 36262, 32014, 15276,  9488,  7104,
         860, 34894, 19020, 21608, 35917,  4066, 27692, 31456, 41059,
       12277, 17705, 21823, 33078, 35894, 25364, 39298, 47376, 26694,
       40368, 35736, 18236,   915,  4603, 10829, 30276, 40735, 27524,
       38777, 21883, 15901, 29169,  7793, 36456, 34796, 26152, 17240,
       13609,  4371, 46629, 47663, 25960, 48949, 24234, 28285, 47511,
        6178, 39453,  5519, 11717, 23444, 17765, 19601, 23229,  5801,
        3407, 21537, 33786, 23749, 34538, 18943, 24615, 33930, 17435,
       48827,  2529, 46833, 43121,  3671, 13586,  5013,   225, 13024,
        5332, 18947, 27622, 26597, 22152, 12182, 37993, 32595,  4665,
       34483, 21866, 39801,  8428,  8517,  8575, 29514, 14695, 20187,
       42681, 22296,  7457,  4701, 19377,  9511,  1223, 41905, 43281,
       40068, 23287, 20943, 41348,  6152, 19593, 13705, 16744, 14100,
       28046, 26447, 48051,  7691, 26508,  5671, 37141, 30412,  7042,
       33900, 29094, 41640, 35498, 19661, 22433, 13577,  4289, 37737,
       47690, 21065, 14077, 29013,  9106,  2379, 38056, 17731, 19289,
        8811, 27439,  6286, 19410, 46345, 29609,  1034, 31638, 27198,
       25883, 32841, 37490, 33668, 10421, 32778, 49238, 40157, 11104,
        1706, 38602, 29335, 42387, 17201, 33671, 21555, 11213,  8951,
         360, 30073, 44157, 10071,  1767, 35330, 38516, 32717, 24152,
        6038, 27566, 31869, 32163, 46298,   657, 40742, 37538, 48743,
       28959, 10533, 15027, 48868, 42278,  7746, 13148, 41634, 32158,
       33856, 43377, 35206, 35961, 10859, 37454, 17406, 21648, 29349,
       35047,   666,  3023, 49694, 37540, 46774, 27675,  3923,  4587,
       47637, 37031,  3697, 36401,  3097, 35657, 20472,  2351, 34158,
       30179, 33639, 40083,  9739,  8159, 29938,  5978, 47944,  9244,
       27588, 48734, 14626, 27589,  9461,  4704, 33839, 13904, 36288,
       35138, 20925,  1774,  8644,  4252, 46656, 16909, 25001, 30678,
       12383, 11533, 37535, 16347, 28166, 40722,   884, 21583, 45860,
       47083, 27997, 17021, 11373, 46781,  6171, 42458, 30215,  3643,
       33982, 39522, 37600, 47448,  2437, 10252, 13889, 48751,  8126,
       31115]), [0, 1, 8, 9]), (array([24658,  3862,  9010,   693, 29770,  6021, 16461, 25898, 18340,
        6527, 37075,  6586, 21753,  2691, 41953, 40195, 39146, 39656,
       46415, 21838, 41770, 14996, 18559, 33434, 39009, 49727, 39772,
       31349,  5940, 15656, 14815, 23455, 42403, 29663,  3587, 42512,
       35624, 25949, 29466, 17033, 45773,  6924,  4053, 43683, 14415,
       30815, 48478, 15503, 41023,  7574, 28489, 23029, 39740, 26653,
       18099, 16254,  5289, 17585, 25441, 38995, 12179, 27853, 20575,
       41518, 36142, 46153, 39878, 40510, 27923,  7407, 38353, 35113,
       28226, 15582, 44473,  9041, 33215, 36960, 25017, 30907, 30911,
        2712, 12103, 26602, 13186, 22536, 20810, 33027, 29629, 25803,
       17937, 33223,  3690, 46526, 20409,  4296, 41305, 33080, 18699,
       25529, 28040,   930, 33735, 23420, 36225, 42303, 21978, 15737,
       24769, 17632,  2327, 38123, 39177, 22750, 20798, 36003, 18279,
         434, 28803, 28654,  3252, 12334, 43222,  1256, 23460, 41003,
       25992, 13412, 18690, 15802, 15428, 42544, 20782,   951, 27455,
       39209, 33501, 45571, 12416,  1158, 18293, 41822,  3089,  3687,
       42695,  3776, 47126, 28334, 20608,  7404, 19627, 44780, 24967,
       22318, 10150, 23057,  4763, 10661, 47716, 44558, 14704, 21341,
       49595, 34365, 27466, 26894, 37827, 31290, 18734,  7775,  4660,
       41699,  2830, 44760, 35634, 43793, 40028, 35250, 13025, 22018,
       34051, 30522, 14181, 32786, 30790, 48812,  1348, 19098, 38660,
       27860, 16195,  5619, 27193, 22361, 18566, 38852, 31143, 30499,
        7382, 25785, 26844, 14937, 44978, 45512, 18836, 25941, 45586,
       38386, 10350, 38103,  5491,  5347, 28100, 17728, 24883, 40665,
       24265,  8523, 37281, 37124, 35180,  8404, 19833, 49597, 36923,
         677,  2216, 17613,  4043,  6475,   543, 25834, 10561, 10970,
       25620,  2155, 23743, 31470,  8131, 25147, 46644,   925, 33266,
       11586, 46416, 29102, 35429, 30225, 31840,  6844, 21155, 31301,
       24242, 38698, 49412, 33953,  8474, 28925, 32859, 31792, 29751,
       16367, 24531, 31270, 19511,  3048, 40041, 31984, 33630,   854,
       21435, 17562, 39680, 19308, 42679, 26197, 28607, 43269, 27346,
         204, 45918,  1342, 36922, 16687, 38771, 16685, 49771, 36913,
       15830, 18282, 35535, 37270, 38061, 31135, 10792,  7025, 32880,
       37940, 11602, 31672, 46271, 21072, 36069, 32869,  8603, 48897,
       31570, 29284,   645, 36602,  9256, 18988, 14864,  9978,  2774,
       42391, 20146, 41916,  6582,  7034, 25892,   488, 45443, 38508,
        3262, 22746,  4824, 37380, 29217, 40396, 31080, 31928, 15497,
       17847, 21146, 23154, 48619, 17427, 41969,  7524, 14065, 49285,
       44590, 10704, 32648, 35891,  3266, 38738, 14188, 11751, 22290,
         103, 44194, 49364, 28776, 26190, 43026,  4926, 33967, 27022,
       38475,  9029, 20293,  9521,  1842, 25964, 23780,  8873, 23050,
        8564, 26387,  4751, 15962, 31508, 25511, 14443, 48400, 34033,
       31312, 14984, 35455,  9381, 33425, 49129, 18396,  4550, 36164,
       10649, 49798, 36232, 44018, 48401, 23230, 26335, 41668,   680,
       30350, 14638,   132, 43182, 47792, 35225, 36373, 16789, 26192,
       39415, 46991, 34805, 16389, 36638, 19870, 13619, 33733, 27688,
        8460, 24298, 22082, 11634, 29009,   655, 31852, 16998,  9953,
       48277,  7264, 26415, 34926, 16287, 38583, 49928, 12459, 23896,
       20853, 24732, 28663, 10600, 19665, 19281,  9924, 46622,  2650,
       19420, 40313, 20025, 29327, 13878, 24424, 23917, 36515, 16562,
       41558, 23216, 14513, 11911, 30089, 14411, 38665, 42641, 23539,
       18224, 41272, 49061, 39544, 17342, 21264, 18090, 27830,  7422,
       42145, 42724,  8956, 28490, 46995, 25835, 13291, 15039,  5443,
        4570, 32958, 23135, 38773, 15498,   437, 20687,  4752,  2925,
        7959,  5474,  9323, 37457, 49404,  3698, 10117, 43074,  9706,
       21719, 10444, 27450,  3567, 21192, 21972, 31652, 44984, 22471,
       18693, 24414, 47021, 29446, 44685, 12426,  2670, 20727, 36371,
       48911, 29175,  2923, 15795, 36390, 49358, 44659, 15676,   888,
        5245, 14823, 46636, 49712, 29530,  1300, 12867, 30639, 19817,
       16435, 33145, 33029, 49389, 13684, 24860, 19618,  1822, 16572,
       24286,  4507,  1174,  1032,  7484, 16147, 10810,  4892, 28295,
       49543, 48164, 29344, 33537, 10864,  9693, 10923, 46029, 19123,
       13115, 23321, 36856, 20298, 35078, 42571, 21546,   870, 27621,
       12206, 27057,  9674, 41314, 19167, 46787,  7534, 47960, 19658,
        9483, 15241, 26214, 18959, 40344, 48127, 34582,   987,  1138,
       48281, 46008, 44438, 28537,  7012, 31872, 18141, 48133, 21164,
        1375, 19219, 44807,  4611, 43612, 29661, 12041, 33372, 11591,
       42202, 34683, 31745, 40408, 49778, 31085, 21515, 30647, 39776,
       16902, 21679, 27124, 43623,  2471, 10524, 39702, 13197, 18125,
       43713, 28287, 18797, 43357, 10511, 23080, 47694, 46985, 22583,
        5643,   892, 47562,  3765, 10068, 40429, 16156, 32611, 37044,
       36949, 17935, 14123, 21427, 19733, 35171,  1236, 33756, 39072,
       21312, 30504, 36942, 32726, 14901, 38625, 17783,   244, 36214,
       26004, 26402, 32351, 15416,  7614, 19605, 28706, 22760, 16084,
        3330,  4210, 38274, 41002, 34212, 28204, 21611, 47854, 12207,
       32265, 15022,  9913, 38619,  4469, 19214, 26938, 35912,  7329,
       49501, 34104,  7310, 41507, 41674,  7281,  2220, 12725, 10975,
        9160, 40110,  4988, 31069, 17685, 23567, 32037,  8766, 12258,
        4916, 26454, 15617, 26608,  6418, 17020, 26239, 28642,  2272,
        2974, 48905,   170, 39956, 23669, 34004, 10817, 25399, 49568,
       18700,  1014, 35536, 12487, 47019, 20786,   795, 22551,  8926,
       38089, 10997, 14046, 10016, 39094, 32234,  1732, 17891, 16532,
       34815, 40179, 16620, 19464, 38008, 42677, 13139, 23765,  9796,
       28012, 30529, 32622, 26209, 24408, 29990, 17404, 29565, 14458,
       46955, 18000, 22898, 48140, 43946, 14510, 32851, 37126, 31014,
       27146, 16302, 13963, 22925, 20765,  7461, 12816,   756, 39330,
       44511, 16317, 24066,  4136,  9358,  1378, 43787, 47876, 44919,
       29972, 15694, 10312, 29926, 12085, 33028, 49793, 21738, 35625,
        8838, 21562, 16601,  2508, 46190, 33806, 33896, 22637, 13358,
       31845, 44410, 30733,  4340, 46269, 33149, 43137, 42764,  2853,
       12008,  2956, 47124,  4131, 19801, 17918, 21219, 41128,  3090,
       44723, 28318,  4169,  1744, 28601, 33208, 18786, 46687, 41409,
       39743,  6784, 15085, 37427, 24372, 19424, 16255, 12835, 37433,
       15337, 37793,  4115,  2199, 24055, 38300,  3129, 16480,  9911,
       10351, 37071, 17798, 42560, 27380, 21129,  5203, 44380, 12902,
        3876, 44056, 18244,  5637, 12405, 46852, 23284, 45993, 49931,
       25051,  5172, 40507,  4541, 40315, 31089, 46861, 29472,   749,
        8850, 49065, 36381,  8539, 30823, 44332, 30392, 11232, 15096,
       16362,  1277, 49662, 21221, 35521, 40750, 29073, 41396,  3479,
        2548, 41395, 36837, 23341, 45755, 16373, 16923, 36875,  3477,
       22150, 23550, 17313, 29418, 35974,    14, 28534, 33858, 22220,
       46090, 22694, 28572,  1638, 25991, 19824, 13767, 45431, 20995,
       19709, 23647,  7920, 42554, 24133,  9211, 49461, 17042,  3705,
       10144, 28879, 19122, 33794,  2001, 41906, 27365, 41126,  2767,
        4111, 10257, 46186, 34645, 10827, 48442, 39894, 26627,  6834,
       14083, 37201, 43942, 36640,  9036, 49945, 42406, 16092, 40885,
       42343, 24731, 47296, 38687, 36866, 23471, 38153, 28781, 14170,
       38324, 16725, 17176, 16917, 21817, 44289, 43696, 45183,  4028,
       17869, 48461,  3793, 30615, 30985, 39183, 49520, 22346,  4223,
       37363, 48178, 14220, 23303, 24391,  8762, 42349, 23403, 15690,
       42213, 39955, 29384, 18488, 20462, 48033, 37132, 17973, 10208,
       35618]), [4, 6, 8, 9]), (array([18202, 17830, 41371,  4579, 25845, 41499, 38106,  6723, 19999,
       10715, 11826, 47524, 17931, 39475, 35543,  5108, 37364, 34846,
       18921, 30290, 27892,  4002, 47659, 18245, 30698, 29077, 21318,
       25606, 39381, 16436, 10597,  1072, 37632, 25886, 33259,  3060,
       38860, 49541, 29615, 40657, 36350, 47977, 24909, 46422, 20809,
       19071, 26700, 33878,  7750,  8756, 15527, 17375,  1274,  8591,
        9999, 49118, 31409,  2305, 34188, 39229,  2013, 43330,  4009,
       19024, 42745,  7406, 33800, 46393, 19362, 11185, 18521, 24585,
       49508, 41680, 11328, 30099, 45417,  4463,  4692, 41034,  1152,
       47158,  1993, 12198, 29321, 10242, 31038, 22470, 18985, 25065,
       14184, 21879, 27947, 14246, 22225, 12728, 44756, 24380, 47783,
       24663, 33629, 19096, 18626, 31059, 29040, 21179, 39403, 46903,
        5100, 48882, 42293, 48736, 18059, 21077, 34790, 24703, 22021,
       20484, 10549,  1588, 26560, 24309, 17921, 17354,  8407,  1111,
       32427, 21678, 39456, 46289, 38807, 11043, 26735, 32877, 48673,
       12839, 27828, 25284, 11145, 13190, 41475, 22671,  5613, 15488,
       45440, 47090, 35155, 20747, 38305,  1662, 44388, 22430, 15549,
       40044, 30140, 33989, 34444, 18721, 46980, 35547, 18598, 45674,
       27835, 27804, 44223, 30684, 37568, 44284, 32862, 40538, 29251,
        9772, 13142, 33172, 32835, 26917, 47485, 19925, 25919,  5175,
       27805, 16798, 15312, 20315, 30085, 44337, 32764, 22590, 40014,
        8928, 38839, 41817, 44377, 18685,  4316, 43543, 49267,  4662,
        2059,  9132, 27401, 19595, 10831, 19757, 18212, 47160, 36532,
        3794, 11507,  3714, 30718, 35510, 26010, 15301, 31129, 46716,
       27885, 45831, 32935, 37396, 29890, 20207, 35819, 44496, 44792,
       44676, 45181, 18966, 39718, 36471, 22174, 11165, 10104,    83,
        2231, 44988, 26313, 11603, 21502, 39647,  3022, 11610, 20348,
       14391, 12716, 38172, 35652,  9235, 37795, 21233, 22647, 44098,
       45147, 47308, 10384, 22626, 39767,  8187, 23988,  3279, 35623,
        2201, 42955, 34900, 32343,  1896, 38890,  5295, 36303, 14503,
        2397, 20099, 45596, 35005,  3272, 21100, 39717, 40026, 22115,
       34733, 26562, 33585, 26702, 35698, 42078, 36270, 14400, 11278,
       28152, 18823, 20565, 42126,  1931, 11697, 31871, 34010, 38252,
       18258, 13409, 29886,  1582,  3261,  9560, 10905, 24237, 40912,
       40744,   492,  6271,  7874,  9719, 46117, 17929, 34412, 41390,
       37800,  6808, 17503,  4396, 49048, 20903, 21244, 29733, 38780,
       19391,  5047,  2140, 29727, 30617,  4480, 36608, 19064, 27161,
       28884, 43360,   113, 48837, 18869, 37601,  2008, 20998, 25617,
       28553, 44076, 40426,  1149, 45945, 32487, 16355,  4264, 18914,
        2813, 34291,   739, 29622, 32399, 27381, 17804, 19119, 48823,
       48741,  1891, 26358, 32815,  5241, 47923,  5564, 24853, 10735,
         191, 45561, 49310,  6821, 10496, 45058, 47246, 15543, 49912,
       25968,  5126,  9184, 14837, 45540,  4736, 47864, 10994, 22198,
        8115, 47346,  9443, 24439,  8038, 46671, 12759, 16198, 38142,
        8897, 44839, 33652, 22595, 33605, 18219,  9689, 36936, 28735,
       23621,  7613,  4325, 35483, 19263, 21113, 12985, 19940, 31839,
       23873, 25804, 20738,  4898,  9486, 19956, 32579, 17643, 10459,
       30925, 47286,  6460, 22267, 47285, 44664, 42500, 29326, 20707,
       40692, 43066, 29778, 42996, 43544, 21410, 43844,  7611,  7577,
       22739,  3104, 40107, 33636, 34599, 22612, 17944, 16764, 42137,
        6680, 34339, 28240, 38597,  9958, 18524, 10778, 47323,  9082,
       44353, 21599, 46167, 40866, 28977, 28206,  3573, 45610, 27998,
       17398, 34868, 35284, 15889,  8277, 18916, 25811, 27682,  6164,
        7831,  1716, 28914, 33254, 37423, 43587, 17467,  4820, 41416,
       37467, 29137,   595,  2195,  5958, 37190, 48055, 35020, 41973,
       38962, 16881, 45874, 21101, 14356,  7437, 12646, 25513,   106,
       22132, 26623,  8235, 48125,  5185, 19706, 25906,  9975, 34983,
       14011,  5546, 48156, 27612, 43048, 49144,  6176, 48462, 26112,
        1255, 45917, 42917, 30664, 17162, 31903, 18397, 39653, 39317,
       40019, 33190, 32897, 27643, 38607, 12172,  8455, 17519, 36766,
        5365,  9206, 35210, 42265,  8830, 32663, 14624,  3094,  3244,
       24127, 31018,  2483, 27252, 25258, 38904, 43198,  6880, 21257,
       46926, 30941, 19283, 14149, 17799, 11244, 17813, 44357, 31692,
       18171, 26098, 26172, 27068,  9023, 14045, 46102,   901, 49681,
       24740, 19748,  4303, 25566, 24444, 13893, 13623, 18076, 31725,
       14826, 32448, 39462, 28599,  1970, 35150, 20089, 18950, 36484,
       17022, 29556, 44490,  3614, 35586, 28829, 26182, 27618, 37420,
       45784, 35169, 28929, 41732, 40147,   503, 25088, 11977, 48597,
        4823, 15429,  8180,  2954, 48737, 37534, 46395, 25987,  6598,
       44577, 19028, 25728, 32560, 47784, 18312,  7678, 42367, 12490,
       30123, 32765, 10003,  3121, 10408, 16430, 47646, 30444, 27061,
       44569,  8940, 38689, 15135,  1821, 17933, 26948, 29014, 39015,
       40415,   943, 23827, 36733, 11001,  9159, 13355, 47814,  9567,
       30788, 23731,  3250, 46477, 12422, 33571, 24855, 41413, 21943,
       33661, 17120, 39210,  1782, 16330, 42770, 24588, 22177, 25744,
       28323, 35562, 13095, 33356,  4580,   627, 11461, 23890,  5987,
        4828, 28153,  9844,   609, 36678,  9951, 19342, 16729, 29479,
        2658, 44414, 49955, 25072, 47881, 24629, 38371, 12452,  1506,
       17630, 22010, 27861, 32578,  7334, 28103, 28066,  2721, 39454,
       46699,  2454,  2523, 32506, 35035, 22524, 38901, 40609, 13923,
       30252, 29690, 36253, 22584, 43807, 26703, 38969, 16847,  5283,
       38158, 30514, 20479, 15422, 46028, 25040, 20662, 21483, 48175,
       39142,  3541, 35617, 47793, 41163, 24347,  4029,  4742, 44382,
       44927, 14866, 46068, 27395, 49405, 44010, 18968, 27975, 37606,
       18467, 36309,  9787, 32249, 41920, 45735, 26506, 39993, 34730,
       11457, 28846, 17977, 20248,  7694, 48123, 25036, 40082, 39586,
       25507, 14278, 18166,  1940, 11547, 39746, 49158, 40923, 18030,
       45465, 39427, 23091, 34772,  6288, 40059, 20176, 29909, 31662,
       32273,    76, 27005, 45829, 36118, 18792, 34573, 27603, 24537,
       19563, 46280, 21954, 30343, 26650, 12653, 39841, 13717, 25988,
       44066, 36521, 49637, 43199,  8567, 41765, 29401, 42454, 45197,
       33739, 10306,   214, 45388, 49398, 32146,  3006, 21507, 47656,
        8402, 46024, 34251, 14564, 10246,  9261, 42452,  8788,  6708,
       30247, 11984, 17296, 15159, 47002, 27575, 17371, 31056, 44212,
       21592, 25994,  5071,  5352, 46243, 45241, 31815, 34175, 36475,
       25285, 12531, 49278, 49244,  1773, 49385, 40237, 26013, 40063,
        3345, 21273, 31637, 15420,  9513, 20441,  8565, 11275, 33656,
       38393, 21300, 22233, 25393, 31162,  8075, 48455, 28159, 46830,
        2757, 39781, 10448, 45405, 32115, 34753, 49300, 13125, 22915,
       23464, 39564, 49453, 34077, 45167, 31483, 21182, 36266,  2477,
        2218, 13531,  5191, 26970, 20069, 32040, 32940, 41786,   541,
       34957, 21010, 30178, 14610, 16293,  8896,  1026, 19248,   306,
       33316, 21542, 45577,  3597,  9683, 42556, 47592,  8821, 22815,
       28888, 29574, 29529, 43325, 18642,  5056, 49125, 35288, 25506,
       46788,   219, 35178, 17052, 27784, 25338,  9971, 49074, 15597,
        7703, 16051, 20013, 40222, 28656,  1602, 20600, 43882, 17336,
       43900,  8394,  1456, 26218, 44810, 23961,  3216, 16107, 42140,
       18394, 16728, 39546, 18252, 16631, 46302, 36818, 25907,  8012,
       11720,  4454, 31905,   883, 18989, 31205,  6149, 37751,  4826,
       41415, 15031, 35280, 42714,  3685, 16234, 17061, 29825,  3719,
       49284, 20250,  3526, 10658, 11311, 48766, 24027, 18846, 10187,
       43634]), [5, 7, 8, 9])]
Competition
DC 0, val_set_size=1000, COIs=[2, 3, 8, 9], M=tensor([2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.226, 0.04464438664913178)
DC 1, val_set_size=1000, COIs=[0, 1, 8, 9], M=tensor([0, 1, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04425914561748505)
DC 2, val_set_size=1000, COIs=[4, 6, 8, 9], M=tensor([4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.244, 0.04420206964015961)
DC 3, val_set_size=1000, COIs=[5, 7, 8, 9], M=tensor([5, 7, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04452222609519958)
D00: 1000 samples from classes {8, 9}
D01: 1000 samples from classes {8, 9}
D02: 1000 samples from classes {8, 9}
D03: 1000 samples from classes {8, 9}
D04: 1000 samples from classes {8, 9}
D05: 1000 samples from classes {8, 9}
D06: 1000 samples from classes {2, 3}
D07: 1000 samples from classes {2, 3}
D08: 1000 samples from classes {2, 3}
D09: 1000 samples from classes {2, 3}
D010: 1000 samples from classes {2, 3}
D011: 1000 samples from classes {2, 3}
D012: 1000 samples from classes {0, 1}
D013: 1000 samples from classes {0, 1}
D014: 1000 samples from classes {0, 1}
D015: 1000 samples from classes {0, 1}
D016: 1000 samples from classes {0, 1}
D017: 1000 samples from classes {0, 1}
D018: 1000 samples from classes {4, 6}
D019: 1000 samples from classes {4, 6}
D020: 1000 samples from classes {4, 6}
D021: 1000 samples from classes {4, 6}
D022: 1000 samples from classes {4, 6}
D023: 1000 samples from classes {4, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.0756532991528511) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.07162128928303718) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.346, 0.10060960775613785) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.262, 0.09178792214393616) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.08844312906265259) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.08061573424935341) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.407, 0.13195230773091315) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.319, 0.1186123097538948) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.373, 0.09776843130588532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.428, 0.0912200716137886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.15404716596007348) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.14389754155278206) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.382, 0.116108429312706) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.1028368262052536) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2008201377093792) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.19386132979393006) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.383, 0.11486499130725861) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.12005862887203693) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.2020772139430046) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.1812336931824684) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.396, 0.12786139059066773) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.12526907271891832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.411, 0.2141107092946768) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.188261329382658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.391, 0.12239557676017285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1390012750029564) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.363, 0.259074219936505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.19005238442867994) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.397, 0.1381242759525776) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.1493007049560547) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.21411252587661148) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.2096376870945096) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.393, 0.1398952288478613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.16985845598578453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.241391664955765) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.22949583377689123) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.15190104061365128) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.1669399950876832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2810998134780675) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.2314559705518186) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO2', '(DO0']
DC 1 --> ['(DO4', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.14819466790556907) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.16509734315425156) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.2698406748874113) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.24066928440332414) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.402, 0.1502597609013319) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.18719358260184527) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.46, 0.30063836242631076) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.2581026356909424) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.416, 0.15784696668386458) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.19852948942035437) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.3118632292384282) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.2808020170852542) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.406, 0.16053531931340695) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.20606785009801387) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.30334638703241945) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.281668808626011) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.413, 0.18466702499985696) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.19969489989802242) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.31387039930187166) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.438, 0.26754720909148455) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO3']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.42, 0.18241956287622452) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.19411430779471994) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.3299275228846818) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.3034121121559292) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.422, 0.17459096457064152) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.18537217374518514) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.46, 0.3537083266917616) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.2878089199066162) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.418, 0.17225475420057773) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.18874617958441375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.3464675769507885) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.26299071485549214) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.421, 0.16901295679807662) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.18409435104206204) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.3480771175827831) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.28313872720301153) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.413, 0.18194218304753304) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.20262362392852082) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.3569809230528772) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.2880335472077131) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.42, 0.18253671583533287) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.19414126037806273) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.3549287891369313) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.435, 0.2790898114461452) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.406, 0.17764553666114807) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.185006136697717) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.3290681131165475) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.31521715366840364) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.419, 0.17762191314995288) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.18613020816631615) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.3342141398712993) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.31975574570056053) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.417, 0.1764321932941675) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.19262803614884616) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.3325129809267819) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.31167995275743304) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.419, 0.17013566634804012) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.455, 0.18318453336786478) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.34679520229902117) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.3002378221452236) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO1', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.1901739945486188) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.1926658388376236) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.34361846156651155) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.2963726092353463) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.423, 0.19573377124220132) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.1976930126901716) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.29754124507494273) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.30634071891754866) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.42, 0.17834052499383687) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.19688121790997684) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.3103572289608419) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.32910937344282865) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.416, 0.18863747468590736) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.191198756987229) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.33107129427883775) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.291440998993814) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.413, 0.20221407169103622) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.20273919802671297) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.3435690187793225) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.30244809708558024) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.416, 0.19331583303958177) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.455, 0.1997794385291636) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.46, 0.33199482193775476) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2878232104685158) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.421, 0.17763643014431) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.2015198844913393) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.3404294975027442) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.3131593371573836) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.20377747366577387) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.452, 0.20457448233058675) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.3156301446855068) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.2689219198450446) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.425, 0.22268471428751946) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.454, 0.18332882268261164) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.35120142030064017) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.26076014878973364) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.416, 0.21465281653404236) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.21998785559041426) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.3508970624320209) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.30189474164694546) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO1']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.417, 0.2094459986835718) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.457, 0.2085678064627573) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.37146556910476647) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.34171447382867337) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.419, 0.22726091001182794) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.457, 0.21111423067795113) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.3186231043059379) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.3585887736864388) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.421, 0.24740395943820476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.2384328167135827) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.3687080460321158) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.32553153825551273) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.414, 0.21853885652124883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.2163343727020547) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.3612777879412752) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.3238630913607776) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.409, 0.2407071293592453) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.2524109378766734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.35901157041732223) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.27589770877733827) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO1']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.42, 0.25476020629704) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.25056258589948993) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.457, 0.33436826402507724) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.435, 0.2945694554373622) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.409, 0.21483455884456634) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.2456296115424484) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.46, 0.3297088329028338) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.31967372220382095) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.422, 0.24940086664259434) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.24096485979156568) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.330707564111799) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.2968519252240658) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.414, 0.23848743838071823) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.2726296443929896) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.35179502390138806) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.31638237251341345) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.418, 0.22980033060908317) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.23349965895805508) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.35507877797819676) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.318288114015013) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO0', '(DO3']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.2443059034720063) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.2471771480580792) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.33479744114540516) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.30472207862511275) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.416, 0.2167256050184369) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.25250412161741403) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.453, 0.31179584946576505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2581638730838895) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.416, 0.22005150156468153) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.458, 0.2471035648449324) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.310456198701635) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.26797134523838756) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.2026735519617796) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.24918111177417449) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.3038789313491434) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.26954679476469756) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.42, 0.1985349648669362) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.2510680604758672) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.3282618778673932) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.2821302154660225) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.226, 0.04464438664913178), (0.25, 0.0756532991528511), (0.338, 0.08844312906265259), (0.373, 0.09776843130588532), (0.382, 0.116108429312706), (0.383, 0.11486499130725861), (0.396, 0.12786139059066773), (0.391, 0.12239557676017285), (0.397, 0.1381242759525776), (0.393, 0.1398952288478613), (0.411, 0.15190104061365128), (0.411, 0.14819466790556907), (0.402, 0.1502597609013319), (0.416, 0.15784696668386458), (0.406, 0.16053531931340695), (0.413, 0.18466702499985696), (0.42, 0.18241956287622452), (0.422, 0.17459096457064152), (0.418, 0.17225475420057773), (0.421, 0.16901295679807662), (0.413, 0.18194218304753304), (0.42, 0.18253671583533287), (0.406, 0.17764553666114807), (0.419, 0.17762191314995288), (0.417, 0.1764321932941675), (0.419, 0.17013566634804012), (0.411, 0.1901739945486188), (0.423, 0.19573377124220132), (0.42, 0.17834052499383687), (0.416, 0.18863747468590736), (0.413, 0.20221407169103622), (0.416, 0.19331583303958177), (0.421, 0.17763643014431), (0.41, 0.20377747366577387), (0.425, 0.22268471428751946), (0.416, 0.21465281653404236), (0.417, 0.2094459986835718), (0.419, 0.22726091001182794), (0.421, 0.24740395943820476), (0.414, 0.21853885652124883), (0.409, 0.2407071293592453), (0.42, 0.25476020629704), (0.409, 0.21483455884456634), (0.422, 0.24940086664259434), (0.414, 0.23848743838071823), (0.418, 0.22980033060908317), (0.41, 0.2443059034720063), (0.416, 0.2167256050184369), (0.416, 0.22005150156468153), (0.408, 0.2026735519617796), (0.42, 0.1985349648669362)]
TEST: 
[(0.236, 0.04354009944200516), (0.25025, 0.07285457700490952), (0.325, 0.08505733188986778), (0.3635, 0.09408129823207856), (0.36725, 0.11148307979106903), (0.381, 0.10974129223823548), (0.39125, 0.12281110650300979), (0.38925, 0.11663438200950622), (0.393, 0.1320787268280983), (0.38825, 0.133778289437294), (0.39625, 0.14531275349855424), (0.3985, 0.13918446958065034), (0.399, 0.14265991061925887), (0.403, 0.15024389266967775), (0.4025, 0.15176785176992416), (0.402, 0.17561580961942672), (0.405, 0.1736242316365242), (0.408, 0.1640420594215393), (0.40075, 0.16187267780303954), (0.40375, 0.15966084522008897), (0.40225, 0.17179893559217452), (0.4025, 0.17390288424491882), (0.39975, 0.16980949836969375), (0.40625, 0.1695454176068306), (0.4005, 0.16696502196788787), (0.403, 0.16178481262922287), (0.405, 0.180226682305336), (0.4045, 0.18638025563955307), (0.40875, 0.1704273986816406), (0.40975, 0.17790584790706634), (0.4085, 0.19040372508764267), (0.404, 0.17980047684907913), (0.404, 0.16850687527656555), (0.40175, 0.19309143257141112), (0.40675, 0.2091173503398895), (0.4045, 0.20220660722255707), (0.40575, 0.19812669610977174), (0.4085, 0.2164909645318985), (0.40625, 0.23505188035964966), (0.40375, 0.21060834205150605), (0.4085, 0.2277601215839386), (0.40475, 0.23956910479068755), (0.40025, 0.20749075317382812), (0.40425, 0.23671267318725586), (0.407, 0.22790457618236543), (0.404, 0.22036996638774872), (0.401, 0.2323450312614441), (0.4035, 0.20971221971511841), (0.40675, 0.2108853130340576), (0.39825, 0.1956424881219864), (0.4075, 0.189319171667099)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.33      0.88      0.48      1000
           3       0.55      0.75      0.63      1000
           8       0.00      0.00      0.00      1000
           9       0.00      0.00      0.00      1000

    accuracy                           0.41      4000
   macro avg       0.22      0.41      0.28      4000
weighted avg       0.22      0.41      0.28      4000

Competition_DC_1
VAL: 
[(0.25, 0.04425914561748505), (0.265, 0.07162128928303718), (0.304, 0.08061573424935341), (0.428, 0.0912200716137886), (0.443, 0.1028368262052536), (0.447, 0.12005862887203693), (0.446, 0.12526907271891832), (0.459, 0.1390012750029564), (0.456, 0.1493007049560547), (0.464, 0.16985845598578453), (0.461, 0.1669399950876832), (0.471, 0.16509734315425156), (0.466, 0.18719358260184527), (0.466, 0.19852948942035437), (0.469, 0.20606785009801387), (0.463, 0.19969489989802242), (0.464, 0.19411430779471994), (0.462, 0.18537217374518514), (0.466, 0.18874617958441375), (0.472, 0.18409435104206204), (0.462, 0.20262362392852082), (0.461, 0.19414126037806273), (0.469, 0.185006136697717), (0.466, 0.18613020816631615), (0.462, 0.19262803614884616), (0.455, 0.18318453336786478), (0.46, 0.1926658388376236), (0.467, 0.1976930126901716), (0.46, 0.19688121790997684), (0.464, 0.191198756987229), (0.46, 0.20273919802671297), (0.455, 0.1997794385291636), (0.456, 0.2015198844913393), (0.452, 0.20457448233058675), (0.454, 0.18332882268261164), (0.463, 0.21998785559041426), (0.457, 0.2085678064627573), (0.457, 0.21111423067795113), (0.463, 0.2384328167135827), (0.461, 0.2163343727020547), (0.456, 0.2524109378766734), (0.46, 0.25056258589948993), (0.468, 0.2456296115424484), (0.464, 0.24096485979156568), (0.466, 0.2726296443929896), (0.456, 0.23349965895805508), (0.461, 0.2471771480580792), (0.461, 0.25250412161741403), (0.458, 0.2471035648449324), (0.46, 0.24918111177417449), (0.46, 0.2510680604758672)]
TEST: 
[(0.25, 0.04342002448439598), (0.266, 0.06890167579054833), (0.31225, 0.07728723338246346), (0.42825, 0.08714401468634606), (0.44875, 0.09824884018301963), (0.45125, 0.11498481222987175), (0.44725, 0.11927160546183586), (0.4605, 0.13327452754974364), (0.4625, 0.14331845676898955), (0.4655, 0.1620286877155304), (0.467, 0.16024554711580277), (0.4695, 0.16026438838243484), (0.4685, 0.18147472405433654), (0.47125, 0.19327810347080232), (0.4705, 0.19787482005357743), (0.46575, 0.19182522523403167), (0.4715, 0.1862547504901886), (0.4655, 0.17858768194913865), (0.47175, 0.17999083524942397), (0.475, 0.17928084653615953), (0.4625, 0.19458712202310563), (0.46975, 0.18649372243881226), (0.4725, 0.17585242146253585), (0.4705, 0.17789169973134994), (0.465, 0.18508085185289383), (0.4655, 0.17352517402172088), (0.4625, 0.18511391365528107), (0.46875, 0.19063417583703995), (0.46775, 0.18973923802375794), (0.465, 0.18481080287694931), (0.46775, 0.19228698056936264), (0.45825, 0.18819004571437836), (0.45625, 0.1916808492541313), (0.45625, 0.19272777634859084), (0.45925, 0.17351037579774856), (0.463, 0.20799257522821427), (0.46025, 0.19815204232931138), (0.46125, 0.19991965460777283), (0.46175, 0.22564921718835831), (0.46425, 0.20370227336883545), (0.4615, 0.241307342171669), (0.4555, 0.23915690308809281), (0.46725, 0.2356609938144684), (0.466, 0.22880741876363755), (0.46475, 0.2602629354596138), (0.4625, 0.2242352940440178), (0.46675, 0.2370127252340317), (0.4665, 0.2407069878578186), (0.462, 0.23635248333215714), (0.46275, 0.23710859894752503), (0.461, 0.23896226906776427)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.40      0.98      0.57      1000
           1       0.56      0.86      0.68      1000
           8       0.00      0.00      0.00      1000
           9       0.00      0.00      0.00      1000

    accuracy                           0.46      4000
   macro avg       0.24      0.46      0.31      4000
weighted avg       0.24      0.46      0.31      4000

Competition_DC_2
VAL: 
[(0.244, 0.04420206964015961), (0.346, 0.10060960775613785), (0.407, 0.13195230773091315), (0.432, 0.15404716596007348), (0.437, 0.2008201377093792), (0.371, 0.2020772139430046), (0.411, 0.2141107092946768), (0.363, 0.259074219936505), (0.442, 0.21411252587661148), (0.431, 0.241391664955765), (0.437, 0.2810998134780675), (0.424, 0.2698406748874113), (0.46, 0.30063836242631076), (0.447, 0.3118632292384282), (0.459, 0.30334638703241945), (0.456, 0.31387039930187166), (0.465, 0.3299275228846818), (0.46, 0.3537083266917616), (0.459, 0.3464675769507885), (0.455, 0.3480771175827831), (0.451, 0.3569809230528772), (0.458, 0.3549287891369313), (0.451, 0.3290681131165475), (0.455, 0.3342141398712993), (0.452, 0.3325129809267819), (0.448, 0.34679520229902117), (0.429, 0.34361846156651155), (0.451, 0.29754124507494273), (0.443, 0.3103572289608419), (0.452, 0.33107129427883775), (0.443, 0.3435690187793225), (0.46, 0.33199482193775476), (0.454, 0.3404294975027442), (0.455, 0.3156301446855068), (0.449, 0.35120142030064017), (0.452, 0.3508970624320209), (0.452, 0.37146556910476647), (0.45, 0.3186231043059379), (0.454, 0.3687080460321158), (0.446, 0.3612777879412752), (0.456, 0.35901157041732223), (0.457, 0.33436826402507724), (0.46, 0.3297088329028338), (0.448, 0.330707564111799), (0.442, 0.35179502390138806), (0.443, 0.35507877797819676), (0.45, 0.33479744114540516), (0.453, 0.31179584946576505), (0.456, 0.310456198701635), (0.459, 0.3038789313491434), (0.458, 0.3282618778673932)]
TEST: 
[(0.2445, 0.04316330614686012), (0.35275, 0.09631808972358703), (0.40675, 0.12594314509630203), (0.4285, 0.14750396132469176), (0.43075, 0.19383546972274782), (0.37875, 0.19488197696208953), (0.4085, 0.20684573304653167), (0.3695, 0.2504873937368393), (0.435, 0.20733263194561005), (0.43075, 0.2325922714471817), (0.43575, 0.27226708149909973), (0.424, 0.26304767322540284), (0.45025, 0.2935598142147064), (0.44425, 0.30121734058856964), (0.4475, 0.2934606242179871), (0.44575, 0.3111679694652557), (0.4555, 0.32137706673145294), (0.45175, 0.338716246008873), (0.45225, 0.34076993048191073), (0.4445, 0.3344610024690628), (0.45, 0.34397612524032595), (0.4485, 0.3475330555438995), (0.448, 0.31899643898010255), (0.44475, 0.32505689370632174), (0.44425, 0.32479588985443114), (0.437, 0.3348876929283142), (0.42825, 0.3298586769104004), (0.44775, 0.2863873020410538), (0.438, 0.30429481863975527), (0.4505, 0.3153159363269806), (0.43425, 0.3326531766653061), (0.4545, 0.3203668365478516), (0.452, 0.3231023094654083), (0.44875, 0.3055412302017212), (0.4455, 0.3413138060569763), (0.452, 0.340118411064148), (0.44375, 0.3563320947885513), (0.44875, 0.3075507928133011), (0.45025, 0.35413435637950896), (0.44075, 0.34626211857795713), (0.458, 0.3423852699995041), (0.45775, 0.32278150045871734), (0.45175, 0.3095604453086853), (0.45025, 0.3167558866739273), (0.44275, 0.3334664114713669), (0.445, 0.3417053098678589), (0.449, 0.32499431920051575), (0.45225, 0.30232376444339754), (0.45325, 0.30090542626380923), (0.45275, 0.2972071648836136), (0.45275, 0.3194928344488144)]
DETAILED: 
              precision    recall  f1-score   support

           4       0.36      0.94      0.52      1000
           6       0.64      0.87      0.73      1000
           8       0.00      0.00      0.00      1000
           9       0.00      0.00      0.00      1000

    accuracy                           0.45      4000
   macro avg       0.25      0.45      0.31      4000
weighted avg       0.25      0.45      0.31      4000

Competition_DC_3
VAL: 
[(0.25, 0.04452222609519958), (0.262, 0.09178792214393616), (0.319, 0.1186123097538948), (0.397, 0.14389754155278206), (0.436, 0.19386132979393006), (0.439, 0.1812336931824684), (0.441, 0.188261329382658), (0.443, 0.19005238442867994), (0.44, 0.2096376870945096), (0.444, 0.22949583377689123), (0.447, 0.2314559705518186), (0.445, 0.24066928440332414), (0.439, 0.2581026356909424), (0.442, 0.2808020170852542), (0.434, 0.281668808626011), (0.438, 0.26754720909148455), (0.437, 0.3034121121559292), (0.442, 0.2878089199066162), (0.437, 0.26299071485549214), (0.444, 0.28313872720301153), (0.445, 0.2880335472077131), (0.435, 0.2790898114461452), (0.443, 0.31521715366840364), (0.444, 0.31975574570056053), (0.445, 0.31167995275743304), (0.455, 0.3002378221452236), (0.455, 0.2963726092353463), (0.448, 0.30634071891754866), (0.442, 0.32910937344282865), (0.442, 0.291440998993814), (0.449, 0.30244809708558024), (0.446, 0.2878232104685158), (0.437, 0.3131593371573836), (0.442, 0.2689219198450446), (0.446, 0.26076014878973364), (0.448, 0.30189474164694546), (0.446, 0.34171447382867337), (0.448, 0.3585887736864388), (0.446, 0.32553153825551273), (0.441, 0.3238630913607776), (0.436, 0.27589770877733827), (0.435, 0.2945694554373622), (0.442, 0.31967372220382095), (0.443, 0.2968519252240658), (0.441, 0.31638237251341345), (0.446, 0.318288114015013), (0.437, 0.30472207862511275), (0.446, 0.2581638730838895), (0.453, 0.26797134523838756), (0.451, 0.26954679476469756), (0.441, 0.2821302154660225)]
TEST: 
[(0.25, 0.04352640879154206), (0.2605, 0.0880089095234871), (0.3135, 0.11392097294330597), (0.38675, 0.1381194160580635), (0.41975, 0.18566381484270095), (0.4255, 0.17218308329582213), (0.42525, 0.18001394498348236), (0.42625, 0.17942561841011048), (0.4315, 0.19832437139749526), (0.43375, 0.21836428129673005), (0.42825, 0.22052164554595946), (0.42925, 0.2250147361755371), (0.43375, 0.24562744039297105), (0.43175, 0.2619908709526062), (0.43325, 0.25831698763370514), (0.43525, 0.2494448093175888), (0.42875, 0.28858103907108307), (0.43425, 0.27230346167087555), (0.43925, 0.25349791085720064), (0.4385, 0.27270054948329925), (0.438, 0.27122811853885653), (0.436, 0.2634687557220459), (0.43625, 0.29604455184936523), (0.434, 0.30416264057159426), (0.4405, 0.3042435612678528), (0.4395, 0.28971967458724973), (0.436, 0.2906726449728012), (0.43675, 0.2862648046016693), (0.43525, 0.3182587596178055), (0.438, 0.2809065008163452), (0.44075, 0.2878268394470215), (0.438, 0.2746860302686691), (0.42775, 0.29125479447841646), (0.438, 0.26490206825733187), (0.43975, 0.24955096304416657), (0.4405, 0.28771874701976774), (0.4385, 0.320407591342926), (0.4355, 0.3347752537727356), (0.43875, 0.3063097046613693), (0.442, 0.3063907022476196), (0.43475, 0.2679449919462204), (0.43375, 0.2758311802148819), (0.43725, 0.2987660740613937), (0.439, 0.28554456055164334), (0.43775, 0.29950819051265715), (0.431, 0.29759628999233245), (0.42925, 0.2859654431343079), (0.437, 0.2422905844449997), (0.44075, 0.2566660780906677), (0.441, 0.25830715560913087), (0.4415, 0.2708529270887375)]
DETAILED: 
              precision    recall  f1-score   support

           5       0.44      0.90      0.59      1000
           7       0.44      0.86      0.58      1000
           8       0.00      0.00      0.00      1000
           9       0.00      0.00      0.00      1000

    accuracy                           0.44      4000
   macro avg       0.22      0.44      0.29      4000
weighted avg       0.22      0.44      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [89]
name: no-alliance-89
score_metric: contrloss
aggregation: <function fed_avg at 0x7730e3651c10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=89
Partitioning data
[[3, 9, 1, 4], [0, 2, 1, 4], [8, 6, 1, 4], [5, 7, 1, 4]]
[(array([13581, 11353, 30571, 14064, 46596, 26097, 45402,  7200, 17497,
        3134,  2665, 24099, 41137, 17655, 33371, 39773, 48088, 45634,
       31198, 16586, 19455,  8179, 40587, 39868,  3000, 45217,  1966,
       26036,  8408, 29979,  6556,   367, 25315, 33443, 44913, 10209,
       21106, 41513,  7075, 23885, 32075, 21687, 40651, 18814, 45216,
       41725, 43262, 47902, 11805, 33980,  1625,  6198, 39135, 35154,
       19496, 13371, 24847, 43254,  2947, 15753, 27722, 13062, 27735,
       37370, 15706,  5876, 24538, 43744, 42590, 43843, 20932,   806,
       19913, 20469, 39287, 48131, 20770, 26022, 14779, 39980, 13253,
       14091, 12076, 15467,   685, 24348,  6422, 39865,  9332,  8162,
        2565, 19726, 37924, 39187, 45502, 21712, 34689, 43017, 17897,
       39165, 42970, 34999, 13205, 39101, 38879, 20371, 44856, 39398,
       43237, 32541, 36113, 35071, 15080,  1164, 38897, 41951, 39158,
       38444, 43428,  5725, 32404, 36159, 47666,  8434,   995, 13686,
       37546, 10101, 17155, 12872, 30809, 44110, 11047, 40433, 14936,
       20488, 43034, 17333, 49430, 22244,  4197,  7869, 18570, 48592,
        3927, 25640, 23526,  7259,   639, 16190, 27683, 19617, 39461,
       40097, 37057, 21551,  7997, 32938, 26132,  6014, 16364, 18400,
       22505, 19422, 12370, 14580, 13302,  7665, 39289, 14993, 31861,
       24922, 45250,  6368, 42997, 38891, 41639,  4672, 49143, 19631,
       36219, 28237, 25259, 26707,  5493, 14938,  6343, 46155, 40809,
       36124, 26357, 30213,  9024, 10534, 24606, 14232, 46595,  5553,
       47196, 18414, 11370, 43726, 45914, 45456,  1696, 40399, 49078,
       20534, 38390, 39885, 24082, 20058, 11359, 47062, 28786, 28554,
       11623,   314,  3910,  8988, 42198,  9300, 27673,  9429, 31449,
       35449, 26558,  8105, 46423, 33422, 27658, 21731,  3875, 12007,
       45901, 25257, 25563, 42646, 41705,  5502,  8386, 35427, 22226,
       24704, 15335, 39689, 26665, 26900, 22523, 42682, 42407, 19380,
       44809, 39319, 38402,  2506, 11918, 12086, 36029, 49360, 13889,
       10533, 12541,  5009, 10138, 24173,  3477, 37536, 36967, 30127,
       14099, 39572, 37857, 44646, 48769, 23990,  7838, 31744, 26597,
       47066, 47687, 24436,  2632, 49031, 42224, 49662, 11901, 41631,
       15141, 33248, 35504, 48431, 10373, 34034, 29148, 41611, 19601,
       26832, 21138, 28932, 40267, 28376, 26908,  6002, 34666, 36050,
        7040, 39993, 42739, 48126,  3974, 15252, 34772, 31918, 38601,
       39699, 38663, 14571, 44606, 42278, 33025, 35064, 35749, 36475,
       45465,  9701,  6158, 34006, 28495,  5079, 23169, 18439, 14572,
       38328, 27807, 39870, 30525, 40030, 46243, 45101, 33053,  6984,
        3929, 34957, 29105, 31637, 35852, 25973, 13346, 12040,  1000,
       36705, 38380, 40254, 45284,  6360,  3479, 27168, 32164, 20013,
        2199,  1115, 40082,  3316,  9302, 32439,   205, 41904, 34480,
       15817, 31378, 27957, 21571, 49029, 49405, 41284, 32879, 40559,
       33281, 16163, 40895, 26531,  8532, 17099, 36590, 18648, 28949,
       32902, 34232, 46698, 35607,  7302, 49074, 19674, 46651, 27752,
       12011, 20799, 14880, 26252, 11111, 17452,   316,   188,  6480,
       33886, 34903, 20787,  4407, 38324, 39611, 30080, 12288, 43171,
        6457,  3700,  3633, 17875, 11517, 18183, 49403, 40693, 10523,
        3876, 43522,  3023, 29628, 10056, 36741, 41535, 23783, 37126,
       19199, 44009,   269, 39331, 40742, 34474, 23657,  4587, 28259,
       32841, 17171, 13128, 48451, 10252, 36109, 49634, 41508, 36859,
        8841, 28304, 41395, 13642, 42885, 24048, 43614, 36165, 24731,
       47303, 43104, 26945, 30580, 10717, 10118, 23341, 41683, 46844,
       15993, 10078,   613, 17607, 24246, 34843, 41746, 34549, 41541,
       19805, 44280, 30733, 33024, 37349, 30678, 34423, 33679, 24893,
       28915, 37968,  4256,  1767,  6610, 46227, 20675, 10859, 26013,
       26674, 15586, 27821, 42452, 24320, 47365, 32937, 37849,  6241,
       14507, 34810, 12929, 10714, 13002, 36868, 20667, 20155, 44725,
       25459, 42976,  4528, 25689, 17275, 33261, 43932, 15386, 40822,
       19354, 31890, 20619, 47470, 29429,  5568, 48128,  6733,  9318,
       46679, 40145, 45126, 31352,   997, 21938,  3379, 29986, 42793,
       29963, 30198,  2731, 18456, 22060, 25046, 20368, 35106, 28001,
       35850, 30782, 23662,  5344, 16986, 12586, 10035,  6383, 19526,
        7992,  3734, 11708,  2259, 16310, 47279, 11804,  9327, 42282,
       19773, 46396,  7861, 35863, 38078, 29318, 45077, 22437, 17279,
       35567, 44541, 29874, 24119, 27215, 11522, 15905, 41472, 45613,
       49933, 22479, 35692, 35021, 38223, 38892, 27868, 46461, 42347,
       41502, 47900, 47178,  9411, 11971, 47588, 44341, 39422,  7723,
       34420, 28861, 29019, 36611, 42081, 10122, 40000, 46162, 20307,
       18752, 14007, 49594, 44767, 16066, 31720, 32736, 48595, 34629,
        7430, 31747,  4047, 15286, 13166, 31348, 31583, 26950, 17534,
       14061, 38175, 33748, 41289, 21407, 18654, 33624, 19046, 16745,
       15738, 26470, 18671, 47377, 41262, 27792, 12169, 35516, 22472,
       41520, 19769, 41723,  7145, 12185, 42163,  7683, 23929, 19324,
       23020,  6608, 18069, 32410,  4553, 13366, 13922, 20463, 37422,
       31282,  4011, 15934, 10655, 18160,  6623, 38577, 13296, 25323,
       29875,  1907, 44472, 24161,  6703,  9380, 47680, 30457, 36331,
       11824, 10079, 16992, 15583,  2469, 42085, 20796, 39906,  8346,
       31203, 34906, 14964, 20937, 22349, 31607, 18526, 48086, 17032,
       45316, 17718, 26740,  6831,  9232, 13108, 33866, 18961, 11558,
       21796, 19485,  6798, 38725, 15897, 44869, 19164, 45212,  2023,
       42007, 40258, 41215, 29023, 13417, 21153, 23203, 13849, 33561,
        2390, 49643, 46357, 28476,  9668, 31523, 26465, 34993, 14878,
       43711, 11782,  4565, 42802, 45704, 48539,  7130, 46514, 36251,
       15114, 46177, 32235, 48807,  6485, 18912,  2363,  3948, 48613,
       25232, 46061, 19568, 13834, 17632,   345, 28372, 36047,  9061,
        3802,  2300, 48351, 19118, 42313, 34562, 21733, 12737, 35915,
       28577, 40757, 24996,  7317, 17878, 14635,  3464, 46526,   505,
         449, 30585, 28407,   887, 11138, 21912, 10447, 15170, 19643,
       23367, 38964, 28040, 23792, 47239, 40642, 40309, 21187, 23022,
       32518, 42700, 37389, 44032, 36383, 25977, 44397, 19475, 19340,
       43139, 10736, 48605, 16216, 41814, 38351, 42750,  3533,  2842,
        6924,  6891, 36817, 25954, 18060, 41162,   632, 19952, 36566,
       40794, 29419, 37923, 36896, 33095, 47837, 43595, 14956, 26860,
       49027, 10865, 36976, 23755, 26892, 23211,  7296, 14036, 24265,
       28319, 25355,  8987, 46133,  7787, 23057,  7851, 42544, 42729,
       12026, 41301, 33796, 48478, 10808, 48824, 38598, 22259, 39656,
        6877,  4043,  9615, 44517,  1695, 19543, 22246,  3027, 49608,
       32494, 27535, 29835,  4305, 30630, 14670, 29737, 13466, 36160,
       47257, 15718,  8396, 23954,  1315, 42910, 48997, 42640, 35877,
       36652, 43032, 28614, 14777, 35180,  4158, 43203, 18043, 15267,
       44200, 38454, 24630, 15036, 13427, 36090, 30353,   669, 35573,
       27259, 26838, 35714, 30900,  4687, 32643,  2905,  9149, 43650,
       10765, 45775, 34217, 41451, 13691, 45644,  1791, 43860, 28600,
       29070, 18289, 42229, 38776,  5940, 41691, 31003, 27247, 29129,
       21676, 27322,   563, 15152, 39127, 26929, 25346, 14879, 41530,
       29184, 22990, 18300, 29827, 46778,  1516,  8784, 44729,  7382,
       49213, 22798, 22698, 39348,  3236,  9771, 22509, 14474,  1350,
       11957,  2178, 47120, 16033, 16897,  6270,  7743, 10633,  3522,
       12301, 33013, 12075,  2703, 36777, 37089,  9507, 14255,  4527,
        9713, 10900, 31143, 30499,  8137, 28799, 37996, 27891, 45234,
       35954, 41186, 41669,  1934, 24778,  5495, 27596, 15508, 31840,
       22580]), [3, 9, 1, 4]), (array([44167, 49992, 38943, 14754, 19493, 22993, 23796, 41606,  9059,
       38238, 10847,  2365, 33117, 47981, 47328, 16460, 30321,  3304,
       41745, 45412, 30160, 49303, 11424, 35788, 43829, 43012, 27434,
       26607, 37178, 21087, 33943, 44143,  7573, 14945, 38039, 34017,
       28354, 18271, 10637, 35121, 30709, 40125, 37022, 13324, 26922,
       18073,    49, 47486, 19041, 32602, 35858, 11963,  8971, 49490,
       42931,  2617, 47834, 41300,  6743, 28794, 44925, 27911, 37160,
       38036, 46738,  7643,  6276, 42476, 26776, 32206, 48254, 32056,
       41324, 46725, 14116, 22973,  3731, 29225, 14698, 26391,  1381,
       36858, 16137, 10690, 27278, 39329, 24113,  3335, 11734,  2720,
         871,  1935, 45163, 41355, 38345, 11356, 24720, 43875,  2714,
       10205,  4721, 37517,  1524, 17677,  4155, 30383, 44967, 36464,
       42446, 29003, 42456, 28972, 42183,  2574,  6190, 40796,  1950,
       34944, 24079, 41907,  4081, 47468, 21974, 26060, 25942, 17917,
       43649, 17762, 40639, 42628, 10340, 41235, 36353, 33258, 48556,
        5864, 48926, 15304,   733,  3789, 36106, 25212, 45344, 10275,
       32369,  8553, 10990, 17717,  2633, 42332,  1473, 25288, 38613,
       16531, 35286, 25190,  1926, 49517,  2962, 24096, 30428,  2401,
       28665,  4854, 34018, 11372, 28456, 29412, 27137, 20891, 49557,
       10577,  8199, 10301, 36357,   115, 12083, 47105,  5457, 32670,
       16186, 48246, 15077, 34083,  3815, 28921, 23825, 49656,    93,
       15946, 38478,  4653, 48327, 38043,  7093, 44175, 26361, 32107,
       10148, 27042,  4353,  2107, 25236, 25616, 30078, 14153, 26296,
       11224, 46945, 39606, 12420, 46287, 24939, 11709, 18080, 43479,
       32934,  3906,  4477, 40485,   965, 24644,  4552, 30172, 38499,
       24750, 40679, 47771, 41600, 27487,  8062, 20813, 20602,  1187,
       26783, 38179,  1759,  3184, 19778, 47865,  8535, 17115, 34036,
       14413, 39650,  4952,  2413, 11612, 40652, 29364,  2393, 11380,
       39646, 11873, 40348, 37878, 26128,  1527, 43781, 10567, 47826,
        3901, 37713, 13848, 10651, 29742, 36481, 16402, 43898, 49457,
       23918,   933, 27484, 36025, 39882, 26386, 10628,  6097, 23537,
       14946,  9597, 19868, 18040,  2626, 47932, 32792,  6488, 36213,
       28933, 35792,  6369, 37298, 38032, 42889, 36377, 25148, 38745,
       30665, 35031, 25929, 21867, 30333, 30746, 22281,  3193, 20216,
       32697,   522, 38459, 17739, 38054, 25381, 11934,  4525,  9503,
       42588, 13179, 20038, 33396, 46564, 39237,  1798, 35486, 21746,
       40527, 44995, 44427,  2927,  2202,  9151, 32071, 46261, 38001,
       40463, 39847, 48090, 46088,  4549,   513, 40333, 46943, 42774,
       14811, 21452,   403,  7761, 36759, 37645, 13270, 33644, 26176,
       13814, 32908, 39407, 13195, 42600,  8032, 17702, 10833, 34196,
       44658,  6843, 13130, 49730, 20700, 24706, 45460, 42928, 46326,
       40282, 35096,  4518, 20547,  9712, 45131, 19056,  6757,  7662,
       20656, 44335, 19932,    24, 31705, 33361, 11730, 17834, 20183,
       17086,  9249,   218, 31501, 37581,    47, 19815, 26795, 45533,
       46509, 14314, 16288, 49093, 12361, 42990, 12589, 26864, 25989,
        5871, 11348,  4370,  5889, 35616,  3259, 32406, 11962, 25695,
       12067, 13901, 21135, 36170,   196, 36726, 13770, 10742, 29625,
       38129, 27097, 10461, 37446, 49784, 15139, 29379,  7664, 49693,
       41925, 44008, 26999, 43523, 48101, 24409,  6946,   724,  7658,
       43747, 32419,  1139, 32067, 26572, 41775, 40087, 48735,  2126,
       18027,  8081, 10756, 42450, 34925,  9011,  9940,  3349, 42581,
       34734, 15065, 37315,  9790, 12148, 27980,  7477, 31007, 10204,
       31707, 42564,  3944, 27877, 11548,  4930, 35104, 41721, 21145,
       26442, 48951, 14088, 29907,  2136, 43041,  6772, 46957, 46584,
        3321, 42158, 19899, 10940, 36994, 14646, 27356, 13934, 10968,
       29637, 13819, 36581,  1789, 27746, 47939, 43853, 12276, 19202,
       34336, 32564,  9549, 32472, 35344,  1574, 43214, 29252, 22298,
        2727, 18048, 39652, 38756, 33207,  2173, 15363, 24385, 14502,
       42921, 26077, 23313, 18890, 27942, 39338, 29777, 17268, 21150,
       31019, 41529, 48820, 16471, 23332, 45769, 29188,  7198, 18078,
       30407, 18844, 27465, 10229, 45675,  1037, 10795, 35965, 20047,
       37702, 34364,  8547, 34850,  9898, 44841, 15885,  5171,  9476,
       37826, 16385, 41710, 45834,  2587, 21152, 45157, 12773,  8529,
       14379,  6814, 32258, 44524, 16077,  4492, 28643,  9717, 44109,
       10039,  7288, 45475, 31253, 12923, 16638, 30313, 21593, 10446,
        3346, 38086, 20168,  9878, 29044, 38420, 42475, 10953, 41248,
       22089,  6074, 42734, 23260, 14376,  7338, 46827, 26550, 19939,
       46544, 49381, 46859,  4100, 22553, 20725,  4132, 49045, 26930,
       28655, 13852, 34163,  8965, 16947, 16594, 48709, 40971,  5811,
       42448, 46900, 33951, 33126, 47529, 24446,  2038, 42239, 29146,
       27447, 33611, 39195, 49348, 47088,  9857, 46993, 40518,  7326,
       49629, 29226, 41706, 26787, 48667,    79,  5422,   840, 46866,
         261, 11631, 44204, 34128, 17112, 29901, 14859, 47310, 31957,
        5157, 28626, 48234, 24284,  6226, 40449, 28127, 21309, 34118,
        7261, 30804, 19185, 45972, 27089, 31959,  8944, 48681, 24228,
       40152, 25993, 23127, 39996, 37666, 10074, 33742,  9400, 12080,
        2072, 45011, 24151, 35635,  4265, 16777,  6349,  7349, 14549,
       15756,  8450, 29828,  1901, 37743, 29512,  4496, 43274,  3414,
       31079, 44591, 11696, 25050,  7605, 12100, 31288, 32462, 28298,
        4993, 42710, 43480, 36400, 12620, 23275, 36663, 10479, 12025,
       28420,  2200, 23213,  8475, 38908, 29029, 23210, 44423, 26295,
       48639, 33314,  6813, 19989, 18527, 36813, 42413,  8127, 24484,
       41963, 24008,  1464, 19571, 30062, 26870, 44537, 15984, 41209,
       27262, 19181, 32743, 20206, 27282,  8913,   272, 47975, 20340,
       42072, 30307, 14305, 38033,  1268, 11582, 17197, 41219, 31446,
       10661, 45293, 20515, 12313, 36317, 46076, 29852, 28823, 15012,
       34350, 28289,  9650,  8405, 16155,  9816, 26844, 14388, 10951,
        2695,  7059, 31733, 40188, 21781, 23013,   399, 37577, 29111,
       18404, 12997, 28084, 28903, 32969, 39413, 25579, 46947, 43228,
       35806, 31953, 29698,  5491, 46531, 31804, 38668,  6879, 31998,
       12417, 27029, 48884, 17953, 48447, 27220, 27390, 19331, 11860,
         660,  8049, 30881, 47139, 49669, 34477, 39772, 25098,  9141,
       41766, 19345, 12489, 40116, 37172, 34114, 48801, 42603, 16880,
       41270,  8184,  7811, 24810,  2647, 18365, 38603,  9568, 29525,
       31549, 48202, 35185, 17039,  1804, 39377, 21629, 16672, 32803,
        7110, 43758,  7269, 14584, 40027, 16749, 32818, 36420, 13510,
       10282, 10966,  3684, 18560, 27946, 23739, 49685,  4987, 43810,
       48902,  6911, 35432,  1904, 28830, 43231, 19264, 40949,  4642,
        3326, 41337, 41056, 22443, 22617, 13764, 16772, 25403, 24594,
       27489, 10154, 24846, 21659,  8930, 36442, 20343,  2155, 33279,
       44760, 42979, 46998, 28809, 15744, 31277, 38995, 11872, 31709,
       25931, 46650,  1952, 14810,  6339, 33482,  1992, 49114, 27630,
       36514, 41103, 28755, 28751, 22949, 36731,  3784, 42907, 25871,
       14284,  2673, 20011, 19943, 19704, 10264,  4297, 14051,  4447,
       22018, 20409, 42982, 18813, 37853, 44614, 31932, 32533, 33226,
        5067, 27455, 10134, 45199,  1007, 23200, 49318, 11021,  1425,
       27221, 36740, 42418, 30449, 17725, 24084, 32413, 24785, 34956,
       21194, 20342, 34328,  6605,  6128, 47079,  7383, 27649, 14492,
       44007, 16890, 39177, 24804,  2225, 20608, 43584, 10904, 33266,
       34182, 21597, 12846, 24776, 22144,  2672, 45114, 39467, 17483,
       11249,  8114, 45400, 15624, 42636, 26631, 19407,  9260, 20521,
       10131]), [0, 2, 1, 4]), (array([48247, 20762, 49551, 29868, 44236, 29075, 15260, 16466, 14891,
       30357, 24692, 25521, 34712, 33441, 16422, 38587, 39802, 23438,
        6651, 34491, 38686,  4283,  9599, 13227, 46892, 31423, 27507,
       39094, 13425, 20455, 49985,  9265, 45024, 28396, 23408, 15743,
       45062, 38416,  1162, 46488, 34473, 10856,  9675, 48757, 19979,
       42333,   716,  4298,  4823, 13582,  1512, 39184, 26055,  8222,
       24554, 46033,  1763, 39254, 49681, 20491,  1751, 34441, 41761,
       49861, 19851, 36221, 26463, 10923,  4955, 34369, 49389, 20650,
       29782, 12426, 19748, 28599, 44384,  8329, 16132,  8495, 39507,
         460,  9722, 42469, 32562,  4204,  1370, 48541,  4780, 49899,
       15250, 42693, 18565, 19509, 17588, 21157, 37318, 27864, 43913,
       40391,  8138, 25501, 31489, 26131, 49388, 16727, 23205, 15046,
       39336, 19887, 45779, 31456, 20822, 39715, 17894,  9192, 22268,
       11327, 42713, 30026, 48984, 13874,  8384, 44180, 21036, 41629,
       33398, 18444,  6153, 37643,  2434, 24435, 47597, 39702, 42254,
       35593, 40345, 29371, 47342, 11180, 13702,  4758, 19666,  8830,
        8248, 12513, 10443, 26147, 38460, 14872, 25855, 38221, 38925,
       20617, 47395, 43492, 37633, 47506, 38065, 31778,  6825, 19605,
       16724, 45104, 20479, 13992,  6773, 27458,  9023, 26784,  5283,
       45912, 13788, 20288, 48905, 15811, 29056, 19991, 45620,  3215,
       33815,  3518, 13386, 28491, 24876, 43109,  2979, 13576, 10836,
       34210, 45447, 19836, 35224, 14642, 10822, 25869, 39317, 45240,
       40702, 30955, 16086, 34656, 27715, 35367, 45868, 15049,  3832,
       12319, 10956, 30844,  5730, 10234, 42933, 23312, 31725, 36390,
       27615, 14693,  6846, 46938, 19398, 46888, 13004,  6789, 31841,
       43183, 16178, 12861, 25618, 17394, 17220, 40062,  8526,  4652,
       26004, 16044, 17335, 19720, 38694, 49851, 27842, 21499,  9346,
       12646, 32213, 27988, 43982,  6912, 11674,  2918,   164, 41870,
       14162, 17035,  7041, 21424,  2267,  2939,  2019, 28678, 30130,
        6918, 47347, 43070, 22534, 45970,  5318, 31382, 43426, 23520,
       19128, 10113, 16507, 24418, 13123, 37574, 48201, 38197, 26151,
        7392, 13796, 32637, 36360, 31616, 27927, 49289, 18801, 46134,
       38061, 26243, 35099, 40653, 39874, 30176, 35114, 44078, 18816,
        3976, 43452, 34652, 34432, 37992, 22654,  7137, 20324, 19511,
       48298, 38865,  4208, 20098, 44612, 17248, 24184, 36192, 41184,
       47734, 26324,  2494, 35499, 25985, 31558, 14039, 41004, 12214,
       47071, 48400,  7215, 36586, 17680,  1031, 26066, 15497, 28378,
        9653, 40274, 36808, 15651, 43488, 44194, 39007, 30427, 16546,
       12321,  5745, 30367, 38438, 27450, 37796,  5679, 46124, 28139,
       12765, 30626,  6572, 11644, 35270, 45585, 10792, 26429, 34804,
       35900, 23821, 49804, 23025, 27134, 33425,  3655, 33012, 32793,
       34362, 25396, 48723,  6367,  6395,  6810, 11319, 34322, 38537,
       48460,   588, 14830, 19834, 37926, 21917, 25417,  5907, 28990,
       39252, 41448, 34505,  3018, 20767, 28986, 31465, 18851,  1766,
       17282, 48229, 12445, 31933,  2729, 20993, 25729, 43439, 16760,
       40811, 17000, 24117, 25085, 48710, 17075, 11150,  7143, 17952,
       30100, 16404,  9303, 40929,  3716, 47464,  5025, 44708, 40164,
       46743, 40357, 45019, 31099, 48932, 44566, 12122, 38993, 28956,
       15294, 45958, 28021, 44518, 49892, 43645, 20930, 47752, 29223,
       47728, 13797,  9468, 24198, 35703, 49305, 20293, 41909,  9808,
       49311,  3070, 41809, 46913, 22100, 26122, 49708, 18336, 45383,
       22345, 17788, 37481, 25067, 41074, 45823, 15220,  1374, 23881,
        7675, 43538, 24882, 49776, 37487, 25747, 36089, 44696,  2965,
       13683, 43298, 20561, 19924, 30741, 38606, 49210, 35208, 46180,
       29677, 43577, 48665, 18407, 22101, 23912,  7976, 31228, 16129,
       41593, 21200, 43481, 32534, 28374,  2851, 21104, 17970,  4858,
       45628, 31935, 43145, 12893, 49416,  3937, 40279, 37844, 48740,
        4099, 24466, 20856, 48182, 18744, 15748, 42438,  7005, 14204,
       27419, 45500, 47749,  3062, 22984, 40537,  1578, 49383, 13996,
       13505, 30136, 23837, 46173, 47616, 19609, 48728, 14048, 10387,
        7529, 10912, 22829, 37494,  9538, 22306, 32580, 16985, 47845,
       11850, 22884, 21675,  6899, 48917, 34442, 29693,  3452, 40815,
       38646, 20376, 28163, 24137, 47035,   206, 35048, 18263, 37512,
       12071, 36641, 42237, 17815, 44889, 34701,   942, 49658,  7777,
       33963, 35364, 17662,  2445, 38822, 13144, 14110, 22227, 33164,
       21218, 47987, 30783, 45535, 39737,  5917, 33762,  8693, 36158,
       36237, 30083, 15299, 24986, 26074,  4459, 34909, 14904,  5144,
       16588, 18425, 28417, 14005,  5197,  4557,  5816, 34426, 15183,
       49616, 45099, 45899, 18178, 18388, 49932, 28532, 25926, 41964,
       36298, 15609, 20931, 32438, 35081,  2656, 15988, 32474, 39811,
       42287, 32418,  9781, 12280, 33392, 49494, 44309,  4533, 22788,
       44308, 17106, 17856, 14068, 27716, 17334, 40453, 19898, 15855,
       47362, 18246,   364, 13234, 35953, 24774,  8576, 13858,  1551,
       42709,  6353, 41627, 38192, 29953, 35666,  2802, 10248, 19351,
       44411,  1021, 35366, 45013, 19490, 10453,  3781, 41307, 20444,
       47633,  8977, 38028, 44395, 39277, 26794, 15832,  2883,  1694,
       15496, 34376, 39745, 32681, 14512, 19265, 26115,   325, 16682,
       19313, 49080, 46359, 14209, 48669, 19598, 32011, 21181,  5635,
       37239, 30553, 13952,  2268, 40341, 43167, 10188, 20746, 28175,
       18518, 33838, 22037, 45210, 27178, 32941, 43684,  1605, 48954,
       14288, 39912, 37213, 42341,  1020, 25377,  4949, 16223,  9095,
       39109, 32083, 34791, 49716, 17454,  9012, 21180, 35508, 37533,
       34233, 16855,   250, 30949,  3444, 21870, 40034, 12800,  7286,
       32042, 44601, 44225, 12805, 30681, 29489,  9202,  9074, 10615,
       38787,  5754, 46153, 14667, 34431, 22149,  5756, 46823, 43746,
        2056, 40121, 24108,  2911, 22183,  7043, 33666,  6894, 39005,
       13464, 27003, 33369, 16240, 13472, 29396, 44857, 10207, 27394,
       30268, 38511, 20192, 35563, 13145,  4438, 13498,  7100, 35517,
       26653,  3577, 19852, 12803, 24674, 49622, 38354,  3635, 35843,
       40665, 21949,  5909, 38805, 32776, 32095,  2123, 33480, 49077,
       33370, 21853, 31579, 11967,  8279,   951, 18065,  5063, 17338,
        3145, 34950, 31755,  5237,  6039, 49002, 44324, 40848, 27572,
       47928, 36915, 12179,  9967,   764, 45618, 42160, 46968, 43049,
       27935, 37719, 20904, 12645, 45397, 37977, 10067, 11481, 12590,
       45408, 14853, 20108, 24997, 26706, 13927, 27790,  3296,  2787,
       43172,  9782, 28913, 38762, 40562, 41810, 38755, 20435,  9155,
       14905,  3288, 10604, 32179, 40793, 47576, 45111, 41665, 14935,
       45340,  2088, 25620, 17877, 15990, 40842, 47664, 36167, 41573,
       12723,  1882, 36536, 25402, 38213, 13469,  7625, 31939, 22683,
       43974, 24419, 48878, 34699, 14152, 41417, 20363, 15737, 19441,
       38517, 49751, 16587,  3776, 46119, 19032, 39701,   162, 35228,
       12453,  2144,  3509,  8236, 28174, 12858, 35522, 45076,   816,
       23445, 21222, 28414, 19098, 11759, 47295, 28654, 27888, 22205,
        9153,  1069, 17556, 41382, 34038, 21041, 17077, 25250, 36096,
       41191,  6387, 11997, 16007,  3377, 49207, 47312,  8688, 34384,
       37741, 32960,  9761, 42312, 14649, 29939,  5180, 43683,  7642,
       40259, 17049, 37688, 12224, 46470,  4763, 45935, 11626, 40473,
        3587, 49984, 20659, 14104, 21176, 32535, 30061, 37342, 26638,
       19702, 19569,  8447, 27313,  6780,  8254,  5624,  6844, 23937,
       33015,  3189, 10105, 10136, 37294, 35956, 45170, 44172, 14377,
        6710,  4781, 43509, 25371, 21929, 22837, 14737, 47570, 27206,
       12416]), [8, 6, 1, 4]), (array([41098, 10258,   686, 36782, 17797, 34541,  4850,  1847, 40471,
       24160, 16797, 47065, 27893, 47289, 30393, 25230, 29432, 45728,
        6873,  6259, 19856, 36851, 44738,   426, 18346, 25465,  9301,
       11852, 13911, 33311, 47544,  3002, 22473, 16812, 48245, 10632,
        7160, 28877,  2794, 31264, 13257, 18764, 11314, 28661, 12326,
       26987, 14965, 46264, 18230, 26125, 33116, 48625, 21128, 25698,
       31047, 26024, 27771, 46720,  6958, 46842, 37252, 36556,  3017,
       11819, 18082,  7476, 30016, 28902, 27879, 14632,  4529, 44260,
       44781, 33295, 18729,  6760, 10397,  3421, 49327, 49680, 38529,
       43467, 18170, 44899, 45436, 23119, 30349,    70, 48645, 21278,
       49797, 47218, 20836, 33702,  8325, 31949, 20445, 34095,  2314,
         359, 11519,  1545,  6131, 17855, 37486,  5962, 31129, 49980,
       39002, 11770, 38695, 33435, 44706, 30622,  9196, 49753, 29110,
       31901, 36956, 22893, 34803, 11185, 12573, 22427, 18748, 13067,
       32511, 41499, 21780, 38854, 25311, 48673, 30662, 20292,   875,
        7094, 14227, 11662, 32184, 36224, 23335, 21077, 48383, 46112,
       45991, 44700, 29304, 29474, 46246, 47496, 28218, 45503, 11365,
       27598, 46494,  1656, 22483,  5393, 27292, 40351,  1849, 36603,
       24650, 41583, 25418, 37096, 36171, 48083,  5305, 23947, 12701,
        7472, 30235, 39228,  7157, 23976, 18061, 43703, 48883, 32530,
       22158, 20075, 45469, 22321, 24006, 29061, 25870, 36899,  5681,
       35165, 41046, 41069, 23146,  1786, 33306, 17346, 29558, 39929,
        8198, 30735, 23150, 44551, 21046, 28693, 39103, 27397, 44290,
       16653, 14875, 43999, 48573, 28442, 23949, 30684, 42337, 25057,
       43071, 39660,  1519, 11201, 33873, 27679, 16436,  2984,  7181,
       27945, 45553, 40091, 32719, 37946, 44555,  5500, 49720, 39587,
       15207, 35660, 49938,  6436,  5379, 21767, 41793, 20020, 21494,
       25610, 28995, 12912, 37813, 36669, 18985, 47625,  4686,  9201,
       11334, 21083, 19346, 29196, 16031, 42284, 16537,  6449, 13525,
       12099,   570,  2045, 11410, 21927, 30167, 42612, 20741, 25283,
         514,  6808,  3146, 31822, 10666,  3398, 29350, 33495, 15539,
       28715, 15081,  3272, 42593, 39425,  1071, 15283, 30030,  7819,
       13749, 10127, 43128,  8311, 32354, 22901, 38104, 11391, 37449,
       29026, 40837, 21967, 30447, 14947,  9082, 28132,  5774, 23880,
       37836, 40892, 42751, 37671, 37624, 41973,  5126, 26752, 49377,
       28150, 18095,  9789, 15679, 17989, 45861,  9795, 10887, 25553,
       40821, 44286, 47856,  8516, 35982, 33755, 32307, 11338, 17129,
       37902, 11846, 28965,   913, 34308,  1915, 37159,   318,   320,
       22725, 10734, 48467, 17997, 26991, 10730,   797, 22128, 43322,
       22934, 39395, 28945, 27568, 24916,  3955, 42668, 37496,  3715,
       45365, 18139, 45061,  8984, 22324, 16666, 15317,  5525, 37935,
       46211,  1135, 27036, 23928, 36364, 21718, 39107, 24091,  7372,
       17886, 16876, 24129, 16198,  4706, 23196, 18556, 10184, 20699,
       12925, 12994,  6752,  2616, 10695, 11796, 45874, 28853, 37458,
       12864, 47777,  9214, 29957, 39498, 11088, 32875, 19973,  7506,
       29727, 31719, 16843, 23547, 16149,  6699,  7609, 12838,  4903,
       12320,   329, 36352,  7939, 30996, 22526, 36136, 25724, 34782,
       28201, 25464, 16040, 18528, 12437, 17254, 43305, 48948, 17636,
       44101, 28243,  3637, 22198, 16022, 46391, 10269, 47372, 47673,
       36589, 46348,    11,   131, 26163, 15136, 38798, 18828, 10481,
       35158, 31036, 38500,  7023, 16589, 39485, 19250, 19361, 38955,
       21773, 32208,  7185, 48041, 34260, 36306, 10538, 43512, 33773,
       42936,  4809, 44151, 48490, 45087, 44694,   152,  3649, 23842,
       26291, 19200, 42909, 14398,  8904, 19794, 45758,  6759, 10034,
       14758,  9496, 37482, 36677, 31807, 40295, 26032, 37472,  4253,
       36691,  8202, 47039, 25598, 14537, 37327, 46213, 33112, 23895,
       47349, 22054, 29473, 25697, 10902, 21706, 49554, 49121, 16332,
        2286,  6928, 33686, 34595, 33114, 42666, 34097, 18698, 35401,
       31737, 43786, 33615, 12592, 36774, 19575, 43641, 42065,  9080,
       44993, 46649, 26652,  2339, 12894, 29409,  8480, 21234, 12560,
       39771, 34043, 44474, 48141, 10569,   396, 41254, 46956,  8119,
       20736, 35530, 38858, 16829, 31251, 20761,  8135, 29485,  1559,
       29494, 23936, 26915, 45602, 34263,  3078, 18331, 37468, 14764,
       25718, 29283, 45485,  1090, 22833, 30446,  7673, 38493,  4696,
       44936, 17126, 43178, 42860, 33693, 41579, 26127, 27268, 46889,
       21529,  4423, 41550, 40248, 34090, 15749, 28381, 47779, 16989,
        8191, 20866, 12903,  2455, 41563, 22217, 18594, 33250, 16412,
       22419,  9766, 39474, 11707, 22165, 31655, 41338,  9263, 38347,
       16549, 42925, 10022,  8771,  3827, 11109, 18649, 21020, 46463,
       35691, 15702, 42912, 42758, 15506, 21959, 18228,  7048,  7722,
       19018, 37309, 37385, 31830,  5207, 14378, 30234, 35338, 34081,
        3315, 49795, 18551,  4200, 28096, 35705,  6121, 18117, 12247,
       24247,  8794,   257,  3085, 23006, 17479, 33640, 28539, 36085,
       34493,  2184,  8884,  6068,  1052, 35674, 18093, 32121,   432,
       15632, 48138, 13899,  3020, 36343, 29230, 27627, 12038, 29203,
        6020, 19869, 36975, 40880, 31945, 20042, 36872, 30458, 40972,
       25619, 25910,  1304,  9809, 46099, 15557,  5810, 23500, 46893,
       30104, 24923, 28876, 47213, 44272, 11645, 19143, 11243,  3366,
       46547, 40713,  2039, 11075, 34068, 18625, 29974, 31138, 20840,
       35651,  3273, 37012,  1394, 43416, 19565, 13122, 23714, 30530,
       34349, 18303,  8432, 22372,  7284, 29633,   262, 49186,  2511,
        6639, 45977, 31599, 46078, 49849, 45317, 10362, 42318, 33970,
       32584, 46406, 45487, 38491, 19628, 18639, 25614,  7214, 27994,
        9442, 18333,    99,  9000, 27547, 32284, 28100, 31290, 37987,
       35538, 41286, 20137, 27050, 48892, 41897, 20730, 10888, 48367,
       47441, 27135, 31769, 27494,  1925,  1001, 46150, 30300, 33027,
       31775, 14397, 10766, 21188, 27631, 21327,   712, 29743, 32253,
       26417,   520, 44834, 27812, 23421, 45820,  2654, 46331, 18161,
       48211, 23072, 31954, 18637, 44850, 23187, 42080, 45782, 38146,
        8338, 27500, 38228, 15155, 35415, 40631,  3354, 24920, 17402,
       18384, 32553, 18056, 32634, 44963, 34751,  4104, 21382,  9147,
       43335, 41675, 44641, 21021,  7563, 21626, 27613, 19462,   489,
        5434, 24582,  5173, 40974, 44587, 37903,  9724, 35526, 23354,
       33129, 47534,  9887, 19256, 19178, 28260, 20906,  4741, 44942,
        9550, 47149, 10303, 36968, 43299,  3519, 33004,  7404, 17713,
       45773, 47644, 40531, 10179,  7295,  6168, 12810,  3217, 27718,
       47611, 37193,  7619, 14937,  9937, 11982, 13210, 30734, 20655,
       15909,  6603, 47907, 17199,  5931, 27469, 45515, 28892,  2106,
       38728, 29101, 25879, 13416,  4773, 39217,  3695, 21946,  7207,
       16527,  9529, 11116, 23252, 14359, 18493, 22137,  5581, 18130,
       48591, 18606,  8792, 47571, 20778, 31281, 44565, 44202,  4195,
       43044, 47738, 32018, 35276, 11994, 11250, 32094, 31515, 26586,
       17727, 45957,  2835, 15203, 19180, 32620, 36302, 43294, 32026,
       18734, 31053,  4695, 37778, 39768, 26819, 45962, 35450, 45496,
       15004, 17623, 27429, 23362, 48738, 33232, 46382, 24829,  2216,
        9962,  4998, 42315, 33971, 13513, 37963, 18011, 18110, 14521,
       24823,  7536, 48889, 43529, 17500,  4359, 17852,  3141, 26715,
       46446,  5298, 43337, 32590, 18374, 30801, 25798, 33401, 42662,
        1903,  4156, 30627, 37234,  7712, 47374, 29071, 43870, 17819,
       14726,  6298,  8131, 16073, 35514, 40705, 16328, 34936, 11757,
       44125, 23869, 30044, 29922, 10890, 26023, 42296, 33523, 15454,
       14704]), [5, 7, 1, 4])]
Competition
DC 0, val_set_size=1000, COIs=[3, 9, 1, 4], M=tensor([3, 9, 1, 4], device='cuda:0'), Initial Performance: (0.25, 0.04431799054145813)
DC 1, val_set_size=1000, COIs=[0, 2, 1, 4], M=tensor([0, 2, 1, 4], device='cuda:0'), Initial Performance: (0.252, 0.044353610038757325)
DC 2, val_set_size=1000, COIs=[8, 6, 1, 4], M=tensor([8, 6, 1, 4], device='cuda:0'), Initial Performance: (0.276, 0.04447247278690338)
DC 3, val_set_size=1000, COIs=[5, 7, 1, 4], M=tensor([5, 7, 1, 4], device='cuda:0'), Initial Performance: (0.218, 0.04446711611747742)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {9, 3}
D07: 1000 samples from classes {9, 3}
D08: 1000 samples from classes {9, 3}
D09: 1000 samples from classes {9, 3}
D010: 1000 samples from classes {9, 3}
D011: 1000 samples from classes {9, 3}
D012: 1000 samples from classes {0, 2}
D013: 1000 samples from classes {0, 2}
D014: 1000 samples from classes {0, 2}
D015: 1000 samples from classes {0, 2}
D016: 1000 samples from classes {0, 2}
D017: 1000 samples from classes {0, 2}
D018: 1000 samples from classes {8, 6}
D019: 1000 samples from classes {8, 6}
D020: 1000 samples from classes {8, 6}
D021: 1000 samples from classes {8, 6}
D022: 1000 samples from classes {8, 6}
D023: 1000 samples from classes {8, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO2']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.375, 0.06283778327703476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07227671495079994) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.07495411774516106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.349, 0.08856414425373077) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.284, 0.06856693160533905) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.08170339134335518) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.08742304396629333) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.09984482651948928) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.08577430075407028) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.357, 0.09504514583945274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.11441597338020802) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.10870000609755516) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.435, 0.11007066434621811) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.11734175483882427) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.16358603571355343) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.13449031338095666) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1351322904229164) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13958969017863274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.17408120495080948) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.1502700263261795) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO4']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.15643265687674285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.419, 0.1466355331838131) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2705200811526738) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.1623449475169182) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.1696251774523407) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.432, 0.15292382569611074) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2703526836093515) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.190651739038527) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19223802105896176) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.425, 0.16991872733086347) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3325524968125392) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.1890639338158071) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19927251303382218) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.418, 0.17540518701449037) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3120443267803639) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.2167533227801323) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.21156952209956945) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.424, 0.16317588014900683) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.3092682206723839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.2174901111423969) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO3']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.21950078430213035) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.436, 0.17199799276702105) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.3495311813998269) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.24179876104742287) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.22700156772509217) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.441, 0.16019078305736184) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.3646612986959517) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.26257471803948285) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.481, 0.2178588998913765) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.437, 0.16250781837850808) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.33664550830668305) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.2640616532117128) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.22197448600269853) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.428, 0.1736603311188519) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.36080756353389004) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.26660772677883504) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.23732823427580296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.18532299120351672) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.483, 0.35184628958895336) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.27060807759314776) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO3', '(DO2']
DC 1 --> ['(DO1', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.22953676644153892) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.438, 0.19263899641670287) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.34806864343662164) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.29054633459448814) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.2348032607715577) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.449, 0.17770124152489006) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.36718032605497863) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.296822370775044) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.2505384323531762) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.20877038723230362) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.35036061135627095) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.301059784129262) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.26663050609268246) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.1944965897500515) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.34682554960239215) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.31282516846060754) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.21093419936764984) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.441, 0.18601853080280126) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.3140784258813947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.3168117549568415) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO3']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.2406758277611807) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.18754410379007458) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.3215669717611745) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.424, 0.3150442706570029) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.21679218859411775) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.448, 0.1935563639588654) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.3143001577947871) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.307403700709343) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.21958002919889988) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.18966161531023681) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.32945706816439635) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.2886795186661184) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.23683868952328338) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.44, 0.17964234866201878) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.3007330510580214) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.30301274540275336) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.22716611827770247) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.438, 0.19967530374322087) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.3109523541457311) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.27935412859916686) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.23024456620961428) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.1877236188892275) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.29278388017776885) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.3037557542733848) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.21138965011388064) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.18339470490347595) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.483, 0.28741396371135486) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.435, 0.2803104966767132) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.22529115900071336) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.18904447115585207) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.483, 0.30241101227112815) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.28708684228360654) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.21194401031406596) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.433, 0.17179374786280097) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.312481773604668) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.26154116474092004) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.2313232481777668) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.437, 0.1768138070181012) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.32260875898823727) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.2657793875038624) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.26393799923185723) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.15966384999081493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.266757999015681) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2969682621732354) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.22010861267661677) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.448, 0.18786952167749404) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.28587221694376785) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.27226965548843146) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.23290677461866288) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.449, 0.16983345877751707) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.3005683971677499) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.25092676883935927) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.23560655796760693) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.1655605873055756) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.3075496287842907) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.438, 0.28600810576975344) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.24986297273309901) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.438, 0.17088397102430464) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.3250825654642103) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.30483676958829165) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO0', '(DO3']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO2']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.2444547769718338) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.16797274084948002) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.2820517913797121) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.28095164384692906) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.23432952560158446) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.44, 0.17503634176403285) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.32658660110770144) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.2823484337627888) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.22649768775864504) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.441, 0.18471072580851614) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.31854911146831) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.2799277594536543) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.27102540808485354) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.441, 0.18347678731754422) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.2781077090037579) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.2977005931548774) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.24563324762159028) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.21285164471901954) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.3064580095095589) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.3009177698940039) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO1']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.25418293265299874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.441, 0.21987079232558607) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.28209984180211906) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.31756336711347105) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.2377180650383234) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.20062226778734477) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.27585056035588784) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.305344022680074) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.24858238188293763) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.444, 0.19480029752664269) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.2914383857632056) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.29840418726205825) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.24002193370088934) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.441, 0.1912561903204769) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.29336443948444685) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.2489983772933483) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.2554475960112177) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.20703809852339328) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.2876747063636576) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.30133221739716826) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO4']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.2705109319156036) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.21659619505144656) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.23474130945996877) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.2879980301149189) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.26252918983856216) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.2037613401794806) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.2556467066414625) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.3195797708109021) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.27091767386649734) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.444, 0.22887655183137395) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.27367898517222783) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.435, 0.3042329463735223) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.2927591007904848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.20410235147178174) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.27829276481936904) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.303373933672905) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.298926624521031) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.19151555746234952) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.49, 0.28412241385181913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.26576829502731564) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.25, 0.04431799054145813), (0.375, 0.06283778327703476), (0.284, 0.06856693160533905), (0.394, 0.08577430075407028), (0.435, 0.11007066434621811), (0.458, 0.1351322904229164), (0.452, 0.15643265687674285), (0.463, 0.1696251774523407), (0.471, 0.19223802105896176), (0.471, 0.19927251303382218), (0.47, 0.21156952209956945), (0.479, 0.21950078430213035), (0.477, 0.22700156772509217), (0.481, 0.2178588998913765), (0.479, 0.22197448600269853), (0.475, 0.23732823427580296), (0.483, 0.22953676644153892), (0.475, 0.2348032607715577), (0.473, 0.2505384323531762), (0.478, 0.26663050609268246), (0.47, 0.21093419936764984), (0.474, 0.2406758277611807), (0.467, 0.21679218859411775), (0.474, 0.21958002919889988), (0.473, 0.23683868952328338), (0.477, 0.22716611827770247), (0.478, 0.23024456620961428), (0.472, 0.21138965011388064), (0.472, 0.22529115900071336), (0.469, 0.21194401031406596), (0.473, 0.2313232481777668), (0.468, 0.26393799923185723), (0.468, 0.22010861267661677), (0.476, 0.23290677461866288), (0.467, 0.23560655796760693), (0.463, 0.24986297273309901), (0.464, 0.2444547769718338), (0.451, 0.23432952560158446), (0.47, 0.22649768775864504), (0.465, 0.27102540808485354), (0.465, 0.24563324762159028), (0.462, 0.25418293265299874), (0.463, 0.2377180650383234), (0.467, 0.24858238188293763), (0.479, 0.24002193370088934), (0.467, 0.2554475960112177), (0.467, 0.2705109319156036), (0.471, 0.26252918983856216), (0.472, 0.27091767386649734), (0.472, 0.2927591007904848), (0.474, 0.298926624521031)]
TEST: 
[(0.254, 0.043263681530952454), (0.37075, 0.060598327726125716), (0.28775, 0.06587039145827293), (0.39, 0.08190187042951584), (0.42925, 0.10396925833821297), (0.45375, 0.12755469673871994), (0.44575, 0.1482144956588745), (0.45525, 0.16008145165443421), (0.46375, 0.17392877840995788), (0.4635, 0.18666653883457185), (0.46075, 0.1964863825440407), (0.46975, 0.20444251197576524), (0.46725, 0.21377579742670058), (0.47375, 0.20144546949863434), (0.46775, 0.20883898067474366), (0.4665, 0.22211952883005143), (0.47, 0.2140796563029289), (0.46525, 0.21865876334905623), (0.46575, 0.23415158826112747), (0.469, 0.2465560958981514), (0.469, 0.1980308844447136), (0.468, 0.2237944858074188), (0.458, 0.20569672149419785), (0.46725, 0.20532076513767242), (0.4655, 0.2189821818470955), (0.46975, 0.21006969356536864), (0.46775, 0.21690725231170654), (0.46925, 0.1980285210609436), (0.46475, 0.20981157851219176), (0.4625, 0.19637717109918595), (0.4635, 0.2177716901898384), (0.462, 0.24795604544878005), (0.466, 0.20355994725227355), (0.47075, 0.21760037076473235), (0.45725, 0.22253134709596634), (0.45375, 0.24175161802768708), (0.45825, 0.2324570380449295), (0.44525, 0.22470740628242492), (0.46025, 0.216304257273674), (0.45975, 0.25475239074230194), (0.46225, 0.23340263587236404), (0.45675, 0.24015850871801375), (0.462, 0.22657127732038498), (0.45725, 0.23461052203178406), (0.466, 0.227919514298439), (0.4575, 0.24514054322242737), (0.46075, 0.2571178842186928), (0.462, 0.2480696622133255), (0.463, 0.25679310190677646), (0.46425, 0.27871375292539596), (0.4625, 0.2867208624482155)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           3       0.42      0.98      0.59      1000
           4       0.00      0.00      0.00      1000
           9       0.52      0.86      0.65      1000

    accuracy                           0.46      4000
   macro avg       0.24      0.46      0.31      4000
weighted avg       0.24      0.46      0.31      4000

Competition_DC_1
VAL: 
[(0.252, 0.044353610038757325), (0.25, 0.07227671495079994), (0.293, 0.08170339134335518), (0.357, 0.09504514583945274), (0.416, 0.11734175483882427), (0.395, 0.13958969017863274), (0.419, 0.1466355331838131), (0.432, 0.15292382569611074), (0.425, 0.16991872733086347), (0.418, 0.17540518701449037), (0.424, 0.16317588014900683), (0.436, 0.17199799276702105), (0.441, 0.16019078305736184), (0.437, 0.16250781837850808), (0.428, 0.1736603311188519), (0.443, 0.18532299120351672), (0.438, 0.19263899641670287), (0.449, 0.17770124152489006), (0.453, 0.20877038723230362), (0.445, 0.1944965897500515), (0.441, 0.18601853080280126), (0.445, 0.18754410379007458), (0.448, 0.1935563639588654), (0.446, 0.18966161531023681), (0.44, 0.17964234866201878), (0.438, 0.19967530374322087), (0.443, 0.1877236188892275), (0.442, 0.18339470490347595), (0.445, 0.18904447115585207), (0.433, 0.17179374786280097), (0.437, 0.1768138070181012), (0.445, 0.15966384999081493), (0.448, 0.18786952167749404), (0.449, 0.16983345877751707), (0.445, 0.1655605873055756), (0.438, 0.17088397102430464), (0.443, 0.16797274084948002), (0.44, 0.17503634176403285), (0.441, 0.18471072580851614), (0.441, 0.18347678731754422), (0.442, 0.21285164471901954), (0.441, 0.21987079232558607), (0.445, 0.20062226778734477), (0.444, 0.19480029752664269), (0.441, 0.1912561903204769), (0.445, 0.20703809852339328), (0.447, 0.21659619505144656), (0.445, 0.2037613401794806), (0.444, 0.22887655183137395), (0.446, 0.20410235147178174), (0.442, 0.19151555746234952)]
TEST: 
[(0.267, 0.04317044323682785), (0.25, 0.06941601613163947), (0.3085, 0.07784869140386581), (0.37125, 0.09040846940875054), (0.41275, 0.11196270486712456), (0.39575, 0.1334228486418724), (0.41875, 0.14155897158384323), (0.43075, 0.1480341517329216), (0.4325, 0.16426597940921783), (0.4275, 0.16917636078596116), (0.4355, 0.1590513065457344), (0.44025, 0.1683790236711502), (0.4415, 0.15597293615341187), (0.4435, 0.15812866628170014), (0.438, 0.1665739552974701), (0.44325, 0.18118612360954284), (0.441, 0.18923471409082412), (0.446, 0.1738680381178856), (0.4505, 0.20462604451179506), (0.446, 0.18891126066446304), (0.4435, 0.1798942756652832), (0.44675, 0.18350669652223586), (0.4455, 0.1882501014471054), (0.44675, 0.18453567278385163), (0.4425, 0.17399568885564803), (0.4405, 0.1935368404984474), (0.449, 0.18116123747825622), (0.441, 0.1781865774989128), (0.44625, 0.18361809366941453), (0.4455, 0.1666873953342438), (0.44325, 0.17061207580566407), (0.445, 0.156663073182106), (0.44925, 0.18183706629276275), (0.4485, 0.16442912542819976), (0.4475, 0.15999988025426864), (0.4495, 0.16464674240350724), (0.4475, 0.163123998939991), (0.44775, 0.16889045464992522), (0.445, 0.17946013396978377), (0.44625, 0.17520158159732818), (0.4455, 0.2049789217710495), (0.44525, 0.21060400933027268), (0.44475, 0.19216559982299805), (0.44575, 0.18572539937496185), (0.445, 0.18155036807060243), (0.4465, 0.1947449392080307), (0.45125, 0.20745463860034943), (0.449, 0.1960327029824257), (0.4425, 0.21770525354146958), (0.44425, 0.19504106801748275), (0.4445, 0.18416222751140596)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.55      0.83      0.67      1000
           1       0.00      0.00      0.00      1000
           2       0.38      0.95      0.54      1000
           4       0.00      0.00      0.00      1000

    accuracy                           0.44      4000
   macro avg       0.23      0.44      0.30      4000
weighted avg       0.23      0.44      0.30      4000

Competition_DC_2
VAL: 
[(0.276, 0.04447247278690338), (0.436, 0.07495411774516106), (0.452, 0.08742304396629333), (0.473, 0.11441597338020802), (0.472, 0.16358603571355343), (0.459, 0.17408120495080948), (0.478, 0.2705200811526738), (0.477, 0.2703526836093515), (0.474, 0.3325524968125392), (0.474, 0.3120443267803639), (0.475, 0.3092682206723839), (0.482, 0.3495311813998269), (0.481, 0.3646612986959517), (0.479, 0.33664550830668305), (0.479, 0.36080756353389004), (0.483, 0.35184628958895336), (0.479, 0.34806864343662164), (0.486, 0.36718032605497863), (0.487, 0.35036061135627095), (0.488, 0.34682554960239215), (0.486, 0.3140784258813947), (0.488, 0.3215669717611745), (0.489, 0.3143001577947871), (0.484, 0.32945706816439635), (0.486, 0.3007330510580214), (0.484, 0.3109523541457311), (0.488, 0.29278388017776885), (0.483, 0.28741396371135486), (0.483, 0.30241101227112815), (0.487, 0.312481773604668), (0.488, 0.32260875898823727), (0.488, 0.266757999015681), (0.488, 0.28587221694376785), (0.486, 0.3005683971677499), (0.484, 0.3075496287842907), (0.487, 0.3250825654642103), (0.481, 0.2820517913797121), (0.484, 0.32658660110770144), (0.486, 0.31854911146831), (0.488, 0.2781077090037579), (0.488, 0.3064580095095589), (0.489, 0.28209984180211906), (0.489, 0.27585056035588784), (0.487, 0.2914383857632056), (0.489, 0.29336443948444685), (0.487, 0.2876747063636576), (0.489, 0.23474130945996877), (0.484, 0.2556467066414625), (0.489, 0.27367898517222783), (0.487, 0.27829276481936904), (0.49, 0.28412241385181913)]
TEST: 
[(0.27075, 0.04346053540706635), (0.4375, 0.07203164887428283), (0.44975, 0.083502028465271), (0.4725, 0.10839265322685242), (0.47325, 0.15421055006980897), (0.45525, 0.16559282863140107), (0.48025, 0.25170764434337617), (0.478, 0.2528487293720245), (0.475, 0.3110364720821381), (0.478, 0.291153617978096), (0.48075, 0.29257084810733797), (0.486, 0.3238119168281555), (0.48075, 0.3414088040590286), (0.4775, 0.3176215124130249), (0.4835, 0.3395236858129501), (0.482, 0.32690863907337187), (0.47575, 0.329922593832016), (0.4875, 0.33991974115371704), (0.4845, 0.32855134284496307), (0.483, 0.32606826186180116), (0.48475, 0.29553295814991), (0.48575, 0.30232701170444487), (0.48675, 0.2962215639352798), (0.48525, 0.31019392573833465), (0.48475, 0.28093940651416777), (0.483, 0.2902458963394165), (0.48125, 0.2762242718935013), (0.48475, 0.2707579513788223), (0.48275, 0.286050302028656), (0.486, 0.2945788165330887), (0.48325, 0.30408952951431273), (0.485, 0.2515049960613251), (0.48425, 0.269664205789566), (0.48325, 0.28357256841659545), (0.484, 0.2876563038825989), (0.4855, 0.3070569022893906), (0.4815, 0.2644195643663406), (0.48525, 0.308089465379715), (0.48425, 0.29788147687911987), (0.48625, 0.26239836275577544), (0.48575, 0.29439723551273345), (0.4855, 0.26522556734085084), (0.485, 0.2600986866950989), (0.483, 0.27418712759017944), (0.4845, 0.27817570757865906), (0.48525, 0.2718156464099884), (0.4885, 0.22359306633472442), (0.48175, 0.24063813829421998), (0.48625, 0.26370851838588716), (0.48475, 0.2646048294305801), (0.48575, 0.2710508328676224)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           6       0.43      0.99      0.60      1000
           8       0.57      0.95      0.71      1000

    accuracy                           0.49      4000
   macro avg       0.25      0.49      0.33      4000
weighted avg       0.25      0.49      0.33      4000

Competition_DC_3
VAL: 
[(0.218, 0.04446711611747742), (0.349, 0.08856414425373077), (0.393, 0.09984482651948928), (0.407, 0.10870000609755516), (0.407, 0.13449031338095666), (0.415, 0.1502700263261795), (0.416, 0.1623449475169182), (0.416, 0.190651739038527), (0.406, 0.1890639338158071), (0.414, 0.2167533227801323), (0.43, 0.2174901111423969), (0.423, 0.24179876104742287), (0.423, 0.26257471803948285), (0.433, 0.2640616532117128), (0.427, 0.26660772677883504), (0.425, 0.27060807759314776), (0.429, 0.29054633459448814), (0.431, 0.296822370775044), (0.43, 0.301059784129262), (0.43, 0.31282516846060754), (0.434, 0.3168117549568415), (0.424, 0.3150442706570029), (0.427, 0.307403700709343), (0.432, 0.2886795186661184), (0.432, 0.30301274540275336), (0.431, 0.27935412859916686), (0.436, 0.3037557542733848), (0.435, 0.2803104966767132), (0.432, 0.28708684228360654), (0.428, 0.26154116474092004), (0.433, 0.2657793875038624), (0.429, 0.2969682621732354), (0.434, 0.27226965548843146), (0.434, 0.25092676883935927), (0.438, 0.28600810576975344), (0.434, 0.30483676958829165), (0.437, 0.28095164384692906), (0.434, 0.2823484337627888), (0.431, 0.2799277594536543), (0.436, 0.2977005931548774), (0.437, 0.3009177698940039), (0.436, 0.31756336711347105), (0.431, 0.305344022680074), (0.432, 0.29840418726205825), (0.431, 0.2489983772933483), (0.433, 0.30133221739716826), (0.431, 0.2879980301149189), (0.428, 0.3195797708109021), (0.435, 0.3042329463735223), (0.436, 0.303373933672905), (0.439, 0.26576829502731564)]
TEST: 
[(0.2305, 0.0433762149810791), (0.34325, 0.08504497039318085), (0.39575, 0.09558765774965286), (0.4115, 0.10409454986453057), (0.40925, 0.12879934746026994), (0.41825, 0.14445116937160493), (0.41375, 0.15593986171483992), (0.4215, 0.18018277913331984), (0.41, 0.17879825294017793), (0.424, 0.20379313588142395), (0.42875, 0.2091999084353447), (0.43125, 0.23005824315547943), (0.42475, 0.2501455147266388), (0.4325, 0.25282575643062594), (0.4325, 0.25739832961559295), (0.426, 0.2603587749004364), (0.43375, 0.28152649796009066), (0.433, 0.28688169705867766), (0.43925, 0.29776426780223847), (0.43375, 0.3086884015798569), (0.43675, 0.3128754670619965), (0.43525, 0.3072377994060516), (0.4355, 0.3002288049459457), (0.433, 0.28420043540000917), (0.435, 0.3026138950586319), (0.43075, 0.275885710477829), (0.43475, 0.2978339821100235), (0.43575, 0.2725295512676239), (0.4375, 0.2819153769016266), (0.43625, 0.259502033829689), (0.4385, 0.26044472765922544), (0.4255, 0.29230487167835234), (0.4335, 0.2698625671863556), (0.43475, 0.2433429410457611), (0.4335, 0.2812371654510498), (0.439, 0.3017186841964722), (0.43775, 0.27828778994083403), (0.4355, 0.2817834320068359), (0.4365, 0.27766941440105436), (0.4355, 0.29035070514678957), (0.433, 0.2936563420295715), (0.43625, 0.312107475399971), (0.43525, 0.2965021311044693), (0.43875, 0.29386374163627627), (0.43525, 0.24768092757463456), (0.43075, 0.2924665641784668), (0.43575, 0.2822889283895493), (0.43525, 0.3130874208211899), (0.43775, 0.29334608602523804), (0.4325, 0.2975906285047531), (0.43525, 0.2613610337972641)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           5       0.39      0.92      0.54      1000
           7       0.51      0.82      0.63      1000

    accuracy                           0.44      4000
   macro avg       0.22      0.44      0.29      4000
weighted avg       0.22      0.44      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [72]
name: no-alliance-72
score_metric: contrloss
aggregation: <function fed_avg at 0x7b3e54ba9c10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=72
Partitioning data
[[7, 9, 1, 2], [6, 8, 1, 2], [4, 3, 1, 2], [0, 5, 1, 2]]
[(array([ 2102, 37010, 13182, 10158, 47275, 19636,  1365, 30216, 31980,
       36407, 45523, 26661, 26983, 44664,  5047,  5597, 35724, 26606,
       14517, 47983, 18828, 43060, 47323, 24898, 36346,  3578, 40182,
        3284, 13516, 13280,  6320, 20701, 22128, 10684, 23000, 17462,
          68, 29789, 31528, 24853,  5774, 34010, 34440, 10045, 24682,
       25310, 45343, 18922, 26291, 29644, 16477,  7598, 31321, 19599,
       15067, 47898, 19909, 28764, 44892, 35176, 40465,  7291, 29962,
         440,   589, 43892, 40340, 16350,  6999, 14202, 34511, 12474,
       30876, 48837, 15999, 42355, 35082, 12844,  3251,  7717, 23621,
       43124, 41850, 24698, 38512,  4809,  8984, 14489, 49610, 37131,
       21051, 10573,  6726, 45708, 16224, 17876, 47372,  5313, 29047,
        6822, 21787, 20581, 24995,  6653, 45054, 11484, 35108, 37019,
       18417,   152, 44440, 31364,  7055, 24013, 18528, 45480, 21394,
       15472,  2207, 32660, 14261, 23682,  7798, 12320, 27529, 10347,
       45747, 44567, 12941, 44135, 31871, 35812, 16828, 43449, 28977,
        5101, 27043, 15013, 35220, 31589, 38616, 43435, 24327, 44661,
       48858,  8521, 29910, 35336,  1742, 12272, 16996, 15712, 40234,
       32211, 40360, 25701, 15324, 21553, 23561, 23627,  8076, 36895,
       10886, 18405, 34015, 37482, 39949, 17537, 18017, 32646, 38826,
       44718, 31156, 10666, 13268, 19159, 39246, 15292, 27697, 21396,
         641, 19940, 12864, 11068,  2366, 26567,  3213,  6239, 28695,
        8887, 38155, 49071,  9279,  4729, 38955, 29272, 30330, 22846,
       26032, 20094, 14144, 49592,   994, 37222,  2801, 34107,  5200,
       19821, 24944, 44262,  1806, 32282, 41319, 15720, 20681,  5241,
       39259, 47092, 17942, 14242, 41129, 11621, 25283, 30163, 49363,
       30030,  1445, 42117, 12744, 49760, 16022, 21876,   789, 40061,
       28832,  7450, 13818, 32061, 40942, 42457, 41784, 32633, 46301,
        3157, 38042, 15271, 23292, 30845, 10041, 11553, 41280,  9606,
       16482, 31378, 47743,  9626, 40599, 48816, 23169, 11312, 22643,
       29228, 38056, 35813, 29306,  2199, 18344, 11876, 30080, 43946,
       27591,  7698, 37993, 12427, 35984, 16601, 23336, 10163, 10144,
       17466, 10231, 14892, 14250, 27551, 20552, 39581, 11258,  6149,
       32040, 17359, 47574, 12555, 16684, 32552, 11472, 23720,  2935,
        1232, 13969, 41765,  8089, 31722, 42407, 43960, 37249, 25303,
       36291, 20947, 14224, 18020, 28189,   102, 39174, 16809, 14913,
       36413, 48332, 33856, 25387, 40043, 45465, 42278, 33813, 23303,
       36434,  8821, 40015, 26229, 44678, 35184, 24712, 30850, 11292,
       15694,  8125,  5332, 45547, 48284,  1765, 44692, 30966, 33437,
       21408, 22156, 19193, 16744, 28846, 46646, 19122,  1638, 40413,
       36475, 12835, 13380, 40059, 21330, 49398, 42638,  2637, 41614,
         668, 45906, 27184, 32945, 38280, 26574, 35667, 32820, 18703,
       12385, 23426, 42106, 24152, 25407, 16054, 40387,  5591, 32112,
       28053, 15263, 16999, 35498, 16092, 30678, 12562, 40790, 19053,
       41495,  8841, 25091, 38267,  7461, 13370, 13881, 42385, 43346,
       16773,  5306, 49823, 39841, 46059, 41630, 47148, 14599, 16170,
       37600, 13869, 36577, 34773, 16725, 10448, 14424,  3175, 23908,
        3143, 20481, 37400, 42208, 36210, 30932, 11889, 44881,  9091,
       16262, 11173, 26647, 14197, 40052, 25752, 26262,  3702, 13529,
       36179, 33417, 16699, 34024, 22802, 23023, 22464, 34474,  9959,
       17945, 48210, 28684, 42851, 37730, 30420, 24938, 26468,  2058,
       10943, 47121,  1987, 16535, 27784, 36918, 24718, 34290,  6708,
        8749, 20176, 12288, 25988, 17097, 42973, 30351, 35379, 16688,
       34485, 24086, 20718, 17526, 12709, 22296,  5668, 37780, 46190,
        3648, 14825, 21696, 34078, 28046, 22240, 29148, 13475, 36384,
       36031, 36649, 33868, 18567, 19120, 37285, 41408, 26620, 28549,
        8146, 34427, 47175, 48577, 23877, 40152, 22533,  7992,  6371,
       11075, 27228,  6813,   743, 20619, 28526,  7513, 46904, 41948,
       16755, 16089, 11791, 34400, 14912, 41695, 37592, 26290,  2469,
       10882, 41392, 37929, 41847, 40076, 18321, 39902, 13323, 43636,
       12169, 49286, 41262, 49085, 39391, 19866, 46741, 48746, 25050,
       12629, 22938, 37844,  2431, 11804, 33034, 24292, 27174, 12064,
        8920, 49196,  4696, 33748, 43847, 10236, 30463, 36334,  9327,
       28347, 38596, 46207,  7358, 32994, 31395, 31208, 49993,  3830,
        7841,  8244,  4654,  3059, 10627, 21593, 40449, 32418, 10522,
       46259, 22747, 36672, 37611, 49426, 25279,  5816, 44886, 38919,
       28559, 49221,  6971, 43297, 49060, 20453,  2286, 10747, 21585,
       24010, 25201, 25488, 10790,  3467, 12373, 31407, 12814,  4145,
       11403, 13523, 44475, 38270, 11130, 47616,   427, 29019, 19667,
       38204, 32011, 21283, 13332, 46674, 18844, 39405, 45289, 15669,
       19446, 35335, 36891, 24008,  2727, 21215, 45599, 43240, 21871,
       33105,  1946, 24024, 16223, 18246, 34673, 45105, 28187, 28688,
       16275, 35666, 30050, 30696, 12917,  5146,  2769, 43068, 35757,
       22145,  9252,  8570, 29315,  5257, 10569, 31552, 28393, 22788,
       21850,  3273, 24194, 44448, 23263, 33292, 37616, 21829, 22636,
       48275, 22833, 47279, 22552, 35058,  1389, 39118, 49151, 20865,
       32681, 16956, 22553,  5769, 22659, 12981, 40145,   848,  8124,
       17091, 42130, 29264, 19454,  1565, 41338, 35207, 31599, 19164,
       49411,    96, 38892, 43932,  7649, 42734, 41663, 26794, 21423,
       44235, 49849, 31300, 20336, 13858, 14844, 15950, 31957, 14717,
        5282, 30120,   226, 47495, 27730, 14538,  8938, 38464, 11822,
       28552, 48234, 18926, 17586, 31297, 16613,  1541, 45687, 28532,
         834, 11574, 34349, 37460, 23607,   262, 39333, 10953, 34145,
       20368, 40220, 13468, 38493, 25910, 22399, 46514,  8191, 43600,
       41087, 29522, 24828, 14423, 29042, 21654, 40327, 48651, 11630,
       20125, 12992, 30767,  9903, 31467, 25161,  6390, 47675, 37581,
        7783, 34566, 19998, 44053,  5603, 20149, 38186,   463, 22803,
       48090, 25695, 32387,  1777, 24457, 48388, 47730, 33699, 32570,
       48594, 30795, 27328, 36778, 36583, 12180,   403, 20876, 45550,
       28705, 35230,  1415, 27430, 45109, 44216, 41764, 10480, 27816,
       49156, 18149, 32420, 26992, 26990, 34293, 19874,  4896, 48012,
        4700, 30531, 45428,  5634,  9580, 33851, 27325,  4168, 34187,
       14588, 43856,  5384,  4220, 43823, 31307,  6558, 47846, 41469,
       42428,  4511, 14814,  8077, 39436, 29612, 35764, 33503, 15813,
       31090, 42155, 38830,  4398, 40231, 49342, 13179, 36245, 42158,
        7868, 16739,  6146,  9001, 27091, 42815, 44842, 25263, 36657,
       22280, 45229, 22006, 25530, 25831, 29715, 39765,  1789, 16123,
       26321, 21650, 15477, 12748, 14063, 23670, 39648, 36465, 44758,
       26982,  1995,   689, 46877, 40860, 12860, 37647, 35880,  3416,
        3760, 43469,   933, 30005, 36355,  5781, 45238, 21500, 42020,
       10935,  8148, 37947,  6616, 37280, 19070, 17344, 21102,  6917,
       18184, 21018, 15820, 27973, 46082, 18287, 22770, 21392,  4656,
       18241, 34181,  1676, 40221,  3484, 31475, 10018, 34608, 45074,
       35319,  9827,  1077, 38502, 36094, 35482, 39390, 28788, 28664,
       48870, 30705, 32166,  6502,  1497, 14834, 43287, 10474, 22375,
       43593,   281, 19714, 23241, 49328, 42564, 18756, 45342,  4242,
       48348, 45855, 49449, 11923,    41, 42234,  7480,  6388, 34497,
       38457, 11606, 38480, 24780,  9455, 15731,  6799, 25130, 46089,
        8713, 20138, 17538, 22864, 44328, 39594, 21208, 12462, 12584,
       49101, 21643, 27186, 22279, 20696, 14620, 48694, 30505, 27746,
       24064, 35823, 23388, 43041, 44121, 37374, 27542, 49395, 35673,
       24254, 44910, 14739, 35226,  6448, 37941, 20737,  1852, 15124,
        1530]), [7, 9, 1, 2]), (array([36373, 22530, 35781, 29310, 42872, 10211,  7403,  1317, 37261,
       41534, 34020, 12031, 12369, 30864, 31558, 30212, 44814, 46252,
        7091, 43390, 12321, 43174,  3716, 42712, 43691,  9653,  3041,
       47580, 46974, 10176,  6938,  5263, 11999, 49512, 21132, 25793,
       18103, 14162,  6025, 18870, 46317, 23370, 17282, 12604, 42584,
       33608, 17925, 43734, 26054, 23152, 34056, 12597, 12445, 15294,
       10600,  5661, 45693, 19184, 19308, 29687, 33173,  3926, 19763,
       28011, 42975, 19489, 16696, 17523, 41533, 48983, 47953, 25169,
       29241, 23087, 43431,  1876, 27626, 17287, 18021, 14660, 43761,
       16960,  1047, 16499, 36463, 36913, 21475, 32248, 39630, 42077,
       31142, 23366, 40206,  2796, 12874, 11392,   836, 13776, 13965,
        7064, 44224, 42859, 35672, 36522, 18615, 30816, 12796, 18554,
        3192, 33988, 18330, 18665, 34612, 20146, 16242, 41552, 16862,
       35891, 23579, 11675, 40316, 24992, 39357, 30001,  7967,  9423,
       46009, 37933, 46334, 20775, 42723, 27678, 45420, 37084, 27051,
       14830, 46050, 35600, 16782, 33436, 40437, 33758, 30644,  9275,
        8436,  7968, 49655,  8474, 34552, 35091, 28370, 21447, 46587,
       11066,  8363, 10517, 46765, 25761, 45597, 42577, 44096, 23593,
        2607, 25729, 13162,  5409, 22295, 33405, 49129,  6566, 29931,
       28504, 49289,    22, 18420, 38771,  7321, 34148, 48203, 27183,
        7172, 17143,  2898,  6442,  9323, 33791, 20008, 10757, 34762,
        3290,  8803, 23956, 29971, 25747, 22282, 33948, 44417, 24852,
       27117, 45300,  8679, 41785, 16611, 44518, 23987, 47858, 44489,
       36111,  1596,  5497, 16778, 45253, 42250, 32397, 30906, 25851,
       44708, 42748, 45719, 23771, 29339,  2276,  2777, 32759, 21125,
       34139,  6145, 12048, 29194, 12151, 24307, 14444, 36998, 18025,
       32080,  4678, 27741, 23373, 41196, 45385,  9112,  3912, 15193,
        4951,   355, 29550,  4143,  7632,  7036, 27367, 41867, 28507,
       32356, 45898, 44057, 32370, 39419, 27386,   958, 41218,  4779,
       41447,   897,  7029, 48805, 36214, 14442, 37692, 40669,  9786,
        1162,  5365, 39069,  9882, 46462, 20719,  5480, 22409, 30844,
       42270, 37953, 33264, 22920, 44043, 23125, 14281, 35617, 16044,
       17034, 23214, 16775, 23625, 48120, 12443, 21679, 24185, 24984,
       23838, 49897, 46543, 15948, 12206, 43858,  6788, 41957,  8414,
       40671, 48864, 28180, 24138, 33613,  4995, 40008,  5091, 34135,
       44594, 13369, 49899, 27727, 33861, 31367, 13587, 22661, 46223,
        7007, 38850, 34910, 48393, 49479, 42506,  1429, 47780, 42840,
        6916, 46710, 41712, 19251, 41008, 47400, 12513, 10840, 43644,
        5910,  3904, 14693,  7083, 23669, 16430, 47052, 28006, 12557,
       27243, 33756, 13576, 48297,  5820, 35825, 45649, 16607, 35707,
       38862,   507, 28066, 23440, 10537, 39038, 11244, 38162, 20727,
        5255, 27067, 44054, 47380, 12792, 42917, 41485, 24765, 39971,
        8657, 37711, 41292, 13481, 27550, 44420, 39362, 40495, 40292,
       37420, 32714, 37643,   609, 32783, 49521, 11490, 13095, 46985,
       38136, 36131, 24455, 28842, 29477, 49681, 27715,  3516, 36547,
       31182,  9188,  5941, 35885, 31714, 34892, 25756, 24083, 25754,
       22465, 24456,  2344, 38754,   259, 21134, 36498, 12249, 27858,
       37633, 17290,  9063, 26583, 46111, 13197,  2635, 42847, 11832,
        3305,  8531, 34319, 49887, 28707, 35540, 47776,  2381,  5985,
       37550, 43713, 49625, 29055, 27561, 38276, 47713, 28012, 30398,
       35150, 47376, 29788,  1794, 16086,  3258,  9669, 23960, 14613,
       46479,  5948, 25924, 21635,  1239, 45325, 36526,  7156, 18112,
       24588, 38734,  4955, 14369, 34695, 17799,  6134,  6219,  8267,
       47021, 20822, 36882, 25573, 30303, 42720,   786, 29877, 44701,
        7310, 41250,   291, 31188, 45243, 24328, 42884, 38797, 25533,
       32271, 47770, 37959, 41256, 15043,  6366,  6453,  3085,  4100,
       25071, 40655, 35129, 10133, 32679, 36813,  4354, 49503, 12903,
       39097, 18671, 18123, 13973, 22564, 28175, 31969, 24607,  8932,
       35041, 38719, 11427, 27194,   396,  2862, 42413, 47745, 33181,
        1559, 17515,  7261, 18649, 39476, 37799,  7019, 47060, 18181,
        6074, 17030, 35705, 34287, 40582, 11698,  6241, 25611, 30782,
       17366, 38785, 49594, 30673, 47839, 41627, 11664, 22358, 13277,
       40505, 15710, 35688, 47618,  4500,  6639, 27773, 31830, 22982,
       29932,  3773, 33257, 16197, 12743, 19018,   482, 27321, 38637,
       30865, 25206, 31539, 46544, 29318,    79, 34138, 11767, 44210,
        7428,  2584, 24678, 43638,  6668, 23933, 27089,  7378,  8977,
       49748, 33207, 18625, 26008, 12451, 35799, 30810,  6670, 28655,
        1304, 32883, 44198, 40987, 49356, 28784, 10450, 29546,  6515,
       24161, 40118, 41051, 26576, 32580,  2570, 13558, 22388, 39769,
       19869, 39692, 26320, 23529, 44168, 43948, 19983, 17893, 17647,
       30951, 23116, 37551, 49345, 40554, 28086, 36237, 40713, 22013,
       40386, 24923, 35339, 44571, 35933,  4858, 20856, 33085, 20680,
       32042, 10022, 44620, 22450, 37588, 13615, 12923, 44423,  2582,
       40880, 47178,  4299, 33897, 36703, 43605, 46865, 16310,  4562,
       32492, 24224,  7587, 44109,  3924, 46953,  7659, 29473,  6933,
       38333, 17593, 19341, 42753,  7198, 21529, 29283,  5076, 41378,
       21080, 49066, 12470, 20114,  4444, 43797, 24191, 44433, 28127,
       13509, 40615,  4200, 46162, 44217, 11707, 45322, 49383, 29901,
       30489, 31703, 39204, 12123,  9602, 19002, 39622, 28027, 15748,
       35567, 36593, 28220, 26168,  5149, 13217, 42735, 42448, 46928,
       23280, 23423, 31193, 37468, 21538, 24986, 40046,  5195, 39674,
        5247, 17128, 39679, 11683, 15776, 41532, 41964, 16989, 34530,
       28844, 26712, 41579, 49362,  8812,  5552,  9442, 36950, 49676,
       45866, 23139, 14336, 25912, 31394, 34968, 38234,  6583, 12721,
       44774, 21189,  3610, 11241, 21711,   425, 49870, 47589,  7074,
       21591, 32480, 34609,  1929, 12742, 19439, 17134, 32071, 42792,
       24636, 40032, 13154, 26181, 27743, 16472,   171,  9089,  4367,
       11992, 22322, 31343, 23843,  8182, 28443, 11137, 17786,  3440,
       45893, 34200, 32153, 34088, 40452,  3669, 21115,  8032, 18882,
        4730, 18477, 13639, 13637, 19916, 14640, 11342, 14918,     6,
        1793, 23138,  1936, 45709,  7951,  5151,  5506, 28518, 18242,
       27580, 31469, 35841, 12519, 27204,  7542, 13340,  9408, 47816,
       33793, 34827, 32377,  4703, 13542, 15522, 36756, 33881, 25830,
       38509, 16406, 13955, 43692, 30969, 37503, 40515, 46541, 25398,
       45710, 24016, 16530, 17682,  8178,  9802, 43476, 23315, 22676,
       35031,  4604,  1492, 48712, 14195, 23919, 42147,  4755, 16306,
       47411,   808, 15929, 42696, 39233, 13970, 12346, 38676, 19695,
       27594, 35259, 18496,  9900,  7056, 19977, 46125, 42661,  9440,
       31410, 42480, 27744, 31979, 35104, 39646, 40371, 46686, 23696,
       23692, 12945,  8616, 43607, 42345, 17571, 21940, 34463, 28348,
       44831,  5818, 38096, 49450, 42185, 48417, 41401,  7661, 33021,
        4376, 20633, 45818, 33396, 36169,  5333, 35958,  3119, 12958,
       38060, 27738, 12697, 22988, 19108, 19504, 48885, 26398, 14899,
       26541,  4776, 48323, 33042,  6976, 35168, 20318, 40305, 38766,
       48364, 27966, 37278, 40087, 17382, 30067, 14017, 27823, 44736,
       19359,  2875, 48973,   544, 44660, 22373, 49678, 45451, 26070,
       39312,  6660, 32582, 20712, 18143, 22843, 49693, 32278,  4811,
       17949, 20073, 13362, 14926, 15560,  6593, 43179,  4633, 12471,
       19220, 31035, 40487, 16165, 20029, 46589,  5226, 38016, 49736,
       46475, 13226, 15589, 42988, 16352, 17092, 32526,  8676, 41157,
        3769, 32430,  5609, 30287, 14317,  5922, 25222,  3856, 20850,
       14619]), [6, 8, 1, 2]), (array([44851, 11250, 43147, 48478,  2710, 43494, 44071,  9885, 11859,
       22133, 30037, 41178, 31893, 39244, 31615, 30425, 36096, 14263,
       15919, 45315, 12333, 23879, 26854,  1832, 10166, 29052, 43386,
       41093, 33586, 20137,  5051, 33279, 13896,  6103, 31775, 35250,
       45555, 10224, 26423, 23654, 36690, 15036, 43228, 49402, 24737,
        7602, 27276, 10282, 43473, 32413, 28219, 21389,  4104, 13513,
        5631, 39615, 31256, 21659, 46011, 13708, 14041,   378, 39451,
        5880, 26047, 32756, 11649, 28654, 48143, 28074,   946,  4193,
       43033, 40840,  6180,  2654,  8260, 34331, 16181, 41582, 30561,
       34727, 14656, 35601, 44525,   153, 23200, 16151, 29125,  2695,
        9352,  1866, 35228,  3723, 29862, 22173, 45122, 23455,  9761,
        5486, 12132, 11565, 43049, 42303, 14810,  4129,  6560, 38701,
       14447, 46251,  5625, 27135, 33703, 13778, 46591, 35634, 37288,
       31075,  4384, 42811,  3089, 10728, 41665, 23252, 27166, 15790,
       31365,  2999, 37294, 19510, 42640, 10006, 31627, 42138, 43584,
       10877, 39293,  3948, 46372,  2856, 27258,  2245, 40862, 29291,
       18705, 28002, 23814,  9260,  3433, 33215, 19528, 41305, 19516,
       32201, 33027, 35526, 39853, 34752, 28881, 42629, 36428, 10134,
       49154, 38161, 10207, 19452,  1795, 28903,  5143, 25893, 40591,
       37627, 33370, 44729, 15918, 14852, 12672, 25992,  8899, 46506,
       27455, 20192,  1212, 33740,  4057, 28755,  3116, 22536,  2418,
       33669, 43203, 28583, 24171, 17800, 27644, 33378, 48425, 45132,
       17045,  5669,  7407, 19614,   904, 32523, 17594, 47311, 14996,
       32459,  2216, 14090,  2144,  5957, 48889,  2900, 29278, 36536,
        8981, 18115, 29613,  7634, 29070, 26975, 45397,  9593, 21041,
       22765, 35954, 33408, 45189, 14060, 27631,  7531, 38124, 30734,
       10604, 48502, 46490, 39284, 19178, 37469, 24846, 19112, 44586,
       13385, 19315, 14262, 42096, 48521, 14316,  9147, 12642,  9943,
       43922, 18414, 36926, 15210, 13330,  8722, 12033,  2564, 13254,
       29561,  9371,   691, 25384, 12016, 24332, 14543, 13991,  5190,
       17455, 32504, 19830, 42585, 26859,  7253, 45128, 31934,  9300,
       46712,  7235,  3177, 38485, 19449, 27114,  9670,  3109, 33132,
       40839,  7852, 42837,  7804, 43123,  3568, 11387, 40446, 18949,
       13925, 14171, 49083, 40370,  9841,  1265,  4674, 17449,    36,
       49280, 30424, 35794, 11623, 22842, 15523, 27115, 32358, 28162,
       23197, 24364, 11836, 21501,  3875, 17821, 40365, 44852, 30772,
        5586, 37304,  9152,  1030, 29096, 38891,  6162, 18222, 10468,
       34136, 33956, 37804, 22866, 48638, 27722, 44354, 47132, 16301,
       42482,  5848,  4310,  9702, 30459, 20367, 49466, 15766, 35016,
       20194,  1098, 21103, 22110, 18329, 49563, 45897, 16120, 19276,
       11103, 36601, 44381, 10209, 43955,  7539, 45789, 11970, 48629,
       18127, 19799, 10255, 11222, 27698, 43551, 11805,  6584, 20153,
       36425,  4994, 25559,  1655, 12342,  9429, 36930,  2432, 31141,
       10506,  4346, 27021,  3807, 12750, 49140,  8330, 39921, 37939,
        6229, 24749, 19853, 22568, 22891, 48452, 46374, 20058, 14018,
        1963, 10587, 28423, 47892, 49408,  6306,  7161, 33358, 36474,
        6905,  6169, 16808, 10929, 36080, 14330, 31868, 39980, 21544,
       32982, 17047, 30960, 29139, 42357, 38992, 35124, 17578, 38066,
       48007, 13793, 28563, 12528, 12425, 21147,  2383,  7517,  5797,
       22223, 23189,  9792, 16604,  6684,  5963,  7153, 36990, 42373,
        8467, 31094,  5007, 13907, 25899, 22660,  2562,  8028, 16796,
       15321, 39158, 33976, 46286,  3709, 49078,  3190, 24652, 45319,
       44543, 14800,  6567, 14014, 44957,  1120, 42650, 10653, 15444,
        8680, 13393, 11668, 47933, 19875, 12546, 27275, 25713, 31460,
       27665,  5183, 39438, 44390, 36828, 37769, 44859, 33978, 32732,
        4108, 46503,  7519,  6135, 37204,  4739, 26877,  7357, 49234,
        6226, 19609, 39058, 44311,  5102, 14498, 24520, 46893, 23463,
       14927, 18890, 12755, 39911, 38710, 43818, 47349, 35753,  2825,
       12335,  1548, 45187,   168, 14967, 42666, 39040, 27802, 18117,
        7370, 23151, 11308, 33426, 40060, 22627, 35406, 28626, 34675,
       19063, 30923, 27994, 49230, 25596, 14593, 34233, 48806, 33838,
        4517, 14668,  2587, 28045, 49186, 37743,  6064,  6967, 18480,
       48899,  4769, 42031, 25061, 47113, 11473,  7821,  3452, 34940,
       14546, 21369, 21137, 33050, 30439, 43098, 23439, 45195, 26424,
       25459, 43918,   714, 36490, 26074, 38037, 33112, 23533, 42347,
       22762,  7390,  8696, 13849, 42171, 26344, 43646, 49392, 37689,
       17219, 23032, 22151,  4983, 44841, 36298, 44601,  6121, 31534,
       25069,  7777,  9070, 20245, 49539, 34453, 19125, 47566, 19546,
        7728, 36844, 24228, 49232,    65, 43164, 18698, 39828, 23332,
        5685, 22043,   432,  7827, 21893, 47423, 20994, 20273, 42860,
       29209, 46796, 22529, 42498, 27402, 44575, 31694, 17409, 32348,
       33643, 31209, 29986,    44, 27792,  6464, 25105, 45202,  5747,
       42292, 41710, 22818, 13994, 37461, 20162,   753, 38902, 27745,
       26532, 24385, 36388, 15577, 40472,  9765, 47001, 28538, 23564,
       18238, 17255, 16766, 39234, 36400, 17126, 48185, 39964, 32886,
       41489, 48003, 32462, 49759,   761, 26470, 31523, 24528,  6544,
        1551, 14209, 38650, 36251, 19598, 24462, 24425, 28694, 41849,
       22217, 25614, 33114,  9863, 45877, 46026, 36379, 43195, 47280,
       15425, 27447, 28081, 48681, 10157, 48840, 20844, 23601, 20222,
       37698, 33907, 16992, 12551, 24932, 41138, 31607, 14425, 37981,
       35674, 20381, 49873,  6673, 17701, 27576, 28192, 21604, 13207,
       34845,  2600, 24979, 14966, 44436, 28643, 35037, 34364, 49959,
       40484,  3505, 49075, 33742,  9781, 13317, 41230, 47288,  8346,
        3851, 49121, 13845, 48074, 48733, 41174,  3488, 15065, 25528,
       20538, 42706,  8157,  2044, 25089, 23537, 13286, 41333, 42120,
       29087,  8234, 33564, 24927,  2343, 47172, 33170, 18646, 39412,
       41667, 28667, 19932,  5717, 42168, 41084,  3245,  4493, 39901,
       11171, 32310,  1787, 17767,  6073, 16349, 28802, 28426, 20959,
       19039, 34873, 20457,   709, 14435, 30834, 26287, 15926, 10391,
       26605, 18162, 47152,  6685,   963,  6856, 14840, 38365,  2133,
        1609, 29229, 41599, 35853, 33379, 23123, 35310, 14473,  3629,
        5707,  9463, 18503, 35148, 11382, 44730, 45252, 33350,  6757,
       16215, 38546, 40134, 28497,  6236, 35834, 16616,  8019, 15326,
       37934, 23064,   630, 43019, 33448, 43545, 35653, 14127,  6069,
        7332,  9368, 38001, 29907, 24134, 14021, 47248, 31884, 19611,
        5340, 30096, 16232, 49858, 48370,  7664,  7340, 24962,   779,
       48300, 31002, 41273,  3982, 40348, 15649, 26730, 11270, 21332,
       25195, 33519,  7662, 38058, 36112, 38592, 40084, 39580,  8819,
       39269, 36293, 43680, 42217, 46053, 22780,  5926, 29354, 45360,
       17883, 37713, 32067,  2846, 25587,  1219, 44238, 12287, 39960,
        1918, 24496, 22960, 47452, 22035, 22432, 41990, 13506, 13814,
       29925,  6469, 30229, 23716, 48608, 28410, 18027,   790, 18234,
       42394, 36726, 35860, 12067,  2354, 46410, 39540,  6488, 36138,
       48790, 40244,  4634, 37488, 21987, 28862, 14459, 29126, 37615,
        8112, 11629,  7919, 15563, 13757, 36883, 16319, 43569, 39112,
       28686,    63, 26629,  1475, 46284, 33277, 35645,  3692, 32257,
       14720,  5638, 35353, 11385, 25491, 26215,  9660, 30025, 37895,
       25174, 42889, 15285, 23217, 46373,  6041, 22667, 17769, 32030,
       11030, 16693, 35157, 49573,  6467, 27470, 41562, 38705,  9992,
       14478,  1189,  4356, 49725,  4091,  8990, 42172, 38750, 11454,
       44574, 40661,  8740, 37932, 23158, 28641,  7449, 48194,  9404,
        9989]), [4, 3, 1, 2]), (array([38010, 31880, 23847, 43277, 35763, 14567, 39230, 48930, 48136,
       30383, 33530, 30645,  7547, 45927,  5404, 11939,   765,  9860,
       48788, 11721,  6400, 42151, 31757, 46303, 33933, 37879, 27609,
       18299, 40851, 40548, 34125, 43289, 22200,  5743, 39396, 10777,
       49387,  7731, 12375, 23240, 41664, 12944, 27425, 42587, 31412,
       47805,  4854, 23419, 48852,  8355,  6428, 32058, 22608, 13650,
       49559,  8660, 10796,   405, 14456, 42754,  2277, 26144, 32508,
       42878, 39739, 24764, 30324, 37181, 26704, 26488, 48908, 17107,
        3362, 46614,  9119, 47754, 20236, 30537, 29280, 29364, 38639,
       34012, 35263, 29173, 49303,  2287, 21360, 16225,  2066, 29844,
        5103,  4599, 36470, 16191,  5249, 13860, 30428, 29940, 48556,
       12328,  3197, 44616, 44884, 33542,  3842, 26391, 23021, 27000,
       28302, 37504,  8258, 43924, 43996, 27440, 34032, 30304, 23767,
       49528,  3049, 24543, 46041, 40287, 35603, 39966, 37061, 17221,
       28469,  5163,  9616,  7509,  1878,  7225, 15571, 46380, 29634,
       15304, 23352, 13457, 26656, 41410, 11114, 30959, 32107, 19445,
       38943, 43442, 11633, 10713, 37189,  4165, 42332,  4477, 24491,
        7203, 28179, 16075, 29515, 43483,  9657, 43475, 12270,  1205,
       33328, 44940, 16634, 17233, 39584, 48324, 29257, 30052, 10729,
       43368,  1178, 45304, 24844, 38387,  6132, 46872, 10146, 14782,
       28584, 44176,  7464,  5174, 35391,  4774, 15900, 34787, 17453,
        3607, 28033, 39138, 14867, 22264, 10987, 43814, 41243, 10955,
       38753, 28014, 19188, 37558, 37034, 12284, 25420, 46019, 41546,
       12052, 33662, 21639, 46369, 48554, 44303, 37102, 47332, 18591,
       42517, 17465, 20393, 33380, 42103, 28636, 44721,  2171, 18106,
       15115,  8655,  8549, 39392, 18271, 29620, 49628, 28833, 37945,
       39196, 33128, 28675,  7930,  2692, 48257,  3292, 47468, 22769,
       29927, 47324,  4552,  2683, 21288, 37062,  9435, 47018, 27671,
       23503, 22895, 45018, 26785, 32999,  9205, 24452, 42820, 32386,
       23432, 34635, 48817, 22387,  5593, 42813, 41501, 31561, 24702,
       11947, 19896, 10104, 33835, 12371, 14185, 10760, 31555, 34941,
       27805, 27141, 19889, 15968, 40783, 21780, 41875,  8683, 29796,
       28299, 43653,  6978, 32944, 32530, 39714, 47440, 44119, 49541,
       12485, 39458,    83, 33702, 26236, 10366, 40503, 49333, 16236,
       42384, 35842,  3002,  4255, 13990, 40269, 28144, 20292, 42691,
       36332, 27422, 44441, 27066,  1991, 23121, 44229, 21810,  5367,
        6575, 39225,  8269, 40205,  5729, 49185,  4548, 24796, 26845,
       47672, 38424, 29268, 27956, 19524, 48454,  6232,  5201, 44665,
       10319, 21957, 12545, 47059, 42987, 30701, 33142, 28854,  7417,
       34007,  4495, 37531, 44046, 49467,  8601, 36350, 33965, 23668,
        8768, 22840, 37676, 13252,  8960,  1538, 37216, 21059,  8954,
        3604, 14759, 21804, 43404, 14533, 38784,  5970, 10194, 25589,
       25517,  1567, 44312, 28845,  7189, 27150, 40905, 38417, 28843,
       28221, 26171,  1943, 20562, 22834, 44106, 15898, 18732, 29996,
       28550, 15602, 49565, 19012, 39147, 39748,  6436, 33989, 47768,
       36246, 18971, 10175, 12069, 28877,  2108, 29398, 26827, 36854,
       42731, 40004, 28069, 26954, 14908,  6815, 47223, 39539, 44345,
       38513,  1204, 31264, 36439, 29818, 15320, 13249, 41997, 11155,
       24226, 14734, 46667, 36487, 44085, 47485, 31973, 35949, 15403,
       15709, 45422, 20076, 19222,  1873, 33889, 16361, 35386, 48181,
       48083, 28286, 10181, 34207,  5510, 14530, 34220,  6082,   784,
       18981, 46222, 46621, 32044, 32804,  4740, 16767, 29603, 17259,
       18894, 21586,  3426, 48797, 32516, 26998, 26676, 32798, 20059,
       47038,  8511, 22650,  7113, 11267,  5760, 24867, 10555, 12573,
       25170, 33723, 40831, 24819, 14748, 12198, 43189, 28595, 13121,
       35254,  3015, 24363, 21834, 42178, 13218, 13843, 23100, 25044,
        2851, 28362, 36715, 30582, 38484, 15114, 40024, 21027, 26636,
       27905, 18518,   991,  3822, 41289, 37136, 27129,  9565, 12311,
       31147,  9209, 14302, 16272, 49163, 15740, 27871, 40537, 14061,
       26787,  4553, 48655, 11163, 41189, 45077, 22048, 23635, 13118,
       40407, 31102, 17312, 31072, 40250, 13505, 45769, 44225,  7124,
       20674, 46406, 33252,   261,  6709,  1631, 25874, 47774, 43369,
       22379,  6161, 44529, 49080, 28344, 21971, 24003,  8529, 34830,
       16410,  8216, 17580, 26471,  6851, 30135,  8090, 37264, 11242,
        4250, 41550, 37491, 34206,   772, 25209, 36594, 21280, 48269,
        5632, 18754, 38818, 14849, 24119, 23662, 44097, 12389, 33314,
       16180, 41455,  5224, 45787, 11196, 49616, 13296, 16087, 38549,
       48898,  8174, 18879, 42684,  2828,  3238, 28617, 39965, 13543,
       40358, 32311, 45859, 25306,  5805, 12964, 33910, 42402, 34063,
       16929, 24746, 24450, 41063, 13999, 48709, 34862, 43954, 48631,
       21720, 21094, 48528,  2259, 22884, 37620, 23203, 22067,  3661,
        2180, 42119, 25558,  9918, 46588, 41666, 11799, 27273, 21795,
        3922, 23379,  4528, 43006,  2280, 27130,  2545, 25881, 21407,
       13327, 18776, 25314, 11692,  1494,  7707,  2731, 11082, 44385,
         466, 35827, 42531, 41502, 16029,  5157, 36181, 29133, 42124,
        6895, 25608, 34809, 26241,  7381, 35651, 20056, 16563, 43145,
        4443, 36445,  8556,  5951, 10476, 28183,  9007, 17861, 21150,
       46598, 14518, 27607, 30840, 35291,  4648,  9400, 32488,  5627,
       15042,  3935, 34548, 46330, 14519, 35605, 14311,  4661, 19930,
       41133,  6120, 39275,  6889, 12893, 27084, 44589, 35953, 26467,
       44643, 38725, 17460, 34358, 46794, 18744, 47040, 20811,   250,
       24206, 17700, 32472, 47701, 37687, 15814, 18066, 12299, 17492,
       10630, 31138, 21985, 36591, 25567, 29188, 17471, 13766, 38878,
       27510, 22270,  3808, 12703,  2202, 22953, 31684, 48282,  4189,
       41498, 10860, 19729, 47133, 35493, 20403,  2619, 11825, 35765,
       13698, 26279, 32711, 45489, 30239, 24699, 15584, 28648,  2436,
       37560, 10406, 44351, 48232,  2784, 46563, 36494,   701, 12549,
       22974, 41143, 45681, 19157, 46642, 44797, 44327, 43826, 39729,
       34482, 39222, 36116, 20990, 32687,  9217, 36616, 27076,  5417,
       49008,  2090,  1050,   796, 28158, 15751, 48354, 35466, 27546,
       22587, 10926, 23264, 40213, 15837, 37008,  3101, 41856, 29636,
        9519, 13643, 21224, 35824, 15789, 37613,  3608, 36658, 29243,
       39308, 40941, 32151,  6499, 27750, 37485, 38989,  4109, 29079,
       33677, 35862, 34789, 44203, 16757, 13626,   778, 32951, 15910,
       15369,  4122, 37127, 29812, 44732, 13418, 31550, 47958,  8340,
       49666,  2719, 36009, 48589, 26449, 22215,   724, 30470, 26757,
       36377, 10866, 13435, 30396, 41454, 16312, 29682, 14715,  1675,
       19296, 34254, 29658, 10898, 12819, 10885,  4588, 39360, 36410,
       42765, 46795, 30377, 13727, 47171, 48205, 34268, 18464, 49248,
       39110,  7151, 34925, 30572, 19453, 11713, 18612, 10160,  7336,
       31454,  9712, 12582, 31687, 17902, 34508,  8173, 35785,  9420,
       36130, 18733, 31057, 40917, 46389, 25989, 36651, 18248,  7688,
       29470, 28291, 47853, 48270, 29389, 26976, 11194, 25030, 48598,
       47492, 49224, 24978, 48365, 39693, 38565, 45335, 30680, 47481,
       14601, 48745, 19806, 21630, 15847, 14174, 43106, 46088,    55,
       28031, 19130, 31707, 29269, 19435, 11197, 10152, 14088, 17850,
       27121,  9876,  5012, 49093,  2993, 29005, 31492, 43285,  8482,
       37121, 38389, 14135, 49329, 46870, 13524, 15922, 20061, 18385,
       39055, 44652, 42328, 24977, 25296, 39881, 12044, 41422,  8726,
       30746, 27326, 33457,  5576,  3259,  9933, 41512,  9448, 38129,
       37797,   646, 21668, 34685, 45367,   885,  3845, 15297, 16906,
       17117]), [0, 5, 1, 2])]
Competition
DC 0, val_set_size=1000, COIs=[7, 9, 1, 2], M=tensor([7, 9, 1, 2], device='cuda:0'), Initial Performance: (0.25, 0.04502433729171753)
DC 1, val_set_size=1000, COIs=[6, 8, 1, 2], M=tensor([6, 8, 1, 2], device='cuda:0'), Initial Performance: (0.26, 0.044506120443344115)
DC 2, val_set_size=1000, COIs=[4, 3, 1, 2], M=tensor([4, 3, 1, 2], device='cuda:0'), Initial Performance: (0.249, 0.04467034757137298)
DC 3, val_set_size=1000, COIs=[0, 5, 1, 2], M=tensor([0, 5, 1, 2], device='cuda:0'), Initial Performance: (0.248, 0.04460585105419159)
D00: 1000 samples from classes {1, 2}
D01: 1000 samples from classes {1, 2}
D02: 1000 samples from classes {1, 2}
D03: 1000 samples from classes {1, 2}
D04: 1000 samples from classes {1, 2}
D05: 1000 samples from classes {1, 2}
D06: 1000 samples from classes {9, 7}
D07: 1000 samples from classes {9, 7}
D08: 1000 samples from classes {9, 7}
D09: 1000 samples from classes {9, 7}
D010: 1000 samples from classes {9, 7}
D011: 1000 samples from classes {9, 7}
D012: 1000 samples from classes {8, 6}
D013: 1000 samples from classes {8, 6}
D014: 1000 samples from classes {8, 6}
D015: 1000 samples from classes {8, 6}
D016: 1000 samples from classes {8, 6}
D017: 1000 samples from classes {8, 6}
D018: 1000 samples from classes {3, 4}
D019: 1000 samples from classes {3, 4}
D020: 1000 samples from classes {3, 4}
D021: 1000 samples from classes {3, 4}
D022: 1000 samples from classes {3, 4}
D023: 1000 samples from classes {3, 4}
D024: 1000 samples from classes {0, 5}
D025: 1000 samples from classes {0, 5}
D026: 1000 samples from classes {0, 5}
D027: 1000 samples from classes {0, 5}
D028: 1000 samples from classes {0, 5}
D029: 1000 samples from classes {0, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.295, 0.06421255218982697) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.06314386874437332) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.279, 0.0991387677192688) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.08198602849245071) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.07024952399730683) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.0713477740585804) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.1353405278623104) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.08995045691728593) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.43, 0.0827924791276455) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.07941373744606972) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.1567333093881607) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.112021663159132) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1102066800892353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.09507753160595894) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.384, 0.17660580858588218) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.1585041108801961) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.1454920856282115) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.1163012564405799) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.392, 0.18973852957785128) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.20079846078529953) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.14613931557536125) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.129363997079432) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.20482386314868928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.22117121162824332) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.1731434683352709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.13845351364091038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.19699945250153542) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2411895220540464) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.17792725160717965) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.16328691100515424) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2086139908656478) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.25990544859599324) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.19964710243418812) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.15808263261057437) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.22641488587111236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.2669675034582615) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.20615564992837607) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.16297005883976817) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.20922902297973633) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.30454038487095386) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO3', '(DO1']
DC 1 --> ['(DO4', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.21504319499060512) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.16668130118772387) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.22777038579434156) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.30058905540267006) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.23819387503713368) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.18574686835519968) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.23755254044383764) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3005353470011614) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.231616054398939) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.18900290916860105) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.2480324033126235) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.2878437988292426) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.24105826675891875) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.19227273540850728) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.26114925211668016) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3167528252019547) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.484, 0.25961474182270466) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.20606490743532777) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.2744803967550397) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.30919506670720875) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.2295115878302604) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.20219349223561586) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.28791689027845857) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.33922992371837607) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.23950351572223008) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.18761784301046283) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.411, 0.28727550749480724) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.34689719696191607) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.481, 0.21812520629167556) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.19892396579124033) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.29907329703122376) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3599770708133001) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.21600804610922933) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.2089225241104141) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.31196764719486236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.4051909456863068) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.486, 0.224650993777439) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.18549195282021538) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.30127439960837366) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.334181351631647) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.1981687380373478) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.19292567820195108) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.3237449593320489) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.388602845916932) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.18727620966918768) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.19991459288774058) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.30502661506086587) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.34852597409498415) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.2193510370068252) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.19866840641503222) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.31539916931837797) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3837817528896267) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.19561979901976884) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.1815408014860004) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.3166354093104601) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.3569843997600838) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.22520013396441937) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.18933413153002038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.3057720410823822) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.39327847629593454) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.20857454464584588) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.1770054064504802) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.32103820266202093) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3741472419258789) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.21274436179175973) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.17545939598046242) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.34354274152219294) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.34880097209464295) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.481, 0.20991024845745415) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.1935583802147303) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.3272699436247349) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3423626564705337) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.2089577343016863) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.21736527660524008) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.33207464952394367) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3805056024626829) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.1756691234856844) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.18128655183152295) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.32331811099871993) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.29513295338395984) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO0', '(DO3']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.486, 0.18969448187947274) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.18827713692979886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.3342276889383793) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.34646871376771016) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.489, 0.19933858058601617) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.17831027152691967) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.309223949611187) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3414383791132132) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.1809608379639685) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.2091223636092618) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.338823700979352) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3596393416970968) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.1817740087080747) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.20199360027187505) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.3073899662718177) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.3861017097304575) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.23254956147167832) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.19786998290923657) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.32892131588608026) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.4210888293838361) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO4', '(DO2']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.17563141816668212) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.20132894361321815) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.3188931811377406) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.32534522757376544) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.1903581324107945) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.1945237827206729) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.305516765858978) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.4063681346332887) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.486, 0.1842534149028361) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.21877453327737748) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.3225031328201294) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.362091146341525) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.17420231490209698) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.20674353157763836) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.35515891468524935) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.37706427461188285) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.20766989109665154) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.20122167175507638) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.30570713701471686) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.40875016451533885) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO5', '(DO4']
DC 1 --> ['(DO1', '(DO0']
DC 2 --> ['(DO3']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.486, 0.21991205832362176) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.25866649427459926) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.28421070058643816) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.37278627350693566) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.21224037080071867) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.22116566362499726) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.2853248918056488) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.41063837724458424) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.481, 0.20927038378454746) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.24743609619996276) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.2859864304959774) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.3677151844451437) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.21049237310513855) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.22137043787472066) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.30172615971416233) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.43400563939253334) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.17115131010115148) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.23289127311973426) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.29221583080291746) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3312948877053568) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO2']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.18839908193796873) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.23475673725399246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.2938560468479991) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.35266712110687515) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.489, 0.18632905080355705) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.24786801558404112) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.32010297233611346) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.371438609671779) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.485, 0.1897370993383229) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.24502220366892288) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.29234746442735193) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.3535156297534704) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.20077261244505643) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.23583061883113987) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.3147590785585344) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3686245207383763) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.17163407515734433) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.2322876978488639) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.29927918428182604) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.39757824506564066) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.25, 0.04502433729171753), (0.295, 0.06421255218982697), (0.447, 0.07024952399730683), (0.43, 0.0827924791276455), (0.458, 0.1102066800892353), (0.448, 0.1454920856282115), (0.472, 0.14613931557536125), (0.477, 0.1731434683352709), (0.47, 0.17792725160717965), (0.477, 0.19964710243418812), (0.477, 0.20615564992837607), (0.482, 0.21504319499060512), (0.482, 0.23819387503713368), (0.48, 0.231616054398939), (0.483, 0.24105826675891875), (0.484, 0.25961474182270466), (0.48, 0.2295115878302604), (0.473, 0.23950351572223008), (0.481, 0.21812520629167556), (0.475, 0.21600804610922933), (0.486, 0.224650993777439), (0.483, 0.1981687380373478), (0.478, 0.18727620966918768), (0.48, 0.2193510370068252), (0.487, 0.19561979901976884), (0.474, 0.22520013396441937), (0.483, 0.20857454464584588), (0.487, 0.21274436179175973), (0.481, 0.20991024845745415), (0.487, 0.2089577343016863), (0.483, 0.1756691234856844), (0.486, 0.18969448187947274), (0.489, 0.19933858058601617), (0.482, 0.1809608379639685), (0.483, 0.1817740087080747), (0.48, 0.23254956147167832), (0.483, 0.17563141816668212), (0.482, 0.1903581324107945), (0.486, 0.1842534149028361), (0.48, 0.17420231490209698), (0.482, 0.20766989109665154), (0.486, 0.21991205832362176), (0.483, 0.21224037080071867), (0.481, 0.20927038378454746), (0.488, 0.21049237310513855), (0.482, 0.17115131010115148), (0.487, 0.18839908193796873), (0.489, 0.18632905080355705), (0.485, 0.1897370993383229), (0.488, 0.20077261244505643), (0.487, 0.17163407515734433)]
TEST: 
[(0.25, 0.04384468254446983), (0.3, 0.06179358550906181), (0.45025, 0.0672670782506466), (0.438, 0.07863861978054047), (0.46075, 0.10416459986567497), (0.4575, 0.13624865448474885), (0.471, 0.13515578013658525), (0.473, 0.1617032497525215), (0.46975, 0.16537044155597685), (0.477, 0.18476930224895477), (0.475, 0.19292769503593446), (0.47625, 0.20240240603685378), (0.47425, 0.22463396966457366), (0.47225, 0.22033803808689117), (0.478, 0.22810182809829713), (0.47675, 0.24103938710689546), (0.47475, 0.2184511885046959), (0.47125, 0.22510431438684464), (0.476, 0.20613496840000153), (0.4765, 0.2041435177922249), (0.47725, 0.20994991832971574), (0.4775, 0.18819649702310562), (0.47675, 0.17770342618227006), (0.47475, 0.20595939362049104), (0.47775, 0.18313016188144685), (0.47175, 0.21280472731590272), (0.47675, 0.19665959751605988), (0.47775, 0.19296522498130797), (0.47775, 0.19316708093881607), (0.47925, 0.18762642079591751), (0.47725, 0.1624404672384262), (0.4795, 0.1665507901906967), (0.48, 0.18237674576044083), (0.47925, 0.16556453466415405), (0.47725, 0.16720762473344802), (0.479, 0.2104753143787384), (0.47825, 0.16080162853002547), (0.477, 0.17814934462308885), (0.478, 0.16767567199468614), (0.47475, 0.16236231660842895), (0.47875, 0.187466546356678), (0.47775, 0.2053444291949272), (0.478, 0.18994769418239593), (0.4785, 0.19372022640705108), (0.47875, 0.19409679287672044), (0.479, 0.16079177814722062), (0.47875, 0.17222134470939637), (0.479, 0.17077467942237853), (0.47825, 0.17453925532102585), (0.4785, 0.18486699306964874), (0.47625, 0.16038857239484788)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.00      0.00      0.00      1000
           7       0.49      0.97      0.65      1000
           9       0.47      0.93      0.62      1000

    accuracy                           0.48      4000
   macro avg       0.24      0.48      0.32      4000
weighted avg       0.24      0.48      0.32      4000

Competition_DC_1
VAL: 
[(0.26, 0.044506120443344115), (0.378, 0.06314386874437332), (0.453, 0.0713477740585804), (0.447, 0.07941373744606972), (0.478, 0.09507753160595894), (0.479, 0.1163012564405799), (0.487, 0.129363997079432), (0.485, 0.13845351364091038), (0.486, 0.16328691100515424), (0.489, 0.15808263261057437), (0.483, 0.16297005883976817), (0.487, 0.16668130118772387), (0.487, 0.18574686835519968), (0.488, 0.18900290916860105), (0.489, 0.19227273540850728), (0.487, 0.20606490743532777), (0.485, 0.20219349223561586), (0.488, 0.18761784301046283), (0.49, 0.19892396579124033), (0.488, 0.2089225241104141), (0.49, 0.18549195282021538), (0.49, 0.19292567820195108), (0.488, 0.19991459288774058), (0.488, 0.19866840641503222), (0.486, 0.1815408014860004), (0.488, 0.18933413153002038), (0.49, 0.1770054064504802), (0.489, 0.17545939598046242), (0.487, 0.1935583802147303), (0.49, 0.21736527660524008), (0.487, 0.18128655183152295), (0.488, 0.18827713692979886), (0.483, 0.17831027152691967), (0.487, 0.2091223636092618), (0.486, 0.20199360027187505), (0.483, 0.19786998290923657), (0.485, 0.20132894361321815), (0.484, 0.1945237827206729), (0.488, 0.21877453327737748), (0.486, 0.20674353157763836), (0.486, 0.20122167175507638), (0.485, 0.25866649427459926), (0.485, 0.22116566362499726), (0.489, 0.24743609619996276), (0.486, 0.22137043787472066), (0.487, 0.23289127311973426), (0.491, 0.23475673725399246), (0.489, 0.24786801558404112), (0.485, 0.24502220366892288), (0.485, 0.23583061883113987), (0.488, 0.2322876978488639)]
TEST: 
[(0.2605, 0.04348890659213066), (0.38275, 0.06072431656718254), (0.45925, 0.06845111122727394), (0.451, 0.07635300573706627), (0.479, 0.09213218602538109), (0.482, 0.1134857217669487), (0.4865, 0.12708065676689148), (0.48525, 0.1371110547184944), (0.48575, 0.1597578677535057), (0.4875, 0.15451923394203185), (0.48675, 0.16020039916038514), (0.4865, 0.16195098465681076), (0.4875, 0.18211392134428026), (0.48925, 0.18490129190683366), (0.48775, 0.1881224390268326), (0.488, 0.2049037985801697), (0.4855, 0.20159202152490616), (0.487, 0.18519271796941758), (0.48775, 0.19561366426944732), (0.4875, 0.20529264402389527), (0.48875, 0.18272214019298552), (0.48725, 0.18955497801303864), (0.4875, 0.1986255949139595), (0.4865, 0.19636040449142456), (0.4865, 0.18111901718378068), (0.48425, 0.18909199273586272), (0.487, 0.17642158484458922), (0.48375, 0.17358618676662446), (0.48575, 0.1921437832713127), (0.4865, 0.2149646154642105), (0.48525, 0.17873469692468644), (0.48625, 0.1842292965054512), (0.483, 0.17271663665771483), (0.485, 0.20731262063980102), (0.48475, 0.19898548996448517), (0.4835, 0.19010673171281814), (0.486, 0.1963182189464569), (0.48575, 0.1915645043849945), (0.48575, 0.21280736643075943), (0.48275, 0.2035612404346466), (0.4835, 0.19609102588891983), (0.48525, 0.25982467567920686), (0.482, 0.21866505235433578), (0.4835, 0.24641787242889404), (0.48175, 0.21889843624830246), (0.47925, 0.22626048702001572), (0.48375, 0.23153397405147552), (0.48525, 0.2418135539293289), (0.48225, 0.23695719814300537), (0.48425, 0.22835021138191222), (0.4845, 0.22516018384695052)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.00      0.00      0.00      1000
           6       0.44      0.99      0.61      1000
           8       0.54      0.95      0.69      1000

    accuracy                           0.48      4000
   macro avg       0.25      0.48      0.32      4000
weighted avg       0.25      0.48      0.32      4000

Competition_DC_2
VAL: 
[(0.249, 0.04467034757137298), (0.279, 0.0991387677192688), (0.306, 0.1353405278623104), (0.354, 0.1567333093881607), (0.384, 0.17660580858588218), (0.392, 0.18973852957785128), (0.403, 0.20482386314868928), (0.403, 0.19699945250153542), (0.404, 0.2086139908656478), (0.422, 0.22641488587111236), (0.416, 0.20922902297973633), (0.422, 0.22777038579434156), (0.426, 0.23755254044383764), (0.427, 0.2480324033126235), (0.43, 0.26114925211668016), (0.421, 0.2744803967550397), (0.419, 0.28791689027845857), (0.411, 0.28727550749480724), (0.425, 0.29907329703122376), (0.418, 0.31196764719486236), (0.425, 0.30127439960837366), (0.419, 0.3237449593320489), (0.431, 0.30502661506086587), (0.42, 0.31539916931837797), (0.422, 0.3166354093104601), (0.422, 0.3057720410823822), (0.433, 0.32103820266202093), (0.425, 0.34354274152219294), (0.421, 0.3272699436247349), (0.426, 0.33207464952394367), (0.419, 0.32331811099871993), (0.43, 0.3342276889383793), (0.426, 0.309223949611187), (0.432, 0.338823700979352), (0.432, 0.3073899662718177), (0.43, 0.32892131588608026), (0.427, 0.3188931811377406), (0.426, 0.305516765858978), (0.427, 0.3225031328201294), (0.422, 0.35515891468524935), (0.426, 0.30570713701471686), (0.43, 0.28421070058643816), (0.419, 0.2853248918056488), (0.423, 0.2859864304959774), (0.427, 0.30172615971416233), (0.423, 0.29221583080291746), (0.424, 0.2938560468479991), (0.428, 0.32010297233611346), (0.433, 0.29234746442735193), (0.431, 0.3147590785585344), (0.429, 0.29927918428182604)]
TEST: 
[(0.2515, 0.043640657246112824), (0.267, 0.09514398372173309), (0.29675, 0.12978019261360169), (0.3455, 0.14937431919574737), (0.3785, 0.16722273707389831), (0.389, 0.17958619666099548), (0.39425, 0.19564056634902954), (0.3885, 0.18772674351930618), (0.39475, 0.1980825604200363), (0.409, 0.21704438626766204), (0.412, 0.19847762775421143), (0.406, 0.21764973628520964), (0.41525, 0.22924898493289947), (0.41425, 0.23498116886615752), (0.41625, 0.24938807940483093), (0.4175, 0.2621987471580505), (0.414, 0.2733523950576782), (0.41325, 0.2740579960346222), (0.4155, 0.2845363744497299), (0.4135, 0.29838720726966855), (0.419, 0.289054198384285), (0.41625, 0.305693461894989), (0.41725, 0.2890230323076248), (0.40725, 0.2984208322763443), (0.4135, 0.29992014622688296), (0.41725, 0.2960949376821518), (0.41725, 0.3067004305124283), (0.41875, 0.3342849631309509), (0.41425, 0.310145103931427), (0.415, 0.31638928973674774), (0.4135, 0.30592397940158844), (0.41375, 0.31828232944011686), (0.416, 0.2920356104373932), (0.41925, 0.32142182779312134), (0.41625, 0.2952197402715683), (0.41775, 0.31665398478507994), (0.41875, 0.31012533390522), (0.418, 0.28789631509780883), (0.41725, 0.3059523937702179), (0.41525, 0.3350169266462326), (0.41825, 0.2914040458202362), (0.418, 0.26948527443408965), (0.41875, 0.2720869052410126), (0.41775, 0.2682575665712357), (0.4175, 0.2911479151248932), (0.417, 0.2824061659574509), (0.4185, 0.28092744982242585), (0.41525, 0.2988461264371872), (0.4145, 0.2829821512699127), (0.416, 0.29850256848335266), (0.41525, 0.28574519288539885)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.00      0.00      0.00      1000
           3       0.44      0.77      0.56      1000
           4       0.40      0.90      0.55      1000

    accuracy                           0.42      4000
   macro avg       0.21      0.42      0.28      4000
weighted avg       0.21      0.42      0.28      4000

Competition_DC_3
VAL: 
[(0.248, 0.04460585105419159), (0.388, 0.08198602849245071), (0.468, 0.08995045691728593), (0.465, 0.112021663159132), (0.476, 0.1585041108801961), (0.46, 0.20079846078529953), (0.457, 0.22117121162824332), (0.463, 0.2411895220540464), (0.468, 0.25990544859599324), (0.472, 0.2669675034582615), (0.467, 0.30454038487095386), (0.474, 0.30058905540267006), (0.475, 0.3005353470011614), (0.476, 0.2878437988292426), (0.472, 0.3167528252019547), (0.475, 0.30919506670720875), (0.474, 0.33922992371837607), (0.477, 0.34689719696191607), (0.475, 0.3599770708133001), (0.478, 0.4051909456863068), (0.475, 0.334181351631647), (0.477, 0.388602845916932), (0.473, 0.34852597409498415), (0.473, 0.3837817528896267), (0.478, 0.3569843997600838), (0.475, 0.39327847629593454), (0.475, 0.3741472419258789), (0.474, 0.34880097209464295), (0.473, 0.3423626564705337), (0.475, 0.3805056024626829), (0.474, 0.29513295338395984), (0.477, 0.34646871376771016), (0.475, 0.3414383791132132), (0.477, 0.3596393416970968), (0.474, 0.3861017097304575), (0.479, 0.4210888293838361), (0.473, 0.32534522757376544), (0.473, 0.4063681346332887), (0.475, 0.362091146341525), (0.477, 0.37706427461188285), (0.476, 0.40875016451533885), (0.476, 0.37278627350693566), (0.475, 0.41063837724458424), (0.474, 0.3677151844451437), (0.477, 0.43400563939253334), (0.471, 0.3312948877053568), (0.478, 0.35266712110687515), (0.474, 0.371438609671779), (0.476, 0.3535156297534704), (0.477, 0.3686245207383763), (0.475, 0.39757824506564066)]
TEST: 
[(0.23875, 0.043380969196558), (0.3875, 0.07882354038953782), (0.4575, 0.0859538711309433), (0.45425, 0.10616131201386451), (0.46675, 0.1488635730743408), (0.45475, 0.19176117146015167), (0.45075, 0.21221389746665956), (0.46, 0.22604575574398042), (0.46225, 0.24731591022014618), (0.46675, 0.25240520119667054), (0.4685, 0.2846794567108154), (0.467, 0.28415266847610476), (0.468, 0.2828246911764145), (0.4695, 0.26910403490066526), (0.4665, 0.2975504261255264), (0.47175, 0.2834343534708023), (0.46925, 0.3156021145582199), (0.47125, 0.3237291921377182), (0.47025, 0.33702639532089235), (0.4715, 0.3686845146417618), (0.469, 0.3113858606815338), (0.4745, 0.35924294352531433), (0.47, 0.3255378580093384), (0.46625, 0.36346805119514464), (0.46975, 0.33161842000484465), (0.4715, 0.3583427724838257), (0.47075, 0.34563383853435514), (0.473, 0.32975980126857757), (0.46975, 0.32031056833267213), (0.47375, 0.3461876404285431), (0.4715, 0.27493783819675444), (0.472, 0.3174939706325531), (0.47225, 0.31983426654338837), (0.473, 0.33319387459754946), (0.4725, 0.3541035957336426), (0.47225, 0.3835439656972885), (0.47225, 0.30407510411739347), (0.47, 0.3818195651769638), (0.4725, 0.3398138995170593), (0.47225, 0.3492579873800278), (0.47325, 0.3797805442810059), (0.47125, 0.3495849779844284), (0.47225, 0.38616932165622714), (0.473, 0.34451068615913394), (0.47375, 0.4050676525831223), (0.467, 0.3120420379638672), (0.473, 0.32881353747844694), (0.4725, 0.34838946640491486), (0.4725, 0.33319389808177946), (0.47275, 0.34545863926410675), (0.4745, 0.3704878097772598)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.55      0.94      0.69      1000
           1       0.00      0.00      0.00      1000
           2       0.00      0.00      0.00      1000
           5       0.42      0.96      0.59      1000

    accuracy                           0.47      4000
   macro avg       0.24      0.47      0.32      4000
weighted avg       0.24      0.47      0.32      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [46]
name: no-alliance-46
score_metric: contrloss
aggregation: <function fed_avg at 0x733763e7cc10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=46
Partitioning data
[[0, 5, 1, 6], [8, 3, 1, 6], [4, 7, 1, 6], [2, 9, 1, 6]]
[(array([41745, 36201, 46585,  2513, 33328, 10243, 10637, 36506, 39196,
        5426, 33363, 17115, 40873,  6858, 31004, 43504, 13419, 38618,
       35855, 20383, 35417,  1214, 20045, 49799, 26565, 31391, 28380,
       36491,  9171, 37841, 22680, 42044, 48869, 35529, 15439,  3295,
        6015, 39619, 39286, 17717, 47865, 49525,  1913,  8072, 44866,
        2707, 32661, 21633, 33950,  5888, 44968, 43828, 29444,  3184,
       44074, 11762, 18451,  9124,  1147, 32290, 33438, 22264, 46287,
         695, 12355, 26049,  9905, 21044, 32369, 12659, 45288, 12553,
       19082, 19106,  2355, 31654, 20862, 41606, 30749, 27434, 33647,
       40761, 39471,  1470, 24383,  5557, 39995, 44430,  3644, 42204,
       44849, 41068,   600, 32839, 29634, 42566, 47645, 10578, 31339,
       30709, 42517, 39793, 32233, 27301, 47215,  1950, 40018, 47969,
        5984,  5765, 12056, 14520, 14025, 34567, 29219, 24695, 25191,
       29286, 41751, 11356, 21143, 24507, 31048,  7972, 39691,  9430,
        1988, 35934, 12325,  3197, 12419,  1329,  4176, 19859, 24101,
       30811, 19634,  7511, 17166,  4165, 35626, 44116, 22320, 34040,
       28865, 30233, 23449, 19332, 12328,   189, 46763,  8062, 39589,
       14666, 14567, 33516, 26018, 32931, 44618, 35414, 16253, 44666,
       15546, 23377,  7251,  2413, 21248, 24351, 41200, 24931,  5371,
       10686,  7971, 29620, 43012, 30240, 18768, 22034, 17107, 20452,
       18572, 17316,  2885, 10146,   457, 43542, 27415, 34783, 14213,
       41238,  6399, 32662, 41821, 47617, 26723, 39329, 49717, 21272,
       35422, 14023, 48378,  7095, 28487,  1338, 18782, 45423,  3136,
       38478,  7581, 36545,  1044, 49507, 33502, 40649, 11913, 22286,
       36052, 46706, 16559, 13457, 19409, 49263, 29104,  5414, 14069,
       25160, 12492,   199, 12704, 49868, 14210, 13894, 35570, 43639,
       48671, 29977, 29518, 33934,  6090,  4524, 20349,  8136, 44476,
       28092,   866, 47379, 17959, 34381, 43378, 18203, 15713, 40957,
        2013, 16199,   984, 38507, 45181, 19437, 33634, 27472, 16949,
       24831, 30356, 30162,  4467,  2884, 46441,  2182, 48890, 39463,
       22941, 11725, 14992, 35543, 18016, 42494, 48135, 36139, 42218,
        1519, 11143, 46112, 48110, 43994, 21977, 12371, 36692, 35534,
       33295, 28283, 15229, 25603, 11861, 43273, 47038, 20732, 23185,
       34074, 13257,  4456, 22879, 48395,    81, 30965, 20043, 26138,
        3021, 37375, 23090, 18212, 44832, 16185, 22681, 32864, 20809,
        6794,  1224, 29037, 42491, 41915, 19024, 14929, 46318, 35194,
       28484, 22434,  7425,  8039, 38529, 23238, 31551, 22445, 27219,
       33477, 13073, 41942, 19933, 32255, 18280,  9383, 12300, 35166,
       40456, 31243,  5645, 11735, 22003, 32926, 10835, 37531,  5208,
       43535, 26589, 30308, 31109, 36235, 22731, 16431, 34884, 43654,
        3069, 36057, 25840, 41886, 19786,  7115,  1157,  3810, 16979,
       14841, 26277, 45142, 36912, 31631,  7729, 44033, 38792, 33067,
       40887, 30188, 34634,  7562, 11077, 40889, 37894,  6058,  1199,
       46525, 30189, 30063, 12682, 25699, 17708, 28999, 36335,  5850,
       13461, 33965, 23437, 16496, 18096, 23556, 29358, 46201, 42323,
       13680, 15378,  5448, 34112, 11274, 30847, 16835, 18189, 46606,
       11004, 25925, 27359, 39456, 24741, 45915, 12832, 44089, 49715,
       11491,  5369, 29804, 27397, 44561, 44687, 24867,  3660, 44740,
       14001, 28505, 38784, 28425, 30126, 21839, 12089,  4319,  3017,
        3005,   999, 30662, 36769,  1280, 43115, 15207,  3420, 48910,
       22671, 35394, 10541, 45581, 42538,  5233, 22483, 33927,  1942,
        2916, 23249, 23544, 30523, 33504, 35604, 35822, 49431, 15177,
       42613, 37341,  3022, 13252, 35390, 27345, 11441, 21075, 35602,
       34048, 22895,  4355,  4968, 40871, 26680,  3060, 17165, 11043,
       31901,  4694,  5698, 13604,   535, 46083, 35748, 18894, 36120,
        8902, 47773,  1998, 47720, 21586, 34493, 23895, 46674, 17384,
       27731, 39792, 12594,  8556, 10571, 37491, 20057, 20366,  5311,
       37799, 11482, 23714, 48021, 16197,  5683,  7683,  6656, 16412,
       43372,  7649, 14096, 39824,  5236,  5301, 35041,  1305, 30486,
       38257, 30912, 49435,  1380, 48496, 29252, 48612, 16250, 47040,
       31776, 35174, 10790,  8149, 35399, 47310, 14448,   547,  9459,
       40505, 11336,   257,  4250,  4533, 45923, 11367, 33686,  7005,
       17606,  7912, 43976, 15386, 22859,  1651, 11515, 15195, 22090,
       42709, 47256,  9766, 48263,  4528,  7358, 41378, 37847, 44269,
       38911, 29792, 38078, 26344,  5663, 28906, 38744,  5149,  6161,
        5099, 18583, 37007, 35803, 47030, 46140, 31534,   855, 11130,
        9232, 26740, 30752, 35847, 23423, 45500, 16077, 45377, 43089,
       31300, 38795, 18388, 47288,  8307,  1064, 16810, 20856, 15662,
       10964, 34906, 18902,   893, 40172, 41519, 31737, 28352, 20616,
        3346, 25799, 49469, 34595, 36400, 41607, 13763, 14643, 21149,
        3661, 43618, 48275, 28027, 45714, 20304, 49973,  1694, 25750,
       19363, 18754, 23142, 11716, 14201, 32913, 26614, 16598, 41579,
       24220, 11683, 22957, 47237,  5392, 26450, 25196, 32937, 30182,
        4945, 40060,  6420,  3808, 21152,  5197, 24566, 15306, 49509,
       14546, 15710, 14040,  8449,    96, 48791, 42143, 22943,   311,
       20155,  2756, 17701, 25582, 45769, 12766,  6244,  7378, 20376,
       25323, 42710,  3444, 43251,  9878, 47900, 24463, 31151, 28848,
        8848, 35322, 45488, 39800, 43416, 31619,  3886, 34809, 36879,
       19181, 11971, 25135, 36838, 25652, 34080,  6967, 45084, 43609,
         282, 19616, 23198, 28580, 28780, 23338, 12875,  7324,   879,
       20074, 43186, 40427, 23407, 37092,  9080, 37779, 28790,  9480,
       35884, 11767, 25208, 12169, 27769, 18932, 42666, 28784, 25874,
       26433,  1631, 46135, 37235, 46448, 42121,  7659, 49777, 28441,
       35757, 41502, 30088,  2475, 44011, 21216,  9953, 49129, 33738,
       42509, 33178, 38769, 13168, 35830,  9727, 26492,  2439, 45944,
       43912, 49279, 37714, 20527, 26232, 36089,  3686, 26618, 34286,
       33733,  4208, 22550, 12302,  4875, 49952, 18604, 36720, 30812,
       48361,  3942, 45889,  3986, 10649, 43166, 24671,  3521, 14443,
       47472, 38475,  3861, 18974, 49550, 37045, 10554,  7393,  2729,
       22066, 24431, 15664, 49046,  7277, 12367, 19508,  3098, 26333,
       47649, 15726, 29973, 39939, 35370, 21917, 29381, 35950, 15771,
       10939, 42762, 16670, 10176,  9360, 22356, 17776, 15291, 11459,
       10635, 25373, 41160,  3403, 29391,  7553, 34662, 34056,  9877,
        5148, 25680, 24143, 22525,   770, 20666, 32359, 21644, 33123,
       31639, 42404, 17317, 17417, 21717, 41297,  7976, 43250,  6599,
        8069, 49928, 10364, 13029, 12796,  4126,  3095,    19,  8956,
        3437, 34684, 48947, 22295, 47465, 40038, 30727, 39203, 19588,
       22631, 14024, 22428, 38230,   807, 30536, 33623,  7675, 22786,
       24918, 40436, 20377,   249, 29217, 42742, 44194, 21497, 24888,
       49516,  2411, 36564,  5680,  8293, 36290, 35571,  2777,  6267,
       25382, 36360, 37048, 18978, 19763, 24155, 25622, 43027, 30212,
       37261, 42049,  2684,  7650, 35170, 35458, 23366, 43157, 32125,
       45622, 13127, 22236, 13479,  4810, 24531, 35745,   235, 35312,
       15193, 46991, 48650, 11101, 28421, 29360, 34139, 20022,  2924,
       40313, 14916, 43298, 23005, 30953, 45019,  4279, 13988, 47331,
        2690, 15823, 14845, 16639, 24354, 32856, 23539, 18682, 37134,
        7918, 29627, 31279, 10374, 18029, 28492,  3341, 15612, 17723,
        4550, 36304, 31118, 32812, 17697,  3739, 38290, 13866, 49804,
       18338,  3683, 41448, 38662, 38397, 12066, 10600, 39205, 27464,
       25234, 18270, 19268, 16218, 31128, 38845,  8256, 40498, 33627,
        3401, 38336, 13596, 27928, 40613,  6832, 38026,  6517, 43414,
       37694]), [0, 5, 1, 6]), (array([10313,  1060, 22492, 47609, 49025, 35505,  9527, 40702, 34894,
       48611, 49501,  2942, 22714, 24062, 26623,  1325,  3258,  4313,
       47376, 39971, 47430, 14149, 29933, 12249, 31714, 27643, 29362,
       25412,  3123, 23399, 31917, 31183, 39299, 17010, 17170,  4283,
       18395, 14166, 20617, 41335, 12907, 43623, 39605, 20259, 49388,
       24260, 18950, 40048,  3240, 11378, 47181, 10369, 12866, 31370,
        5546,  1239, 36295, 39433, 32671, 30510, 42917, 11248, 30433,
       34296, 47793, 21451, 45095, 35766, 34369, 32714, 35730, 22776,
       49712,  6717, 45461, 33954,   723, 27692, 34892, 23497, 37002,
         580, 13929, 40578, 42162, 37036, 41247, 41413, 15164, 20909,
       18081, 42898, 23960,  9427, 29075, 39243, 47212, 25513, 10096,
       32619,  7610, 41078, 44659, 25163, 29954, 40470, 37959, 24328,
       46223, 16942, 41727, 23757,  5280, 15022, 13386, 11063, 21050,
        3319, 38315, 45398,  9922, 30997, 10810,  4204, 29437, 35416,
       46028, 11965, 49379, 25691, 48168, 42713, 40785, 16978, 39335,
       48124, 37722, 29868, 28230, 24526, 13504, 40881, 10856, 29895,
       12207, 13859, 30443, 11674,  1141, 11352, 46543, 47561,  3777,
       36299, 36444, 39577, 27846, 36498,  7693, 29559,  1398, 17168,
       14281, 10923, 23723, 46975, 10172, 35911, 42467, 13886,  5658,
       12772, 23567, 25126, 29490, 17482, 35074, 30509, 29782,  6166,
       29676, 13338, 48432, 37072, 27808, 49654, 47342, 28436,  5378,
       17820, 18492, 45104, 46101, 20749, 28768,  7129, 46309, 37642,
       32532,  1751,  7083, 47978, 49127, 20984, 19186, 40402, 10702,
       12087,  2819, 16630, 37307, 12699,  4001, 23672, 22920,  8006,
       30691, 25237, 19097, 16792, 34274, 29479, 38689, 20726, 12543,
       24045, 29371, 25072, 46580, 14693, 23872, 39198, 12615, 13388,
        9122,  5348, 26487, 30770, 49851, 27068, 30664,   170, 39653,
        6529, 26720, 48261, 23595, 35491, 36438, 43381, 43490, 15872,
       35459, 25934, 40670, 13189, 11743,   785, 44451, 47100, 38193,
       36080, 43606, 38894,  7171,  2406,   395, 46230, 27441, 39871,
        8710, 14623, 38445, 37415,  7432,  3486, 26657, 42604, 26097,
       13259, 45928,  6783, 46950, 25705,  4672, 40712, 13201, 40249,
       45282, 48592,  9973,  3795, 45914, 26862, 20210, 18780, 40954,
       13873, 16983, 47067,  2051, 38472,  3222, 23389, 23763, 38360,
        3320,  6284, 22979, 45362, 36425, 30339, 30452, 30687,  4350,
       12786, 31479, 45433, 20721,  9051, 10753, 47062,   869, 10468,
        3447, 44103, 28368, 42646,  4628, 15467, 27608, 32134,  3108,
       19414, 40651, 43762, 48762, 45117,   740, 15470,   922, 17658,
       46682, 33600, 49478, 33236, 15873, 35472, 21655,  5553, 19061,
       44120, 38338,  4876, 18324, 36140,  6332, 26964, 11103, 39420,
       22913,  6199, 16583,  4697,  1895,  4392, 19255, 28697, 17214,
       33345,  3218,  9667,  4428, 38999,   241,  6714, 10452, 26707,
       41827,  9760, 12798, 15388,  4405, 33238,  2132, 42953, 41421,
       48540, 34199, 20425, 44944, 23286, 43123, 12973,  4558, 47897,
       41853, 25899, 44943,  1841,  7411, 21703,  2082, 17062, 39604,
       17565, 24991, 23702, 10186, 37194, 16335, 22042, 46604, 46977,
        2447, 39865, 33653, 34517, 22861, 25218, 47586, 14107, 21274,
        7667, 26482,  8846, 33033,  6151,  4519, 41271,  8635,  7107,
        5838, 47950, 22377, 26849, 22919, 35790, 22244, 30192, 42122,
       49742, 46058, 17096, 35307, 15545, 32230,  7615, 43487,  5502,
       37421, 35697,  3242, 15860, 23584, 21835, 25627, 45949, 23409,
       48758, 17655, 43598, 34160, 30318,  9846,   792, 25545, 28657,
       45933,  7122, 49726,  8686, 44197, 47603,  5539, 12904, 22484,
       21898, 14986, 46605, 21085, 16526, 45667, 27796,  5260, 36930,
       38764,  2359,  9008, 33709, 35380, 18462,  4110, 49032, 48368,
       48085, 25509, 23992, 30947, 34170, 13999, 12164, 40936, 21796,
        5125, 25046,  8058, 41710, 24268, 36848, 27419,  9584, 34364,
        6383, 31599, 44889,  5282, 27174, 10107, 43520,  6779, 46099,
       35688, 15669, 39964, 49022, 15577, 14662, 37727, 17772, 40964,
       40713, 30696, 37830, 44415, 11134, 30039, 17964, 43274, 10795,
       32737, 35863,  6915, 41072, 23263,  4132, 22500,  8476,  5759,
        2433, 49225, 27953, 24544, 19886, 43806, 38064, 44051, 14967,
       14376, 31890, 35344, 33181, 41153, 43480, 41051, 15114, 31720,
       27260,  2204, 28671,  6411, 34802, 14384, 16613, 38658, 29710,
       16543, 39322, 24292, 20674, 22227, 41075, 33085, 38710, 27132,
        7148, 25331, 30019, 46904, 12977, 41189,  9335, 37455,  9998,
       36067, 30545,  7390,  4993, 14238, 48465, 31665,  2771, 40220,
        5616, 46547, 17169, 39340,  6226, 32462, 22788, 47670,  1790,
       42437, 43655, 42438, 12516,  9085, 34400, 37203, 41760, 38482,
       15315, 14367, 41445, 32584, 34258, 34905,  9646, 16736, 35576,
        2362, 10168, 12306,  4958,   250, 44204,  4648, 35910, 29874,
       25600, 23768,  4200, 47989, 22060,  7371, 38420, 12340, 28268,
       45289,  3025,  8127,  9668, 27405,  9317, 16855,  7875, 18908,
       31645, 44266, 30183, 43954, 48970, 18178, 30083, 25165, 44273,
       47190, 26368, 41439, 45821, 47423, 30999,  4354,  7284, 12629,
       39892, 45308,  7887, 30650, 40449,  1736,   593, 43608, 12451,
       45068, 40046, 12292, 34278, 29429, 10826, 13044,  4244, 47226,
       19571,  4099, 23439,  8189, 45972,  3827, 22790, 38223,  7428,
        6819,  6121, 47264, 17460, 34794, 40076, 24075, 15183, 48269,
       13311, 35465, 26350, 10564, 18825,  7124, 21706, 45014, 41946,
       49847, 33658, 40472, 17031, 47033, 49977, 49769, 17205,  3142,
       18907,   761, 44601,  3131,    60, 39204,  1413,   176, 14676,
       12247, 42163, 13523, 16211, 49959, 16286,  6851, 45317, 11799,
       33832, 21416, 33118, 43482, 24673, 46043, 15028, 24872, 41260,
       33566, 31748, 19877, 28042, 35535, 31943,  7091,  6025, 29332,
        1063,  4682, 34322, 16493,  3994,  1876, 10828, 21636,  3865,
       11239,  7843, 47984,  8557, 17675,  2152,  9233,  9428, 31232,
       35221, 45355, 49464, 27295, 15657,  3168, 47752,  3307, 16768,
       26727, 18407, 25851, 42619, 18533,  7025, 16696, 24275, 23412,
       18866, 47976, 38834, 36666,  3091, 45836, 33990, 26192, 12641,
       24397, 19366, 18024, 25709,  4430, 38677, 39678,  6768, 30864,
       31598,  4966, 39323, 13213,  9974, 47878, 37593, 12635, 25139,
       15592, 47790, 39159, 20482, 29846, 27367, 25913, 48103, 40438,
       44128, 35090,  9543, 38069, 35332, 33787,  1017, 20628, 44696,
       40274,  2937, 14084,  2479, 22746, 17966, 37426, 40879, 25612,
       41328,  9530,  1031, 11911, 39464, 33491, 17938, 23888, 10117,
       16789, 30302, 46835, 23771, 11147, 41188, 31928, 45078, 20663,
       16915, 29136, 36274, 47451, 31570, 19107, 24973, 36285, 17562,
       29752, 42510, 29436, 41656, 33364,  2810, 30130, 30481,  3588,
       20885,   931, 13600, 42261, 13216, 34804, 10510, 48065, 12879,
        4086, 32228,  9847, 33192, 40836,   473, 37922, 18542, 31142,
       43269, 44612, 33948, 29357, 20106, 31396,  8873, 36842, 46340,
       36645,  9131, 14951,  9103, 19239, 46612,  5682,   488, 12609,
       20027,  6672, 36704,  1248, 10001, 19927, 41534, 10026, 27765,
       19562,  7272,  4899, 33758,  3775,  1989, 30202, 41293, 17796,
       20003, 33704, 13331,   819, 21572, 45607, 16404, 46473, 10718,
       32010, 18261, 46066, 49754,  1175,  3018, 38396, 33759,  7669,
       10379, 12293, 24321, 14864,  9515, 49636, 18342, 13076, 23036,
       17715, 31181, 33812, 26383, 20113, 44917, 27610, 13956,  9808,
       21756, 14621, 14694, 13066, 14065, 11429, 23628, 36847, 29103,
       25828, 23680, 29516,  8091, 46721, 24701, 33256, 34922, 12048,
       11751]), [8, 3, 1, 6]), (array([ 1937, 21260, 10485, 22369, 36168,  1657, 33408,  4753, 15238,
       19471,  5742, 28170, 40846,  6270, 40739, 23468,  5570, 19360,
       32799, 39182, 11765, 41618, 47630,  2573, 35228,  8423,  1169,
       11810, 16889, 42096, 13012, 42967,   665, 35324, 12406, 33401,
        8765,  6206, 27469, 15587,  9852, 15342, 27604, 21304, 16702,
       19340,  8462,  8379, 48087,    86,    20, 16173, 28658,  5880,
       42716,   816, 33749, 21966, 16878, 30037, 26838, 43509,  3908,
        8487, 38492, 18115,  4199,  3897, 44397, 37288, 48172,  1158,
       33209,  7984, 29065, 10477, 18293, 39947, 45605, 48484, 47857,
       19702, 29445,  1826,  3560, 32593, 37850, 23869, 21197,  2734,
       18493, 46790, 25871, 28555,  5631, 10629, 38123, 46251, 30561,
       42314,  9234,  6652, 33016, 11499,  3189,  1018, 18099, 21364,
        2792, 21016, 42104, 28049, 43151, 48945, 11435, 39265, 46164,
       33574, 30869, 22148,  7382,  4750, 29723, 20782, 16936,   934,
       15718, 42863, 22990,   844, 27649, 46036, 15781, 18821, 30384,
       31932, 26434, 28755,  1746,  3635, 14134, 30358, 41777, 15508,
       43241, 43637,  8046, 30527, 18690,  7100, 33245,  2155, 34777,
       49097, 19441, 49488, 14532, 18766,  3973,  8523, 20978, 43640,
        2486, 26301, 36552,  6432, 28892, 27390, 26233, 23611, 43055,
       23255, 21693, 24464, 47239, 15253, 18418, 36517, 37089, 12691,
        5653, 26823, 48699, 33947, 33272, 12843, 15656, 37313, 27479,
       27923, 18052, 43486, 11254, 11957, 42769, 30225, 40650,  6844,
       12282, 31505, 39125, 39036, 42442, 44629, 16659, 41246,  1264,
       13855,  3717, 38821, 30242, 42303, 31349,  7625, 42653, 19284,
        4394, 15663, 15538,  1182, 27614, 10426,   381, 24884, 25813,
       19790,  2943, 13896, 40665, 43044,  4290,  4781, 19833, 38281,
       48787, 14440, 27017,   563, 48801, 21254, 14397,  6452, 15760,
        1866, 43925, 24067, 23634, 32069,  8890, 45611, 44845, 37302,
       49763, 29158, 23880, 43531, 16260, 41624, 42423, 28759,   178,
        2275, 36495,   948, 39810, 40541, 42500, 32546, 15824, 49648,
       10305,  4216, 15002, 11935, 22136,  2807, 44986, 44798, 48857,
       23860, 45476,  8595, 37262, 38196, 36037, 16088, 11323, 21660,
       34733, 24050, 44338, 44858, 46747,   824,  5842,  3914,  2015,
       48423, 44105, 47082, 21317, 30032,  3135, 47263,  7385,  8498,
       24547, 18938, 40447, 43316,  6966, 49470, 49162, 45758, 35316,
       11656, 35922, 25724,   954, 39465,  3104, 48104, 19062, 19259,
       37893, 11428, 24129, 37472, 43498, 41418, 18667, 26165, 25310,
        5035, 22541, 46691, 42126, 28812, 39297, 46300, 44447, 33681,
       19772, 20900, 33019, 23304, 36680, 19642, 40461, 20846, 32668,
       19461, 30671, 23784, 35814, 23356, 18258, 35277, 19321, 17463,
       11761, 10528, 49231, 38131, 39246,  2601, 30195, 33220, 48727,
         163, 39961, 48759, 29733,  9462,  5809, 30036, 34788, 10852,
        7758,  3606,  3147, 44295, 44378,  6588, 41956, 40637, 34807,
       35407, 38178,   825, 41381, 37292, 39762,   636, 38358, 25889,
       26133, 30993, 27930, 32779, 21927,   641, 10573, 18443, 45561,
        7611, 29648, 13540, 32383,  8461, 16171,  6010,  5854, 31212,
        2783, 38220, 29592, 13482, 37882, 25090, 23196,  8367, 23298,
        6032, 19909, 43748, 41438, 14375, 37763, 17110, 17886, 33804,
       13373,  6235, 44297, 12364, 21787, 44249, 40010,  4554,  3487,
        6759, 41717, 48486,    11, 48837, 39494, 17503, 31262,  8351,
       27145, 22291,  5269,   131, 17872, 27672, 25778, 27400, 32660,
       10998, 20960,  7016, 46353, 29196, 12466, 48436, 27157,  9369,
       18997, 35164, 36654, 28936, 38688, 13185, 23829, 13083, 23993,
       29355, 20369, 12864, 49529,  3534, 25156, 31982,  3592,  4714,
       13493, 17589, 40694, 16824, 41185, 41877, 47828, 10602,  7101,
       26669, 22626, 28727, 41626, 28152, 38330, 24194,  9565, 19351,
       13852,  6668, 17312, 44725, 44436,  8856,  5827, 13323, 37257,
       39692,  1494,  3238, 29325, 29522, 43061, 12920,  2455, 38908,
        9854, 33897,  6371, 21752, 20312,  6295, 47529, 28183, 33126,
        1660, 16848, 29675, 34723, 11444,  7147, 10576, 16474, 12424,
       27073, 45600, 33230, 35914, 25718, 28938, 20313, 46238, 12100,
       46376, 40207, 33532, 15903, 32847, 14425,  2037,  9048, 34052,
        8965, 11910,  5257, 37505,  2615, 27601, 16925, 11163, 18160,
       32827, 16953, 46051, 14978, 47377, 32941, 42490, 21933, 42927,
       41291,  4101, 26298, 27284,  8625, 44591, 26937, 27195,  5287,
        9263, 41262, 49554,  7640, 37687,  4230, 41984, 35850, 32262,
       10359, 44767, 41654,  5917, 24975, 15051, 19667, 27802,  1037,
         978, 30489, 38822,   137, 38262,  6130, 43195, 41948, 22175,
       16089, 46731, 24680,  5722, 33845, 39250, 44841, 21331,    45,
        1079, 28655, 38725,  3907, 40935, 11571, 23959, 48528, 10035,
       23058, 26315, 11804, 15770, 29494, 45296,  8600, 10495, 17534,
        7832, 47568, 26899, 28005, 29485, 29285, 44541, 25209, 46717,
        3483, 14182, 48901, 32418, 35933, 19522, 34571, 30104, 32652,
       24398,  5552,  7374, 26026,  6639,  3992,  4334,  6589, 37801,
         493,  1571, 15885, 25611, 28896, 14502, 37551, 23648, 46649,
       44225,  7738, 40594, 17984, 21218, 40422, 35454,  4326,  9494,
       28265, 44272, 47606, 17626, 16400,  5422, 47470, 40081,  3020,
       36388, 49060,  9745,  3034,  9017, 19581, 22055, 13264, 44625,
       47088, 38579, 39654, 30458, 49629, 39679, 39771, 10569,  3285,
       38211, 46396,  7338,  6931, 17492, 16365, 29963, 45968, 39668,
        4636,  2545, 41563, 24527, 13006,  7312, 25279, 10022, 16937,
       44589, 22833,  8174, 21651, 13667, 32458, 21992, 21142, 15248,
       13608, 12335, 47162, 49121, 37920,  2762, 36376,   991, 44679,
       47370, 30227, 37630, 16003, 16069,  2888,  1159, 48400,  5833,
        8171, 40322, 28258,  7064, 44915, 43021, 14178, 30146,    25,
       27660, 14457, 36147, 21072, 10356, 40767, 17143, 12369, 49171,
       18224, 16389, 36620, 44985,  6747,  6941, 18816, 48866,  5398,
       32191,  5380, 39190, 35435, 13120,   863, 20717, 17908, 34786,
       44417, 30922, 47792,  6127,  4881, 24660,  9828, 46306, 17132,
       11038, 37997,    95, 45793, 41399, 42671,  5169,   103, 47396,
       35600, 34245,  7252, 42334, 13573, 23244, 26245, 34092, 30685,
       32637, 28432, 25657, 21560, 25747, 18025, 48664, 31500, 38936,
       10738, 36625, 32397, 49624, 48325, 27504, 29976, 14889, 10444,
       12396,  9762, 20362, 24418, 42027, 44163,  9732, 31820, 25406,
       21704, 39986, 26524, 11904, 31112, 36493, 37865, 40551, 18021,
        2019, 43933, 12014, 29214, 18325, 27577,  2915, 14547, 20878,
       13838, 48043, 32880, 43284, 10063,  6395, 40804, 47302, 11823,
        9126,  3589, 16822,  2965, 16762, 32105,  7339, 45878, 32762,
        4663, 45009, 34157, 26419,  7591,  6777, 12433, 38462, 46856,
        5708, 38298, 26798, 18334, 10783, 32739, 43431, 39759,  8215,
       22027, 36638, 29009, 12019, 28672, 37918,  2872,  3716, 33367,
       16097, 48977,  1023, 45225,  3603, 31616, 40047, 19731, 44626,
       12168,  9246, 30222, 23915, 48176, 10579,  8815, 27486,  3041,
       48819, 26054, 10936, 48710, 24519, 17597,  5558, 23881, 28501,
       17688, 45247, 22610,  7232, 29327, 14824,  2602,  7881, 14973,
       15343, 45443, 31891, 42327,  4751, 34542, 23470, 46791, 36586,
       39052, 19128, 36808,  7172,  5402,  8545, 19823, 11212, 31135,
        9356, 47953, 31784,  6664, 13806, 43353, 13143, 21722, 32212,
       15517,  2874, 43995,  3439, 43390, 39343, 15232, 31575, 23697,
       29426, 30033, 42358,  6035, 43439,  1176, 15231,  8827, 45663,
       46952,  8832, 18621, 18884,  7638, 14884,  3192, 22968, 40323,
       34564]), [4, 7, 1, 6]), (array([  742,  9067, 29220, 49888, 47450,   910,  6227, 41198, 43122,
       20779, 10433, 40281, 15176, 13786, 12711, 44392,  4588, 48893,
        3037, 12669, 21819, 44075, 41681, 45864, 28497, 41619, 32274,
       47632, 25041, 13433,  8543,  1492, 36125, 49397, 40701,  9552,
       38745, 12504,  7166, 38096, 26321,  7948,  5979, 14192, 29678,
       46114, 26541, 40398, 12742, 35311, 26186,  8881, 49802, 31495,
       34088, 38594,  7303, 28802, 14172, 26807, 43465,  9033, 21942,
       12583, 27825,  9001, 32951, 27498, 48589, 39062, 42267,  9989,
       32278, 29912,  1768,  1788, 26893, 13081, 43478,  3848, 37742,
       20547,  8186, 39019,  9526,  9455, 48545, 10160, 18652, 37745,
       24170, 30454, 22667, 48833,  3629, 47956, 44658, 48365, 41542,
       20580, 25449, 19964,  1530, 36025, 17844, 25491, 41933, 45074,
       28705,   108, 43010,  7853, 37560, 14229, 19950, 40177,  6408,
       22297, 42803, 40372, 15522, 33051, 34396,   820,  1444,  3462,
       11443, 31308, 18863, 13648, 20572, 48531, 42302,  1122, 43118,
       17955, 12046,   975, 31333, 28994, 28164, 19914, 43469,  1693,
       15696, 42648,  1040, 42468, 39991,  3476, 39051, 31813, 46819,
        6484,  5727, 28708, 19932, 49104,  7807, 40087, 33519, 45926,
       46522, 14829, 47491, 23911, 29728, 18027,  3119, 29448, 23530,
       19137, 12148, 22602, 33032, 10018, 37321,  2372, 16919, 35681,
       36450, 40583, 43766,  8871, 39436, 31562, 25859, 12602,  8843,
       18092, 32470, 25830, 23692, 20660, 47016, 31026, 31519, 24636,
       31759,  8800, 13564,  1533, 33449, 32816, 35482, 40403,  3101,
       22228,  9437,  3889, 41362, 16266, 18101, 31040, 14270,  1606,
       24694, 39687, 44739,  6228, 29181, 23453, 46530, 24496, 40225,
       44277, 15182, 36339, 12156, 27580, 37278, 27557, 41255, 38058,
       37816, 10767, 39928,  2033, 32321, 21167, 43134, 16279,  4426,
        6448, 20526, 19695, 38704,  8418, 49297, 17735, 29776,   369,
       22921, 42739, 21786, 10208, 38363, 28543,  6066,  7127, 45231,
       25319, 48017,   886, 17716,  3513, 27548,  3820,  6937, 29992,
       26069, 13881, 38379, 39884, 40699,  5437, 14099, 11230,  5611,
        1390, 39583, 40068, 47073,  2281, 41560, 13155, 27190, 30175,
       43039, 17468, 25009,  5671, 27420,  7793, 49926, 40303,  9213,
       17905, 22016, 27523,  3477, 24718, 10494, 49637, 49785, 46685,
        7457,    31, 31566,  1706, 25631, 36050, 24086, 26955,  2214,
       43511, 25159, 48371, 42346, 11312, 30853,   883, 34943, 32810,
       25837, 15750, 32242, 41976, 31287, 17218, 44052, 19842, 45906,
       10138, 21203,  3628,  5453, 35746, 42627, 37074,  1115, 37678,
       33200, 28391, 20587, 14809, 26061, 37709, 16641, 12835, 39806,
       27983, 39816, 26053, 40117,  2477, 24391, 33896, 43591, 11104,
       26496, 44132, 10178,  9025, 33865, 17228, 27784, 38380, 33025,
        6579, 43803, 36807,  6480, 36859, 46777,  8501, 15838,  6237,
       39984, 19302, 43346, 20713, 24542, 10390,  8505, 34796,  2061,
       27957,  5161, 23075, 27799, 41183, 46346, 36529, 38961, 22614,
       16720, 14224,  3850, 45063, 10467, 26057, 16347, 48192, 13795,
        6539, 32129,  1572, 41226, 14661, 39688, 19670,  3838, 29784,
        3409,  7010, 44058, 22651, 13827, 44716,  1231,   186, 20995,
       12104,  4364, 29193,  3652,  5465, 17090, 40450, 49442, 11755,
       45008, 38789, 35542, 39418, 31162,  5300, 40833,  8650, 35183,
       46781, 13096, 46269, 11777, 27336, 15142, 47512, 39453, 44339,
       42140, 23225, 11373, 34867, 47765, 36684, 30615,  4473, 45198,
       32115, 33417, 49147, 21584, 44469,  2908, 10312, 45498, 17798,
        9807, 13421, 11555, 28879, 32041, 15384, 19367,  3846, 24408,
       17068, 49003,  1972, 31403, 27380, 42973, 43619, 23137, 31490,
       34872, 14220, 14462, 41228, 27196, 12086, 46645,  3537, 26446,
       16591, 18878, 10781, 46861, 49385, 43646, 30814,  6353,  2143,
       43669, 43267, 48995, 46507,  9549, 29299, 46966, 13509, 19810,
       29473, 43290, 44487, 30821,  4856, 30006, 14593, 49663, 27404,
       45946, 16180, 49283, 14048, 47213,  2851, 13766, 40554, 24294,
       35866, 10868, 27130, 43710, 23313, 18670,  8427, 28963, 49426,
       29049,  5337, 46634, 28685, 43984,   834, 49585, 38157, 18527,
       14716, 31935,   325, 44185, 32811, 47706, 16471, 24336, 44235,
       35827, 27510, 30316, 30049,  4172, 19575, 36850, 18744, 31193,
       17319, 28383, 38958, 14964,  8729,  2428, 23060, 19745, 49326,
       13327,  7774, 11418, 38735, 41307, 40808, 47349, 19565, 34519,
       13191, 33615, 29959,  8893, 35207, 24151, 19678, 12981, 42237,
       47824,  4360, 47390, 49989, 41063, 11920, 32189,    97, 31502,
       45565, 10377, 44966,  2849,  5356, 35948,  5816, 31407,   134,
       34238, 41739, 23275, 10771,  6325, 19341, 10832, 21877, 22552,
       12158, 14455, 42753, 48917, 14074, 27538, 40290, 20047, 37309,
       42085, 49401, 43132, 41532, 43918,  2038, 27002, 28335,   364,
        9066,   936, 38452, 44474, 27582,  6429, 21947,  2844, 38542,
       36991, 26578,  1731, 11109, 37440,  4654, 38902, 19202, 21003,
       14249, 27730, 31583, 26818, 20680,  6272,  6831, 35508,  2101,
       15797,  4423,  9583, 16962, 35058, 16252, 29908, 32486, 34128,
        2318, 24338, 35531,  7297, 29999,   323, 36853,    79, 21150,
       27970, 14381, 28541, 29555, 34618, 17860, 29469,  3734, 21181,
        8124, 18495, 40431,  7019, 24923, 22499, 14101, 26158,  9113,
       39256, 37620,  6347,  8389, 26115, 43205, 45332, 43547,  7375,
       26331, 18238, 28122,  4171, 30009, 14855, 13997, 49148, 22906,
       30119, 27509, 38858, 19481, 46588, 23789, 33097, 42802, 40655,
       16777, 49933, 35756, 16981, 10693, 31655, 25300, 46889,  5971,
       28552, 26310, 22930, 29149, 48639, 38028, 34426, 41392,  2149,
       18425, 24807, 38818,  7137, 13890, 11654, 15390, 40298, 42291,
       24510, 31257, 34422, 47546, 15498, 13018,  6145, 18871, 36164,
       14392, 45806, 42660,  2989, 33701, 37907, 37926, 12359, 44431,
       20934, 11748, 24279, 44342,  5022,  3707, 31448, 47047, 12508,
       44018, 23025, 11066, 40640, 25704, 29749,  7848, 20332, 41379,
        2939, 17283, 48448, 41799, 37603, 29889, 11699, 21956, 40455,
       11090, 17238, 49076, 48789, 28194, 33069, 31812, 34926, 16584,
        4590, 31753,  1842, 45863, 40627, 46126, 19967,  5745, 17245,
       37119, 26335, 12077,  1837, 39252,  1678, 32922,  6754, 19701,
       40617, 29310, 49704, 27379, 12852, 15360, 11369, 28395, 11672,
       12305, 33543, 34550, 30193, 44684, 26190,  9477, 29506,  3833,
       30910, 15116, 45266, 11827,   326, 29223, 32385, 36880,  1955,
       40568, 33663, 15534, 15994, 22686,  9406, 46009,  5019,  7036,
       41537, 32859, 45958,  8820, 11875, 45585, 40153, 41909, 33594,
       38844, 35480,  6545, 31508, 13550, 10220, 34033, 35349,  3873,
       13484, 39337, 29457, 17484, 15804,  9367, 46429, 18380, 12434,
        6473, 38508, 42156, 10128, 29434, 48067, 17766,  7041, 23747,
       36576, 29802, 16337, 22266, 13188, 47454, 40982, 21084, 39046,
       24928, 21362, 35831, 11346, 13617, 45169, 12897, 22362, 35506,
        3926, 16177,  7267, 26206, 46092,  4789, 44712, 34942, 48337,
       12657, 39010,  8009, 15110,  4144, 23411, 22065,  7491, 34829,
       18742, 25519, 46913, 34031,  2267, 15179, 21177, 35351, 29408,
       35817,  7950, 21800, 35136, 11303, 18992,  4786, 25168, 22752,
       15991, 33726, 26872, 12273, 32381,  5025, 23563, 12933, 21935,
        4234, 43308, 25396, 27376, 40874, 40927,  5374,  4415, 33044,
       28869, 30469, 23618,  3241,  7820,  3070, 30569, 41749, 23328,
       22528, 28188, 31270,  8212, 22101, 48888, 28956, 38125, 33791,
       27937, 28454, 46948, 17680,   960, 10774, 48532, 31808,  4377,
       26066]), [2, 9, 1, 6])]
Competition
DC 0, val_set_size=1000, COIs=[0, 5, 1, 6], M=tensor([0, 5, 1, 6], device='cuda:0'), Initial Performance: (0.259, 0.0446454519033432)
DC 1, val_set_size=1000, COIs=[8, 3, 1, 6], M=tensor([8, 3, 1, 6], device='cuda:0'), Initial Performance: (0.249, 0.04491214168071747)
DC 2, val_set_size=1000, COIs=[4, 7, 1, 6], M=tensor([4, 7, 1, 6], device='cuda:0'), Initial Performance: (0.246, 0.04495431101322174)
DC 3, val_set_size=1000, COIs=[2, 9, 1, 6], M=tensor([2, 9, 1, 6], device='cuda:0'), Initial Performance: (0.251, 0.04425755798816681)
D00: 1000 samples from classes {1, 6}
D01: 1000 samples from classes {1, 6}
D02: 1000 samples from classes {1, 6}
D03: 1000 samples from classes {1, 6}
D04: 1000 samples from classes {1, 6}
D05: 1000 samples from classes {1, 6}
D06: 1000 samples from classes {0, 5}
D07: 1000 samples from classes {0, 5}
D08: 1000 samples from classes {0, 5}
D09: 1000 samples from classes {0, 5}
D010: 1000 samples from classes {0, 5}
D011: 1000 samples from classes {0, 5}
D012: 1000 samples from classes {8, 3}
D013: 1000 samples from classes {8, 3}
D014: 1000 samples from classes {8, 3}
D015: 1000 samples from classes {8, 3}
D016: 1000 samples from classes {8, 3}
D017: 1000 samples from classes {8, 3}
D018: 1000 samples from classes {4, 7}
D019: 1000 samples from classes {4, 7}
D020: 1000 samples from classes {4, 7}
D021: 1000 samples from classes {4, 7}
D022: 1000 samples from classes {4, 7}
D023: 1000 samples from classes {4, 7}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.418, 0.06142205399274826) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.411, 0.06509904915094375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.0901500660777092) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.08961409723758698) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.06931790328025818) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.07178343132138253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.285, 0.12618956410884857) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.273, 0.1021047906279564) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.0813574980199337) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.08620012563467026) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.322, 0.1603555630147457) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.12693029174208642) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.09731255877017975) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.11036875423789025) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.1944475924372673) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.15093016832321882) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.12286224584281445) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.15348628028482197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.21103904109448193) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.1675949716344476) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.13750439224019648) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.15476095771044493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.21013511917740108) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.21405988819152116) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.14960554386116565) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.17682396379858256) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2106591824889183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.2431645128428936) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.15374461515340954) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.18366238740086555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.24523430716246367) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.27802466233447193) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.16089225522428752) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.19264739524573088) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.2547052926644683) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2662605528570712) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.17724084822554143) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.1974615651369095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.2696862921901047) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.29413447110913693) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO0', '(DO2']
DC 2 --> ['(DO5']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.17452540120435878) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.24310164262354375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.414, 0.28580697695538404) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.3387425405215472) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.18162821408640592) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.19475076502375305) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.29260633084923027) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.34481240368448196) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.20459620343148707) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.21231901431456207) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.30507677368447184) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.3093372963573784) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.19936620865343138) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.21525420495960862) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.2813997602574527) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.317076409380883) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.20470537373470143) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.21485905333980918) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.2768659417182207) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.38765156974736603) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO0']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.1992196844238788) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.22097925977176056) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.3257474353611469) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.38170421731984244) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.18942805074248462) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.18283524945005775) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.2903843613564968) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.454, 0.33106693962588907) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.19134141582110897) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.1975837275274098) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.33401283707283436) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.30770798567682506) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.20544685142161326) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.19592551789432763) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.30817436511442065) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.461, 0.35932473118649794) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.21082936951308512) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.22816629043221473) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.28003920658305287) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.30606729954108597) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO0', '(DO4']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.20754672225750984) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.20889357138425113) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.32250401337072254) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.34874038782855493) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.18630593120865524) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.21444857031852008) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.3388949249051511) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.33281355514982713) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.23085263347404544) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.20094849235750734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.3147721752300858) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.3341119632311165) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.2290267192358151) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.21319095825590192) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.30371652669645843) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.3405675527364947) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.230588948335906) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.19133538994006813) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.3051612301766872) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.32230345746083183) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO2']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.20816293217847123) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.18628517237491907) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.3093570705279708) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.3336176608442329) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.22064314524782822) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.195092939697206) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.296700023829937) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.33127928837342185) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.2399094582789112) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.18772582675516605) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.2986483151149005) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.32703069562255405) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.22232803877745755) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.20692382625862957) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.30051402477920053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.313741766794119) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.20834482615021988) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.2043636593548581) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.2897961160726845) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.2812294906745665) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO1', '(DO2']
DC 2 --> ['(DO4']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.22941495414823293) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.1611406158003956) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.320643137127161) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.3368820718713105) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.24850910579023183) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.18947752058226616) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.30330591551586983) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.3260640354345087) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.26502255495765714) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.1712429580707103) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.27528968581184743) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.31859335691994056) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.2803955555350403) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.17800236310064793) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.2758454231470823) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.336166100793751) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.2470606305692345) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.1720784338861704) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.2919650531560183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.461, 0.29784526348719376) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.28604880791844334) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.16928083777055145) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.27047052323818205) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.30839182644663377) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.26647685901704243) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.19390874000638722) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.29217139663919806) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.29748786987457426) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.285527285702352) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.15203374621272087) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.3134702882282436) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.35624338509421793) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.2717486012395238) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.19366400813311338) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.26752157987840475) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.464, 0.3292596841128543) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.2534507645292906) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.18387241404503585) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.32109696853905917) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.3003175199967809) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.30507025046786296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.1804656303524971) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.3000661358870566) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.3182412069635466) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.28539117642323253) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.477, 0.2026815267316997) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.310692980915308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.31966987494123167) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.2961449518838199) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.17167639578133823) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.29841178146004677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.3356656242487952) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.2811104843700887) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.16818082922697067) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.3089642498679459) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.31458546660421416) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.29197014055791076) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.19131469286978245) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.30513216265663506) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.464, 0.3050848887269385) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO1', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.29298091941417076) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.1455394022464752) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.2860830928571522) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.31299096696963535) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.2844506639477331) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.17061675046384334) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.31349579164385794) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.30518429840914907) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.26939503943000453) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.1568082693219185) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.3008026895429939) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.31848667106591166) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.3014287285883911) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.18339949927851557) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.3160086400546134) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.2846618461720645) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.2492514036251232) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.15985954826325177) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.29805697982758284) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.3001477854624391) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.259, 0.0446454519033432), (0.418, 0.06142205399274826), (0.46, 0.06931790328025818), (0.467, 0.0813574980199337), (0.461, 0.09731255877017975), (0.458, 0.12286224584281445), (0.462, 0.13750439224019648), (0.452, 0.14960554386116565), (0.453, 0.15374461515340954), (0.469, 0.16089225522428752), (0.472, 0.17724084822554143), (0.458, 0.17452540120435878), (0.472, 0.18162821408640592), (0.471, 0.20459620343148707), (0.466, 0.19936620865343138), (0.461, 0.20470537373470143), (0.461, 0.1992196844238788), (0.463, 0.18942805074248462), (0.468, 0.19134141582110897), (0.465, 0.20544685142161326), (0.465, 0.21082936951308512), (0.465, 0.20754672225750984), (0.47, 0.18630593120865524), (0.468, 0.23085263347404544), (0.467, 0.2290267192358151), (0.469, 0.230588948335906), (0.462, 0.20816293217847123), (0.47, 0.22064314524782822), (0.468, 0.2399094582789112), (0.462, 0.22232803877745755), (0.47, 0.20834482615021988), (0.466, 0.22941495414823293), (0.464, 0.24850910579023183), (0.458, 0.26502255495765714), (0.461, 0.2803955555350403), (0.459, 0.2470606305692345), (0.465, 0.28604880791844334), (0.466, 0.26647685901704243), (0.461, 0.285527285702352), (0.464, 0.2717486012395238), (0.465, 0.2534507645292906), (0.465, 0.30507025046786296), (0.461, 0.28539117642323253), (0.462, 0.2961449518838199), (0.462, 0.2811104843700887), (0.464, 0.29197014055791076), (0.462, 0.29298091941417076), (0.466, 0.2844506639477331), (0.466, 0.26939503943000453), (0.465, 0.3014287285883911), (0.467, 0.2492514036251232)]
TEST: 
[(0.259, 0.043567125290632246), (0.40925, 0.05918567603826523), (0.46025, 0.06647206515073777), (0.46125, 0.07821208986639977), (0.4535, 0.09296726769208909), (0.4575, 0.11661085388064385), (0.45925, 0.12995803904533387), (0.44975, 0.14063709473609926), (0.44975, 0.1442626576423645), (0.4685, 0.15252220165729521), (0.4685, 0.16789883202314376), (0.4575, 0.16432510966062547), (0.47125, 0.17241170465946198), (0.473, 0.19327269798517227), (0.469, 0.18849174624681472), (0.46725, 0.19413237357139587), (0.469, 0.187497507750988), (0.469, 0.17939056265354156), (0.469, 0.18303806799650194), (0.46825, 0.19538378721475602), (0.469, 0.198890649497509), (0.47, 0.1959552915096283), (0.4715, 0.17732540088891982), (0.46975, 0.21995283710956573), (0.4705, 0.21783882701396942), (0.4735, 0.21954261934757233), (0.46925, 0.19452857345342636), (0.46975, 0.2079318492412567), (0.47, 0.2282570593357086), (0.4685, 0.21113820427656174), (0.467, 0.19733257621526717), (0.469, 0.21435819160938263), (0.4695, 0.23099086421728135), (0.46575, 0.246867356300354), (0.4675, 0.2646700425744057), (0.4665, 0.23144233131408692), (0.46875, 0.27306826317310334), (0.46925, 0.24865126079320907), (0.4705, 0.2656941918730736), (0.47125, 0.2559556704759598), (0.4685, 0.23862758260965347), (0.4715, 0.2874121372699738), (0.46975, 0.2689654735326767), (0.4675, 0.278944708943367), (0.468, 0.26420057541131975), (0.47025, 0.2754345613718033), (0.46825, 0.2729244655966759), (0.47125, 0.266884348988533), (0.469, 0.25344922477006915), (0.46975, 0.2855261733531952), (0.4705, 0.23289774417877196)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.63      0.91      0.75      1000
           1       0.00      0.00      0.00      1000
           5       0.38      0.97      0.55      1000
           6       0.00      0.00      0.00      1000

    accuracy                           0.47      4000
   macro avg       0.25      0.47      0.32      4000
weighted avg       0.25      0.47      0.32      4000

Competition_DC_1
VAL: 
[(0.249, 0.04491214168071747), (0.411, 0.06509904915094375), (0.459, 0.07178343132138253), (0.462, 0.08620012563467026), (0.462, 0.11036875423789025), (0.467, 0.15348628028482197), (0.47, 0.15476095771044493), (0.467, 0.17682396379858256), (0.467, 0.18366238740086555), (0.47, 0.19264739524573088), (0.468, 0.1974615651369095), (0.471, 0.24310164262354375), (0.471, 0.19475076502375305), (0.474, 0.21231901431456207), (0.471, 0.21525420495960862), (0.468, 0.21485905333980918), (0.471, 0.22097925977176056), (0.476, 0.18283524945005775), (0.47, 0.1975837275274098), (0.473, 0.19592551789432763), (0.474, 0.22816629043221473), (0.471, 0.20889357138425113), (0.473, 0.21444857031852008), (0.468, 0.20094849235750734), (0.47, 0.21319095825590192), (0.469, 0.19133538994006813), (0.473, 0.18628517237491907), (0.472, 0.195092939697206), (0.471, 0.18772582675516605), (0.47, 0.20692382625862957), (0.476, 0.2043636593548581), (0.473, 0.1611406158003956), (0.475, 0.18947752058226616), (0.472, 0.1712429580707103), (0.474, 0.17800236310064793), (0.471, 0.1720784338861704), (0.473, 0.16928083777055145), (0.472, 0.19390874000638722), (0.474, 0.15203374621272087), (0.473, 0.19366400813311338), (0.47, 0.18387241404503585), (0.473, 0.1804656303524971), (0.477, 0.2026815267316997), (0.471, 0.17167639578133823), (0.472, 0.16818082922697067), (0.473, 0.19131469286978245), (0.476, 0.1455394022464752), (0.475, 0.17061675046384334), (0.475, 0.1568082693219185), (0.475, 0.18339949927851557), (0.472, 0.15985954826325177)]
TEST: 
[(0.25425, 0.04382993084192276), (0.417, 0.06269155278801918), (0.465, 0.06890970841050148), (0.4675, 0.0824232129752636), (0.466, 0.1052005781531334), (0.47175, 0.14798526340723037), (0.474, 0.14918128669261932), (0.47475, 0.1711675282716751), (0.474, 0.17799552154541015), (0.4715, 0.1876283633708954), (0.47575, 0.19261900579929353), (0.4775, 0.2377548621892929), (0.474, 0.1887073370218277), (0.47425, 0.20711637407541275), (0.4765, 0.2080047606229782), (0.47325, 0.2103859583735466), (0.47475, 0.21231141448020935), (0.47625, 0.17717109674215317), (0.4785, 0.1903467625975609), (0.4755, 0.19139571142196654), (0.4745, 0.22051576673984527), (0.47675, 0.20263164234161377), (0.4775, 0.2050201324224472), (0.4745, 0.19299305003881453), (0.4785, 0.20450656884908677), (0.47775, 0.18448906177282334), (0.478, 0.18066396361589432), (0.4765, 0.1885836181640625), (0.476, 0.18173391556739807), (0.47875, 0.20135471302270888), (0.47475, 0.19518174868822097), (0.47825, 0.15572380048036574), (0.4775, 0.18152812242507935), (0.47725, 0.16640253233909608), (0.47575, 0.1727100470662117), (0.477, 0.16596768939495088), (0.475, 0.16444268929958344), (0.476, 0.1858092246055603), (0.477, 0.1501574193239212), (0.4775, 0.18983388018608094), (0.4765, 0.1802344017624855), (0.4725, 0.17416790360212325), (0.4775, 0.19607840061187745), (0.476, 0.16742282676696776), (0.476, 0.16419282937049864), (0.477, 0.1861873089671135), (0.47475, 0.1435419904589653), (0.47325, 0.16708642017841338), (0.47425, 0.15253106915950776), (0.4765, 0.17501181715726852), (0.476, 0.15772882342338562)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           3       0.46      0.94      0.61      1000
           6       0.00      0.00      0.00      1000
           8       0.50      0.96      0.66      1000

    accuracy                           0.48      4000
   macro avg       0.24      0.48      0.32      4000
weighted avg       0.24      0.48      0.32      4000

Competition_DC_2
VAL: 
[(0.246, 0.04495431101322174), (0.25, 0.0901500660777092), (0.285, 0.12618956410884857), (0.322, 0.1603555630147457), (0.379, 0.1944475924372673), (0.396, 0.21103904109448193), (0.418, 0.21013511917740108), (0.404, 0.2106591824889183), (0.419, 0.24523430716246367), (0.417, 0.2547052926644683), (0.416, 0.2696862921901047), (0.414, 0.28580697695538404), (0.42, 0.29260633084923027), (0.427, 0.30507677368447184), (0.424, 0.2813997602574527), (0.419, 0.2768659417182207), (0.426, 0.3257474353611469), (0.427, 0.2903843613564968), (0.413, 0.33401283707283436), (0.425, 0.30817436511442065), (0.431, 0.28003920658305287), (0.427, 0.32250401337072254), (0.428, 0.3388949249051511), (0.433, 0.3147721752300858), (0.43, 0.30371652669645843), (0.426, 0.3051612301766872), (0.428, 0.3093570705279708), (0.427, 0.296700023829937), (0.43, 0.2986483151149005), (0.42, 0.30051402477920053), (0.421, 0.2897961160726845), (0.428, 0.320643137127161), (0.431, 0.30330591551586983), (0.424, 0.27528968581184743), (0.425, 0.2758454231470823), (0.424, 0.2919650531560183), (0.429, 0.27047052323818205), (0.426, 0.29217139663919806), (0.429, 0.3134702882282436), (0.434, 0.26752157987840475), (0.433, 0.32109696853905917), (0.427, 0.3000661358870566), (0.43, 0.310692980915308), (0.433, 0.29841178146004677), (0.429, 0.3089642498679459), (0.428, 0.30513216265663506), (0.425, 0.2860830928571522), (0.434, 0.31349579164385794), (0.433, 0.3008026895429939), (0.44, 0.3160086400546134), (0.439, 0.29805697982758284)]
TEST: 
[(0.2375, 0.04377405685186386), (0.25, 0.08629248949885368), (0.2875, 0.1204801225066185), (0.33075, 0.15272677546739577), (0.383, 0.18404866141080856), (0.396, 0.1998128536939621), (0.4165, 0.19864237356185913), (0.41075, 0.20010809701681137), (0.4185, 0.232751971244812), (0.4215, 0.2435290548801422), (0.42125, 0.25688072097301484), (0.41425, 0.27224383717775347), (0.4225, 0.27599601781368255), (0.4255, 0.2898790020942688), (0.4245, 0.2695707421898842), (0.4205, 0.2641191765069962), (0.4325, 0.31043143177032473), (0.43225, 0.27497595846652984), (0.42325, 0.3188502966165543), (0.431, 0.29158095574378967), (0.42975, 0.26664701211452485), (0.43175, 0.30419712138175964), (0.43125, 0.3243544111251831), (0.43325, 0.29766247272491453), (0.43275, 0.28774963986873625), (0.4315, 0.2916992120742798), (0.43475, 0.2933608845472336), (0.43525, 0.27862371361255645), (0.4305, 0.2838956176042557), (0.4325, 0.28864956212043763), (0.431, 0.2768022824525833), (0.43325, 0.30203667747974394), (0.432, 0.28462108409404757), (0.4315, 0.26118179762363436), (0.434, 0.26195249247550967), (0.43225, 0.2779258360862732), (0.43275, 0.2576329982876778), (0.4295, 0.27827318596839906), (0.43075, 0.297844291806221), (0.43475, 0.2554222394824028), (0.4365, 0.3035473923683166), (0.43725, 0.28348870956897737), (0.439, 0.29391534411907194), (0.4375, 0.2827485238313675), (0.43675, 0.29067070496082303), (0.434, 0.2864248914718628), (0.433, 0.2709306319952011), (0.43175, 0.2980406345129013), (0.434, 0.28813714945316315), (0.438, 0.29933908092975614), (0.43625, 0.2849744699001312)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.35      0.92      0.51      1000
           6       0.00      0.00      0.00      1000
           7       0.61      0.82      0.70      1000

    accuracy                           0.44      4000
   macro avg       0.24      0.44      0.30      4000
weighted avg       0.24      0.44      0.30      4000

Competition_DC_3
VAL: 
[(0.251, 0.04425755798816681), (0.25, 0.08961409723758698), (0.273, 0.1021047906279564), (0.392, 0.12693029174208642), (0.402, 0.15093016832321882), (0.411, 0.1675949716344476), (0.459, 0.21405988819152116), (0.462, 0.2431645128428936), (0.46, 0.27802466233447193), (0.463, 0.2662605528570712), (0.458, 0.29413447110913693), (0.46, 0.3387425405215472), (0.463, 0.34481240368448196), (0.457, 0.3093372963573784), (0.447, 0.317076409380883), (0.463, 0.38765156974736603), (0.462, 0.38170421731984244), (0.454, 0.33106693962588907), (0.427, 0.30770798567682506), (0.461, 0.35932473118649794), (0.457, 0.30606729954108597), (0.456, 0.34874038782855493), (0.459, 0.33281355514982713), (0.445, 0.3341119632311165), (0.462, 0.3405675527364947), (0.457, 0.32230345746083183), (0.46, 0.3336176608442329), (0.46, 0.33127928837342185), (0.458, 0.32703069562255405), (0.46, 0.313741766794119), (0.462, 0.2812294906745665), (0.463, 0.3368820718713105), (0.455, 0.3260640354345087), (0.452, 0.31859335691994056), (0.455, 0.336166100793751), (0.461, 0.29784526348719376), (0.458, 0.30839182644663377), (0.457, 0.29748786987457426), (0.457, 0.35624338509421793), (0.464, 0.3292596841128543), (0.457, 0.3003175199967809), (0.459, 0.3182412069635466), (0.457, 0.31966987494123167), (0.468, 0.3356656242487952), (0.46, 0.31458546660421416), (0.464, 0.3050848887269385), (0.469, 0.31299096696963535), (0.466, 0.30518429840914907), (0.466, 0.31848667106591166), (0.468, 0.2846618461720645), (0.462, 0.3001477854624391)]
TEST: 
[(0.2505, 0.04323132234811783), (0.25, 0.08614569437503815), (0.2725, 0.09757168874144555), (0.40125, 0.1206749073266983), (0.39925, 0.14345088809728623), (0.415, 0.15829271918535232), (0.46625, 0.20534850442409516), (0.4725, 0.2301350491642952), (0.46875, 0.26313387537002564), (0.4705, 0.2506633816361427), (0.4695, 0.2793130035400391), (0.471, 0.32386793756484983), (0.46825, 0.3281647443771362), (0.4685, 0.2928442845344543), (0.45575, 0.2985740348100662), (0.4745, 0.36796041786670686), (0.47175, 0.3670652259588242), (0.466, 0.31857512378692626), (0.4355, 0.29032988762855527), (0.46775, 0.34341199827194213), (0.46625, 0.29071081244945524), (0.46275, 0.3321743873357773), (0.46725, 0.3211867201328278), (0.45175, 0.31837464463710785), (0.46825, 0.32786627757549286), (0.466, 0.3070304647684097), (0.46875, 0.32189498317241666), (0.4685, 0.32162894928455354), (0.471, 0.3171179577112198), (0.4695, 0.30395765841007233), (0.46725, 0.2725648090839386), (0.471, 0.32570687735080717), (0.462, 0.31124314093589783), (0.4615, 0.3030807273387909), (0.46075, 0.32248801374435426), (0.46625, 0.2879319895505905), (0.465, 0.2997551327943802), (0.46625, 0.2888154267072678), (0.4695, 0.3391680109500885), (0.4695, 0.31507995545864104), (0.46475, 0.2894139819145203), (0.46625, 0.30852552461624144), (0.464, 0.3037671537399292), (0.4715, 0.325587486743927), (0.46825, 0.3001358675956726), (0.46575, 0.29049881851673126), (0.4685, 0.3006644300222397), (0.46825, 0.29460767495632173), (0.47175, 0.3059438211917877), (0.471, 0.2761298110485077), (0.46775, 0.2906709071397781)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.44      0.98      0.61      1000
           6       0.00      0.00      0.00      1000
           9       0.51      0.89      0.64      1000

    accuracy                           0.47      4000
   macro avg       0.24      0.47      0.31      4000
weighted avg       0.24      0.47      0.31      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [42]
name: no-alliance-42
score_metric: contrloss
aggregation: <function fed_avg at 0x79a6fc337c10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=42
Partitioning data
[[6, 3, 1, 0], [4, 9, 1, 0], [2, 7, 1, 0], [5, 8, 1, 0]]
[(array([44073, 21183, 23596, 47728, 22362, 44695, 31128, 11101, 38769,
        9468,  7705, 30033, 16760, 33704,   313,  4389,  8937, 40664,
       11672, 12356, 23451,   326,  9481, 31835,  9625, 38853, 40544,
       13754, 33212, 15185,   347, 14513, 38677,  1596, 13137, 27554,
       36188, 47513, 39889, 26364, 46971, 40041, 15983, 12491, 13390,
       18884,  5526, 42015, 31564, 19941,  8679, 39252,  7516,  1529,
          22, 22337, 32010, 20734,  8171, 26966, 24289, 36360, 15498,
       17372, 18655, 10058, 47115, 15052, 26940, 36841, 46052,   525,
       17596, 18621, 22795, 25168, 27678, 40685, 41088,  7964, 47396,
       31706, 33824, 13479, 35439, 48841,  8597, 37887, 15612, 15492,
        7392, 25819, 46429, 22384, 18257, 10532,   124, 35928, 12804,
       33731, 14231,  7823, 42460, 39637, 37574, 20717, 42186, 49537,
       22058, 31948,  9367, 34028, 37332,  3833, 25967, 48565, 29798,
        6810,  4143,  2888, 48753,  1360, 43697, 19959, 42690, 11363,
       34805, 20106, 25904, 43581,  3290, 38662, 46188, 29538,  2623,
       23876, 12238, 17317, 34912,  2674,  6749,  7215,  2411,  3048,
        4644, 19955,  9320, 37974,  7867, 45247, 18656, 45900, 34706,
       12550, 40128, 17680,  7265,  2503, 24261,  5223, 32339,  7092,
        2997, 37682, 29241, 11048, 14042, 49200,   154, 32198, 27385,
       34563,  8983, 30143, 13331, 35672, 25067, 35830,   327,  3567,
       40836,  1733, 30193, 29680, 11644, 28322, 45305, 10938, 22065,
       26240, 36788, 32082, 11466, 41641, 42778,  5921, 49082, 14257,
        3861, 30323,  9126, 23771, 48658, 49094, 26054, 39986,  7003,
        7968, 14894, 48607, 29123,  1228, 35732,  6125,  3227,  3780,
       43661, 49059, 38166, 17759,  3849,  6859, 10029, 39678, 33488,
         409, 15192, 48047, 36285, 37033, 49829, 10828, 24852, 35904,
       11875, 40462, 30074, 23787, 14749, 17151, 48446, 40797, 17603,
        3266,  2115, 36486, 12996,   125, 14889, 36282, 11192, 42141,
        3178, 42131,   315, 42949, 20520, 39924, 26609, 38931, 33925,
       32215, 37175, 37162, 30708, 48686, 20868, 39609,  3763, 45152,
       19465, 40314,  2525, 34407, 28717, 43813, 10869, 31626,  3320,
       39166, 22922, 39130,  7049, 41918, 11211, 26357, 31535, 39500,
       33422, 36212,  9304, 43123, 28472, 21070,  4745, 38317, 10967,
       32451,  9976, 10843, 25882,  8656, 15912,  4657,  7909, 37770,
        6557,  6167, 36664, 43283, 33371, 37187, 30838, 35009,   159,
       42647, 36781, 10254, 23995,  5389, 30683, 15175,  1030, 32392,
       21574,  7412, 25492, 41952, 46568, 29692, 21909, 37215, 35163,
       10468,  1573, 18452, 32145,  6071, 27270, 13552, 35124,  5610,
       44047, 13471, 35071, 11028, 29235,   895,  9694, 38702,  7230,
         367, 34682, 23322, 15844, 10611, 14125, 22701, 38879, 44859,
        5578, 49468,  9052, 48315, 26154, 14574,  4439, 11449, 39822,
       47819, 18889, 15083, 46402,  7906,  8781, 39871, 49855, 31141,
        9710, 42789, 12821,  1059, 46594, 39150, 20601, 21415, 20631,
       10746, 36930,  4449, 42206, 29177, 46880,  4428, 47217, 12601,
         258,  2723, 14578, 21524,  1265, 22030, 47413, 35804, 46109,
       31261, 14155, 26711, 16827, 29950, 47523, 42223, 39420, 29004,
       30479, 43744, 19187, 49582, 20496, 15954, 31479, 44103, 32043,
        5859, 30586, 45379, 47851, 27619, 40584, 12013, 42865, 13678,
       17449, 46232,  7460,  9609, 24082, 16705,  5870, 39300, 19352,
        6304,  3447, 12776,  3591, 35976,  8255, 37109, 15383, 10948,
        6200, 26126, 17841, 39220, 26472, 17018, 36745, 43554, 23977,
       41369, 26575,  3289, 21015, 29074, 36051, 19847, 17124, 39575,
        8330, 17879, 43034, 26859, 34756, 17814,  8507, 21156, 30737,
       12174,  4564, 25933, 21147, 15397, 25535, 46838,  9738,    74,
        8056, 42633, 19820, 14122,   334,  5065, 48601, 37863, 17565,
       22008, 41057, 28731,  7128, 12454, 36929, 21685, 43908, 46396,
       12773, 48122, 38084, 42606,  6751, 18908,  8812, 30135, 19522,
       30182,  9189,  2180, 15583, 45214,   354, 11750, 30154, 24725,
       24463, 47109, 15770, 48358,  7255, 24400,  8232, 10190, 23921,
       24620, 47266, 28671, 28615, 36641, 38577, 21971,  7379, 41922,
        8667, 29188, 37801,  4047, 11407,  2582, 35861, 48595, 38596,
       39953,  6064,  9690,  2143, 35576,  9646, 47616, 20994, 35910,
       13277, 38514, 22399,  2204, 21528, 38725, 38682, 48740, 10292,
       10755,  4648, 25150, 22072, 15867,  9658, 19481, 37588, 12982,
       22111,  1559, 49409, 47470,  4983, 42805,  5197, 37030, 47863,
       42918,  1304, 46674, 30227, 13908,  9138, 16726, 10644, 28910,
       14859, 44448,  3064, 24284, 31737, 25323,  1660, 31512, 41439,
       19533, 30457, 49356, 21094, 12127, 45470, 34794, 35850, 12276,
       26304, 10363, 36791, 12920, 15299, 34347, 17752, 35649,  4100,
       22212, 12451, 13895, 24288, 17309,  9318, 20079,  9595, 28937,
       32087,  9509, 36028, 30747, 19886, 33059, 13608, 47772,  3934,
        8107, 36879,  6779, 40410,  9878, 30570, 11824, 29347, 16766,
       20746,  1907, 20753, 46840, 24774,  5137, 47987, 26141, 13849,
       44146, 23151,  2286,   568, 19354, 49447, 47980, 24833, 31274,
        7147, 31268,  9946, 48820,  9889, 40250, 40713,  4334,  1731,
       36040, 35106, 26467, 48264, 47151, 42413, 37618, 13755, 37179,
       46237, 22564, 33126, 46544, 43655, 24527, 38347, 12628, 43935,
       41701, 38174, 10824, 41471,  7130, 23578, 23864,  4993, 40958,
       18311, 13940, 49438, 16810, 24224, 33198,   536, 25459, 16012,
       21631, 16853, 24520, 36774, 13462, 35366, 24999, 48845, 45228,
       49998, 30560, 32684, 27488, 14549, 34336,   917, 42658, 16723,
        5420, 26794,  5202, 36736, 23232, 30047, 14048, 32665, 16986,
       42437, 10910,   815, 40204, 40595, 39735,  6831,  2775, 39387,
       34809, 30344, 37468,  5969, 40275, 43245, 18009, 23386, 29944,
       29364, 47106, 44599, 48378, 12469, 29596, 26162,  2718, 44906,
         332, 27961, 22026, 28972, 31164, 47678, 12030, 40747, 46576,
       17485, 23478, 36858, 27026, 38344,   349,   628, 47320, 31837,
       45242, 34856, 47691, 16238, 10553, 30977, 47379, 20265,  8382,
        7727, 27104, 26430, 20759, 11989, 18857, 31521, 44294,  6165,
       21881, 23194, 15473, 13326,  5873, 10394, 25337,   752, 26842,
       38551, 40945, 36750, 40287, 32247,  4295,  9014,  9140, 26546,
       18218, 33596, 47991, 17196, 30743,  4957,  4859, 25763,  4759,
       37558, 39716, 28978, 40644, 28208,  4645, 24387, 48470,  4504,
       47802, 31654, 15594,  2490, 23964, 10215,  6085,  2707, 15886,
       29480, 47579, 25350, 10334, 22962, 38855,  9813, 21874,   165,
       13357, 19294, 30378,  3318, 36149,  6242, 44715, 16840,  7660,
       42476, 19945, 36055, 34106, 36163,  8857, 18291, 41238, 39739,
       21825, 48671, 22847,  4861, 41300, 46898, 10043,  6396, 20924,
       23786, 21144, 49420, 23825, 29819, 36836, 42399, 10686, 13051,
       38977, 13835, 31459,  6276, 47550, 14949,  4765, 21195,  2578,
       43292, 45096, 17490, 46200, 19860, 31083, 36623, 12836, 38224,
        3549,  8383, 12767, 32592, 15863, 44167,   940, 42755,  1988,
       14534, 11897, 37628, 19237, 49556, 41258,  3362, 14213, 38656,
       49856, 38187,  8623, 11416, 21469, 23684, 12010, 47711, 36254,
        3618, 40411, 15541, 17054, 11572, 46614,  9493, 34017,  2513,
       47560, 48516, 14194, 25748, 42678, 32194, 42205, 14441, 40022,
        7347, 41731, 37646, 36786, 12429, 39670, 18953,   695, 28456,
       16100, 34919, 18047,  6904, 31488,  2435, 14594, 37729, 29246,
       11783, 24521,   694, 35758, 18677, 37806, 33750, 28636, 40479,
       16230, 49018, 29540,  5533, 18955, 43375,   748, 31119,  8199,
       35343, 39179, 10732,  5699, 35209, 21119, 26269, 27749, 32267,
        1039]), [6, 3, 1, 0]), (array([ 4666, 12143, 39947, 36083, 20608,  5946, 45609, 19936, 21684,
       29799, 48902,  8772, 28174, 30790, 33066, 20342, 43758, 26854,
       43923,  7100,  7062, 42938,  8117, 23393,  7383, 41270, 34261,
       48008, 19957,  5143, 26417, 49207, 14303,  4773,  6475, 27914,
       20485, 30341, 24377, 49024, 48879, 24933, 14664, 13787,  2922,
       35630, 41927, 30616, 16936, 48429, 31799, 18014,  3566, 45311,
       16085, 10385,  2710, 41635, 29131, 32026, 22727, 44473, 14082,
       13292, 44790, 12213, 16155, 22139, 30881, 37234, 43974,  7154,
       27469, 27213, 36896, 24236, 46940, 10629, 25949, 38802, 31433,
       24827, 22619, 29613,  6311,  5364, 26897,  1557, 12797,   982,
       25346, 27663, 28414, 30384,  2976, 32803, 15909, 34784, 27787,
       38353,  3802, 28885, 35925,  9452,  1639,  3393, 41788, 29368,
       22480,  3985, 48533, 44254, 29922, 14385, 14923, 10426, 17179,
       17819,  8523,  5594, 14810,  1535, 47305, 41820, 48478, 49114,
        8378, 31993, 28260, 31514,  2444, 31848,  6594, 33222, 40844,
        8208, 16294,  3970,  8379, 26653,  1148, 26351,  3556, 46339,
        3957, 24241,  7099, 49758, 39061, 27817, 41669, 42969, 22519,
       20956, 23823, 47441, 41097, 48945,  9848,  5721, 48163,  4943,
        6844, 22133, 18912, 34384, 26638, 42418, 12898, 26540, 20192,
       14041,   420, 18821, 36843,  6437, 49352, 34808, 44846, 37871,
       11092, 29756, 34082, 25846, 48884, 13282,  3688, 39734, 15018,
       28649,  3383,  5625, 15105, 26475,  4497, 47928, 23474, 18669,
       38755, 39202,  3141,  1407, 27214, 36533, 34215, 27429,  3326,
       27596, 36914, 45170, 18060, 41547,  6421, 33094,  9771, 30719,
       15674, 46255, 42087, 22246,  1349, 36777, 48521, 18320, 12106,
       36247, 27417, 29083, 21771, 17845,   398, 10865, 36600,   945,
        6532, 36199, 15487, 35066, 31549, 22944, 35809, 18300, 41453,
       49645, 27647, 36976,  1366, 35522,  6144, 27180,  4865, 36734,
       44242, 28459, 22989, 48431, 39556, 23908, 47690, 27717, 29073,
        8323, 29438, 19525, 31909, 36807, 26530, 19486, 48672, 36223,
       11678, 12708, 10071, 25499,  3876, 13588, 43596,  1117, 24609,
       40931, 32273, 16143, 20231, 44013, 26439, 24893, 40315, 34995,
       22009, 32952,  4584, 10092, 23645,  5595, 37600,  9673, 10030,
         749, 35178, 14471,  5300, 42058, 46685, 31458,  8506, 14723,
       20517, 26150, 36887, 32778, 27569, 35203, 39721, 19076, 14954,
       18235, 26050, 41904, 25510, 45055, 18668, 45197, 29238,   443,
       47343, 11258,  1827,  4643, 26627, 13609, 16803, 40693, 23302,
       43325, 18982,  8402,  8754, 28215, 18681, 49833, 40324,  9277,
       41899, 11388,  9959,  4142,  1767,  2935, 44315,  2251, 21887,
       30285, 27581, 10717, 35251, 21067, 36878, 46335,  4665, 29197,
       19432, 16642,  8126, 15221, 23886,  2219, 41539, 11620, 21801,
       23624, 22285, 16152, 21478, 42704, 49181, 31638, 45059, 33547,
       36834,  2129,  5133, 21714,  9027, 30175, 23979, 26121, 16718,
       49427, 23325, 14868,  1437, 40885, 27369, 33746, 28901, 34370,
       26848, 47755, 18968, 42624, 44380, 17747, 37347, 32439, 16773,
       48568, 15978, 25285, 31517, 23961, 41505, 26681, 34774, 42911,
       15384, 19891,  5801, 32084, 45735, 25945, 43104,  8567, 33676,
        9513, 48868, 32041, 27040, 23798, 35676, 44669, 10762,  7629,
       48165, 26763, 43760, 43560, 31768, 36557, 45174,  9936, 14198,
       39930, 24453, 23840, 34402,  3819, 21093,  5394, 46438, 28235,
       15350, 20296, 25936, 11430, 31790, 34423, 48919, 43616, 12986,
         683, 37604,  5382, 10467, 28379,  7583, 44056, 48449, 13208,
       19826, 17600,  3510, 15501, 48921, 45302, 38687, 26466,  6934,
       38035, 36794, 44469, 46351,  5485, 23471, 31012, 49904, 31618,
       13962, 43652, 46852, 35852, 41156, 41436, 47419, 37115, 10781,
       29384, 18488, 31771,  4028, 13780,  7587, 38858,   238, 33505,
       20327, 23431, 41963, 32311, 46519, 28876, 26008, 25389,  9620,
        4354, 10145, 22656, 34595, 10747, 12311, 21418, 48269, 29829,
       44412, 28530, 27802,  2771, 18670, 30912,  2339, 30086, 12360,
       40297,  8191, 38658, 11196, 41105,  7738, 14167, 21283,  7019,
       30103, 22490, 32121, 49674, 42239, 29763, 40431, 33845, 26298,
       40185, 14721, 19575, 13323, 17726, 38549, 37145,  8149, 28008,
       25689, 49652, 36046, 33314, 15261, 35666, 25463, 14900,  1287,
       37301, 32474, 31969,  8189, 37260,  8559, 12123, 27538, 15306,
       46953, 16286, 44876, 18331, 49705, 35757,  3735, 33646, 43520,
       35406, 25544, 45856, 15934, 16774, 40936, 43797, 33292, 39902,
       22217, 49269, 29794, 37995, 25105, 34128, 49769, 44466, 45099,
        4803, 28054, 39912, 18238, 26093, 21215,  6969, 47370, 16309,
       25750,  9478,  6639, 13240, 33561, 38078, 49286, 32003, 18562,
       43988, 42130, 31890, 36086,  9251,  4927, 22586,  2630, 17275,
       31300,  4099, 33951, 31147, 23100, 32235,  5827,    94, 44824,
       24936, 17205, 24146, 18456, 33643, 46794, 49121, 37183, 25069,
       34618, 23524, 26742,  5195, 40964, 19095, 10642, 11515, 23311,
       39990, 27330,  6813, 18844, 48639, 16412, 37327, 34862, 41946,
       28268, 10249, 16956, 14358, 25619, 45988, 39208,  2039,  5635,
       45910, 16958, 17017,  8757, 30236, 24297, 26550, 10458, 19574,
        5895, 21152, 44183, 37512, 25682, 20463, 41862,  4949, 20475,
       21429, 28617,  1586, 38637, 26368,  1790, 43984, 43585, 15855,
       25048, 27260, 49381, 31552, 18649,   874, 48263, 20595, 37461,
       22545, 38463, 49777, 27700,  8628, 42335, 13056,  7349, 46786,
       17106,  2615, 23357, 38231, 13191,  4132,   561, 24075, 19760,
       42976, 43290, 39437, 18160, 15783,  6269,  9584,  2727, 11580,
        2227, 42709, 25007, 34913, 13922, 38157, 42912, 44936, 29283,
       20336,  8085,   947, 27460, 45188, 44319, 40590, 40194, 28133,
       45348, 24735, 36889, 14689, 12192, 29666, 40131,  7470, 25362,
       10855, 29380, 21674, 43002, 47483, 18767, 14488, 22326, 40861,
        8453, 17200, 29854, 36370, 29367, 28653, 32076,  7168,  6374,
       40928, 47805, 44307, 32149, 41355, 47912,  8376, 10323,  4387,
       19632, 19386, 46910, 23767,  9860,  7826,  2006, 24939, 40607,
        1119, 17692, 25198, 32484, 42110, 45123,  2895,   341, 17520,
       17237, 20682,  7547, 11280, 21231, 29378, 24445, 28681,  3940,
        3536, 31728,  5529, 39959, 22800, 30539, 14334,  6015,  3903,
       16838, 15554, 38233, 37049, 39396, 37504, 28311,  7093,  4524,
       21760, 12181, 25457, 19329, 20257,  2613, 18525,   317, 26124,
       41179, 35028, 39431, 34173, 32874, 49510,  4516, 22521, 29444,
        9945, 49656, 37519,  7360,  5642, 40635, 41019, 35927, 45615,
       31450, 10487, 29641, 20233, 31927, 28068, 46839, 24750,  3180,
       48729,  6663,  2478,  1376, 43564, 45106, 30487, 48851,  4281,
        3564, 19277, 44645, 34780,  6328, 23024, 26178, 13817, 26296,
       16559,  4176, 23126, 15236, 21522, 24661, 25422,  5346, 12944,
       42241, 32683, 20473, 30913, 28865, 44552, 21774, 33003,  8478,
       26599, 19662, 39927, 16346, 10031, 43160,  1329, 35599, 18999,
       28665,  8549, 35377, 21227, 37032, 44785, 26391, 39504, 37905,
       13656, 35883,  4574, 35509, 23522,   598, 35763, 43667,   129,
       38307, 27080, 45384,  5094, 24795, 34189, 40343, 36363,  3128,
       48618, 21439, 27703, 13613, 21782, 48071, 46046, 35626, 49423,
       40556, 36415, 34646, 32631, 33509,  5924, 19445, 12764, 49174,
       15044, 12916, 36862, 38478, 43838,  7647,  4348, 47274, 33542,
       19730, 29189, 37250,  2169, 41792, 44619,  6735, 30408, 27315,
       31556,  8569, 29492, 41414, 18211,  3184, 24592,  3042, 17093,
       46367, 30321, 23491,  1382, 48958, 16253,  7196, 36632, 15414,
       41456]), [4, 9, 1, 0]), (array([37315, 14106, 29220, 39412, 37615,  8641, 37821, 20651, 44177,
       33293, 46673, 25455, 25038, 31269, 17779, 48074,  8219, 47587,
       19204, 10417, 43190, 48493, 40189, 33550, 25478, 40256, 15303,
       26990, 13290, 16221, 39931, 34529,  9319, 15751, 40385, 48146,
       31926, 13671,  2557,  8121,  1107, 31941, 18837, 42588, 28403,
       41312,  5922,    54,  3639, 28834, 40148, 15477, 13837, 22618,
       38476,  2247, 15563, 48518,  5016, 32499, 44302, 20880,  7844,
         830, 34495, 15522,  9424, 28224,  4902, 22045, 32141, 39508,
        8341, 43116, 28935, 37403, 14926,  7410, 46491, 14404, 12448,
        8234, 43312, 17826, 36585, 44670,  1334, 18086, 12782, 30102,
       40213, 22287, 42144, 12141, 30874, 26551, 14299, 19343, 43873,
       37613,  4023,  8776,  4719,  7807, 37352, 47335,   218, 30542,
        7919, 34630, 12346, 35460, 27560, 28948, 33183, 30466, 17817,
         411, 31026, 11921, 30023, 33564,  7948, 31040, 13556, 49224,
       18964, 29150, 13727, 40371, 31882,  1110, 19163,  7178, 49397,
       19789,  9227, 18863, 38007,  1789, 48244,  6946, 24902, 35537,
       36726,  3963, 39550, 25901, 15638,  9577, 21836, 41538, 31041,
       30478, 24739, 29581, 39976, 14146,  4491, 40115, 38022, 24706,
       19287, 24904, 17980,  8543, 33081, 19231, 21521, 24978,  9660,
       12721, 47449,  9240, 27580, 12536, 47481, 35734,  9740, 21442,
       25379, 42821, 13408,  5131, 26696, 32103, 14768, 25200,  7452,
       39514, 17206, 13786, 17380, 10474, 19655, 30654, 37549, 31420,
        8032, 23692, 17070, 47492, 14120, 43162, 23237, 43513, 31361,
       17262,  7194, 47533, 48287, 17834, 17844,  6392, 46810, 45341,
       45021, 29637,  7633, 20957, 44218, 23493, 14317,  3254, 32047,
       30669, 35230,  9954, 25743, 35294, 45238, 11528,  6502, 37766,
       31058, 35022, 13448, 43631, 24429, 11991,  7576, 34373, 35862,
       46179,  5527, 49991, 11813, 12656, 19964,  4588, 33450,  9224,
       24908, 39518,  3135, 39755, 30761, 14400, 28945, 26702,   623,
       28446,  4157, 32296,  8681, 18258,  7182, 38611,  8533, 31990,
       47092, 48767, 27748, 39494, 43919, 32673, 11881, 27634, 10259,
       44578, 14503, 22782, 10496,  1898, 47665, 36133, 19849,  2969,
       12658, 45851, 40930, 47153,  6739, 37211, 43671, 18941, 21482,
       18411, 43491, 10077, 48649, 14680, 41700,  1650, 34487, 39378,
       35814, 13124,  4736, 14745,  8814, 29588, 33921, 33151,  3487,
       23621, 14144,  2392, 23506, 20186, 46454, 38252, 30987, 21025,
       30319,  2813, 34899, 13268,  4933, 46348, 16025, 14977,  1276,
       14319, 25213, 32660, 21001, 23860, 45027, 37200, 35108, 40300,
       46988, 23834, 43083, 40912, 19962, 33981, 43580, 25968, 41052,
        7894, 37601, 38955, 49407, 32335,  2000,  4790, 44222, 43515,
         440, 13376, 14333, 21108, 13272, 18491, 41624, 40723, 29137,
       44356, 32696,  5058, 38110, 35975, 47170, 43163, 22031,  1054,
       29916,  6218,  1365,  4238, 40604, 19250, 33397, 47241, 49942,
       27781, 26796, 15296, 33496, 41001, 13423, 45058, 38474, 20016,
       43544, 48826, 19956,  6326, 33349, 14095,  3348, 34584,  6748,
       27916, 24012, 40439, 49264, 31520, 25365, 26032,  1488, 47748,
       19794, 36513, 10386, 10562, 43629,  2554,  6726, 43791, 14983,
       23744,  5737, 10348, 24970, 23053, 34228, 12381, 16914, 39345,
       23292, 30216,  5560, 47455, 48853, 25788, 27915, 17956, 37908,
       30030, 34372, 27086, 43293, 28150, 12683,  3835,  5771, 13058,
       41000, 22267, 14556, 37884, 24547, 15081, 17023,  1490,  7204,
       18601,  8751, 41959, 29062, 23880, 24698,   178, 33855, 22566,
       19461, 45722,   181, 28639, 19326,  5890, 24982,  9778,  8817,
       21822, 35623, 13818, 33684, 32507, 47185,  4076, 34583, 25410,
        5317, 41843, 30935,  3651, 12763, 27930, 29669, 26259, 26343,
       41805, 26017,  8461, 44826, 24954, 15662, 39696, 45859, 41519,
       29044, 46463, 28761,   565, 33532, 10630, 13973, 27024,  7875,
       19226, 44282,  9066, 33625,  3404, 40472, 29485, 28526, 31522,
       37412, 44198,  9822,  6703, 36751, 30446, 33539, 38327,  9365,
       25596, 23514, 30676, 34358, 45613, 24398, 39769, 32319,  9127,
       15609, 38491, 26937, 22822, 46730, 15100, 23379, 47256,  5311,
       23317, 27009,  8801, 13701, 40822, 41133, 25363, 39974, 32083,
       23139, 48515, 29616,  6429,  7102, 45475, 13740, 11007,  1631,
       17746, 24008, 43089, 23164, 34364, 41336, 14311, 11264, 31765,
        1689, 31711, 13509, 16882, 28896, 27545,  7495, 24807, 42113,
       20057, 46679, 24220, 28476,  8475, 39674, 22421, 45977,  4533,
       35635,  3230, 30183, 20811, 39352, 12260, 19898,  8825, 44960,
       40075, 42171, 28187,  9809,  5831, 42669, 11767,  5112, 22316,
       46956,  6683, 13244, 10377, 17209, 32402, 35508, 32486, 48749,
       37937, 20088, 49494, 42925, 14129, 22145, 28297,  3025, 11640,
       26471, 34802,  5515, 19564, 26474, 16278, 34069, 16938, 35264,
       18438, 18907,  8938, 25196, 10655, 25799, 39622, 10090, 36015,
       43547,  8181, 25857, 49993, 22261,  4101,  3922, 20661,  5811,
       21398, 20764, 30083, 45332, 42143,  8977, 22372, 41051,  2568,
       40808, 22037,  1820,  6130,  6668, 28394, 24157, 45317, 44835,
       28163, 27194,  8573, 12849, 26331, 26456,   617, 43618, 22818,
       48128, 13534,  1020, 49973,  5146, 29120, 21280, 21720, 19526,
       45403,  8809, 49977, 15512,  4171, 47839, 27607, 28858, 35357,
       45329,  1320, 25611, 17002, 39490, 40615, 35070, 25697, 27512,
       44751, 21003,  6494, 30120, 29875, 40090, 42150, 43480, 43061,
       29066, 32847, 30166,  4609, 20353, 24228,  8307, 31158, 25135,
       24151,   962,   991, 41248, 37687,  3483, 31925,   206, 18415,
       36593,  5627, 45427, 39275, 22055, 14040, 16953, 31319, 23275,
        5552, 41663,  6959,  5010, 49507, 19858, 10399,  8522, 36386,
       45604, 47468,  9859,  1335,  4991, 25439, 14520, 42309,  5477,
       24543, 42622, 10617, 19749, 36311,  1960, 26045, 35470, 16625,
         179, 46554,   284, 10516, 13649, 44637, 49633,  8292, 44932,
       44350, 12131, 11176, 25489,  9619, 40752, 22993,  2391, 10265,
       47271,  6570,  8889, 14754,  9616, 39628,  2863, 13260,   407,
       11474, 28305, 27646, 47456, 18847, 40190, 36925, 42754,  6428,
       46287,  4096, 39904, 29295, 36491, 44037, 15581, 13436, 22109,
       24212, 28512, 40525, 30426, 17203, 10115, 29099, 25546, 37432,
       21217, 17107, 30895,  1338, 12785, 42934, 17076, 21721, 18630,
        9124, 16564,  9059, 11107,  1514, 34989, 17328, 44609,  2946,
       45358, 32885, 21308, 49439, 15003, 21892,  9181, 44770, 41628,
       47163,  7299, 38618, 42952, 43700,  7925, 11304, 48327, 41164,
       38943, 11122,  9966, 22692, 33229, 25037, 12363, 11255, 49671,
       46876, 12078, 48119,  6373, 44116, 29732, 32403, 32460,   557,
        6920, 22129, 33249, 47969, 47916, 48197, 37493,  9328, 12328,
       32033, 32629,  3053, 38479,  5955, 41740, 44027, 44662, 42784,
       17183, 10811, 38613, 18381, 24258, 14350, 12133, 36252, 35042,
       10847, 40241,  7788, 24360,  7183, 28063,  3859, 28400, 27751,
       15030,  7060,  2967, 46553, 48289,  3382, 10024, 45716, 48359,
       44082, 12660,  3842, 24021, 43811,  3918, 43951, 39104, 25531,
       31339, 34270, 31214, 24802, 28267,  6640, 42867, 15396, 26427,
        3665, 22716, 12435, 25106, 32659, 14023, 17100, 13864, 39926,
       13850, 39899, 36886, 17011, 34568, 45509, 39113, 42563, 35286,
       16892, 32669,  4726,  1473, 10690, 42456, 20537, 20831, 45232,
       36068, 45262, 44145, 30172,  5640, 11224, 48691,  8118, 13876,
        5414, 22458, 45982, 48852, 11276, 12270,  4906, 38617, 25190,
         843,  6954, 14528, 43219, 18603,  9696, 48986, 10326, 16214,
       24367]), [2, 7, 1, 0]), (array([17346, 10744, 37508, 24120, 21089,  9397, 47629, 28022,  8204,
       25333, 15019, 41531, 10119, 39649, 15499,  2963, 31064, 29603,
        8400, 25248, 41098, 46780, 23268, 17186, 37880, 21379,  5201,
       42776, 37589, 47524, 48403,  1411, 49638, 27445,  1545,  5305,
       36378, 22430, 46767, 16698, 11496, 11174, 11636,   784, 48625,
       31132, 26889, 33319, 43991, 45653, 32079, 26543, 12573, 26173,
        5057,  9035, 36308, 13352, 20122, 20615, 37395,  4067, 44154,
       30757,  4424,  3152, 17375, 13111, 10541, 15485, 35748, 39677,
       16643, 24953, 20647, 20020, 49715, 26010, 48645, 13252, 31631,
       44024, 47905, 18939, 39558, 33295, 28754, 22390, 17773, 21148,
       17425, 21961, 14681, 28099, 35390, 28340, 44632, 11753, 18262,
        8502, 20322, 23639, 48161, 34216, 16212, 48135, 25202, 10586,
         450, 24656, 42494, 48208, 26917, 46471, 42152, 25057, 49052,
       30140, 27968, 34757, 47334, 44211,  1331, 20672,  5288, 36127,
       32636, 33795, 45278,  2167, 19105,  7864,  1786, 10294,  4893,
       20939, 32889, 24128, 15915, 14696, 17670, 10740, 49644, 27093,
       11189, 47672,  1810, 24037, 45222, 49333,  1298, 28967, 44112,
       15133, 33768,  6807,   607, 10759, 11396, 18001, 20185, 28306,
       12791,  8276, 22424, 21534, 38851, 31543,  5547, 21075, 27671,
       24006, 45831, 47012,  3748, 33552, 46165, 16496, 28902, 32782,
       20776, 34859, 47781, 28756, 43467, 12326, 21689, 17619, 40681,
        8457, 14663,  4874, 29774, 19252, 18003, 21266, 37706, 20092,
       44033,  1717, 30310, 48477, 17354, 48048,    40, 46010, 19537,
        9987, 20484, 48139, 44229, 43455, 44832, 15544, 20495, 24130,
       48341, 38788, 40310, 39176, 31248, 42645, 45184, 43338, 29660,
       26738,  1033, 21436, 47621, 31240,  8768, 17882, 34403,  8671,
       34887, 27890,  8287, 11259, 38629, 44306, 32452, 36030, 26073,
       12581, 24452, 13190, 42384,  4817, 29816, 40528, 41403,  7668,
       25618,   240, 26980, 25104,  8668, 21036,  4594, 42275, 20962,
       25472, 44720,  9648, 29695,  7496, 15676, 40881, 40137, 26481,
       26437, 30148, 16435, 22255, 23572, 31663, 22920, 12258, 34814,
        3740, 15241,  6756, 10702, 15223,  4652, 10810,  9324, 41314,
       21905,  1771, 44384, 14402, 14883, 16676, 18312, 12172,  2586,
       41264, 42048,  7306,  2166, 18382, 17854, 20150, 45017, 34983,
       28807,  2754, 20492, 42207, 34096, 23962, 34712, 37823, 29493,
        5523, 37994,   777, 11141, 47793, 16738, 32322, 49393, 16381,
        1357, 19195, 34963, 23538, 33231, 49830,  8278,  8445, 16483,
       38458, 43713,  1391, 23839, 12353, 33554, 49184, 22635, 31370,
        5732, 34276, 21456, 27490,  4272, 13004, 12891, 38525, 12201,
       34521, 24093,  9262,  7384, 13388, 13883,  9226, 10096, 41288,
        1994, 37002, 49960, 35150, 40500,  1616, 10098, 32698, 11842,
       17899,  5054, 47212, 29903,  5911, 38524, 43348, 30788,  8045,
       28037, 36584, 26220, 38002, 24787,  4974, 34152, 35864, 14587,
       29661, 45777, 13227, 46001, 40093, 48678, 26923, 34523, 27621,
       49744, 23080,   741,  3065,  2434, 27034, 38436, 40789,  7156,
       22056,  3541, 14245, 16159, 42032, 16898, 20198, 35374, 43899,
       23560, 20209, 35736, 45966, 14711, 41078, 37483,  3881,   456,
       42693, 42325, 19557, 44659, 36553, 27692, 38408, 45626, 25450,
        1720,  1250, 15743,  7790,  8337,  1686,  9826, 25521, 38120,
       15276, 14176,  5360,  7934, 20219, 31996, 36939, 21759, 47992,
       13693, 10088, 30514, 23219, 45648, 32819,  1914,  5427,  8489,
       41225,  1506, 36765, 15927, 16846, 20594, 39507, 30762, 45650,
        9883, 26951, 24271, 40124,  6912, 47445, 22862, 42557, 32543,
       39766, 24281, 22758, 21823,  8968, 23625, 30164, 36893, 39079,
       48299, 12954, 30745, 35274, 18182,  2284, 32800, 14934, 13958,
       43913, 47960, 21579, 38857, 48164, 30949, 12917, 32168, 48496,
       27262, 11571, 40358, 42507,  4050, 21137, 14855, 29791, 14481,
       12977, 45677,  7448, 48915, 17112, 21407, 30902, 49151, 14776,
       25201,  8090, 41489, 29007,  3120, 25488, 34442,  8944, 18594,
       16925,   942, 17255, 12080, 46259, 23901, 41733, 37027,  6851,
       42753, 41445,  6161,  8174, 36237, 31445, 13218, 42292, 39749,
       37203, 25209, 13217, 30712, 41602, 12648,   140, 16543, 17995,
        7913, 10826, 19865, 26288, 45655, 38950, 44726, 25306, 48075,
       28027,  4858, 14145,  7597, 18495, 42652, 46547, 25210, 38919,
       15172, 22710, 30374, 21661,  4565,  2928, 10248, 28810, 31534,
       49435, 17223, 40178, 38482, 10251, 32305, 42833, 27794,  3212,
       48030, 19446, 15740, 33970,  2844, 48294, 14381,  1570, 23662,
       11966,  7288,   997, 32366,  1476, 39039,  8300, 34153, 12283,
       43186, 39940, 35465, 31749, 13924, 21150, 45488, 17750, 43167,
        5076, 26519,  1547, 45923,  4040, 37938, 32252, 12981, 48574,
       26905, 31241, 15655,  1037,   105, 21498, 12158,  6922, 11558,
       37799, 40762,  2795, 19581, 22395, 36537, 29410, 12841, 36868,
       24255, 31288, 20914, 18276, 24561, 11464, 18698, 26424,  8875,
        8600, 22522, 16128, 41463, 30281, 27868, 34778,  3399, 12927,
       45866, 16576,  7640, 36672, 33034, 23564,  6786, 19484, 43602,
        3257, 38929, 18321,   743, 22984, 34162, 38630, 39532,  3954,
       44411, 21627,  4379, 22396, 34923, 21328, 36039, 28818, 13332,
       23635,  2600, 19628,  1611, 15710,  4953, 11791,  3072,  6020,
        4360, 28098, 10711,  1464, 30439,  5301,   840, 20197, 18527,
       46859, 16089, 47226, 35587, 15195, 41110, 45466,  6453, 24263,
       38522, 41844, 16594, 33624, 37491, 41525, 46966, 24979, 16829,
       27871, 19539, 20244,  6122, 47968, 19869, 12183, 46078, 16385,
       49045, 20546, 26881, 46147,  2422, 33206, 33907, 30104, 14964,
        6731,  9549, 15248, 21564, 47981, 23854, 41274, 11963,  3689,
       21488,  2421,  5266, 45373, 19210, 26081, 28428, 47771, 15502,
        7530,  4523,  8833, 23866, 32251, 14309,  4468,  2258, 14407,
       41751, 24534, 13660,  1950, 27155, 34828,   637,  7757, 18765,
       13880, 49169, 46420,  2598, 31880, 12915, 37324,  5516,  5037,
       46548, 42674, 33242, 14567,  8160, 40654, 44001, 40807,  2824,
       44296,  7700, 17998, 32378,  7663, 13561, 34746,   371, 12461,
        2355, 23449, 19682, 15931, 16529, 43442, 11440, 22308, 22907,
       31726, 34381, 11238,  5702, 18605, 26007, 27638, 49530, 38683,
       22665, 21815, 44468, 27873, 37165, 39213, 46113, 30749, 35524,
       11305, 38586, 36180, 39723, 31246, 47401, 34547, 46434, 44866,
       47763, 17963, 27522, 34676, 18315, 38898, 20606, 34711, 15884,
       40516,   189, 38254, 49245, 20622, 23096, 31600, 27071, 33700,
       12922, 46782, 16002, 27426,  3361, 36286, 30078, 20694, 20916,
       33510,  2451, 40649, 17717, 31888, 20980,  1243, 38937, 21605,
        4030,  3713, 33336,  5194, 44470, 38518, 41848, 17947, 44742,
       45085, 28273, 35285, 36134,  2171, 39080, 42844,  1926, 43867,
        7537, 19527, 36573,  7972,  3979, 30904, 16553, 13043, 13225,
         415,  9384, 38052,  8102, 16225, 40802, 37881, 36004, 26764,
       19103, 44884,  2855, 40876, 19347,  9495, 20985, 18200, 49555,
        2932,  2413, 31648,  2652,  1999, 16420,   687, 20708, 40843,
       46294, 28408, 11721, 27179, 22080, 20611, 24589, 40612, 35059,
       18659, 42231, 28401,  2010, 14413, 28126,  4559, 39477, 20035,
        5719, 41467, 34998, 44143, 13594, 21647, 47799,  5216,  1216,
       25288,  8842, 14727, 19639, 33076, 36358, 47089,  9759, 15240,
       39372,  3900, 24721, 11762, 22628, 17763, 28467, 12993, 41234,
        9287, 26722,  6923, 25083, 31947, 17838, 16920,  2060, 34722,
       27147, 16939,  8098, 43944, 21533, 20105, 16371, 31004, 18680,
       14124]), [5, 8, 1, 0])]
Competition
DC 0, val_set_size=1000, COIs=[6, 3, 1, 0], M=tensor([6, 3, 1, 0], device='cuda:0'), Initial Performance: (0.242, 0.04443864643573761)
DC 1, val_set_size=1000, COIs=[4, 9, 1, 0], M=tensor([4, 9, 1, 0], device='cuda:0'), Initial Performance: (0.218, 0.044824156880378725)
DC 2, val_set_size=1000, COIs=[2, 7, 1, 0], M=tensor([2, 7, 1, 0], device='cuda:0'), Initial Performance: (0.243, 0.04442102932929993)
DC 3, val_set_size=1000, COIs=[5, 8, 1, 0], M=tensor([5, 8, 1, 0], device='cuda:0'), Initial Performance: (0.244, 0.044828352451324466)
D00: 1000 samples from classes {0, 1}
D01: 1000 samples from classes {0, 1}
D02: 1000 samples from classes {0, 1}
D03: 1000 samples from classes {0, 1}
D04: 1000 samples from classes {0, 1}
D05: 1000 samples from classes {0, 1}
D06: 1000 samples from classes {3, 6}
D07: 1000 samples from classes {3, 6}
D08: 1000 samples from classes {3, 6}
D09: 1000 samples from classes {3, 6}
D010: 1000 samples from classes {3, 6}
D011: 1000 samples from classes {3, 6}
D012: 1000 samples from classes {9, 4}
D013: 1000 samples from classes {9, 4}
D014: 1000 samples from classes {9, 4}
D015: 1000 samples from classes {9, 4}
D016: 1000 samples from classes {9, 4}
D017: 1000 samples from classes {9, 4}
D018: 1000 samples from classes {2, 7}
D019: 1000 samples from classes {2, 7}
D020: 1000 samples from classes {2, 7}
D021: 1000 samples from classes {2, 7}
D022: 1000 samples from classes {2, 7}
D023: 1000 samples from classes {2, 7}
D024: 1000 samples from classes {8, 5}
D025: 1000 samples from classes {8, 5}
D026: 1000 samples from classes {8, 5}
D027: 1000 samples from classes {8, 5}
D028: 1000 samples from classes {8, 5}
D029: 1000 samples from classes {8, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.07661269408464431) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07654814468324185) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.09640297073125839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.07325242793560029) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.07945466989278793) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.08400962515175342) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.278, 0.12523775127530098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.09349438312649727) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.377, 0.09021134239435195) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.09025510634481906) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.15705580693483354) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.14240596529841423) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.11596032911539078) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.412, 0.11271193671226501) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.19727019335329532) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.16480454625189303) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.13193382388353347) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.12182052563875914) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.20265718460083007) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.21888766702078283) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO5']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.12846777415275573) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.140342478916049) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.21342508929222823) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.24219384985789658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.12581581400334835) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.14166802729293704) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.2270521096549928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.27305388302356004) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.11846597637236118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.15023153780028223) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.24203452307730913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.26939495775196703) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.388, 0.12462650395929814) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.17387545125558973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.2579819469116628) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29477349990280344) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.39, 0.14789085798710586) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.19303536528348922) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.267256860435009) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29640268966881556) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.37, 0.14851210363954306) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.18982051305659114) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.2791272814180702) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.32740656919078903) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.15376150463148952) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.17739704525098204) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.2831452851369977) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3219985390071524) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.14906439793109894) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.20037036059889943) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.31269943139329553) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.31018744532438) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.1569977222830057) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.20456270048394798) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.3097561857998371) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.320400069300551) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4, 0.14811746790260077) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.20482745793834328) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.2844408188611269) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.33842417648562695) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO3', '(DO0']
DC 2 --> ['(DO2']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.14386818970367313) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.19776326462626456) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.2824747913479805) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.3509555250343983) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.15879193133860828) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.20542398431152106) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.28964401492103936) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.33427575846167745) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.395, 0.1340394599735737) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.19681755885109306) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.31401582246646287) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.33689308744372104) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.406, 0.14167974326014518) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.1879777118638158) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.31150976188480856) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.33796656001050723) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.399, 0.13834849840030075) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.1990556422844529) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.2976327456459403) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.327462995605194) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.406, 0.13481522457301617) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.2097464298289269) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.31176387302577496) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3351097931523109) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.1296702182739973) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.20845887804683297) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.30313393192738297) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.3500386247083661) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.404, 0.1359424711763859) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.477, 0.2092889818660915) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.2989666960537434) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.3295234278558637) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.404, 0.14306917793303728) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.234892349393107) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.3005758037529886) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.3316002800039714) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.399, 0.14974810869991778) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.22327381694177165) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.28791885712370274) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.34540718945805565) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO4', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.13855322375893592) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.2103223379473202) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2614233381357044) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.33284264135343256) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.397, 0.1554116261973977) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.23166357744298877) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.2822694267220795) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.31103640926862136) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.414, 0.15414098528772593) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.2192023781184107) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.28651084794290366) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.41656259073165713) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.412, 0.14740326567739248) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.2033724361974746) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.3125077411234379) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3483515387904481) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.4, 0.13440883610397578) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.455, 0.19110835985466837) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2759255507439375) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.334191621279635) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO3', '(DO1']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO4']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.402, 0.14634772650897504) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.458, 0.19643070898391307) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.2934264206960797) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3552559633151977) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.406, 0.13566934490203858) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.1938421790068969) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.2806850909255445) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.31433461333275775) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.16002342265099287) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.455, 0.201566500662826) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.27843353617563843) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3043370987721719) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.403, 0.1676571141630411) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.19289084255229683) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.3209043859820813) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.29898542969091796) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.414, 0.1533103419318795) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.18365641978615896) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.27407246227562426) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3276335444805445) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO0']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.15076112741231917) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.465, 0.19306907402630896) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.2986684958972037) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.33278928859950974) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.409, 0.1800819434300065) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.22803566214488818) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.2830679712481797) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.33176187749212843) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.16465339220315217) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.18364069886505605) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.28957929712533953) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3214667870735284) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.396, 0.1862295311652124) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.20537952718185262) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.2584320536889136) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.33559890278032983) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.18141478003934025) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.2573384885995183) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.30650516852736476) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.36277116534044035) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.401, 0.17806619422882794) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.21440491841407494) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.27941040381789206) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.3273681647563935) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.403, 0.1703040035367012) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.23186082356912083) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.2671235649921) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.33094918780072474) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.407, 0.17071770748496057) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.2147401677609887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.29438757172971963) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.337706051707064) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.1788861323520541) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.2233409703532234) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.28239737781882285) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.31888942913815843) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.403, 0.18002160538733006) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.1813652432183735) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.26874194884300234) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.28105420114030133) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.18171740266680717) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.2041845447833184) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.2731604818254709) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.32291103902141916) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.414, 0.18447005375102163) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.19288941281894223) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.2513611997961998) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3346411893485929) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.407, 0.14299646154046058) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.23978685877704992) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.28145974727347495) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3037918626168393) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.415, 0.1562790735512972) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.2265144782504067) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.3019949389696121) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2854965086688753) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.18425048004090785) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.2595546960220672) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.27289400836452843) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.32120112006120327) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.242, 0.04443864643573761), (0.354, 0.07661269408464431), (0.275, 0.07945466989278793), (0.377, 0.09021134239435195), (0.41, 0.11596032911539078), (0.408, 0.13193382388353347), (0.398, 0.12846777415275573), (0.394, 0.12581581400334835), (0.378, 0.11846597637236118), (0.388, 0.12462650395929814), (0.39, 0.14789085798710586), (0.37, 0.14851210363954306), (0.398, 0.15376150463148952), (0.411, 0.14906439793109894), (0.408, 0.1569977222830057), (0.4, 0.14811746790260077), (0.378, 0.14386818970367313), (0.398, 0.15879193133860828), (0.395, 0.1340394599735737), (0.406, 0.14167974326014518), (0.399, 0.13834849840030075), (0.406, 0.13481522457301617), (0.405, 0.1296702182739973), (0.404, 0.1359424711763859), (0.404, 0.14306917793303728), (0.399, 0.14974810869991778), (0.411, 0.13855322375893592), (0.397, 0.1554116261973977), (0.414, 0.15414098528772593), (0.412, 0.14740326567739248), (0.4, 0.13440883610397578), (0.402, 0.14634772650897504), (0.406, 0.13566934490203858), (0.41, 0.16002342265099287), (0.403, 0.1676571141630411), (0.414, 0.1533103419318795), (0.405, 0.15076112741231917), (0.409, 0.1800819434300065), (0.408, 0.16465339220315217), (0.396, 0.1862295311652124), (0.405, 0.18141478003934025), (0.401, 0.17806619422882794), (0.403, 0.1703040035367012), (0.407, 0.17071770748496057), (0.41, 0.1788861323520541), (0.403, 0.18002160538733006), (0.408, 0.18171740266680717), (0.414, 0.18447005375102163), (0.407, 0.14299646154046058), (0.415, 0.1562790735512972), (0.408, 0.18425048004090785)]
TEST: 
[(0.2285, 0.04340292808413505), (0.35375, 0.07347347635030746), (0.27775, 0.07620753061771393), (0.3925, 0.0865502817928791), (0.4165, 0.11079292529821395), (0.403, 0.12526093649864198), (0.401, 0.12224474203586579), (0.3945, 0.11966084939241409), (0.38425, 0.1124050145149231), (0.38725, 0.11857743835449219), (0.3925, 0.13983969384431838), (0.38675, 0.14101330214738847), (0.39975, 0.1462789415717125), (0.42025, 0.14004940390586854), (0.40575, 0.14686847776174544), (0.404, 0.1386795619726181), (0.38025, 0.13400667655467988), (0.40575, 0.15049056893587112), (0.40625, 0.12811378186941147), (0.41825, 0.13574945586919784), (0.4015, 0.1327925960421562), (0.41175, 0.1277933230996132), (0.40875, 0.1228965649008751), (0.4145, 0.12823222970962525), (0.41125, 0.13586860126256942), (0.41225, 0.14180636107921601), (0.4285, 0.13200641638040542), (0.41425, 0.1480312581062317), (0.41975, 0.14762557685375213), (0.4155, 0.13975916188955306), (0.41575, 0.12945627224445344), (0.41325, 0.13979653817415238), (0.41175, 0.12895563226938248), (0.421, 0.15132844644784926), (0.418, 0.15778289157152176), (0.424, 0.14115130561590195), (0.41875, 0.14400228369235993), (0.41825, 0.17128049516677857), (0.4175, 0.1553829777240753), (0.4025, 0.17736535459756853), (0.4145, 0.17367788046598434), (0.41725, 0.16963999336957933), (0.41125, 0.1617773300409317), (0.41825, 0.16272103399038315), (0.42, 0.16981344586610794), (0.41575, 0.17120299512147905), (0.41925, 0.17161890214681624), (0.42025, 0.17242608499526976), (0.42275, 0.13324748760461808), (0.4255, 0.14831519603729248), (0.4225, 0.17261117631196976)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.00      0.00      0.00      1000
           3       0.42      0.78      0.55      1000
           6       0.42      0.91      0.58      1000

    accuracy                           0.42      4000
   macro avg       0.21      0.42      0.28      4000
weighted avg       0.21      0.42      0.28      4000

Competition_DC_1
VAL: 
[(0.218, 0.044824156880378725), (0.25, 0.07654814468324185), (0.252, 0.08400962515175342), (0.381, 0.09025510634481906), (0.412, 0.11271193671226501), (0.434, 0.12182052563875914), (0.467, 0.140342478916049), (0.461, 0.14166802729293704), (0.474, 0.15023153780028223), (0.469, 0.17387545125558973), (0.466, 0.19303536528348922), (0.471, 0.18982051305659114), (0.476, 0.17739704525098204), (0.475, 0.20037036059889943), (0.475, 0.20456270048394798), (0.479, 0.20482745793834328), (0.476, 0.19776326462626456), (0.478, 0.20542398431152106), (0.47, 0.19681755885109306), (0.462, 0.1879777118638158), (0.467, 0.1990556422844529), (0.474, 0.2097464298289269), (0.471, 0.20845887804683297), (0.477, 0.2092889818660915), (0.469, 0.234892349393107), (0.463, 0.22327381694177165), (0.473, 0.2103223379473202), (0.46, 0.23166357744298877), (0.461, 0.2192023781184107), (0.469, 0.2033724361974746), (0.455, 0.19110835985466837), (0.458, 0.19643070898391307), (0.46, 0.1938421790068969), (0.455, 0.201566500662826), (0.466, 0.19289084255229683), (0.461, 0.18365641978615896), (0.465, 0.19306907402630896), (0.462, 0.22803566214488818), (0.466, 0.18364069886505605), (0.47, 0.20537952718185262), (0.46, 0.2573384885995183), (0.462, 0.21440491841407494), (0.47, 0.23186082356912083), (0.46, 0.2147401677609887), (0.471, 0.2233409703532234), (0.466, 0.1813652432183735), (0.472, 0.2041845447833184), (0.47, 0.19288941281894223), (0.464, 0.23978685877704992), (0.473, 0.2265144782504067), (0.472, 0.2595546960220672)]
TEST: 
[(0.22, 0.043847873479127886), (0.25, 0.07381470426917076), (0.2505, 0.08048938992619514), (0.386, 0.08601839923858642), (0.4165, 0.10769178631901741), (0.44125, 0.11695740818977356), (0.4685, 0.13551646131277084), (0.4675, 0.13753553491830825), (0.4745, 0.14645577955245973), (0.47025, 0.1688867705464363), (0.47025, 0.18691782051324846), (0.4715, 0.18412074053287505), (0.474, 0.17214942955970763), (0.47625, 0.19219536703824996), (0.47375, 0.19791988199949265), (0.4765, 0.1980437103509903), (0.473, 0.18979322528839113), (0.478, 0.19969263136386872), (0.471, 0.19138740587234498), (0.46525, 0.182431585252285), (0.46875, 0.19263064330816268), (0.4745, 0.20224344474077224), (0.47325, 0.19972367733716964), (0.4765, 0.20046433037519454), (0.4735, 0.22535573476552964), (0.47, 0.2148646547794342), (0.475, 0.20328205305337907), (0.465, 0.22169152975082398), (0.46775, 0.21357261157035828), (0.4725, 0.19417308688163756), (0.46425, 0.1856329609155655), (0.47, 0.18987459778785706), (0.46575, 0.18874844187498094), (0.46225, 0.19487351047992707), (0.47, 0.185402683198452), (0.4655, 0.17700550293922424), (0.46975, 0.18619784873723982), (0.46425, 0.222285160779953), (0.4695, 0.17748342078924179), (0.473, 0.19954346042871476), (0.46575, 0.24958326363563538), (0.4675, 0.20688105130195616), (0.4735, 0.22731203615665435), (0.46425, 0.21064735531806947), (0.473, 0.217827427983284), (0.46875, 0.1763574988245964), (0.4765, 0.19638756567239762), (0.47, 0.1871941089630127), (0.46875, 0.23114412665367126), (0.47325, 0.21892288202047347), (0.4745, 0.25075363492965697)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.00      0.00      0.00      1000
           4       0.49      0.99      0.65      1000
           9       0.46      0.91      0.61      1000

    accuracy                           0.47      4000
   macro avg       0.24      0.47      0.32      4000
weighted avg       0.24      0.47      0.32      4000

Competition_DC_2
VAL: 
[(0.243, 0.04442102932929993), (0.25, 0.09640297073125839), (0.278, 0.12523775127530098), (0.396, 0.15705580693483354), (0.42, 0.19727019335329532), (0.426, 0.20265718460083007), (0.421, 0.21342508929222823), (0.43, 0.2270521096549928), (0.427, 0.24203452307730913), (0.435, 0.2579819469116628), (0.429, 0.267256860435009), (0.429, 0.2791272814180702), (0.436, 0.2831452851369977), (0.437, 0.31269943139329553), (0.437, 0.3097561857998371), (0.439, 0.2844408188611269), (0.433, 0.2824747913479805), (0.44, 0.28964401492103936), (0.43, 0.31401582246646287), (0.433, 0.31150976188480856), (0.438, 0.2976327456459403), (0.437, 0.31176387302577496), (0.44, 0.30313393192738297), (0.444, 0.2989666960537434), (0.441, 0.3005758037529886), (0.442, 0.28791885712370274), (0.437, 0.2614233381357044), (0.44, 0.2822694267220795), (0.443, 0.28651084794290366), (0.441, 0.3125077411234379), (0.437, 0.2759255507439375), (0.441, 0.2934264206960797), (0.439, 0.2806850909255445), (0.444, 0.27843353617563843), (0.446, 0.3209043859820813), (0.438, 0.27407246227562426), (0.447, 0.2986684958972037), (0.447, 0.2830679712481797), (0.445, 0.28957929712533953), (0.439, 0.2584320536889136), (0.445, 0.30650516852736476), (0.441, 0.27941040381789206), (0.445, 0.2671235649921), (0.444, 0.29438757172971963), (0.451, 0.28239737781882285), (0.447, 0.26874194884300234), (0.443, 0.2731604818254709), (0.447, 0.2513611997961998), (0.445, 0.28145974727347495), (0.447, 0.3019949389696121), (0.451, 0.27289400836452843)]
TEST: 
[(0.246, 0.04339280295372009), (0.25, 0.09244508910179138), (0.2815, 0.12015084260702133), (0.38875, 0.15123850136995315), (0.42325, 0.18991599303483964), (0.4235, 0.19383455294370652), (0.42825, 0.20367060768604278), (0.42975, 0.21929274678230284), (0.4355, 0.23072633934020997), (0.44425, 0.245837830722332), (0.442, 0.25503220057487486), (0.43475, 0.2687557027339935), (0.44625, 0.27152720391750335), (0.44375, 0.3034352866411209), (0.44575, 0.29895502924919126), (0.4435, 0.27215663266181944), (0.4465, 0.27079713547229767), (0.44525, 0.2797148900032043), (0.4385, 0.3019742078781128), (0.44575, 0.2963073241710663), (0.44575, 0.28582518875598906), (0.4425, 0.29595067155361177), (0.4465, 0.29006805408000946), (0.44575, 0.28555374312400816), (0.44825, 0.28799543309211734), (0.44925, 0.27083354568481444), (0.44375, 0.24548497760295868), (0.444, 0.26920243334770205), (0.44525, 0.2717694520950317), (0.4495, 0.29883013701438904), (0.447, 0.2652339768409729), (0.449, 0.28417994344234465), (0.4495, 0.26804008102416993), (0.4495, 0.26721519249677655), (0.448, 0.31058620965480804), (0.4455, 0.26399966794252394), (0.45275, 0.28776334691047667), (0.444, 0.27219732534885405), (0.44925, 0.2814755718111992), (0.4475, 0.24820995008945465), (0.44925, 0.29421694147586824), (0.44825, 0.26953321051597595), (0.44425, 0.25601214629411695), (0.4465, 0.28205868005752566), (0.44975, 0.27038702023029326), (0.44825, 0.25750741201639177), (0.44825, 0.2636362808942795), (0.45275, 0.2410560103058815), (0.448, 0.26974331378936767), (0.45, 0.29249433159828186), (0.45, 0.26181478476524356)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.00      0.00      0.00      1000
           2       0.34      0.95      0.50      1000
           7       0.70      0.85      0.77      1000

    accuracy                           0.45      4000
   macro avg       0.26      0.45      0.32      4000
weighted avg       0.26      0.45      0.32      4000

Competition_DC_3
VAL: 
[(0.244, 0.044828352451324466), (0.392, 0.07325242793560029), (0.46, 0.09349438312649727), (0.474, 0.14240596529841423), (0.469, 0.16480454625189303), (0.469, 0.21888766702078283), (0.47, 0.24219384985789658), (0.477, 0.27305388302356004), (0.471, 0.26939495775196703), (0.477, 0.29477349990280344), (0.477, 0.29640268966881556), (0.475, 0.32740656919078903), (0.477, 0.3219985390071524), (0.475, 0.31018744532438), (0.475, 0.320400069300551), (0.47, 0.33842417648562695), (0.481, 0.3509555250343983), (0.474, 0.33427575846167745), (0.475, 0.33689308744372104), (0.48, 0.33796656001050723), (0.475, 0.327462995605194), (0.473, 0.3351097931523109), (0.476, 0.3500386247083661), (0.481, 0.3295234278558637), (0.478, 0.3316002800039714), (0.477, 0.34540718945805565), (0.474, 0.33284264135343256), (0.475, 0.31103640926862136), (0.476, 0.41656259073165713), (0.472, 0.3483515387904481), (0.474, 0.334191621279635), (0.477, 0.3552559633151977), (0.475, 0.31433461333275775), (0.477, 0.3043370987721719), (0.472, 0.29898542969091796), (0.475, 0.3276335444805445), (0.478, 0.33278928859950974), (0.479, 0.33176187749212843), (0.477, 0.3214667870735284), (0.474, 0.33559890278032983), (0.477, 0.36277116534044035), (0.47, 0.3273681647563935), (0.479, 0.33094918780072474), (0.477, 0.337706051707064), (0.48, 0.31888942913815843), (0.476, 0.28105420114030133), (0.478, 0.32291103902141916), (0.477, 0.3346411893485929), (0.477, 0.3037918626168393), (0.48, 0.2854965086688753), (0.477, 0.32120112006120327)]
TEST: 
[(0.2495, 0.044014592796564105), (0.40075, 0.07023401990532875), (0.46475, 0.08952063956856728), (0.47275, 0.135820614695549), (0.476, 0.15784210336208343), (0.47525, 0.20845930409431457), (0.47475, 0.2282492750287056), (0.48075, 0.2596552656888962), (0.47775, 0.25685111624002455), (0.48, 0.28122049593925474), (0.47975, 0.28348508703708647), (0.4775, 0.3122968578338623), (0.47875, 0.30703748667240144), (0.478, 0.29499329364299776), (0.47875, 0.3069189611673355), (0.4775, 0.32238943457603453), (0.48125, 0.3351306792497635), (0.48125, 0.32002444756031034), (0.48125, 0.3203385022878647), (0.48225, 0.3238044285774231), (0.4815, 0.3109853951931), (0.4795, 0.3164355581998825), (0.4795, 0.3330020550489426), (0.4795, 0.3157064325809479), (0.481, 0.3223124290704727), (0.48075, 0.32832982325553894), (0.4785, 0.3122135469913483), (0.48175, 0.2980784100294113), (0.48225, 0.3971365647315979), (0.481, 0.3296705913543701), (0.48175, 0.3182362228631973), (0.48, 0.34184331715106964), (0.47975, 0.30084889006614685), (0.479, 0.29000491762161257), (0.47675, 0.28347239208221436), (0.4815, 0.31368222415447233), (0.4815, 0.31669794845581056), (0.481, 0.31457761192321776), (0.482, 0.3044179995059967), (0.48075, 0.3184809573888779), (0.484, 0.3472733117341995), (0.4765, 0.31231359648704526), (0.48175, 0.3198040039539337), (0.4825, 0.3258276290893555), (0.48175, 0.3029968949556351), (0.48225, 0.26890351724624634), (0.48175, 0.3064676060676575), (0.48175, 0.3178592348098755), (0.4815, 0.2912901680469513), (0.48225, 0.2748877217769623), (0.48125, 0.3089771515130997)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.00      0.00      0.00      1000
           5       0.72      0.95      0.82      1000
           8       0.36      0.97      0.53      1000

    accuracy                           0.48      4000
   macro avg       0.27      0.48      0.34      4000
weighted avg       0.27      0.48      0.34      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [31]
name: no-alliance-31
score_metric: contrloss
aggregation: <function fed_avg at 0x7a421af21c10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=31
Partitioning data
[[2, 8, 0, 7], [5, 3, 0, 7], [1, 9, 0, 7], [4, 6, 0, 7]]
[(array([32045, 13820, 18987, 26795, 19845,  5574,  2772, 11006, 30325,
       24699, 30286, 16712, 47074, 45079,  7576, 28872, 34555, 22733,
        8186,  4375,  9827, 25022, 17379, 46652,  2007, 28176, 28479,
       39508, 20226, 26282, 39765, 49710, 20651,  2387, 25296,  8627,
       32706, 31375, 19963, 21987, 15380,   300, 48344, 44660,  3019,
       29517,  2133, 48242, 46589,  4183,  1122, 31707, 36186, 48270,
       13655, 43715, 20563,  4806,  7868, 46214, 29351, 48835, 20038,
        1523, 19163, 49756, 36242, 31293, 40832, 24734, 48314,  1129,
       48472, 46244, 15888, 34203, 17817, 19231,  6805, 46633,  5531,
       47632, 43795,  6614, 43469, 48739,    42, 42854, 34747,  4209,
       44351, 11050, 27399,  7962, 35686, 37855, 33183, 43541, 33137,
       27169,  5836, 44186, 12720, 46810, 14085, 44938, 46759, 17329,
       30751, 16530, 22732, 35958, 15754, 41604, 43106, 19364,  9498,
       33134,  9255, 20850, 40720, 34580, 11500, 23775, 32730, 43959,
       27980, 13698, 40281, 17981, 39466, 35740, 40791, 23650, 32981,
       27592, 15054,   271, 21034, 31501,  1789, 27877, 37754, 43930,
       20323, 26648, 27091, 22275, 48549, 27368, 25130, 46870, 43287,
        7662, 39145, 21162, 48300, 49972, 42856,  2938,  9579, 28568,
       25998, 40112, 45359, 34363, 47411,  3796, 18733, 48989, 14423,
       16165, 23653, 22570,  9089,  7761, 12752, 36245, 40917, 21057,
       24555, 15944,  6976,  9553,  1372, 32332, 49274, 19296, 20212,
       31614, 42988, 28164, 18489, 17481, 20308, 22376, 47593, 23074,
        6445, 23038, 43658, 35901, 35787, 23705,  4189, 34050, 33550,
       30003, 15820, 29076, 12070,  5554, 29682, 34959, 30596, 13413,
       22877, 29678, 34222, 13177, 28291, 27744, 18855, 49395,  9403,
       14887, 16786,  1576,   924, 29925,  2224, 34663, 18248, 34266,
        3769, 45006, 26730, 48095,  3446,  8395, 12694, 46972,  2833,
       36469, 23237, 15477,  1438,  7807,   303,  1838, 12258, 12614,
       16697, 33195, 24083, 34342,  7716,  2735, 10613, 13641, 39658,
        5687, 29782, 40255, 28424, 32034, 10763,  4863,   716,  7973,
       43755, 23583, 15011, 47993, 26563, 31950, 28154, 48911, 32138,
       30076, 23839, 10663, 48168,  1060, 39011,  9844, 19910,  5272,
       48698,  2344,  9226, 34655, 35266, 34718,  8961, 13674, 20612,
       45045, 44577, 48604, 22426, 15743, 30788, 36670, 19788,  1957,
       45446, 36571, 41413, 33322,  8852, 14969, 46138, 44594, 47713,
       45017, 20500, 18274, 29056, 36484, 21919,  2797, 45372, 45161,
       35078, 14971, 43916, 15648, 24142, 49379, 20898, 24140, 26981,
       27919, 10785, 26703, 18177, 12206, 15721, 25472, 15768, 31258,
       35617,  7668, 49042, 15617, 26337, 42506, 15927, 32618, 48959,
       42236, 18055, 17063,  7109, 11095,    69,  7248, 19622,  9623,
       21880, 25088, 46210,   259, 16479, 48457, 25118, 39106, 34636,
       49696, 42799, 37858,  4287, 28269, 48541, 37681, 44059, 22504,
        3268, 29747, 11368, 40070, 14183, 14402, 23793,  1720, 31787,
       46582, 24260,  3800,  2687, 13116, 37023, 16476, 27896,  9682,
       41002, 47652, 46926, 35489, 19149,  1229,  6646,  8495, 36939,
       14208, 18236, 21608, 48107,  3269,  6346, 11265,  2333, 12605,
       26055, 21840, 33109, 24748, 31182, 18036, 29868, 42316, 39579,
       25513, 48774, 27057, 37420, 30504, 40093, 18565, 40122, 20596,
        9514, 22551,  3670, 37411,  6435, 48670, 36933,  1397, 13266,
       13425,  6823, 40344,  3881, 37398, 45025, 18718,  3359,  2740,
       34319, 28450, 17729, 35236,  2368, 28396, 32663, 17345,  1439,
       17416, 42822,   723, 12935,   901,  4979, 18675, 27864, 17588,
       42247, 12731,  8787, 38332, 27337, 35177, 10346, 32562, 30872,
       29530, 43334, 38322, 14728, 26038, 30147, 41714, 27839, 33815,
       43381, 49654, 34208, 48752, 42245, 28940, 16156,  4303, 44981,
       48481, 19415, 11832, 26672,   566, 40241, 36771, 14708, 44961,
        1878, 20267, 46486, 32905, 23010,  8941, 34250, 49869, 13707,
       18925,  3184,  8102, 46586, 39959, 19974, 24764, 22769, 14456,
        1454, 24792, 35059,  7647, 46224, 47242, 40159, 49535, 39288,
       12009, 43944,  7464, 35354, 39709, 41222, 32483, 10493, 13016,
       45827,  8759,  1243, 46041, 25352, 15628, 18402, 41741,  3128,
       23182,  1188, 31087,  4721, 41842,  4141,  9800, 45364,  5371,
         757, 10548, 38586, 34856, 20831, 39793, 33767, 16004, 16920,
       34259, 42044,  6015, 28653, 34601, 24971, 29412, 46749, 13578,
       46277, 39553, 36337, 13549, 25144, 39780, 34998, 33923, 25816,
       14698, 29944, 22622,  1205, 16313, 31358, 10741, 17103,  9860,
       36501, 46116, 19395, 35281, 24285, 47486, 29140, 36289, 10872,
       17387, 41040, 18538, 36981, 49891, 35731, 48850, 29666, 23371,
       37322, 38479,  2446, 35738, 32346,  4571, 34780, 43292, 15580,
       17080, 49513, 33243, 41030, 42364, 27426,   694, 11240, 44875,
       24589,  4079, 43211, 48246, 47444, 35858, 40022, 10812, 33363,
       38342, 38635, 30205, 23507, 43831,  9390, 22249, 32365, 27855,
       24859, 10365, 19004, 47771, 27431, 33700, 42151,  9292, 42808,
       42797, 17438, 46689, 33229, 23175,  2169,  3073, 10690, 16497,
       22293,  4577, 12010, 24661, 48968,  4097, 14407, 33087, 34454,
       16169, 31249, 37664, 43992, 25930, 47799, 30280, 47274, 30429,
       24383, 46200, 24600, 42621, 42272, 17026, 15044, 35700, 24404,
       33943, 10401, 26412, 30408, 43054, 26913, 36148, 36634, 10713,
       30415, 41792,  1168,   752,  8623, 35079, 42883,  5828, 40906,
       35411, 46839,  9096, 26286,  5969, 29293, 28400, 34607,   348,
        8242, 11849, 36789, 47215, 38098, 41266, 42945, 17458, 11691,
       43301,  4621, 31976, 19639, 26178,  4778, 26007,  8200, 27104,
       19982, 13011, 25212, 38716, 18080, 10516,  6947, 25023,  8072,
       34026, 16840, 12130, 18973, 35316, 15120, 38286, 45034, 39231,
       48041, 31851, 41462, 21410, 25917, 39767, 37082, 19250, 28566,
        3606, 31965, 45365,  5200, 12535, 43717, 12980, 39164, 22599,
       27505, 32700, 13333, 27327, 43272,  5058, 33902, 41959, 22541,
        3844, 13146, 32586, 38803,  8115,  2360, 20062, 41704, 19300,
       39944, 37724, 42330, 44374, 37286, 32282, 29461, 38104, 25411,
       19074, 17913, 40055, 33299, 38938,  9279, 23699, 16224, 13039,
       18227, 43227, 24794, 44406, 36619, 36773, 22554, 45409,  3920,
       18723, 31980, 22725, 22324, 48596,  8966, 41660,  2611, 45643,
        3284,  2834, 34487, 15079, 12838,  7945, 32704, 11611, 28117,
       48720, 27998,  8498,  4747, 42299,  1931, 19636,  9456, 37980,
       41449, 49451, 23794,  7291, 28249, 12658, 19010,  8640, 31871,
        9079, 28296,  6839, 24612, 46711, 37763, 42052, 37802,  8694,
       33884,  5996, 35164, 24406,  2612, 32995, 49772, 12220,  1679,
       40374,  6998,  4238, 42126, 32032, 30498, 24499,  8901,  2502,
       34113, 34239, 11621, 14671, 22592, 42381, 32948, 29588, 28911,
       38110, 33343, 33382, 21317,  7624,   589,  4900, 22723, 12529,
       13284,  8595,  8909, 43445, 34380, 13336, 17587, 27039,   739,
       26389, 10672, 44958, 16565, 26037, 20309,  5044,  9422, 19037,
       45352,  5989, 42137, 38611,  3611,  7611,  1650,  7185, 29245,
       23643, 16091,  4820, 29789, 35767, 33843, 16000, 19121, 11741,
       12842, 49341, 14630, 15167, 27574, 22204,  1561, 28914, 19691,
       11323, 32646, 18481,  7939,   842,  8461,  4499, 42555, 38249,
        2458, 39314, 35982, 30401,  8604, 12566, 29823,  7641,  9958,
       28710, 12415, 15332, 22359, 34708,  4386, 44413, 26661, 14955,
       13818, 44262,  2397, 33648, 35120, 21843, 20356, 17775, 39835,
       36017, 31873,  2201, 33074,  8924, 26820,  6372, 23585, 43844,
       33527, 35988, 46098,  2776, 28987, 18326, 17489, 11377, 13525,
       48420]), [2, 8, 0, 7]), (array([16110, 17711, 49675, 41400,  8457, 27977,  8989,  1634, 18317,
       36308, 37436,  8497, 19007, 22355, 20644, 42846, 23335, 17305,
        8824, 46381,   515, 48083, 28438, 45583, 28475,  8204, 31877,
        1346, 31129,  3060, 15133,  9677, 34859, 29683, 39845, 44583,
       45326, 41703, 38199, 27903, 45134, 21436, 38313, 30072, 30359,
       23689,  9337, 25390, 48220, 22978,  8795, 42038, 27397, 30482,
       31273, 36150, 49867, 18732, 25141, 46853, 19042, 38854, 48630,
       27292, 35452, 45184,  9301, 47625, 24160, 38471,  6222,  3040,
       42731, 12687, 20031, 48882, 47192,  3746, 40800, 27956, 31376,
       12931, 41221, 47662, 38302, 33304, 35821, 41942, 30519, 33629,
       23733, 16079, 38632, 10315,  8274,  5840, 40421,  5981, 40717,
       45973, 46911,  1661, 16351, 16534, 36335, 44929,  3767,  1847,
       18966, 35495, 19825, 11463, 49531,  5690,  1580, 23233, 33720,
       15006, 14931, 48495, 19293, 43345, 29304, 11551,  7435, 26914,
       45882,  9274, 16444,  1976, 27590, 17152, 17065, 47480,  1072,
       31064, 40351, 14530, 27771,  8768, 25205, 22099, 45501,  8805,
       45153, 26581, 49988, 32255,  3206, 22079, 47223, 10319, 17102,
       13829, 20361, 40272, 30042, 11819, 18516,  9264, 19319, 27711,
       35802, 37484, 18945, 32924, 17721, 29836, 33800, 43878, 19362,
       30395,  6455, 36617,  5305, 48139, 28659, 21678, 46997, 46918,
       17882,  1983, 41559, 33161, 28999, 25886,  2906, 45271, 40997,
       26751, 14627, 42194, 28047,  4728, 37151, 25894,  4990,  2384,
       14734,  3604, 38343, 34391, 24714,  1134, 28756, 30502, 33295,
        9622, 46811, 27345, 48750, 21379, 31347, 28982, 18708, 38564,
        5205, 18459, 24006, 29896, 25157, 19487, 23285, 37708, 36962,
       36217, 45629, 35604, 14465, 36769, 23039, 19663,  7364, 17795,
       28070, 40871, 19177, 34117, 13610,  6017, 15775, 22679, 19271,
       47629, 16166, 46112, 47768, 47913, 13111,  5905, 17785, 13463,
       35026, 40712, 38931, 42501, 31260, 49430,  4128, 20345, 33113,
       20965, 23168, 40249, 13941, 37297, 27235, 47722, 48524,  9421,
       48933, 40314, 22463, 43462, 38679, 45542, 43456, 47933, 40129,
       37408, 39614, 42206, 35154, 37756, 16301,   992, 20806, 18393,
        4697, 21921, 37809, 26711, 36140, 33267, 32509,  6151, 37223,
       48376, 11476, 35581, 41144,  1546, 22042, 11294, 33355, 49609,
        8386, 30318, 25745, 10960, 25447, 37038,  1499, 16120,  5147,
       23700, 19234, 38066, 39438, 39294,  8790, 41497, 41182, 14380,
       31150,  3749, 25273,  4329, 23884,  8420, 49227, 23270, 46542,
        6905,  9185, 49835,   922, 20153,  7122, 44635, 49545, 33944,
       37215, 39175, 25997, 45931, 44354, 15778, 35935,   377, 26472,
       43971, 10418, 29038,  1281,  3646, 20685, 41154, 44543, 47100,
       12952, 10427, 33101, 32219, 38982, 29692, 25103, 13064, 16449,
       21510, 15345, 25378,  1345, 40922, 37135, 45251, 32649, 17919,
       10993, 40365, 23761, 38432,  5088, 29923, 49713, 43779, 20625,
       37429, 24961, 45319, 37300, 13928,  8319, 34742,  2785,  6422,
       40668, 25706, 26323, 40007, 21373, 18089, 44550, 12601,  6167,
       35770, 46402, 46349, 31861, 47866, 33463,  6637, 46799,  9343,
       47840,   494, 39681, 28839, 18412, 45236,  9904, 25062, 27413,
        5354,  3758, 33607, 33211, 15181, 29213,  8381, 49970,    80,
        8164, 39979, 15996, 11544, 19449, 17058, 30479,  8864, 36970,
       37417, 20917, 33346, 42647, 10341, 48019, 32938,  3617,  3813,
       14422, 17036,  4982, 34658, 33393,  8876, 47204, 33238,  8916,
       46337, 37733,  2414, 20968, 43827, 15321, 19327, 33779, 49054,
       36772, 19093, 43318, 22759, 47475,  8151, 28592, 46812, 29397,
       11291, 25639,  6071, 13082,  5800, 42417, 26064, 24538, 37904,
       47823, 24839,  4657, 49325, 43313, 15279, 30777, 10501, 41012,
       34160, 46727, 34901, 28880, 14703, 23061, 11169, 11879, 31556,
       32016, 19094,  1306, 31481, 13594, 21874, 26376, 41971, 43451,
        9385, 16559, 32318,   448, 39027, 47969,  4787, 49671, 15542,
       34938, 43989, 18215, 28742, 23924, 38828, 22924, 26137, 18047,
       43252, 45102, 17232, 16002, 22408, 11238, 12757, 31015, 48327,
        8392, 21469, 28300, 23999, 19730, 13473,  8292,  5314, 11975,
       10425, 18657,  3721,  2855, 14079, 15102, 13084, 47525, 13522,
        1626, 22557, 10553, 18765, 45156, 44618, 28178,  7179, 47685,
       26493,  4202, 30517, 39196, 45919,   401, 32217, 36195,  2613,
       32033, 35594, 25905, 25004, 44484,  6682, 35608, 13289,  1340,
       40194, 16015, 31635,  5266, 26592, 29468, 44275, 37869,  7592,
       32662, 36763, 17328, 38900, 17140, 24931, 12345, 33130, 30272,
       34831, 32839, 21243, 39301, 11424, 23746, 37619, 33347, 42148,
       30160, 45096, 24887, 21272,  7328, 11002, 45064, 32062, 33558,
       41960,  7970, 25763, 24447, 35434,  9121, 11682, 25814, 12420,
        2804, 27686,  1477, 34948,  1980, 13225, 24690,  6008, 16793,
        8889, 32316, 27931,  7655, 37083,   284, 10010, 23462, 19860,
       11014,  1144, 33451, 45564,  2885,  9687, 26040, 32206, 43245,
       21105, 48355, 20817, 24212, 16253, 45845, 25160,  8190, 18381,
       31970, 34018, 32076, 25422, 28080, 27301, 21467, 36322, 40211,
        1270,  9819, 24676, 27522,  4523, 23113, 25316, 20709,  9064,
        1473, 25167, 12469, 44122, 47048, 10689, 37432, 31904, 49164,
       35403, 28223, 35952, 35546, 22326,  2773,  3466, 40334, 23667,
       42205, 20924, 44968, 25546, 11400, 17056, 11263, 30400, 28794,
       33732,  8778, 13668, 35761, 48317, 22991,  7196,  3180, 22609,
        5614, 17969, 15008, 37165, 37103, 42166, 28428, 47332, 15309,
       30383, 24695, 16553, 11919,  4338, 44785, 11237, 13629, 33011,
       14712, 19804,   783, 23863, 32267,  8963, 14460, 45387,  2617,
        8147, 40401,  3618, 37556, 44815, 10278, 43748, 21589, 35008,
        7598, 19832, 46117, 42964, 42184, 34528, 37810, 34717, 31073,
       19349,  3573, 10879, 47189, 21378, 40977, 26358, 32000, 30419,
        5563, 20179, 27423, 31982, 32152, 30617, 43246, 42951, 12770,
       24129, 44789, 11414, 44921, 23842, 37919, 47372,  9294,  2783,
       46034, 40563, 47856, 27320, 12848, 40306, 31468, 26259, 28152,
        3598, 20757, 35987,  3209, 40141, 27672, 17627, 26104, 33763,
       24091, 12099, 15644,  3627, 36248, 29208,  9044, 32888, 12502,
       24439, 23909, 23993,  9186, 39180,  4088, 39518, 36932, 44115,
       11527, 47110, 22846, 38425,    84, 36680, 25741, 25080,  6441,
       17463, 16881,  9823, 18712, 45054, 36535, 39717, 29671, 17472,
       24768,  5887, 45172, 13900, 41985,  7833, 16764,   926, 14233,
       26910, 44249, 38062, 20417, 41898, 23381, 37660,  1445, 18504,
       11870, 22198, 38798, 35890, 26826, 26733, 10932,  7686, 30993,
       20369, 31529, 40238, 41795, 36637, 35962, 15099, 30803, 37458,
       27182, 37430, 48927, 49456, 47724, 47996, 33981, 32633, 47308,
       33450, 38667,  6239,  2746, 32546, 17575, 31513, 24613, 17467,
         492, 49203,  4119, 38435, 43247, 44762, 38505, 23676,  3543,
        2396, 32287, 36776,  7304, 29619, 47748, 26673, 49794, 43320,
       14057, 37655, 32097,  2135, 29618,  6992, 43536, 46946, 36346,
       16114, 47270,   133, 10358, 36936, 41791, 18443, 17521, 10858,
        2801, 22128, 23402, 16144, 48829, 43128, 19802, 47696,  8142,
       13638,  1835,  1054, 37273, 30556,  6552, 45724, 10816, 24052,
       37010, 38713,   256, 12722, 48466, 13494, 23034, 49814,  5121,
       21773,  2151, 44559, 15514, 32329,  5854, 31364, 41039, 44025,
       21120,  4357,   652, 29533, 24930, 13632, 47522,  9581, 47286,
        6748, 24508, 17037, 28398, 21430, 28392, 40465, 29359, 36730,
       12439, 14745, 42887,  5809, 49139, 40360, 16031, 29998, 33618,
       11334]), [5, 3, 0, 7]), (array([31193, 38816, 44841, 42424, 19783, 23139, 26637, 45118, 38211,
        3620, 46233,  9322,  8305, 11130,  9372, 16012, 15528, 20742,
         978, 22270,  8973, 35106, 14129, 34832, 33970,  9849, 47642,
       38522, 35004, 41404, 12164, 43711, 45157, 44575,  5822, 19351,
       35454, 14764, 39192, 26550, 49875, 40076, 47365, 15704, 33198,
       43098, 34300,  6899, 29283,  7401, 37937, 47113, 37689,  5149,
        4050, 37494, 11804, 46259, 38047, 12551, 25874, 35587, 11336,
       36379, 48917, 18345,  8107, 11645, 11850,  7358, 46127,  6779,
        2389,  5247,  5102, 47264,  8576, 49572, 31345, 26115, 49733,
       12100, 27037,  2101,   160, 16989, 28054, 17334, 17666, 31665,
       16598,  2844, 31100, 40204, 28640, 30553, 21088, 27794, 16682,
        6420, 35605, 47618, 15646,  9863, 15583,  7600, 49919, 16958,
       16256, 10495,   176, 19574, 19865, 25910,  3231, 33599, 36237,
       47377,  2289, 30198, 43207, 15069,  7841,  7381, 41807, 46865,
       45296,  4261, 47616, 43684, 23275, 29117, 49847,  2023, 38177,
       41837, 30457, 20680, 37792, 23648, 46928, 30570, 41637, 22063,
       10479, 25731, 42163,    45, 41061, 35048, 32860, 31158, 49959,
       35068, 42352, 10544,  9392, 37401, 11619, 32135, 39638, 10292,
       48639, 21073,  4172, 17673, 42912, 45014, 18354, 24484, 21020,
       22298, 49021, 41896, 24110, 20391, 42188, 34385, 44671, 41733,
       38804, 11109, 43413, 44751, 33941, 16577, 31300,  6871,  1446,
       30006, 42916,  8932, 44415,  3922, 23607, 41844, 22303, 19399,
        3808, 26498,  8389, 36838, 42060, 33897, 22212, 22952, 24520,
       32319, 40207,  5722, 48375, 11037,  9610, 16172, 15783, 21871,
       28089, 22975, 33050, 49489, 33611, 23357, 34206, 38637, 45070,
       17586, 38078, 39566, 25405, 49759, 24590, 36208, 24561, 31352,
       42921, 11692, 44960, 38919,  8812, 45602,  1455,  3505, 35810,
        3483, 28981, 43668, 18551, 31072, 11543, 34890,   202, 22685,
       30566, 37295,  2845,  3228, 29438, 46236,  4865, 45547,  2935,
        5562, 39548, 45841, 15480, 11834, 13370, 41436, 46335, 18343,
       23019, 36717,  8532, 14436,  1034, 46755,  9955, 11688, 42407,
        8281, 22166, 20176, 21355, 49833, 42208, 37538, 24133,  5002,
       33746,  4407, 26447, 16143, 29142, 41173, 13148, 14387,  8103,
        4246, 12662,  3930, 47343, 49073, 33856, 27365, 34175, 11773,
       18805, 48034, 19114, 47154, 11227, 46278, 45943,  1241, 33766,
       45426, 32072, 35384,  3392, 23001, 25361, 47076, 38752,   511,
        9919,  8975, 44564, 32552, 23679, 14467, 32550, 43614, 13208,
       28359, 37443, 27570, 30360, 16378,  7556, 47896, 35618, 41427,
       38324, 20481, 18800, 14401, 37634, 43900, 16488,  4071, 30343,
       37678,   166,  8413, 23336, 46355, 31662, 15758,  7453,  5692,
        1114, 45860,   438, 34474, 40079,  3613,  6546, 36117, 21489,
       31387,  2991, 42106, 31986,  8428, 29519, 42362,  9701, 25839,
       18513, 43169,  7854, 19199, 36508, 38056, 36859, 23243, 37968,
       26888, 13577,  7167,  3983, 26229, 46298,  5203, 13381, 27811,
       35498,  6002, 49260, 24078, 19048, 30364, 36397,  5382, 49785,
       24627,  6937, 42733, 37132, 19534, 48996,  8840, 36413, 23287,
       17171, 19367, 16454,  1472, 45612, 29956,  5602, 45302, 23284,
        5974, 11918, 24477, 41605, 37070, 42727,  7479, 23975, 20720,
       40233,  2655, 47767, 32156, 40494, 12986,  2002, 47086, 23819,
       10178, 49278, 43176, 41560, 47641, 32612, 35233, 14197, 36878,
       15821, 36507, 19190, 15005, 35019, 46685, 49864, 26021, 17367,
        4041, 17228, 13343, 41813, 37031, 29179, 29167, 32952, 28351,
       27889,  7657, 33088, 39816, 31298, 32209, 33991, 31210, 15096,
       36461,  8673, 44157, 40157, 27622, 18692, 21470,  6036, 14475,
       29551,  8575, 44809, 24731, 15056, 29805, 16517, 29700, 17990,
        5668, 26061, 31186, 21999,  5300, 44178, 23822, 32107, 34989,
       46992,  5426, 36886, 19009, 35283, 32721, 21044, 18803, 43628,
       16541, 16918, 26656, 36491, 34270,  9314, 27278, 46367, 36993,
       25578, 32081, 24104, 48560, 32713,  2660,  9890, 20035, 49856,
       46876, 37949,  6663, 19273, 19216,  8882, 17865,  2429, 37519,
       39927, 19196,  5702, 27155, 34040, 47381,  5331, 40418, 14138,
       16103, 39121, 34123, 31023, 45556, 12817, 48476, 12095, 25624,
        1664, 42456, 11280, 21023, 46756, 42630, 34799, 17303, 38368,
        9281, 26000, 42784, 40485, 32903, 49556, 27315, 25339, 36632,
       40343, 48389, 17093,  7256,   605, 28548, 37747, 15483, 45166,
       31144,  8365,  7377, 15622, 36482, 47243,  5873, 15636, 46576,
       45666, 12740, 45304, 32232, 28828, 49263, 33179, 28917, 24319,
       20233, 15707, 29879, 44476, 15949, 24834,  2675, 29099, 21440,
        6954, 19173, 32369, 37328, 41220, 42399, 44067, 36909, 22350,
       10911, 48105,  8444, 43138, 42628, 14325, 19664, 37358, 40448,
       20824, 39480, 37334, 43368, 38003, 46046, 48174,  4726, 29596,
       44749, 40420,  5216, 21903, 29634, 29444, 19812, 12764, 47456,
       21217, 34676, 20242, 38796, 33905, 44970,  2322,  4935,  7015,
       36506,  4941, 37738, 20124, 19370, 24814, 12730, 13660,    77,
       10267,  9619, 11643, 23224, 41457, 35490, 35548, 48096,  4337,
       47865, 19682, 34715, 23254, 46922, 15502, 18630, 46934,  5323,
       32444, 19828, 26027, 13223,  8524, 35524, 36415, 15473,  6919,
       36183, 24851, 40607, 17948, 49359, 37566, 47227, 21288, 31213,
         373, 22716, 25532, 10782, 19210, 25862, 27286, 33000, 44721,
       43220,  9697, 47754,  4311, 45457, 16554, 32761,  7528, 27358,
       42332, 18905, 16450, 22684,  6396, 33780, 17233, 34009, 38633,
       43667,  1142, 26744, 13650, 13055, 13649, 27533,  4229,  5589,
       48979, 48363,  4869,  6711, 40479, 43993, 30093,  6440, 35690,
       24051, 46987, 27434,  3958,   847, 38780, 23344, 27915, 42238,
       48697, 43150, 17398, 38595, 25539, 42996, 48204, 18017,  3364,
       27421, 40684, 39168, 22380, 38220, 27187, 46783, 34046, 30805,
       49183, 13058,  5784,  7621, 23970, 30385, 23456,  6876,  7076,
       16839, 15296, 38067, 43435, 40061,  1742, 29281, 31034, 43848,
       11974, 14983,  7325, 31316, 45103,  4399, 24898,  8817,  3434,
       17782, 13026, 15433, 44135, 46167, 39181, 46875, 19973, 33254,
       16589, 35093,  6776,   952,  1698, 22179, 27230, 12097, 32640,
       26494, 19064, 25122,  7836,  7756, 42360, 13703,  5117,  7030,
       34067, 16109, 15712, 48265, 32239, 27086, 25809, 40502, 21508,
       12690, 23778, 39307, 30195, 44874, 47556, 22046,  2049,  7849,
       18577, 28719, 25711,  8580, 12683, 25948, 26285, 41239, 40570,
       23265, 20550, 33191, 24974, 41626, 22782, 32298, 42685, 16227,
       34446, 48759, 47046, 24758, 32898, 30296, 22219, 10891,  8312,
        4903, 26249, 42659, 17314, 17181, 35523, 11059, 34329, 11469,
        8996,  1215, 17581, 39494, 24941, 38767, 16025,  8360, 37293,
       38440, 11520, 21852, 47184, 45027, 17469,  9741,  6398, 11393,
       12163, 41416, 22222, 17979, 49728, 27078, 11502, 44922, 26697,
       36839,   972,  6696, 47673, 18339,  5934, 26628,  6864, 18411,
       35253, 47039,  3413, 30972, 18823, 28150,  3208, 10884, 16171,
       40106, 41781,  8202, 21891, 29621, 42390, 24786,  5991, 11950,
       14692, 24995, 20893, 44457, 18108, 37930, 38176, 39077, 46754,
       48695, 28240, 16504, 33082, 22669,  8744, 19346, 43130,  2319,
       36065,  3989,  7986, 25637, 29370,   478, 39276, 37837, 13950,
        8646,  2311,  3191, 29016, 36897, 30004, 34277, 23547, 42959,
        8369, 40198, 33945, 31847, 39425, 19777, 19467,  5401, 46301,
       36265, 36409, 48453,  3553, 41496, 34226, 31263, 25329,   181,
       30984, 38394, 27936, 30781, 39547, 30053, 43255, 35133, 23486,
       32568]), [1, 9, 0, 7]), (array([15790, 19345,  2235, 18272, 28885,   247,  2385, 49024,  9395,
        9845,  9528,  7634, 37691, 31393, 49598, 47922, 32368,  7536,
       35429, 33666, 41294,  1923, 15533, 10861, 35526, 40375, 12108,
       41659, 16085, 21171, 22820, 30499, 32525, 41202,  4224, 44629,
       36630, 31124,  1728, 16968, 16426, 14957, 12094, 45255, 36105,
       44525, 18566, 25692, 28313, 42553, 28271, 45315, 28620, 25474,
       39740, 17498,  6652,  3981, 35978,  7110, 37954, 24883,   420,
       42310, 22749, 27199, 35395, 42795, 14429, 10697, 25371,  5042,
       19784, 49187,  4695, 21346,  3322,  2442, 23604, 17323,  4905,
       19168, 30958, 27205, 23071, 41836, 21693, 43032, 37903, 27801,
       47293, 49138, 17145,   632, 17552, 27757,  3556, 31585, 43620,
        3612, 40550, 18065, 43575, 31998, 37053,  5075, 30291, 19406,
       15773,  4158, 27276, 14284, 35450,  1644, 27923, 32224, 42656,
       29835, 41216, 40071,  4032, 37214, 38375, 27812, 30885, 13038,
       45571, 28577, 28649, 36896, 41737, 39265,  5756, 30289, 32643,
       11872, 34770, 27725,  3776, 35877, 40949, 40301, 46998, 16461,
        2858, 44994, 20654, 23638, 33369, 42229,  4129, 27391, 31143,
       45132,  1925, 42219, 25781, 21779,  9592, 37741, 46823, 23231,
       45515,  3176, 46276, 34352, 25401, 49743, 46446,  4640, 30707,
       23739, 33586,  1832, 30268, 20282, 36764, 45170, 36884, 31699,
        1406, 43364, 47928, 41127, 43180,  8263, 22318, 42398, 12896,
       18977, 23574,  9937,  6569, 49114, 31588, 10131, 47907, 49515,
       18606,  7218, 30413,  5625, 38248, 18628,  1001, 12208, 35113,
        2830, 27534, 42138, 13652,  5944,  1407, 41610, 39891, 15112,
       44844, 19721, 25024, 29202, 36276, 39958,   661, 18043, 22371,
       35641, 39853, 30911, 41955, 43857, 21949, 43746, 46648, 32494,
       14105, 38386, 20120, 28904, 35143,  4231, 13708, 43810, 42277,
        3283,  1953, 31515,  2155, 20498, 31683, 42040,  4234,   200,
       36365, 11911, 12635, 28395, 40031,  4606, 34394,  8215, 37759,
       14553, 14719, 18179, 45582, 43426,  5443,  9877,  7650, 12359,
        6664, 41049, 33178, 46123, 29436, 31784, 15060, 29097, 41641,
       24231, 28722, 13890, 15338, 37481, 22393, 10676,  3139, 24155,
       15351, 39511, 16387, 23082,  8877, 44342, 38396, 37261, 15349,
       49061, 17105, 23087,  1371, 34674, 20297, 24321, 23987, 46550,
       36069,  2100, 25234, 34786,  2729, 10025,  5159,  8431, 25396,
       26921, 37188, 48710, 31112, 44251, 16451, 21846,  4926, 28588,
        6096, 48854, 24882, 38671, 21024, 26139,  4389, 13802, 27708,
        1678, 47650, 19823, 26243, 36696, 27567,  5857, 19707, 32768,
       47513, 32739, 23108, 23616, 32381,   451,  3500, 23084, 15211,
       41707, 11233, 24722, 19731, 49636, 20977, 49200, 28252, 19798,
       26782,  1403, 27357, 47302, 31598, 20021,  6267, 47580,  2299,
       11647,  8363,  7911, 29426, 31629, 40921, 18309, 25681, 37564,
       32240, 17715, 46677, 12447, 49464, 30193, 24673, 24204, 45518,
       40549, 13965,  9241, 40302, 48070, 13306, 27240, 12004,  9084,
       43390, 23152, 23324, 27648,  1228, 46092, 32027, 31421,  7316,
       43764, 42641, 22746, 28859, 21292,  6686, 11729, 16883, 42046,
       12512, 40501, 40685, 36995, 39982, 39969,  3241, 22795, 49039,
       18615, 31175, 39161, 40437,  9994,  6273, 13209, 28998, 25430,
       12831,  8128,  5419, 28533, 44597, 14608, 48897, 44244, 13939,
        6750,  9208,  7064, 31140, 13455, 12168, 42616,  5689,  3070,
        1978,  2874, 39068, 42460, 30176, 38141,  6960,  1038, 49209,
        4659, 15185,  2267, 13878,  3686, 31118,  8697, 21588,  9303,
       20003, 39007, 26228, 49948, 17697, 28980, 26619, 35830, 23747,
        6294, 33630,  3341,  7473, 16442, 38786, 44999, 12293, 41466,
        9233,  9788, 35001, 25298, 39986,   862,  2941,  4421, 33244,
        7575, 38707, 49798, 18085, 13988, 29244, 28750,  8116, 33530,
       46631, 29343, 24445,  9925, 27080, 11451,  6904, 45348, 32479,
       17917,  5441, 44149, 41300, 21573, 10334, 43998, 12052, 42083,
       17453, 30257, 39149, 29732, 26723,  6609,  2034, 44709,  7674,
       13902, 11633,  1960,  7925, 46420, 26269, 33516, 22847, 38335,
       21758, 38217, 42984, 19386, 33003, 27706, 26824,   457, 25070,
       35603, 13110, 28126, 38921, 28360, 20127, 45190, 41435, 29336,
       24144, 21248, 38044, 40890, 13086, 32140, 41557,  3903,   940,
       12916,  8044, 34235, 25082, 24641, 15225, 41242, 26840, 13436,
       35179, 35269, 39914, 40648, 44443,  1594, 30968, 45393, 37955,
       47375, 43849, 14520,  5194,  5883,  7174, 27645, 22540, 41848,
       41055,  7251, 38036, 44770, 48489, 27562,  1234, 15466, 28408,
       13915, 16831, 40542,  8953, 15620, 19065, 49006, 24291, 29844,
       16116, 45621,  4940, 37693, 38307, 43657, 27004,  7168, 27850,
        3725, 28401, 19527, 38154, 20014, 20910, 46366, 17088,  8553,
       17107,  7212, 34917, 42103, 25270, 28003, 36149, 27758, 29641,
        6276,  6923, 42226, 26505, 38977, 19961, 39628, 44454, 35544,
       34448, 42199, 20015, 34421, 29772, 29888, 17156, 27873, 18436,
       40768,  4543, 49507, 36573, 25177, 28305, 25016, 22286, 41124,
       16903,   223,  5642, 11583, 34280,  9069, 37240, 26361, 23004,
       35381, 29578, 39083, 17226, 15735, 33368, 16191, 42241, 38341,
       49308, 10487,   905, 35798, 41019, 37199, 39435,  7537, 15509,
        7971, 10729, 18578, 11671,  5010, 36464, 49130, 44175, 48851,
       46362, 38433, 38222, 47678, 12634,  3438, 12785, 40287, 11178,
       33186, 43771, 48842, 19493, 27944,  7423, 30295,  1871,  7527,
       28515, 24685, 15255, 35859, 19371, 32538, 12509,  2345, 39606,
        3644, 21064, 45516, 32631, 33325, 34283, 35162, 22628, 26003,
       38320, 29665, 42998, 16346, 27724,  8136, 35201,   293, 28212,
       37718, 35683, 32432, 48464, 11446, 28256, 14403, 46869, 43229,
        5188,  5336, 45087,  9957, 42962, 14636,  6072, 34807, 36562,
       10547, 33524, 13022, 23298, 17069, 36495, 25617, 49141, 40119,
       26067,  5608,  7223, 49961,  4795,  5804,  2074, 43309, 20699,
       10529, 27248, 46097, 18828,  5313,  8859,  4933, 29526, 26895,
        7874, 43193, 44923,  7416, 15932, 17377, 21904, 42322, 38057,
       40567, 26166, 16654, 32286, 17804, 27583,  4325, 29441, 15824,
       17910, 35214, 30253, 15850,  7863, 25156,  2897,  4216, 34257,
       46172, 44753, 11951, 43060, 12394, 21967, 40439, 25410, 33850,
       18519,  4285, 14582,  5710, 33349, 37884,  1828,  4833, 25271,
       28201,  1577, 25197, 39328, 27952, 22678,  7609,  2063, 26515,
       25575, 24327, 18447, 23835, 39219, 25102,  8984, 37472, 24311,
       21301, 34240, 47125, 15081, 25228,  5536, 19408, 17129, 31528,
       45171, 45459, 27782, 38539, 22854, 42788, 34592,   956, 25916,
       22076,  3933, 20773,  1806, 33388, 37474, 30348, 10121, 24468,
        4871,  1808, 23753, 44845,  8907, 28264, 36833, 28206, 42831,
       19200, 47249, 44839, 37683, 34126, 37897, 47638, 37205, 49226,
       44514, 24235, 42338, 14095, 16868, 26107, 26562, 14144,  3261,
       25778, 20158, 49695, 33496,  1113, 16260, 28868,  5317,  8499,
         329, 16666,  2977, 31326, 17997, 12023,  5126, 41974,  6301,
       39782, 11278, 21876, 23435, 20476, 37635, 34569,  7798, 33203,
       36098, 42117,  5802, 49476, 28243,  9133, 14837,  6793, 25454,
       41195, 10988, 18041,  7186,  2480,  4872, 45833, 29800,  7845,
       28920, 31472, 12751, 14050, 44567, 14862, 33475, 16633,  8481,
       11214, 45999, 47285, 20218, 33553, 32148, 10552,  4809, 36034,
       40709, 40539,  7331, 28977, 29155, 16095,  2481, 24159,   382,
       11350,  9981, 39406,  7450, 10386,  9560, 46524, 42751,  4078,
       26669, 33860, 43531,  8391, 32992, 45407,  7725,  5560, 36829,
        6986]), [4, 6, 0, 7])]
Competition
DC 0, val_set_size=1000, COIs=[2, 8, 0, 7], M=tensor([2, 8, 0, 7], device='cuda:0'), Initial Performance: (0.222, 0.04485496115684509)
DC 1, val_set_size=1000, COIs=[5, 3, 0, 7], M=tensor([5, 3, 0, 7], device='cuda:0'), Initial Performance: (0.25, 0.044860809803009036)
DC 2, val_set_size=1000, COIs=[1, 9, 0, 7], M=tensor([1, 9, 0, 7], device='cuda:0'), Initial Performance: (0.221, 0.04458320701122284)
DC 3, val_set_size=1000, COIs=[4, 6, 0, 7], M=tensor([4, 6, 0, 7], device='cuda:0'), Initial Performance: (0.25, 0.04539239990711212)
D00: 1000 samples from classes {0, 7}
D01: 1000 samples from classes {0, 7}
D02: 1000 samples from classes {0, 7}
D03: 1000 samples from classes {0, 7}
D04: 1000 samples from classes {0, 7}
D05: 1000 samples from classes {0, 7}
D06: 1000 samples from classes {8, 2}
D07: 1000 samples from classes {8, 2}
D08: 1000 samples from classes {8, 2}
D09: 1000 samples from classes {8, 2}
D010: 1000 samples from classes {8, 2}
D011: 1000 samples from classes {8, 2}
D012: 1000 samples from classes {3, 5}
D013: 1000 samples from classes {3, 5}
D014: 1000 samples from classes {3, 5}
D015: 1000 samples from classes {3, 5}
D016: 1000 samples from classes {3, 5}
D017: 1000 samples from classes {3, 5}
D018: 1000 samples from classes {1, 9}
D019: 1000 samples from classes {1, 9}
D020: 1000 samples from classes {1, 9}
D021: 1000 samples from classes {1, 9}
D022: 1000 samples from classes {1, 9}
D023: 1000 samples from classes {1, 9}
D024: 1000 samples from classes {4, 6}
D025: 1000 samples from classes {4, 6}
D026: 1000 samples from classes {4, 6}
D027: 1000 samples from classes {4, 6}
D028: 1000 samples from classes {4, 6}
D029: 1000 samples from classes {4, 6}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.06002717798948288) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.0816035498380661) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.304, 0.09341755312681198) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.0939168337881565) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.06308483767509461) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.08910550057888031) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.352, 0.13188596749305725) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.11786670678853989) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.07532635158300399) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.289, 0.10358087551593781) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.15207326704263688) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.15404279613494873) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.105178024366498) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.11446061944961548) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.17986074367165567) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.19359765738248824) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.12571522931009532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.351, 0.12731943529844283) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.17620865178108217) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.21025887111574412) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.14941069076955318) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.357, 0.1479798818230629) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.17783011223375797) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.2532032940760255) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.15933214899897574) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.16829006430506707) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.21066069585829975) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.24311571361124515) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.19129687175899743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.365, 0.16548691460490228) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.20810078812390567) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.27205386623740196) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.1845419636927545) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.15964543256163596) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.23719379422068596) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2844371333681047) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.2080296144457534) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.367, 0.12997722536325454) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.219299509100616) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.3079838444199413) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO5', '(DO0']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO3']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.22164799582213163) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.14846084129810333) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.24144368553906678) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.3212093838695437) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.23276726327463984) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.371, 0.15193505269289018) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.2591338819079101) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.3412908539138734) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.2496528520192951) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.15373130029439927) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.2591255188733339) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.3341454144194722) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.2546190532660112) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.379, 0.1311723072230816) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.25957327950000764) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.37167221363447606) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.24562297463417054) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.365, 0.14725519776344298) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.2831150761693716) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.36633644412457944) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.28261474082805216) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.1545028160959482) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.3060157789234072) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.37304128459095953) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.24957743223384024) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.17166774249076844) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.27757851117104293) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.37334966015536336) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.2680230436725542) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.15716772776842117) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.32404195982962847) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.3616837582625449) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.2571596061969176) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.15028832617402077) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.318012645766139) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.38789463948458436) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.2899721881723963) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.13473687019944192) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.30622690772265193) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.39739035521866756) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO2']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.26724233825085686) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.15575531199574472) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.3028921430781484) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.3819490233696997) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.2846208300013095) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.16614112934470177) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.3492756843417883) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.42511164185777306) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.2604365832488984) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.14393641975522042) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.35270276774466036) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.38211484385700895) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.2762858480643481) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.16364624834060668) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.32634551722556354) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.3642001649560407) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.2797283517597243) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.15382701510190963) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.3193540616035461) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.39237209195178) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO2', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.28878076645638795) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.1711645911037922) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.3253219989165664) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.35336962567456065) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.2765385628370568) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.38, 0.14692287108302116) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.30715678926557305) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.41888885699771344) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.481, 0.2958295125274453) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.1380216095149517) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.2900372635349631) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.32967351163737474) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.2600212829289958) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.375, 0.16271821501851083) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.29993019038438795) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.3464073397973552) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.3044312476837076) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.14162077829241754) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.27179008984565733) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.32988454222492875) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO5', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.25957458720635623) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.379, 0.13756267668306826) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.2857032580003142) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.3568630316690542) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.2669140110146254) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.372, 0.15967141336202623) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.31189778324216605) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.37216003253590313) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.2857985948417336) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.367, 0.14228911823034288) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.3177468853369355) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.41753327230818105) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.3140357398013584) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.16193076556921004) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.2991255126148462) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.35227529709460215) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.2940704781487584) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.14715608179569245) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.3277261270582676) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.35337148728303147) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO2', '(DO5']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.3037456775694154) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.368, 0.15084642374515533) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.2969722108319402) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.3903241931345547) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.29413228055275975) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.1595377848148346) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.303361414514482) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.3812777430182323) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.3180584222087637) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.13791195234656334) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.32507729904353616) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.38585277219186537) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.30252735678758474) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.379, 0.16986797556281089) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.32321188324689865) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.4113360491711646) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.3028932454306632) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.369, 0.16615919303894042) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.3285794519037008) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.42791004631877877) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.2991276709022932) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.374, 0.16398749747872352) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.33310689356550577) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.38854703069338575) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.31825694877235217) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.18787831410765649) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.3354609297551215) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.3878414505124092) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.32915982673177496) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.363, 0.18317391523718835) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.33480278483405707) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.39410965870134534) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.31374293626844885) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.362, 0.16607925364375115) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.28811496014893057) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.36884636418521405) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.2995411054706201) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.364, 0.1530155448615551) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.3111110518202186) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.39562486331909896) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.3387081741923466) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.369, 0.16036631521582603) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.3295398713238537) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.4002407399527729) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.30041896225558595) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.377, 0.16631250992417335) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.31144636862725017) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.4555961642123293) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.481, 0.3125159708047286) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.37, 0.16793424129486084) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.3371724978014827) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.39985293611697853) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.325196070862934) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.1793139777779579) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.2861403175368905) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.43214685546187687) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.3084586132038385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.372, 0.19643388283252716) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.25501906851679085) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.4176707308795303) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.222, 0.04485496115684509), (0.362, 0.06002717798948288), (0.451, 0.06308483767509461), (0.463, 0.07532635158300399), (0.463, 0.105178024366498), (0.465, 0.12571522931009532), (0.47, 0.14941069076955318), (0.464, 0.15933214899897574), (0.469, 0.19129687175899743), (0.468, 0.1845419636927545), (0.456, 0.2080296144457534), (0.468, 0.22164799582213163), (0.47, 0.23276726327463984), (0.467, 0.2496528520192951), (0.473, 0.2546190532660112), (0.475, 0.24562297463417054), (0.474, 0.28261474082805216), (0.469, 0.24957743223384024), (0.472, 0.2680230436725542), (0.473, 0.2571596061969176), (0.476, 0.2899721881723963), (0.476, 0.26724233825085686), (0.474, 0.2846208300013095), (0.473, 0.2604365832488984), (0.472, 0.2762858480643481), (0.477, 0.2797283517597243), (0.476, 0.28878076645638795), (0.475, 0.2765385628370568), (0.481, 0.2958295125274453), (0.472, 0.2600212829289958), (0.474, 0.3044312476837076), (0.468, 0.25957458720635623), (0.474, 0.2669140110146254), (0.475, 0.2857985948417336), (0.472, 0.3140357398013584), (0.472, 0.2940704781487584), (0.473, 0.3037456775694154), (0.468, 0.29413228055275975), (0.48, 0.3180584222087637), (0.473, 0.30252735678758474), (0.473, 0.3028932454306632), (0.472, 0.2991276709022932), (0.477, 0.31825694877235217), (0.474, 0.32915982673177496), (0.476, 0.31374293626844885), (0.469, 0.2995411054706201), (0.474, 0.3387081741923466), (0.478, 0.30041896225558595), (0.481, 0.3125159708047286), (0.477, 0.325196070862934), (0.473, 0.3084586132038385)]
TEST: 
[(0.21525, 0.0437830693423748), (0.36075, 0.05810387420654297), (0.442, 0.06061009407043457), (0.4575, 0.07175614351034164), (0.465, 0.09945581325888633), (0.45975, 0.11858130222558975), (0.4675, 0.140495234310627), (0.4625, 0.14899606961011885), (0.46325, 0.1790197706222534), (0.465, 0.17290244770050048), (0.4485, 0.19369042420387267), (0.4675, 0.20631295865774155), (0.468, 0.21763194817304612), (0.47175, 0.23206564152240752), (0.47175, 0.23807056307792662), (0.47125, 0.23116178518533706), (0.47, 0.26488201558589936), (0.47175, 0.23413760256767274), (0.4705, 0.2495270293354988), (0.47125, 0.2376789336204529), (0.472, 0.26761830097436906), (0.46975, 0.2477291995882988), (0.471, 0.2641924188733101), (0.471, 0.24247460359334946), (0.47225, 0.25436827254295347), (0.4715, 0.2583319815993309), (0.4695, 0.26955756413936616), (0.47175, 0.2571821073293686), (0.472, 0.2770528621673584), (0.4665, 0.2416895712018013), (0.471, 0.2826537078022957), (0.465, 0.2421914696097374), (0.47275, 0.24911587828397752), (0.4755, 0.26527589160203935), (0.46975, 0.2895742530822754), (0.4715, 0.27021715068817137), (0.468, 0.2826398239731789), (0.46775, 0.27418121498823167), (0.47225, 0.2971754130721092), (0.47425, 0.28183693933486936), (0.47125, 0.28284866058826447), (0.47475, 0.28099479860067367), (0.47275, 0.2990216951966286), (0.471, 0.3082265901565552), (0.4735, 0.2920561851263046), (0.46925, 0.2811931135058403), (0.47425, 0.31144042694568635), (0.473, 0.27972717881202697), (0.4745, 0.2970392920970917), (0.474, 0.30400481599569323), (0.472, 0.290870863199234)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           2       0.37      0.98      0.54      1000
           7       0.00      0.00      0.00      1000
           8       0.66      0.91      0.76      1000

    accuracy                           0.47      4000
   macro avg       0.26      0.47      0.33      4000
weighted avg       0.26      0.47      0.33      4000

Competition_DC_1
VAL: 
[(0.25, 0.044860809803009036), (0.25, 0.0816035498380661), (0.25, 0.08910550057888031), (0.289, 0.10358087551593781), (0.366, 0.11446061944961548), (0.351, 0.12731943529844283), (0.357, 0.1479798818230629), (0.366, 0.16829006430506707), (0.365, 0.16548691460490228), (0.373, 0.15964543256163596), (0.367, 0.12997722536325454), (0.377, 0.14846084129810333), (0.371, 0.15193505269289018), (0.366, 0.15373130029439927), (0.379, 0.1311723072230816), (0.365, 0.14725519776344298), (0.373, 0.1545028160959482), (0.374, 0.17166774249076844), (0.364, 0.15716772776842117), (0.364, 0.15028832617402077), (0.366, 0.13473687019944192), (0.381, 0.15575531199574472), (0.364, 0.16614112934470177), (0.368, 0.14393641975522042), (0.373, 0.16364624834060668), (0.37, 0.15382701510190963), (0.377, 0.1711645911037922), (0.38, 0.14692287108302116), (0.378, 0.1380216095149517), (0.375, 0.16271821501851083), (0.37, 0.14162077829241754), (0.379, 0.13756267668306826), (0.372, 0.15967141336202623), (0.367, 0.14228911823034288), (0.374, 0.16193076556921004), (0.368, 0.14715608179569245), (0.368, 0.15084642374515533), (0.366, 0.1595377848148346), (0.364, 0.13791195234656334), (0.379, 0.16986797556281089), (0.369, 0.16615919303894042), (0.374, 0.16398749747872352), (0.37, 0.18787831410765649), (0.363, 0.18317391523718835), (0.362, 0.16607925364375115), (0.364, 0.1530155448615551), (0.369, 0.16036631521582603), (0.377, 0.16631250992417335), (0.37, 0.16793424129486084), (0.373, 0.1793139777779579), (0.372, 0.19643388283252716)]
TEST: 
[(0.25, 0.04378252673149109), (0.25, 0.07828381612896919), (0.25, 0.08545986431837081), (0.2835, 0.09934729871153831), (0.34575, 0.10978565776348113), (0.357, 0.12227776199579239), (0.35475, 0.1418950108885765), (0.36225, 0.16125718301534653), (0.361, 0.15870756310224532), (0.36225, 0.15440980315208436), (0.36025, 0.12609325283765793), (0.3665, 0.14366705095767976), (0.364, 0.14556807696819304), (0.36, 0.14795290505886077), (0.3635, 0.12773992711305618), (0.3575, 0.1412552430033684), (0.36175, 0.14881945645809175), (0.35825, 0.1646391059756279), (0.357, 0.1528313524723053), (0.359, 0.14520136880874634), (0.36375, 0.1309311063885689), (0.363, 0.1517469048500061), (0.363, 0.15894452404975892), (0.36425, 0.1386854211091995), (0.36075, 0.1581806703209877), (0.36275, 0.14719137352705003), (0.36275, 0.16424984735250472), (0.36475, 0.14343618881702422), (0.3645, 0.13216421961784364), (0.36225, 0.15688837146759033), (0.35825, 0.1339915115237236), (0.362, 0.1338554180264473), (0.365, 0.15348418617248535), (0.365, 0.13875438463687897), (0.3675, 0.15645065367221833), (0.36475, 0.1429812416434288), (0.36475, 0.1450922875404358), (0.35775, 0.15304979652166367), (0.36125, 0.13359021651744843), (0.36275, 0.16365108954906463), (0.3605, 0.16233466780185699), (0.3675, 0.15812049043178558), (0.36275, 0.18559427350759505), (0.36025, 0.1742768208384514), (0.36025, 0.16198753190040588), (0.36075, 0.15133019369840622), (0.36125, 0.1562173880338669), (0.361, 0.16181222581863403), (0.359, 0.1650394247174263), (0.36025, 0.1739106638431549), (0.367, 0.1889769366979599)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           3       0.31      0.80      0.44      1000
           5       0.47      0.67      0.56      1000
           7       0.00      0.00      0.00      1000

    accuracy                           0.37      4000
   macro avg       0.20      0.37      0.25      4000
weighted avg       0.20      0.37      0.25      4000

Competition_DC_2
VAL: 
[(0.221, 0.04458320701122284), (0.304, 0.09341755312681198), (0.352, 0.13188596749305725), (0.397, 0.15207326704263688), (0.413, 0.17986074367165567), (0.423, 0.17620865178108217), (0.408, 0.17783011223375797), (0.403, 0.21066069585829975), (0.425, 0.20810078812390567), (0.425, 0.23719379422068596), (0.431, 0.219299509100616), (0.427, 0.24144368553906678), (0.436, 0.2591338819079101), (0.429, 0.2591255188733339), (0.432, 0.25957327950000764), (0.436, 0.2831150761693716), (0.426, 0.3060157789234072), (0.432, 0.27757851117104293), (0.434, 0.32404195982962847), (0.44, 0.318012645766139), (0.44, 0.30622690772265193), (0.433, 0.3028921430781484), (0.434, 0.3492756843417883), (0.434, 0.35270276774466036), (0.439, 0.32634551722556354), (0.447, 0.3193540616035461), (0.44, 0.3253219989165664), (0.443, 0.30715678926557305), (0.446, 0.2900372635349631), (0.446, 0.29993019038438795), (0.443, 0.27179008984565733), (0.444, 0.2857032580003142), (0.44, 0.31189778324216605), (0.445, 0.3177468853369355), (0.445, 0.2991255126148462), (0.445, 0.3277261270582676), (0.444, 0.2969722108319402), (0.443, 0.303361414514482), (0.443, 0.32507729904353616), (0.442, 0.32321188324689865), (0.439, 0.3285794519037008), (0.439, 0.33310689356550577), (0.435, 0.3354609297551215), (0.446, 0.33480278483405707), (0.442, 0.28811496014893057), (0.444, 0.3111110518202186), (0.449, 0.3295398713238537), (0.448, 0.31144636862725017), (0.443, 0.3371724978014827), (0.442, 0.2861403175368905), (0.442, 0.25501906851679085)]
TEST: 
[(0.231, 0.043520193308591845), (0.31675, 0.08954067873954773), (0.36975, 0.12627470338344574), (0.4045, 0.14565940433740615), (0.3995, 0.17283708488941193), (0.41275, 0.16908486545085907), (0.3955, 0.17159293580055238), (0.3935, 0.20377891224622727), (0.42225, 0.20144247567653656), (0.41775, 0.2316529153585434), (0.41525, 0.21458645927906037), (0.41725, 0.2340280283689499), (0.42175, 0.25247314655780795), (0.422, 0.2506298280954361), (0.42725, 0.25349046546220777), (0.42725, 0.2773769792318344), (0.4195, 0.297966924071312), (0.438, 0.2702222961783409), (0.4365, 0.3134315768480301), (0.43275, 0.3038997223377228), (0.43775, 0.2975861769914627), (0.4315, 0.29862761580944064), (0.43575, 0.3426181004047394), (0.433, 0.3506606947183609), (0.4405, 0.3203036288022995), (0.4345, 0.31274220037460326), (0.43975, 0.3213425762653351), (0.43375, 0.3026667101383209), (0.433, 0.2871693157553673), (0.4405, 0.2965142420530319), (0.43575, 0.266223925948143), (0.43675, 0.2835199303627014), (0.43825, 0.3114169362783432), (0.4325, 0.312628387928009), (0.4335, 0.3021200456619263), (0.437, 0.32525139677524567), (0.43325, 0.2950660983324051), (0.43525, 0.2983211594820023), (0.4365, 0.31444405353069305), (0.43275, 0.3188811783790588), (0.433, 0.31676283597946164), (0.43225, 0.3282854187488556), (0.43075, 0.3268899772167206), (0.4365, 0.32817186558246614), (0.43275, 0.28153341841697693), (0.438, 0.3061331173181534), (0.4375, 0.32695393562316893), (0.4405, 0.3042414892911911), (0.437, 0.32961552011966705), (0.43775, 0.28222845351696013), (0.43725, 0.25099723613262176)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.60      0.86      0.70      1000
           7       0.00      0.00      0.00      1000
           9       0.35      0.89      0.50      1000

    accuracy                           0.44      4000
   macro avg       0.24      0.44      0.30      4000
weighted avg       0.24      0.44      0.30      4000

Competition_DC_3
VAL: 
[(0.25, 0.04539239990711212), (0.25, 0.0939168337881565), (0.396, 0.11786670678853989), (0.425, 0.15404279613494873), (0.428, 0.19359765738248824), (0.41, 0.21025887111574412), (0.425, 0.2532032940760255), (0.445, 0.24311571361124515), (0.433, 0.27205386623740196), (0.429, 0.2844371333681047), (0.428, 0.3079838444199413), (0.428, 0.3212093838695437), (0.439, 0.3412908539138734), (0.439, 0.3341454144194722), (0.436, 0.37167221363447606), (0.441, 0.36633644412457944), (0.444, 0.37304128459095953), (0.445, 0.37334966015536336), (0.441, 0.3616837582625449), (0.445, 0.38789463948458436), (0.452, 0.39739035521866756), (0.45, 0.3819490233696997), (0.445, 0.42511164185777306), (0.448, 0.38211484385700895), (0.451, 0.3642001649560407), (0.444, 0.39237209195178), (0.447, 0.35336962567456065), (0.453, 0.41888885699771344), (0.456, 0.32967351163737474), (0.45, 0.3464073397973552), (0.446, 0.32988454222492875), (0.444, 0.3568630316690542), (0.44, 0.37216003253590313), (0.443, 0.41753327230818105), (0.446, 0.35227529709460215), (0.444, 0.35337148728303147), (0.451, 0.3903241931345547), (0.449, 0.3812777430182323), (0.445, 0.38585277219186537), (0.441, 0.4113360491711646), (0.442, 0.42791004631877877), (0.449, 0.38854703069338575), (0.451, 0.3878414505124092), (0.449, 0.39410965870134534), (0.445, 0.36884636418521405), (0.451, 0.39562486331909896), (0.449, 0.4002407399527729), (0.452, 0.4555961642123293), (0.452, 0.39985293611697853), (0.448, 0.43214685546187687), (0.453, 0.4176707308795303)]
TEST: 
[(0.25, 0.044226822316646576), (0.25025, 0.09019724974036217), (0.40925, 0.11303245615959168), (0.4305, 0.14766822332143784), (0.43575, 0.18539260530471802), (0.41875, 0.20131079435348512), (0.4305, 0.24464485538005828), (0.4475, 0.23358500742912292), (0.4465, 0.2583946565389633), (0.439, 0.27387344324588775), (0.4385, 0.2942857506275177), (0.4395, 0.30852769804000857), (0.4445, 0.3275069435834885), (0.45175, 0.3206368762254715), (0.45025, 0.3514910981655121), (0.45225, 0.3517350007295609), (0.4505, 0.35967285645008085), (0.45075, 0.36082094311714175), (0.45025, 0.34783536243438723), (0.453, 0.37184458565711975), (0.456, 0.3787680997848511), (0.4485, 0.3660115554332733), (0.4515, 0.41048767399787905), (0.451, 0.36619995880126954), (0.45225, 0.34467012238502504), (0.44775, 0.374288019657135), (0.453, 0.3391002712249756), (0.45475, 0.4001018178462982), (0.4565, 0.3154532380104065), (0.4545, 0.33464035952091215), (0.45075, 0.31657400202751157), (0.44925, 0.34104800486564635), (0.44575, 0.3569097377061844), (0.45175, 0.39746986925601957), (0.45025, 0.3386169570684433), (0.44875, 0.33906273353099825), (0.454, 0.3719165667295456), (0.45375, 0.36311691069602964), (0.445, 0.3685846248865128), (0.4465, 0.3932365417480469), (0.4455, 0.40962013220787047), (0.45525, 0.37651519012451173), (0.45225, 0.37124410462379454), (0.45275, 0.3795389442443848), (0.451, 0.35775496113300326), (0.4555, 0.38440630161762235), (0.4555, 0.38339038848876955), (0.455, 0.4370608813762665), (0.4545, 0.3871783598661423), (0.4515, 0.4168396810293198), (0.4535, 0.39900654554367065)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           4       0.33      0.94      0.49      1000
           6       0.77      0.88      0.82      1000
           7       0.00      0.00      0.00      1000

    accuracy                           0.45      4000
   macro avg       0.27      0.45      0.33      4000
weighted avg       0.27      0.45      0.33      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [23]
name: no-alliance-23
score_metric: contrloss
aggregation: <function fed_avg at 0x7519f2cfac10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=23
Partitioning data
[[0, 6, 4, 1], [5, 7, 4, 1], [9, 8, 4, 1], [3, 2, 4, 1]]
[(array([32659,  7396,   871,  8190, 33489, 44742,  8515,  5082,   965,
       11208,   700, 39329, 40740, 22304, 44430,   974, 28487, 44470,
       46964, 24750, 26632, 28921, 43184,  6663, 28273, 30029, 46367,
       39397, 38683, 15767, 48108, 33229, 13937, 41040,   276,  9831,
       45344,   927, 35963,  8842, 30591, 38617, 45559, 43659, 27768,
        2617, 28750,  1234,  5549, 34259, 20709, 33584, 29856, 34421,
       39553, 49734, 33380, 49130, 11897,  7445,  5003, 11958, 18763,
       26018,   600, 46936, 26667, 12477, 25023, 32870, 43077, 35570,
        9077, 16939, 46873, 32990, 37041, 39271, 47483,    93,  9121,
       33662, 34724, 33478, 49672, 12807, 45297,  6328, 20985, 35633,
       46625, 12674, 31391, 16554,  3347,    77, 48059, 30820,  9945,
       18102, 22924, 12297, 48777, 17222,  5557, 21815, 36415, 42231,
       12944, 47726, 47068, 30888,  7256, 33268, 22951, 42165, 39628,
       29380,  3564, 21143, 37035,  2427, 28000,  2010,   757,  7203,
       22118, 21803, 41745,  5349, 28272, 47329,  7925, 47339,  1715,
       21458, 16196, 38711, 12492, 17316,  5414, 48001, 21964,  1831,
       28305, 11218, 24113,  5642,  7263, 47994, 25199, 43548, 40956,
       42456,  4567,  4726,  9614, 28716, 32233,  2622, 13656, 27202,
       11372, 38791, 22458, 26656, 43427, 17620,  3731, 45232,  5093,
       45594, 20452, 16553, 11122, 14666, 44552, 24983, 19094,  6952,
       13850, 14434, 32107, 43475, 26764, 38307,  1376, 22194,  3721,
        1887, 43253, 34989, 10141, 10334,  4239, 17200, 33573,   348,
       14732, 31450, 46419, 39196, 41880, 14325, 11002, 38748, 45652,
       29881, 16647, 13031, 11615,  3987, 32056, 23776, 21469, 40556,
       20574, 49349, 13348,  1757, 29593, 44770, 13229, 10265,  2459,
       31083, 41242, 46749, 19004, 42929, 11682, 16235, 33258,  7772,
       48317, 37646, 17237,  5640,   436, 37078, 42829, 16632, 36386,
       38217, 44806, 26918, 39774, 45166, 43045, 28014, 37774, 45607,
       29833,  8942, 35340, 16404, 26339,  2252, 39512,  6096,   745,
       36348, 13137, 46342,  7848,  6581,  5785, 23691,  5176, 40551,
       24184,  6941,  5708, 42510, 18816, 20104, 13825, 39091, 17072,
       41444, 14293, 31042, 24955, 39889, 17957,  3307, 34684, 27177,
       15726, 14678, 49462, 33216, 46404, 23050, 45862,  8950, 19228,
       40879, 35455, 35585,  6093, 47497, 26275, 30222, 23451, 17105,
       31894, 42415, 40675, 18924, 42671, 25551, 47821, 10327, 13188,
       34804, 20310,  4094, 15552, 41672, 11070, 23539, 44258, 20141,
       18334, 31820, 12930, 21831, 15220,  5497, 13838, 23683,  5263,
        3655, 36913, 36192, 39148,  6521, 20734,  5419,  9125, 42046,
       16611,  7824,  9862,  9237,  5055, 42712, 31808, 40568, 34986,
        4785, 21155, 23760, 42850,   914, 34056, 19230, 35435, 21192,
       22909, 28106, 35600, 49747, 14024, 40929, 42778, 23390, 42460,
       11303,  8466, 35891,  6179, 33278,  7825, 15664, 12411, 48526,
       44422, 25680, 29381,  7632, 15517, 31806,  7335, 10431, 29241,
        3403, 22157, 30800,  3823,  4959, 44314, 10176, 31780, 38747,
       44612, 45637, 38864, 22986,  3435, 38893, 47513,  3841,   819,
        7715,  7705, 35444, 45361, 48176,  1154, 36704, 48460, 39756,
        6750, 30212,  3976, 47352, 23412, 19949, 39464, 35977, 47962,
        1529, 14274, 46322, 47276, 35900, 27240, 20771,  6968,  4926,
       34394, 29391, 28860, 24394, 49088, 21572, 41311, 36110,  2179,
       32060, 39326, 23517,  7025, 15536,  9021, 35480, 14940, 33690,
       42679, 33298, 32340, 38853, 26085,  2898, 11204, 47858, 26355,
       14163, 25793, 25430, 12641, 26415, 28501, 26478, 19239, 36396,
       45140, 30588,   863, 21941, 48027, 22295,  1035, 21958, 44573,
       35131, 38771, 34447, 39010,  7137, 44579, 35577,  3528, 48664,
        2810, 39537, 10665, 45895, 42836, 34829, 22332, 24673, 34139,
       12407, 24577,  3554, 42311,  1053, 31775, 32608, 35185, 30911,
        8254, 44215, 31281, 36711, 43486, 42665, 47605, 45665,  1269,
       36457, 34752, 45749, 38906,  1310,  9937, 34450, 43714,  3112,
        3225, 43032, 48447, 40088, 15777,  8375, 45767, 30600,  5677,
       43172, 47570, 42700, 23603,  6022, 39626, 22438,  4666, 47348,
       37656, 40827,  7822,  7362, 43758, 13778, 30435, 40529, 39501,
       25879, 11565, 42979,  1315, 41148, 16505,  9013, 36182, 29891,
       21382, 22139,  5173, 48334,  6844,  3442, 46333, 30950, 19585,
       22902,   158, 35839, 33872, 21161, 21659, 18363, 31902, 26710,
       15963, 36631, 18507, 38640,  6603, 48367, 39932, 22259, 49605,
       48881,  6569, 34425, 32302, 47015,  8804, 28649,  1826,   520,
       29771, 11586, 43914, 15667, 44465, 43683, 31932,  5742, 30501,
       35413,  5726, 48889, 38708, 48159, 39803, 29083, 27787, 20511,
       33998,  3360, 32388, 12416, 23525, 21473, 33038, 21281, 25153,
       21702, 27644, 32026,  4497,  5786, 22183,  2415, 27946, 47571,
       16893, 25781,    82,   429, 46660,  1804,  3077, 38593, 39169,
       34819,  3999, 41897, 24529,  4741,  9873, 20439, 18699, 24874,
        7163,  1923, 17608, 13959, 14258, 10925, 37172, 38794, 18293,
        2573, 20828, 41058, 44717,  2629, 44760,  3577,  2832, 16383,
       14857, 10076, 35318, 25024, 13464, 25444, 45812, 32784, 45644,
       17420, 15009, 49055, 31431, 45609,  2486, 34881, 35324,  6770,
       37004, 17513, 42915, 39833, 44572, 46521, 47318,  6990, 43975,
       22949,  3322, 17161, 10477,  4857, 10766, 31239,  8792, 42675,
        1569, 47530,    89, 43055, 31514, 23958, 10890, 14075,  2078,
       34816, 48521, 15415, 35915, 26926, 37089, 44850, 41249, 32786,
       39036, 23591,  8405, 22246, 33066, 12136, 28823, 32797, 28074,
       43228, 13310,  5607, 27428, 44516, 35187, 46036, 26844, 24239,
       31751, 48997,  6879, 18875,   343, 17266, 39356, 14490, 13903,
        4561, 19358, 46234, 46649, 24428, 18698, 44549, 26293, 24979,
       45470, 41404, 29318, 34327, 23317, 39769, 49314, 18562, 22270,
       22335,  2800,   547, 45258, 14035, 22067, 44824, 43359,  4326,
        1242, 10236, 29473, 14005, 43600, 11558, 29876, 24625, 37652,
       48113, 40520, 31288, 27089,  5301, 27994, 44266, 45455, 14904,
        6889, 36251, 36107, 32502, 36067, 12743, 37849, 14117, 36929,
       11955, 35291, 15756, 15299,  7992,  8374, 37260,  4909,   212,
       49234, 22816, 35613, 22422,  8244,  6908, 16848, 17986,   606,
       25710, 21615,  4171, 16882,  5282, 42007, 36400, 32462, 44488,
        7312, 23685, 31072, 20595,  5616,  4648, 30999,  3678, 31951,
        6122, 11037, 43932,    45, 15506, 49787, 49075, 47836,  1660,
       42802, 12914, 34097, 10248, 36611, 20079, 25463, 24470,  2306,
       40758, 48610, 11134, 19522,  3085, 33832, 16673, 19866, 31100,
       34802, 41607, 47824, 48294,  1565, 10122, 37098,  5885,    64,
        9610, 11744, 17539, 17950,  5287, 32235,  9524, 15797, 13240,
       49768,  9717,  5144, 11403,  7408, 14336, 46078,  4244, 29616,
       11308, 19564, 30218, 30712,  8339, 20536, 16755,   119, 40751,
       26937, 31147,  8093, 17014,  3034, 30445, 21850, 40311, 45201,
       44311, 42658,  9781,  4500, 38735, 19382, 26065, 26652, 36614,
       40431, 19341, 11163, 30103, 43290, 12080,  7587, 37428, 24541,
       19481, 21181, 48215, 47362,  1421, 32295, 10450, 28798, 27576,
       22419, 41922, 19125, 41768, 11109, 31720, 15447, 35357, 17772,
       19581,  4533, 13884, 12275, 40352, 41908,   561, 10678, 30154,
       30553,  4040, 29205, 17460, 17128, 37743, 19454, 29143, 18276,
       42663, 15814, 24873, 34118, 42976, 17017, 26294, 11954, 23020,
       31297, 18994, 38031, 12348, 14512, 34721, 35134, 11972, 48749,
       47001,  7019, 36040, 25388, 38192, 25135, 40250, 22499, 32988,
       29044, 16077, 41807, 15114, 43893,  9088, 12946, 12169, 29911,
       27371]), [0, 6, 4, 1]), (array([33576,  1922,   839, 20020, 33494,  4878,  8287,  3388,  4639,
       26548, 44538, 47095,  9288, 24553, 17293, 25774, 18928, 25740,
       29970, 33454, 31240, 18574,   339, 49070,  7518, 32122,  8282,
       17670, 25356,  9284,  8261, 46436, 47209, 14026, 47757,  8020,
       48181, 32199,  9391, 47884, 47211, 45043,  7846, 48843, 49552,
       22099, 42348,  5755, 15186, 37632, 23641, 47463, 41115,  4319,
       30140, 31439, 42645,  1849,  9537, 25722, 26836, 11560,  4579,
        4990, 15980,  1198, 22059, 26969, 43455, 39929, 47386, 44532,
       19012,  1280, 35534,  6338, 38600, 15301, 27691, 28659, 45731,
       21186, 29256, 49380, 49327, 15371, 34755, 18083, 14437,  3810,
        4920, 25583, 45108, 35034, 16852, 48189, 23234, 31782, 30196,
       11201, 47388, 24344, 10803, 14932, 28730, 20669,  5439, 29608,
       48555,   173, 36227, 43399, 35762, 26747, 23874, 19756, 40620,
       13872, 12912, 22890, 45215, 38366, 43979,  6958, 42088, 34763,
       22779, 25057,  8252,  4312, 29961, 48011, 23515, 46916,    27,
       41062, 29713, 41789, 13121, 17740, 37749, 25515, 28660, 11770,
       45448,  9665,  6697, 49188, 36954, 27377, 13073, 44706, 31355,
       18131, 22932,   784, 42776, 48207, 19182,  1618, 18569, 26418,
       46708, 16470,  2686, 29857, 41358, 10768, 19334, 24744,  7981,
       32598,  8240, 30931, 35892, 26587, 23689, 22941, 22473, 48161,
       46264, 19757,  8198, 25903, 27094, 45636,  1860,  1993, 43654,
       37921,  5729, 30370, 14875,  5561,  8894, 25870, 40934, 37451,
       47661,  3229, 16510, 22590, 21233,  7140, 49867, 22021, 26143,
       11347, 14033, 31781,  4862, 39757,  5305, 32957,  2916, 32889,
        6140, 39662, 41406, 41759, 30042, 37434, 35640,  7082, 25517,
        5690, 28306, 43599, 30235, 33578, 49661, 29302, 32364, 27633,
       33534,  5411,   670, 12051, 28321, 30523, 33142,  4641, 19652,
       17343,  1928, 10597, 41442, 39742, 26501, 46030, 47046, 46816,
       32347,  1898, 15073, 19466, 21995, 22115, 10584, 27374, 33469,
       49231,  2319, 34592, 25090, 20094, 20618, 12994, 30434, 39408,
       37683,  5258, 23773, 42598, 41795,   968, 42338, 26249, 38142,
       17627, 32374, 23486,  1783,  5710, 30883, 37695, 12985, 45147,
        8238, 49630, 23000, 12163, 38678, 47777, 48773, 29359, 39064,
       36495, 42284, 26733,  4088,  8343, 28262, 26217, 13442,   329,
       14319,  8907, 49109, 10277, 13039, 45127, 21394, 21576,  8498,
        7969, 46709,  7922, 45523, 47056, 14219, 21820, 26166, 35980,
       16582, 22980, 16187, 18802, 25241, 25132, 40010, 21409, 17129,
       29036, 32995,    11, 37100,  3213, 23828, 21857, 15647,  3364,
        5989, 19931, 33775, 36205, 33712, 49310, 32660, 17113, 31350,
       25370, 33921, 44986, 44374,  5812, 29937, 49116, 33585, 31341,
       12690, 45561, 27529, 28152, 23794,  3138, 34900, 44378,  4782,
       19372,  8744, 20576, 24948, 35407,  1927, 28257,  8360, 22625,
       30030,  4513, 10772, 46368,  6164, 13237, 31876, 40570, 18590,
        2119, 39136, 16091, 11414,  3473, 33527, 13547, 43229, 27916,
       27039, 31276, 22457, 24729, 44091, 17913,  3146,  1544,  4554,
        2211, 49648, 30797,   114, 23903,  1900, 28639, 36174, 15850,
       29557, 27098, 27910, 45747, 49103, 41473, 28392, 18938, 34990,
       22901, 27574, 25594, 42651, 24311,   825, 38196, 17616,  3627,
        6235, 33299,  5116, 31924,  9347, 49925, 37980, 47321, 44228,
       43724,  1113, 26422,  2620, 28977, 48579, 34260,  5792, 18339,
       29872, 46228, 29369, 41180, 38194, 10927, 40998, 18431,   388,
        6734, 23627, 14947, 38667,  6398, 21096, 20170,  9906,  9472,
       22031, 16162, 13284, 10687, 38286, 38225, 22669, 11260, 20175,
       40478, 39246, 10259, 29859, 15072, 47275, 48960, 12432,  3054,
       20423, 12364, 23585, 38094, 29486,  9719, 11990, 16025, 42887,
       42238,  3151, 43322, 13950, 29499, 30917,   669, 30794, 29582,
       13815,  8630,  2293, 40156,  6249, 24884, 46319, 30228,  3494,
        4103, 34852, 44963, 42662, 16426, 33667,  7563, 31075, 14090,
       14474, 31227, 24080, 46132, 10593, 41294, 13636,  7536, 37577,
       22673, 40523, 13164, 12108,  3952, 45221, 12406, 16194,  1557,
       26745, 13764, 10415, 16135, 11649, 32186, 39047, 40377, 43926,
       36945,  6212, 32969,  9963, 14744, 13282,  3936, 44705, 15503,
       10404, 47244, 46276, 37775, 49154, 21016, 33458, 18669, 31003,
        4973, 41216, 11862,  5067,  9962, 46415,  4286, 32224, 15715,
       31701, 38995, 40597, 37345, 16689, 14789, 25785, 40798,  2673,
       33666,  9695,  7595, 18014,  5919, 14957, 33370, 41101,  9254,
       42849, 41665,  5051, 27479, 48801, 48488, 29237, 38263, 13231,
       41093, 15131, 37469,  9780, 32229, 34144, 47735, 42312,  7100,
       27461, 40240, 19178, 46476,  8639, 13363, 39473, 18314, 33482,
       49204, 38824,  7365, 46789, 21222, 21081, 47868, 49902, 48018,
        6234, 14113, 31755,  4058, 22946,  2131, 15663, 39505, 20303,
       25692, 17437, 20060, 28407, 49597, 17845, 21708, 25834, 18473,
        7383, 47622,  7984, 44761, 44002, 23231, 21694, 30799,  9776,
       41191, 13102, 31429,  3695, 29635, 16659, 17123, 20373,  9153,
       28387, 23179, 41202,  9544, 28076, 37639, 25941, 48191, 27801,
       16020,  9342, 34119, 44747, 14469, 18690, 48309, 22580, 29254,
       46968, 30156,  6505, 46940, 36372, 18434, 16254, 43567, 15036,
       41100, 23637, 45645, 29404,  1952, 16073, 20054, 15668, 36671,
       45205, 34671,  3507,  4156, 30593,  1238,   674, 32590,   435,
       33971, 42313, 38092, 23022, 49951, 18912, 23421, 35598, 32280,
       26047,  2787,  5178, 31033, 41931, 49355, 28785, 32284, 15196,
       19721, 25383, 13663, 44891, 40116, 39853, 21642, 12333, 28614,
        3560,  9915, 30915, 33862, 23200, 41246,  5940, 20904,   831,
       40757, 17634, 49421, 35605, 12412, 25117,  6851, 15275, 43050,
        9582, 27238,  5076,   375,  3512, 34982, 27272, 49509,  8450,
        4261,  7738,   250, 23504, 40509, 11776, 31079,  5224, 22013,
       24486, 33640, 18885, 25479, 37611, 16711,  1724,  4206, 31395,
       43583, 26452, 26878, 44309, 44499, 31345,   493, 49326, 17409,
       36686, 34830, 20057, 49483, 38596, 27402, 29146, 48279,  9938,
       19226, 44575, 34533, 40518,  5084,   565, 12800, 40024, 18594,
        9407,  7147, 17626,  5947, 20731,  5971, 25459,  9232, 28640,
       16813, 31634, 44876, 37248, 49491, 24075,  8432, 30943, 38610,
       14501, 34902, 27716, 23489, 13217, 15456, 46135, 23107, 14524,
        8371,  9946, 29894, 41869, 34778,  5515,  5841, 12920, 11799,
       43251, 45672,  3212,  5629, 40386, 25278, 45602, 21245, 26450,
        9584, 47980, 18057, 44647, 14851, 19202, 49221, 41602, 21661,
        5666, 21199, 36086, 34813,  8124, 24043, 27404, 44589,  7401,
       26292, 11532, 32679, 42669,   427, 27330, 34400, 21627,  7659,
        7605, 12516, 14182,  3020,  6074, 31348,  5559, 18345, 39820,
        5791,  1973, 34064, 28393, 33194, 43398, 47540, 13843, 16272,
       21020, 25162,  3523, 40713, 15610, 46408, 39132,  6928, 34385,
       48182, 45834, 24580, 34940, 13877, 13849,  8995, 26267, 28626,
       21154,  6589,   599, 10298,   761, 47423, 17726, 16937, 34701,
       25852, 31351, 36058, 18134,  8944, 39349,  2259, 35033,   874,
       10479, 37844, 16576, 38650,  9646, 46240,  5567, 39692, 45988,
       24923, 33261, 27607, 14409, 17995, 13615, 30792, 48799, 26115,
       49409, 32168,  8625, 33561, 38911, 35783,  5834, 19265, 30716,
       27792, 26405, 33198,  6040, 20353, 17334, 22358, 36941, 42335,
       41259,  6736, 13701, 27970, 43984, 29875,  9889, 40422,  4021,
        9442, 49588, 43608, 20616, 21933, 29999, 41948, 44217, 12964,
       17309, 46219, 43609, 45866, 29974, 42785,  8305, 17205, 21808,
       40076]), [5, 7, 4, 1]), (array([ 2818, 19498, 43780, 29418, 43121, 36659, 32407, 28871, 21289,
       37575, 23657, 40875,  4041, 10219,  5140, 42253, 41920, 27900,
       23111, 40015, 28713,  4675, 47967, 42420, 39043, 24627, 21008,
       41905, 22482, 19724, 15215, 13711, 41089, 26016, 31291, 44860,
        2637,  2985, 26222, 47315,  7571, 37413,  2853,  1612,  9434,
       10806, 16396, 31881, 37678, 43903, 39581, 41899, 16046, 17404,
       18796, 17142, 30823,  5776,   428, 15082, 21960, 31956, 22285,
       17176, 41840, 17158, 38406, 26828,  6671,  3977, 47262,   269,
       33568, 22240, 22538, 28010, 29059, 15936,  7708,  1638, 22464,
       13429,   915, 41128, 18878, 27025, 38793, 22127,   408, 32766,
       38017, 36713, 19805, 22182,  4489, 33784, 49360, 47073, 40638,
       31869, 20518, 26561,  1767, 16803, 39930,  7583, 18779, 22623,
        4121, 42473,  7302, 38378,   270, 33483,  2313, 49583, 17289,
       27311, 46280, 22199, 47512, 35218,   867,  5668, 37074, 41415,
       39371, 40976, 24076,   881, 18963, 28747, 41771,  2609, 15901,
       36549, 13407, 20380, 46346, 30220,   672, 49831, 29073, 37040,
       15652, 21614, 34767, 39264, 21887, 44404, 10698, 14970, 35197,
       25094,  4584, 41161, 43882,  8992,  1000, 33813, 26769, 18943,
       21745, 36250,  6903, 41363, 20172,  3846,  2308, 48816, 26101,
        2890, 42994, 27975, 47697,  1078, 40043, 34223, 21170, 26511,
       23908, 19086, 42058, 29000,  2281,  9229, 26622, 48606, 19876,
       37070, 46654, 38481, 37762, 27755,  2829, 28783, 22543, 39449,
       33538, 34390,  1043, 12568,  9880, 17201, 15252, 11139, 15495,
       49057, 10762, 40819, 15027,  4289, 26012, 49208,  9511, 25736,
        9028, 25433, 28390, 40166,  6258, 44605, 23840, 36031, 35019,
       42139, 27815, 14112,  6364,  1940,  2671,  5203, 47435, 36797,
       19824, 28514, 39567, 24027, 49783, 21401, 16078, 31233, 34402,
       15249,  9432, 15031, 13705,  6984, 26506, 10309,  4003, 13157,
       42617,  6238,   190, 44059, 39613, 26623, 47713, 40070, 31462,
       46453,  1479, 14054, 17167, 27352, 35896,  6667, 30087, 18644,
        4272,  1512, 28016, 30639, 44416, 39336, 20762, 43408, 22340,
        9068, 26440,  3305,  9960, 14728, 43705, 12544, 39702, 30601,
       16700, 20942, 10876, 10139, 13233,  7455, 49253, 26105, 17130,
        4222, 35671, 23669, 27410, 29600, 47378, 21028, 36803, 23883,
       36744, 39075, 43175, 39139, 17617, 28768, 45617, 34615, 21541,
       14710,  3482, 33124,  4828,  1914, 48374,  9648, 20873, 36892,
       25783, 15322, 28134,  5600, 15630, 27153,  9567, 22518, 46068,
       48959, 27618, 21998, 42371, 21710, 48805,  3430, 25632, 31037,
       33225, 48081, 37710, 34468, 26538, 10688, 42270, 23960,  5687,
       15951, 37335, 25047, 28707, 32465,  7287,  8229,  5855,  1357,
       18593, 40270,  2721, 35726, 36108, 41240, 23872, 48641,  1261,
       39802, 23148, 43699, 15699, 27027, 18295,  6459,  4916,  1721,
       13355, 10016, 21568, 36255,   987, 35778, 31938, 29362, 39916,
       33119, 47791, 37064, 45635, 22862, 36876,  1289,  3681, 22440,
       14049, 13684, 12128, 18307, 47391, 45608, 28985, 32207, 44362,
       41281, 22147,    62, 33888, 17854,  4287, 30483, 45115, 44570,
       35909, 12319, 33651,  4474, 22600, 38808,  1702, 48457,  3313,
       30488,  3770, 13836, 13883, 46392, 32974, 21716, 23707, 40881,
        4794,  8678,   901, 19748, 23845, 20962, 45784,  7443, 10004,
         259, 38339, 14866,  3765, 34825, 48874, 46386, 35828, 41647,
       34248,  7535, 20740, 37641, 29734, 21830, 23399, 19768,  9269,
       41403, 30210, 15914, 24270,  2663, 49567, 41941, 10763, 19336,
        8831, 39350,   430, 26108, 48943, 34527, 13720, 11993, 23011,
       28450, 17252, 33570, 38322,  5904,  8384, 32281, 35628, 23818,
       12286, 28295, 41187,  8968, 30660,   971, 37483, 10245, 22218,
       27436, 21164,  2640,  4936, 46617,  7218,  7751, 23279, 42434,
        3296, 18627, 44270, 43549, 22050, 41265,   336, 19180, 35151,
        7110, 35443, 16749, 23133, 49352, 26512, 16636, 29471, 15625,
       15155, 31053, 33408, 32300, 24067, 11045, 11254, 30967, 49981,
       31549, 15773, 30815, 47611, 14649, 41554, 28200,  2488, 31939,
       40707, 41878, 33480, 47144, 12921,  1624,  2842, 49685,  2691,
       36523, 39977, 28711, 17145, 24659, 43239,    28,  7634, 15958,
       46077, 44450, 14921, 22820, 47591, 43586,   979, 15643,  2703,
       37466, 43238, 38869, 20810, 48235, 37214, 37987, 37977, 11997,
       42967, 31547, 48505, 18512, 22749, 24664, 12997, 18550, 23817,
       37234, 28219, 13005, 41788, 15409, 46049,  5844, 26351, 15864,
       43460, 10489, 15737, 45255, 13772, 37154, 18841, 47126, 40285,
       22677,  5998, 47374, 48969, 32017, 37960, 40111, 22750, 30125,
       20253, 43529, 40981, 28084, 43682, 46884, 15140, 34255, 19426,
       36965, 37241, 22203, 26360,   526, 20173,  8279,  1595,  9550,
       47855, 20769, 39752, 17402,  8361,  8504, 46185, 35564,  3957,
       34193,  8541,  6415, 47344, 49292, 42629, 26953, 20343,  1813,
       13466, 27535,  7382, 45729, 29670, 17150, 30425, 28228, 29329,
        8551,  8233,  2155, 36160, 16897,  5957,  5187, 26114, 39067,
       23654, 40486, 39596, 23671,  4924, 47384, 27417, 11810, 45712,
        5790, 28260, 47664, 36428,  2442, 12124, 32519, 44942, 37875,
       44162,  8939, 10237, 28769, 29376, 20485,  2461, 13855, 39005,
       43746, 39467,   372, 31476,  2327, 14787, 10447, 34784, 25017,
       19307,  4527, 30869, 37805,  7119, 19990, 25147, 24658, 45764,
       29361, 42643, 48860, 28892, 22985,  3383, 37288, 44563, 27860,
       34744,  8263,  4989, 24241, 28744, 29278, 36976, 11957,  1526,
       28903,  5501, 39565, 12261,  5931, 30681, 42769, 32929, 19746,
       45226, 25429, 41822, 16058, 19452, 15256, 42938, 18502, 24799,
       49390,  1974, 33209, 29121, 46507, 25817, 47519,  2339, 37512,
       48000,   617, 14659, 34832, 27506, 42113, 34862, 36594, 15779,
       21585, 21333, 17515,   261, 14871, 28784, 31899,  7706, 14859,
       23338, 33063, 16286, 32161, 29330,  7077, 24566, 44412, 15512,
       35105, 10100,   227,  1052, 49381, 45977, 31890, 29633,  4609,
       39256, 11890, 22450, 47968,  3485, 36938, 42292, 28958,  8221,
        1631, 42065,  4101, 15740, 32736,   168,  8475, 36388, 40046,
       23598, 11580, 22882, 16250, 35145, 29314, 25608, 16850, 49616,
       20777, 14713, 25700, 26059, 15891, 26737, 19616, 38278, 41289,
       11664,  2769, 38420,  6295, 48617, 40537,  8175,  4230, 19710,
       33114,   699,   396, 42281, 16275, 19399, 36672, 11683,  1571,
       36751, 36033,  2390,  8812, 19867, 20197, 48995, 15205,  6429,
        6191, 48636, 37269, 24284, 29311, 42918, 30014, 32603, 20214,
       43447, 34145, 40576, 28347,   311, 22165,  1410, 46731, 18926,
       38974, 20510, 26578, 39109, 44541, 45704, 31512, 38725, 41279,
       19628, 49894, 16855, 48970, 21437, 14900, 41794, 17856, 29325,
       25652, 39204, 49705, 20783,  5959, 10491, 11791, 18063, 43217,
       38892,  6272, 20988, 37995,  6120, 10393, 48740, 20914, 11427,
       17755,  8294,  3386, 24398,  2039, 28906, 43641, 29188, 49494,
       13526, 34068, 38571, 26637, 45580,  2268,  1694, 45859, 42211,
       26794, 15832,  2280,  9549,  7827, 23662, 21153,  4423, 12923,
       25150, 30245,  5759, 41322, 38549, 29285, 32410, 42734, 34426,
       43847,  6371, 14722, 22529,  6701,  2173, 22586, 18495, 10748,
       39240,  3840, 31107,   947, 11579,  6899, 37203, 14067, 46259,
       46359, 28539, 43132, 30486, 22163, 19500, 35454,  8875, 15496,
       21073, 12774, 33327, 23311, 41356, 19609, 11698, 39974, 12622,
       20874, 48845, 10953, 23578, 49121, 36085, 45011, 49436, 18206,
        2511, 15704, 43605,  9765,  8571, 12373,  3414,  2227, 36872,
        3059]), [9, 8, 4, 1]), (array([ 6304, 37804, 27012, 40216, 43318, 23884,  9846,  6309, 25738,
        3864, 20721, 17440,  3813, 15453, 26707,  4197, 47851,  2127,
       29685, 20447, 10869, 41892, 32250,   846,  8210,  6607, 40165,
       41826, 44173, 15421, 19820, 16619,  9293, 11998, 11477,  6014,
       40246, 37973, 43462, 37662, 11387, 16366, 46825,  7753,  7086,
        1608, 46058,  6405, 34099, 41285,   998, 13700, 16301, 15603,
        9034, 11564, 22420, 38304, 12428, 28953, 25490, 15383,  4558,
       27233, 35124,  1510,  8319, 30868, 33133, 29810, 42752, 16547,
       22030, 49970, 24726,  1958, 43922,  4674, 18814, 37770,  6619,
       12111,  8356, 13001, 30817, 38992,  1057,  7432, 35092, 22620,
        9024, 18127, 29012, 14368, 31178, 18998, 28864, 21686, 43843,
       46121, 21053, 22913, 14108, 26849, 10193, 24749, 47100, 10746,
       41412, 35128, 25882, 32451,  1539, 40584, 12174, 42930, 39108,
       27407, 15974, 10575, 47901, 23376, 11231, 13349, 20252, 13013,
       11120, 22406, 41016,  8781,  6913, 30266,  4402, 49079,  1556,
       34984, 44263, 27909, 20329,  5321, 47554, 38679, 21085, 19126,
       32297, 46542, 31179, 32846, 22919, 14118, 28253, 25281, 14070,
       32541, 33092,  2447,  2011, 20820,  1895, 23498, 22793, 49741,
       46182, 41229, 10958, 29215, 43411,  7906, 39294, 15545,  5455,
        2353, 45502, 11047, 39885, 11714, 27275, 48907, 29705, 20770,
       46215,  3571, 26357,  5823, 36211, 29520, 33267,  3905, 22294,
       36422, 32936,  8645,  9830, 11105, 43328,  7671,  6649, 46157,
       14089, 29839, 48040, 14171, 14703, 40232, 26044, 24060, 38364,
       17615,  1961, 17243, 45646, 16070,  3194, 24081, 48999, 11724,
       37906, 15053, 16295, 23145, 41917, 10942, 17047, 33463,  9720,
        4025,  5520, 44550, 22662, 27114,  5218, 22411,  1573, 38781,
       35323, 15247, 38932, 46417, 46605,  6540, 47722, 17018, 26559,
       45382, 38634,  9183, 23594, 18570, 21510, 42744, 14472, 27738,
        7151, 37255,  2784, 32899, 11137, 20558, 15977, 37686, 18143,
       25200,  9940, 44644, 32377, 33474, 40372, 33083, 30632, 11286,
       16833, 48733, 14595, 32807,  6626, 16739, 26425, 21099, 14811,
        7466, 12583, 17981, 45951, 39499, 20651, 31621, 36423, 38349,
       31040, 38038, 37856, 20673, 43826,  8055,  6616,  2957, 11454,
       16215, 20038, 30940, 10926, 40924, 28664,  2789, 14619, 10440,
        4616, 16462,  1258, 16581,   802, 11270, 34307, 43990, 34488,
       25101, 38592, 15314, 33519, 14785,  4038,  1107, 14157, 33054,
       35653, 30572, 24773, 38801, 44535, 20073,  3963, 14383, 38216,
        3569, 29625, 19280, 14285, 10671,   912, 14601,  4168, 15398,
       15789,   990, 28667, 26386, 47171, 18163,  3484, 37373, 41024,
       13934, 33183,  7807, 47936,  1475, 13487,   885,  2514, 17693,
       27239, 42227,  2004, 30437, 11003, 24927, 29779, 33175, 33350,
        6028, 30786, 26999, 39368,  7521, 20009, 30631, 27186, 17302,
       43244, 31884, 23673, 40398, 27546, 25440, 40403, 31454, 41884,
       11843, 42894, 34203, 13290,  7330, 27948, 42428, 47710,  4511,
       48779, 30665, 17834, 36502, 26976, 24064, 39847, 15443, 14106,
       40348, 33498, 40217,  9803, 46564, 43572, 21262, 29180, 34222,
       48169, 22511, 26588, 46819, 11884, 16446, 18204, 14229, 27498,
        6334, 26176, 43823, 33531, 32409,    90, 25982, 49914, 13814,
       11779, 10412,   522, 23062, 15369, 32763, 32480, 15589, 44634,
         804, 38459, 11089, 13273, 34873,  8706, 11587, 32057, 34885,
       21500, 13081,  2191, 27865, 47368, 27685, 33842, 36382,  5647,
       44879, 48414, 25030, 39062,  1798, 28834, 46575,   283,  3558,
       38182, 48092, 48608, 11443, 46214, 27594, 25398, 23453,  4237,
       26308,  6177, 43287, 10378,  5219, 31361,  4656, 17955,  8219,
        4634, 17273, 24623, 45533, 15929, 16609,  5384, 43692, 21063,
       26630, 19615, 33776, 47584, 12060,  2450, 32094, 22619,  5704,
        6943, 18099, 23634, 28174, 47908, 32478, 22743, 16908, 45133,
        8688,  7903, 14134, 40530, 40794, 28885, 33383, 45598,  5706,
       45786, 41625, 38354, 47795,  5298, 24221, 48195, 40098, 27017,
        5676,   621, 46153, 26497, 29070, 36142, 26329, 19059, 15126,
        9018,  1284, 27372, 14786, 30522, 34969, 20280,  8913,  8631,
        1704,  9750, 21052, 10604, 45022,  7043, 45110, 26943, 49687,
        6652, 12717,   477,  1348, 14853, 35378,  4137, 49586, 39945,
       36552,  1657, 30725, 21693, 33099,  9592,  4642, 41770, 43420,
       16105, 16880, 23943, 21794, 17450, 10299, 32523, 48179, 31306,
        8701, 11845, 17707, 23076, 48360, 30614, 48605, 41608, 39209,
       40301,  1984, 43897, 19704, 26882, 39878, 29852,  9364, 47576,
       46145, 27531, 41077, 39293,  9500,  1396, 45396, 21582, 45410,
        2190,  5225, 33221, 45756, 21188, 44994,  9507, 35395,  3556,
       14726,   959, 25779, 41103, 31954, 17613, 26886, 33215,  1937,
       45293,  9041, 27259, 18409,  9285, 14798, 36345, 24265, 43494,
        1339, 29128, 15734,  5829, 15454, 20341,  3283,  4846,  8987,
       40036, 22274,  7670, 49475, 37528,  9273, 19528, 17260, 43222,
       42695, 34776, 41097, 34727, 20629, 24925, 32494,  8137, 22385,
       20967, 22369,  3046, 38728, 16672,  5227, 37677, 47232, 48796,
       21649, 33080, 10500, 36071, 36731,  8114, 30077,  9779,  4885,
       11794, 47147, 44750, 47120, 32201, 20435, 18197, 44061,  2528,
       32253, 16263, 25798, 24572, 26975, 42907,  6344,  7394, 25142,
        3509, 40219, 17039,  5880, 48429, 46591, 47796,  2105,  7957,
       35457,  8066, 37741, 31898, 32029, 12975, 39946, 15238, 14635,
        9250,  4199, 15909, 16101,  9749, 18185, 15730, 27303, 44283,
       25643,  2297, 18264, 26233, 23739, 39541,  1406,  1640,   381,
       33688, 27762,  3913, 40077, 30243, 16525, 35050,  6049, 44525,
       17552, 48945, 31588, 37588,  5769, 38929, 27584, 25300, 35694,
       33390,   524,    99,  8085, 28089, 36331, 19575, 26424,  3836,
        3103, 45856, 18458,   301, 31102, 13108, 31445,  1240, 48669,
       23533, 28559,  7300,  8556, 34530, 31251, 30039, 15662, 39277,
       12741, 49769,  8001, 20840,  5897, 14593,  6375, 10403, 16023,
       36236, 33625, 36703, 43145, 19063, 33650, 49820, 17030,  4764,
       44818, 42707, 20605, 14519, 11007, 20442, 14598, 33126, 33624,
       44415, 16834, 42606, 12894,  2958,  4299,  2656, 48375, 41984,
       17621, 41888, 25596, 33050, 16964, 34263, 38078, 26741, 16588,
        8690, 30471, 17732, 47178, 29029, 14643,  9085, 22697, 16962,
       31523, 26746,  6779, 24268,  4942, 31329, 45001, 31830, 13002,
       17860, 22745,  6241,  8893, 47060, 17268, 22303, 41579, 31245,
       49230,  7540,  7375, 24620, 44725, 27636,  1494, 29394, 31587,
       43195, 33399, 31711,  3009,   226,  4578, 42031, 49447,   126,
       22545, 28394, 11817, 13037, 42121,  4464, 25377, 46323, 11242,
       14662, 45105, 11829,  4661, 23546, 13470, 42341,  6988, 12893,
       30182, 18927, 33615, 19301, 45946,  7214, 25910,  1251, 27194,
       37239,  5635, 28245, 46679, 32348, 16745, 32737, 19983, 24068,
        7503, 25961, 44971,  3233, 14964, 26471,  6495,  8779, 45715,
       42991, 20273, 23142, 40749, 32311, 38902, 30062, 13444, 45968,
       21888, 46730, 45097, 13311, 33598, 19880, 12560, 23088, 28957,
        8090, 26230,  1548, 30244, 41563, 22689, 41736, 25827, 29230,
        9012, 37755, 37320, 33237,  6456, 47453, 21893, 24561, 38682,
        5975, 49875, 35878, 46928, 22145,  4607, 41862, 26074, 21530,
       34791, 37253,  2067, 27228, 34618, 19484, 33569,  2597, 32042,
       13899, 24157,  8201,  2320,  4352, 30165,  9318, 33646,  9459,
       18223, 19242, 21230, 17274, 13647, 49392, 41739, 41026, 38838,
       17457, 28344, 27773, 41701, 47208, 30086, 33196, 39824, 41378,
       40762]), [3, 2, 4, 1])]
Competition
DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.255, 0.04439314603805542)
DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.245, 0.04447525298595428)
DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.244, 0.045177888870239255)
DC 3, val_set_size=1000, COIs=[3, 2, 4, 1], M=tensor([3, 2, 4, 1], device='cuda:0'), Initial Performance: (0.209, 0.04494925320148468)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {0, 6}
D07: 1000 samples from classes {0, 6}
D08: 1000 samples from classes {0, 6}
D09: 1000 samples from classes {0, 6}
D010: 1000 samples from classes {0, 6}
D011: 1000 samples from classes {0, 6}
D012: 1000 samples from classes {5, 7}
D013: 1000 samples from classes {5, 7}
D014: 1000 samples from classes {5, 7}
D015: 1000 samples from classes {5, 7}
D016: 1000 samples from classes {5, 7}
D017: 1000 samples from classes {5, 7}
D018: 1000 samples from classes {8, 9}
D019: 1000 samples from classes {8, 9}
D020: 1000 samples from classes {8, 9}
D021: 1000 samples from classes {8, 9}
D022: 1000 samples from classes {8, 9}
D023: 1000 samples from classes {8, 9}
D024: 1000 samples from classes {2, 3}
D025: 1000 samples from classes {2, 3}
D026: 1000 samples from classes {2, 3}
D027: 1000 samples from classes {2, 3}
D028: 1000 samples from classes {2, 3}
D029: 1000 samples from classes {2, 3}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.06242937207221985) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06935057532787323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.10054781334102154) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09110854172706603) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.44, 0.07027108192443848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.07565721029043197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.10466695122420788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.323, 0.11809220957756042) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.08399075710773468) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.405, 0.08414981603622436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.10775397792458534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.355, 0.1430300070643425) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.09190552899241447) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.10456353434920311) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.15356273053586483) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.356, 0.16536943846940994) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.11256972487270832) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.11904050388932227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.17955271257087588) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.354, 0.1911297039538622) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.12555305766686797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.392, 0.12016960312426091) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.20252362125553192) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.376, 0.1672702697366476) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.13640131926350296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.12200692869722843) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1993113804385066) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.384, 0.1897293770611286) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.1368831439372152) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.1383049213439226) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.22230591307021677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.387, 0.205445771753788) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.15904230438452213) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.13590771255642176) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.251293695660308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.379, 0.1797317993193865) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.15739924981538206) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.422, 0.15744038719683887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.24946794794872404) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.2138650175333023) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO3']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.1844995580916293) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.425, 0.14759176074713468) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.453, 0.26954294674191626) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.2175305056720972) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.1748997853421606) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.394, 0.15057681515067817) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.27528590631484984) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.21681640201807023) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.18771850892249495) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.419, 0.16554798210039734) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.453, 0.27847595897689464) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.21323319018632172) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.18247125481208787) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.435, 0.17070766422897576) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.2834350539254956) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.395, 0.22056705191731452) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.18285514527070337) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.403, 0.18131607419252396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.31474325494887306) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.24355234099924564) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO5', '(DO3']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.18780017406749538) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.435, 0.16874198664724827) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.3316115417033434) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.26456464114785194) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.19670781180937774) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.417, 0.16220802706107498) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.2959042695024982) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.26752902963757513) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.19840400235541164) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.423, 0.1583345806375146) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.31961913318140434) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.29490510165318845) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.18598916844069027) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.43, 0.16195029193907975) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.3184883202421479) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.2883051442503929) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.19598884385882412) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.1877165179681033) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.31142598558124157) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.391, 0.28157265159487727) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO0', '(DO3']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.19697045290190726) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.429, 0.15464684414491056) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.3155714855492115) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.318990552932024) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.2031793672554195) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.433, 0.1795035262480378) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.3166995062762871) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.30872878755629063) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.21787757984537165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.406, 0.15639037918113172) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.2970145103922114) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.3136509017944336) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.18780296317650935) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.423, 0.163649878077209) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.3114716867348179) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.3074020326733589) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.22059571801614947) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.433, 0.1893907681265846) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.3092413095533848) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.3183441966474056) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO0', '(DO1']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.20916514804796316) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.423, 0.15984360825829208) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.32391816652193667) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.32452325627207756) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.22344490158307598) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.438, 0.15795182242617012) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.31389554242417217) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.29364362175762654) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.20980295378435404) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.16256980556435882) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.3094558977368288) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.30902393336594103) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.2427927883748198) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.438, 0.172209537031129) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.3004639228673186) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.3077876227349043) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.21943246288714) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.433, 0.1693664630856365) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.3018302199500613) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.3147669750750065) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO5', '(DO3']
DC 2 --> ['(DO0']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.20464108648221008) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.15779004772379995) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.3228493012492545) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.32545424076914786) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.2249987903048168) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.425, 0.15129904162697494) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.33258654280263) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.2981713104248047) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.2189047840454732) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.432, 0.16478929401561618) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.3131268822331913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.2898639529198408) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.23976967538380994) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.437, 0.13680738067626952) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.3240914299529977) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.31098702225089075) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.26743132545152914) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.435, 0.15336642744392157) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.3183338465003762) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.29397807869315146) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO0', '(DO1']
DC 2 --> ['(DO3']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.22200926326244372) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.16637110737338662) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.463, 0.3277354846068192) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2849781015962362) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.2310325984666124) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.43, 0.1780539224576205) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.34082126631680876) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.305679500490427) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.24331555450049927) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.426, 0.1806828739885241) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.2700339290930424) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.2903761856853962) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.24536206599330762) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.428, 0.1455563203766942) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.29866808546939866) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.3321658974289894) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.24411513333395124) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.426, 0.16899508057907223) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.463, 0.3368644592440687) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.3022850620001554) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO2']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.22766082035488217) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.439, 0.19103899770975113) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.453, 0.30517439023416953) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.29840491399168967) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.26170665217895295) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.435, 0.17531635687127708) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.46, 0.3181938112052157) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.311349544018507) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.2512911185588455) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.437, 0.1855864342227578) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.32490480185870546) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.30253161615133284) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.26836299225781113) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.44, 0.1780636076964438) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.3042046786075225) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.26503587636351583) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.2644183736159175) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.433, 0.18419042419269682) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.3092969073378481) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.3289306920841336) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO4', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.27090902762525365) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.16702529519051312) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.29270061531022656) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.2703943748921156) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.30554528541429316) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.444, 0.1992294986769557) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.3011443825309398) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.3237344397753477) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.3230813010636048) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.44, 0.1589848610498011) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.46, 0.3282448150955024) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.3125765206068754) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.3191213323500342) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.1908817695900798) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.30966182364488487) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.32222379998862744) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.3425175072739803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.433, 0.17851381738483907) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.453, 0.3314584676202794) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.31453494529426096) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.255, 0.04439314603805542), (0.276, 0.06242937207221985), (0.44, 0.07027108192443848), (0.455, 0.08399075710773468), (0.465, 0.09190552899241447), (0.459, 0.11256972487270832), (0.464, 0.12555305766686797), (0.461, 0.13640131926350296), (0.475, 0.1368831439372152), (0.461, 0.15904230438452213), (0.469, 0.15739924981538206), (0.478, 0.1844995580916293), (0.472, 0.1748997853421606), (0.476, 0.18771850892249495), (0.469, 0.18247125481208787), (0.473, 0.18285514527070337), (0.473, 0.18780017406749538), (0.47, 0.19670781180937774), (0.476, 0.19840400235541164), (0.479, 0.18598916844069027), (0.469, 0.19598884385882412), (0.479, 0.19697045290190726), (0.473, 0.2031793672554195), (0.477, 0.21787757984537165), (0.474, 0.18780296317650935), (0.479, 0.22059571801614947), (0.479, 0.20916514804796316), (0.475, 0.22344490158307598), (0.474, 0.20980295378435404), (0.472, 0.2427927883748198), (0.475, 0.21943246288714), (0.475, 0.20464108648221008), (0.477, 0.2249987903048168), (0.474, 0.2189047840454732), (0.476, 0.23976967538380994), (0.475, 0.26743132545152914), (0.469, 0.22200926326244372), (0.477, 0.2310325984666124), (0.478, 0.24331555450049927), (0.479, 0.24536206599330762), (0.476, 0.24411513333395124), (0.474, 0.22766082035488217), (0.474, 0.26170665217895295), (0.48, 0.2512911185588455), (0.479, 0.26836299225781113), (0.475, 0.2644183736159175), (0.475, 0.27090902762525365), (0.478, 0.30554528541429316), (0.478, 0.3230813010636048), (0.48, 0.3191213323500342), (0.476, 0.3425175072739803)]
TEST: 
[(0.2515, 0.043328432649374006), (0.2885, 0.05995646780729294), (0.4465, 0.06739776784181595), (0.4545, 0.0804382722377777), (0.46875, 0.08793868029117584), (0.46075, 0.10819000673294067), (0.46475, 0.12106116461753845), (0.46375, 0.1310386021733284), (0.475, 0.13236147272586823), (0.4615, 0.15334183567762374), (0.46875, 0.1515594100356102), (0.4725, 0.17810695588588715), (0.473, 0.16848605448007584), (0.4765, 0.1825898666381836), (0.46875, 0.17689818626642226), (0.47325, 0.17631846034526824), (0.47125, 0.18025645953416825), (0.471, 0.18914113581180572), (0.47375, 0.19263679319620133), (0.47625, 0.18182708764076233), (0.47225, 0.1914410907626152), (0.47825, 0.19095187890529633), (0.4725, 0.19708204543590546), (0.47725, 0.2125972318649292), (0.46925, 0.1841808511018753), (0.477, 0.21543486577272414), (0.47625, 0.2060390738248825), (0.47725, 0.22048716807365418), (0.47375, 0.208019160926342), (0.47275, 0.23706007981300353), (0.47225, 0.21386522436141966), (0.475, 0.19966941350698472), (0.474, 0.2190055533051491), (0.47175, 0.21194699239730835), (0.477, 0.2335251748561859), (0.47625, 0.2583850386142731), (0.47475, 0.2172045665383339), (0.47725, 0.2250151913166046), (0.4765, 0.23922903335094453), (0.4765, 0.23930497610569001), (0.4765, 0.23581532025337218), (0.4775, 0.2246263192296028), (0.4745, 0.25337376999855044), (0.47825, 0.24094891667366028), (0.478, 0.259063800573349), (0.4765, 0.2578876039981842), (0.4745, 0.2638760942220688), (0.4765, 0.29328053796291353), (0.4775, 0.3122776997089386), (0.4765, 0.3099437972307205), (0.47675, 0.3312773758172989)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.61      0.92      0.74      1000
           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           6       0.39      0.99      0.56      1000

    accuracy                           0.48      4000
   macro avg       0.25      0.48      0.33      4000
weighted avg       0.25      0.48      0.33      4000

Competition_DC_1
VAL: 
[(0.245, 0.04447525298595428), (0.25, 0.06935057532787323), (0.352, 0.07565721029043197), (0.405, 0.08414981603622436), (0.389, 0.10456353434920311), (0.376, 0.11904050388932227), (0.392, 0.12016960312426091), (0.382, 0.12200692869722843), (0.384, 0.1383049213439226), (0.416, 0.13590771255642176), (0.422, 0.15744038719683887), (0.425, 0.14759176074713468), (0.394, 0.15057681515067817), (0.419, 0.16554798210039734), (0.435, 0.17070766422897576), (0.403, 0.18131607419252396), (0.435, 0.16874198664724827), (0.417, 0.16220802706107498), (0.423, 0.1583345806375146), (0.43, 0.16195029193907975), (0.431, 0.1877165179681033), (0.429, 0.15464684414491056), (0.433, 0.1795035262480378), (0.406, 0.15639037918113172), (0.423, 0.163649878077209), (0.433, 0.1893907681265846), (0.423, 0.15984360825829208), (0.438, 0.15795182242617012), (0.431, 0.16256980556435882), (0.438, 0.172209537031129), (0.433, 0.1693664630856365), (0.431, 0.15779004772379995), (0.425, 0.15129904162697494), (0.432, 0.16478929401561618), (0.437, 0.13680738067626952), (0.435, 0.15336642744392157), (0.434, 0.16637110737338662), (0.43, 0.1780539224576205), (0.426, 0.1806828739885241), (0.428, 0.1455563203766942), (0.426, 0.16899508057907223), (0.439, 0.19103899770975113), (0.435, 0.17531635687127708), (0.437, 0.1855864342227578), (0.44, 0.1780636076964438), (0.433, 0.18419042419269682), (0.443, 0.16702529519051312), (0.444, 0.1992294986769557), (0.44, 0.1589848610498011), (0.443, 0.1908817695900798), (0.433, 0.17851381738483907)]
TEST: 
[(0.23225, 0.043421665340662004), (0.25, 0.06652063864469528), (0.345, 0.07260621449351311), (0.4005, 0.08074362868070603), (0.388, 0.10054817044734955), (0.381, 0.11419055730104447), (0.39725, 0.11531978017091751), (0.3775, 0.1179304256439209), (0.38325, 0.13432818323373794), (0.41475, 0.1329578117132187), (0.41625, 0.15371146613359452), (0.41625, 0.14334875351190568), (0.39575, 0.14661630326509476), (0.41325, 0.16119537097215653), (0.42625, 0.16676013445854188), (0.39775, 0.17844326263666152), (0.431, 0.16712770015001296), (0.41725, 0.16140947777032852), (0.41675, 0.1569194628596306), (0.42, 0.160773213326931), (0.42075, 0.18597963523864747), (0.414, 0.1536278073787689), (0.42375, 0.18069982481002808), (0.405, 0.15448224306106567), (0.41825, 0.1610268297791481), (0.42525, 0.18876399570703506), (0.4205, 0.1604743772149086), (0.42725, 0.16096442544460296), (0.4225, 0.1605348720550537), (0.427, 0.17135988730192184), (0.43025, 0.168985504925251), (0.4235, 0.15737647610902786), (0.4175, 0.15085196936130524), (0.42225, 0.1649650495648384), (0.4295, 0.13756153827905654), (0.427, 0.15510260909795762), (0.42625, 0.16518861263990403), (0.42075, 0.17800199216604232), (0.41775, 0.18026472240686417), (0.42325, 0.14664495515823364), (0.424, 0.1673314517736435), (0.42675, 0.1894923552274704), (0.42375, 0.1728304969072342), (0.4255, 0.183792786359787), (0.4285, 0.17680415755510331), (0.42225, 0.18127749091386794), (0.434, 0.16508620476722718), (0.435, 0.19644439554214477), (0.43325, 0.15606456923484802), (0.43075, 0.1858932104110718), (0.431, 0.17487990391254424)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           5       0.35      0.95      0.51      1000
           7       0.60      0.78      0.67      1000

    accuracy                           0.43      4000
   macro avg       0.24      0.43      0.30      4000
weighted avg       0.24      0.43      0.30      4000

Competition_DC_2
VAL: 
[(0.244, 0.045177888870239255), (0.25, 0.10054781334102154), (0.252, 0.10466695122420788), (0.38, 0.10775397792458534), (0.413, 0.15356273053586483), (0.395, 0.17955271257087588), (0.436, 0.20252362125553192), (0.441, 0.1993113804385066), (0.441, 0.22230591307021677), (0.464, 0.251293695660308), (0.456, 0.24946794794872404), (0.453, 0.26954294674191626), (0.449, 0.27528590631484984), (0.453, 0.27847595897689464), (0.458, 0.2834350539254956), (0.454, 0.31474325494887306), (0.468, 0.3316115417033434), (0.452, 0.2959042695024982), (0.449, 0.31961913318140434), (0.466, 0.3184883202421479), (0.459, 0.31142598558124157), (0.45, 0.3155714855492115), (0.446, 0.3166995062762871), (0.436, 0.2970145103922114), (0.465, 0.3114716867348179), (0.456, 0.3092413095533848), (0.459, 0.32391816652193667), (0.464, 0.31389554242417217), (0.464, 0.3094558977368288), (0.458, 0.3004639228673186), (0.461, 0.3018302199500613), (0.458, 0.3228493012492545), (0.461, 0.33258654280263), (0.456, 0.3131268822331913), (0.462, 0.3240914299529977), (0.461, 0.3183338465003762), (0.463, 0.3277354846068192), (0.466, 0.34082126631680876), (0.458, 0.2700339290930424), (0.459, 0.29866808546939866), (0.463, 0.3368644592440687), (0.453, 0.30517439023416953), (0.46, 0.3181938112052157), (0.454, 0.32490480185870546), (0.466, 0.3042046786075225), (0.459, 0.3092969073378481), (0.451, 0.29270061531022656), (0.456, 0.3011443825309398), (0.46, 0.3282448150955024), (0.455, 0.30966182364488487), (0.453, 0.3314584676202794)]
TEST: 
[(0.22875, 0.04402987053990364), (0.25, 0.09631045669317245), (0.25525, 0.0999217810332775), (0.39225, 0.10327717873454094), (0.41875, 0.14785195577144622), (0.405, 0.17185581016540527), (0.43225, 0.1947092747092247), (0.43925, 0.19275807696580888), (0.451, 0.21640128707885742), (0.4615, 0.24251160019636153), (0.452, 0.24052831733226776), (0.4555, 0.2614407564401627), (0.44975, 0.26470827758312226), (0.45675, 0.26862468004226686), (0.46175, 0.2718281054496765), (0.456, 0.3027725839614868), (0.4645, 0.31927266120910647), (0.45275, 0.2836358767747879), (0.45925, 0.3063711584806442), (0.46325, 0.3066280846595764), (0.459, 0.3001040562391281), (0.45275, 0.3053544330596924), (0.454, 0.3034462823867798), (0.445, 0.28102861273288726), (0.46575, 0.3001960818767548), (0.4595, 0.2957115868330002), (0.45375, 0.3148636918067932), (0.46425, 0.3077071692943573), (0.46225, 0.3001407998800278), (0.4575, 0.2923114575147629), (0.461, 0.2863028743267059), (0.46175, 0.30827062523365023), (0.468, 0.3215647881031036), (0.45725, 0.30963619446754453), (0.4655, 0.3137202750444412), (0.46475, 0.3080700739622116), (0.46625, 0.31860306227207186), (0.4645, 0.32508527612686156), (0.4595, 0.25769072473049165), (0.45825, 0.28778002178668977), (0.4585, 0.32158911418914793), (0.45225, 0.2952032376527786), (0.45825, 0.3063347613811493), (0.45825, 0.30909205627441405), (0.46425, 0.29330505084991454), (0.45575, 0.2984836157560348), (0.456, 0.2823461526632309), (0.46175, 0.2919759418964386), (0.4605, 0.3222467622756958), (0.4555, 0.3028401862382889), (0.45475, 0.31845692205429077)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           8       0.45      0.97      0.61      1000
           9       0.46      0.85      0.60      1000

    accuracy                           0.45      4000
   macro avg       0.23      0.45      0.30      4000
weighted avg       0.23      0.45      0.30      4000

Competition_DC_3
VAL: 
[(0.209, 0.04494925320148468), (0.257, 0.09110854172706603), (0.323, 0.11809220957756042), (0.355, 0.1430300070643425), (0.356, 0.16536943846940994), (0.354, 0.1911297039538622), (0.376, 0.1672702697366476), (0.384, 0.1897293770611286), (0.387, 0.205445771753788), (0.379, 0.1797317993193865), (0.393, 0.2138650175333023), (0.396, 0.2175305056720972), (0.398, 0.21681640201807023), (0.392, 0.21323319018632172), (0.395, 0.22056705191731452), (0.392, 0.24355234099924564), (0.398, 0.26456464114785194), (0.396, 0.26752902963757513), (0.402, 0.29490510165318845), (0.396, 0.2883051442503929), (0.391, 0.28157265159487727), (0.403, 0.318990552932024), (0.4, 0.30872878755629063), (0.402, 0.3136509017944336), (0.403, 0.3074020326733589), (0.402, 0.3183441966474056), (0.404, 0.32452325627207756), (0.404, 0.29364362175762654), (0.392, 0.30902393336594103), (0.399, 0.3077876227349043), (0.399, 0.3147669750750065), (0.406, 0.32545424076914786), (0.396, 0.2981713104248047), (0.405, 0.2898639529198408), (0.405, 0.31098702225089075), (0.404, 0.29397807869315146), (0.408, 0.2849781015962362), (0.393, 0.305679500490427), (0.406, 0.2903761856853962), (0.403, 0.3321658974289894), (0.404, 0.3022850620001554), (0.404, 0.29840491399168967), (0.403, 0.311349544018507), (0.403, 0.30253161615133284), (0.407, 0.26503587636351583), (0.403, 0.3289306920841336), (0.402, 0.2703943748921156), (0.404, 0.3237344397753477), (0.399, 0.3125765206068754), (0.397, 0.32222379998862744), (0.399, 0.31453494529426096)]
TEST: 
[(0.20325, 0.043842192202806475), (0.2555, 0.08742595201730728), (0.3265, 0.11303198945522308), (0.359, 0.13696498495340348), (0.36825, 0.15801430696249008), (0.3605, 0.18254355472326278), (0.382, 0.1589944772720337), (0.3905, 0.17838659977912902), (0.39525, 0.1921792615056038), (0.3875, 0.17070769482851028), (0.395, 0.20351121479272843), (0.40075, 0.2035170789361), (0.39625, 0.20020900970697403), (0.39225, 0.19915851640701293), (0.40475, 0.204878058552742), (0.4035, 0.22438795864582062), (0.402, 0.24783937466144562), (0.40525, 0.2545144789218903), (0.40575, 0.28099216747283934), (0.411, 0.2624399304389954), (0.40375, 0.2627547526359558), (0.4075, 0.29115752935409545), (0.4105, 0.287689280629158), (0.40875, 0.29863530266284943), (0.41125, 0.2861009433269501), (0.408, 0.3021454675197601), (0.4075, 0.31033181631565093), (0.40775, 0.28004286527633665), (0.40125, 0.2928027055263519), (0.40125, 0.29111969804763793), (0.40925, 0.29704948496818545), (0.40925, 0.30287106084823606), (0.40975, 0.2815231506824493), (0.40425, 0.28389797747135165), (0.41075, 0.29260297811031344), (0.41, 0.28402004969120026), (0.40875, 0.26901402390003204), (0.4085, 0.29012160968780515), (0.409, 0.2756014257669449), (0.4105, 0.3136548269987106), (0.41175, 0.2908609752655029), (0.40775, 0.288501935839653), (0.40925, 0.2942900993824005), (0.4075, 0.2891348291635513), (0.41125, 0.25376821517944337), (0.40875, 0.3053006329536438), (0.4075, 0.25422313117980955), (0.4045, 0.3094757913351059), (0.406, 0.293011976480484), (0.408, 0.29956152403354647), (0.41, 0.29624699795246123)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.38      0.86      0.52      1000
           3       0.45      0.78      0.57      1000
           4       0.00      0.00      0.00      1000

    accuracy                           0.41      4000
   macro avg       0.21      0.41      0.27      4000
weighted avg       0.21      0.41      0.27      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [12]
name: no-alliance-12
score_metric: contrloss
aggregation: <function fed_avg at 0x7301a97a3c10>
alliance_size: 1
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=12
Partitioning data
[[6, 1, 7, 4], [5, 0, 7, 4], [8, 3, 7, 4], [9, 2, 7, 4]]
[(array([40565, 32339, 39457, 16008, 30469, 15600, 36274, 46340, 27651,
       15424,  4824,  4421,  6548, 25892,  7823, 36318, 14392, 32170,
       25039,  1579, 38310, 46334,  6486,  2424, 38993, 15349, 25294,
       15048, 20755,  9406, 18256, 28382, 47734, 42975, 20535,  3401,
       25877,  3412, 11070, 32198, 33824, 45836, 20800,  7137, 49129,
       49110, 36522,  4201, 49584,   234, 25526, 49754,  7481, 43933,
       46846,  8734, 10007, 32904, 32752, 25704, 28177,  4118, 45759,
       14701, 26235,  6101,  2978, 18816, 45863, 35591, 20025, 42881,
       21719, 19834, 35577,   248, 37835, 22889, 35615, 39258, 36625,
       15657,  3122, 34245, 30390, 33367, 28504, 42209, 16064, 36613,
       23105, 48229, 16960, 22004, 30031,  6581, 36348, 46550,   298,
       30759, 12254, 45140,  1159,  8344, 17554,  1222,  2596,  3018,
        3676, 10783, 22567,  8053, 18025, 37045, 25448, 25569, 46134,
       14959, 41142,  3223, 12635, 49966,  8820, 33990, 36062, 17173,
       17858, 20324, 22058, 24231,  3873,  1585, 22363, 27843, 38337,
       27708,  9130,  4105, 25612, 25155,  2583, 13390, 42380,  5206,
       48206, 29214,  7247, 20789, 35780, 26542, 23760, 41491, 13176,
       34602,  4731, 15517, 37748, 27454,  9303, 20278,  1917, 33812,
       27602, 16508, 10176, 44573,  8171, 47961,   620, 25641,  8015,
        2777, 34564,  2439,  5497, 21730, 37796, 34264, 45637,  3926,
       31099, 43624,  5679, 39469, 42960, 31651, 12221, 21673, 37715,
        2179, 49808, 39337, 18396, 47407,  8867, 29089, 37404,  2043,
       32637, 33178,   738, 38786, 25551, 14291, 42584, 30089,  8827,
       30249, 46418, 29357, 21965, 32497, 40824, 49094, 40929, 34552,
        7044, 35060,   132,  5964, 23927, 39133, 41275, 41045,  6046,
       38959, 10757,  3554, 18571, 30100, 26231, 40206, 31826, 22295,
       11917, 26502, 15338, 42015,  4805, 11233, 27087, 25035, 30292,
       39808,  8170,  5169, 11310,  8128, 20717, 16927, 27868, 35550,
       13763, 14074, 34287, 19144, 44273, 37301, 39990, 33228, 23514,
       32603, 41576, 33112, 41445, 39936, 21088, 28347, 30923, 11580,
         466, 32418,  8124, 28847, 19310,  5211, 30676, 46219,  7722,
       44541, 28193, 34258, 30269, 30166, 12384,  7326, 16925, 43600,
       32458, 10363,   873,  9946, 34830, 49993,  9138, 45672, 42655,
       12741, 23275, 29966, 34376, 39097, 41087, 14101, 44097, 45569,
       16233, 25363, 36046, 38812, 13755, 22532, 24450, 40178, 36614,
        3231, 37153,  5171,   841, 39338, 13839,  5685, 18117,  2584,
       44109, 12865, 16083, 23933,  9118, 42734,  7735, 36736, 38756,
        4360,  2582, 13899, 12064, 23923, 13488, 20312, 23648,   262,
       37235, 26994, 33452,  7475, 26452, 13056, 39654, 45704, 10112,
       48612, 31512, 13897, 44483, 44971, 21933, 34637, 20475, 24873,
       39848, 26578, 40987, 21706, 36595,  5810, 11421, 21073, 42709,
        1304,   536, 31113,  9754, 19574,  4358, 37620, 18333,   690,
       19810, 21796, 14005,  6895, 28137, 27002, 24297, 32959,  7841,
       41525, 15295, 16320, 31072, 28882, 45488, 10271,  2910, 34700,
       37461, 32979,  4132, 44217, 40972, 31747, 10236, 19930, 44185,
       48294, 34064, 49186,  7622, 24284, 12676, 31195, 44686,  6318,
       26080, 24219, 36107, 49416, 46519, 40505, 34238, 16673, 27538,
       18066,  1694, 45444, 22762, 49937, 23100, 21651, 24417,   364,
       44134, 39805, 43251, 47540, 44726, 46177, 29791, 15655, 44993,
       45988, 19500,  8573, 37666, 24121, 21547, 11782, 27132,    75,
       33114, 15883, 28122, 32684, 19301, 15557, 34690, 29314, 29894,
       16368, 16275, 26637, 46396,  1781, 12516,  3620, 10614,  6068,
       48465, 38858, 12923, 30049,   815, 40751, 31539, 48312, 24466,
       44524, 11646, 15756, 20376, 38047, 36813, 23714, 12054, 49225,
       18832,  7130, 19546,  7832, 45736, 27871, 38950, 33859, 44487,
       37979, 44311, 16800, 14507, 29309, 38286, 33712,  1445,  9587,
       38703, 14641, 19461, 24917, 35908, 23785, 31777, 36114, 10281,
       10573, 40637, 24850, 17181, 15079, 29272, 17737, 10873, 26991,
       18165,  6573,  8897,  4037, 48567, 48055, 48041, 16026, 16750,
       37150, 26046, 34707,  8859, 27006,  7813, 44922,  7223, 33825,
       38195, 20678, 37930, 39259, 14680, 35682, 49903, 16537, 49745,
       12995, 27288,   181, 32374, 26107, 38279, 49058, 24197, 17436,
        3135, 10278,  8129, 26420, 38243,   972,  9133, 48837,  3637,
        5326, 11088,  5774, 39378, 18447, 42485, 41758,  2135, 48104,
       15070, 15511, 14449, 12658, 42895, 17521, 39408, 21660, 48697,
        9110, 41526, 12889, 42514, 49226, 26166, 43625, 13569, 28328,
        3955,  3157, 10858, 28987, 46754, 43360, 27985, 45000, 39307,
       10602, 21107, 36608, 23186, 15245, 42268,   797, 40300,  1930,
       13703, 28366, 17463, 48204, 44353, 25079, 37842, 28498, 20780,
       13039, 26017, 48256, 29196, 46620, 32208, 40820, 35696,  7969,
       31944,  3050, 39257, 14775,  8106, 21482, 42593, 19844, 45999,
       22204,  7331,  1742,   289, 41805, 44493,  6329, 39711,  1218,
       36278,  8595, 11996, 17636, 29535, 20109, 38693,  1071, 12787,
        1042, 31357, 40912, 20565, 17910,  6677, 46412,  8038, 16504,
       49925, 18227, 18468, 49183, 11426,  9805, 29644, 46672, 36589,
       35241, 28197, 35290,  5363, 40692,  9374, 21335, 34876, 27775,
        5842, 48848, 31117, 26733, 36369,  5269, 14493, 14290, 26947,
       37262, 36936, 42463, 37896, 10818, 34584, 43060, 40356, 22569,
        7621,  1309, 12900, 26056, 19200,  2801, 27187, 49627,  2458,
       35483, 18579, 12961, 16171, 33485, 40893, 16914,  6449, 23571,
         771, 44610, 46988, 44798, 35164, 28639, 10235,  1908, 34044,
       15235, 30422, 21535, 38667, 44424,  2103, 18517,  1308, 23910,
       24901, 31520,   973, 39633, 19638, 12244, 20016, 16654, 35869,
       21269,  2301, 38308, 45767, 15196, 44147, 43595, 31464, 14670,
        1169, 48505, 10250,  6838, 44336, 40146,  1387, 38135,  7436,
       23545, 28250, 24883, 28167, 19528, 33171,  9724, 15168, 45170,
       35066, 23779, 10861,  6144,  9529, 36671, 27494, 14557, 43549,
       49687, 10865, 39005, 46883, 49390, 44450, 11092, 46753, 19742,
       27461, 24448, 28837, 20515, 13453, 32818, 30464, 25579, 45515,
        2123, 26819, 28576, 19643, 45315, 10633, 10392, 10539, 35774,
       44400, 15024, 17801, 40228, 47716, 28464, 13846, 45723, 23460,
         856, 25779, 26638,  3577, 42750, 30616,  2106,  1022,   665,
       44125,  1882, 40962, 15482, 37214, 19264, 37854,  3945, 19846,
        1784, 47144, 37089, 23634, 28561, 22944, 13987, 10519, 28666,
       23329, 36294, 22243, 18409,  3897,  3936, 37431, 32893, 48634,
       20888,  1813,  9349, 23337,  7365, 41237, 31689, 47287,  5229,
        4057, 31033, 15703, 41825, 12444, 45408, 13469, 40674, 18404,
       29291, 49504, 20412, 18340, 41518,  1863, 23817, 23468, 33666,
       18052, 37274,  6654, 26584, 25434, 18875, 25620, 46940, 27384,
       26118, 14911, 30300, 40259, 39470, 16073, 30230, 30715,  6344,
        1575,  6844, 12136, 33342, 16802, 22536,  5998, 42725,  9848,
       30077,   153, 44705, 26772,  8523, 23211, 48429,  1516, 24046,
        1069,  2839, 32928, 26911, 19689, 18490, 32415,  6234, 46789,
       41263, 48211, 40210, 47345,  2842, 45323,  2835, 43553,  5405,
        6270, 41523, 26854, 17623,  1814, 10925, 21753, 40531, 45820,
       16323, 33408, 22461,  2647, 36976, 18304, 20782, 49402, 49954,
       10536, 41101, 38350, 24308, 47798,   572, 15515,   660, 19475,
       14262, 33140, 49902, 34670, 27817,  9061, 27558, 35714, 48997,
        1832, 43926,  1238, 14726,   868, 46528,  5514, 12522, 14019,
         490, 17211, 46446, 36523, 36317,  7656, 11033, 38395, 20543,
        8541, 13292, 40033, 30020, 32413, 12505, 41852, 31256,   934,
        8473]), [6, 1, 7, 4]), (array([ 9035, 26874, 11513, 48114, 30657, 23488, 40417, 41232, 30871,
       32769, 18463,  3173, 13676,  2864,  8494, 43759, 36121, 42293,
       34833, 19274, 41046, 28982, 42866, 41094, 31646, 29268, 27653,
       44838,  4125, 15861,  4627, 33957, 14495, 46608, 37114, 32849,
       32530, 16949, 40069, 20801, 27991, 42194, 20348, 45992, 46428,
       49267, 44284, 46165, 37508, 15653, 31409,  6580, 13251,  6436,
       16626,  1091, 20207,  6982,  9871, 41139, 27947,  5367, 26194,
       23273,  3451,  3080, 21677, 11995, 40632,  9337,  4890, 45553,
       38990, 28779, 30155, 26079,  7090, 18966, 14004, 45215, 39817,
       44358, 25333, 22890, 37899, 48935, 48832, 46310,  6561, 47218,
       12177, 16580, 17009, 22059, 30723, 37880, 40434, 38632,  5993,
       46853, 31059,  2089, 40051, 43038, 13579, 47103, 49988, 14627,
        2148, 18399, 27348, 40904,  8252, 40763,  6220, 13283, 43651,
        6807, 25751, 18347, 32044,  1199, 14259, 41340, 24877,  1545,
       29450,  2182, 19169, 24656, 47602,  9772, 47919, 11740, 32559,
       45587, 39124, 32658, 28425,  4120, 15450, 21233, 46130, 14992,
       27265, 39783, 47920, 41499, 26072, 46022,  6689, 42613, 36202,
       49547,  2374,  7398, 38944, 41886, 47659,  6058, 29630, 15544,
       34112, 34728, 49675, 27223, 47647, 31120, 49316, 11980,  1355,
       28090,  7140,  9445, 29422,  4990, 27941, 42549, 32124,   424,
       38739,  1061, 21598, 11396, 40688, 20493, 48754, 38403, 15019,
       21046, 37708, 48083, 30231, 47230,  8274, 20809, 29804, 28889,
       22483, 29836,  2194, 20075,  7446, 30952,  6037, 38709, 30054,
       10258, 23392, 24648, 18852,  8511, 27397, 20421, 19652, 36123,
       18772, 13057, 27666, 34299, 30162, 24909, 25552, 20171, 28438,
       19933, 20277, 36235, 33079, 18986, 49810,  9929, 33039, 41817,
       47555,  6707, 22908, 15336, 11600, 30393, 24953, 40105, 39846,
       24650, 46483, 46690, 43599, 27305,  4632, 40765,  2895, 28767,
       40432, 48556, 31023, 38312, 43659, 47560, 21930,  8753, 34070,
       16375, 38716, 15959, 43253,  3609, 28138, 38307,  1329, 28919,
       16513, 19409, 46137, 23010, 35528, 42183, 39396,  7808,  5684,
       41164, 21573,  8102, 45390, 20295, 43731, 46192, 44731, 10578,
       28003, 22123, 31481, 13964, 49130, 15153, 22271, 19273, 44967,
       12469,  2490, 20902,  4295, 18694,  8437, 44592, 25489, 10804,
       43138,  8660,  2156, 47805, 11218, 27278, 25668, 34711, 29732,
       16316, 16065, 15115, 34748, 29189,  2863, 37068, 29039, 26546,
       46934, 17221,  6904, 39819,  4721, 25477,   782, 28063,  6688,
       32363, 13714, 12936, 19945, 19974, 44430, 37788, 10031, 39138,
        9171, 38321, 18614, 43077, 31600, 24111,  3909, 45357, 13060,
        7588, 10486,  4619,  7643, 35873, 36530, 43245, 17082, 38320,
       11431, 37664, 16752, 48148, 34341, 11056, 21322, 35546, 11276,
        3987, 19493, 37146, 23629, 47557,   650, 27211, 36323, 47994,
       30539,  4952, 37165, 11709, 35952, 27901, 32140,  7599, 11178,
       38586, 11114,  9435, 20396,   284, 35608, 29293, 49513,  9236,
        5738, 48069, 19106, 22026, 16146, 21472, 32683, 12028,  4746,
       36497, 45186, 48239, 49799, 41167, 14468, 41212, 29518, 27042,
       46041,  4861, 31015, 46303, 22993,  4524,  2617, 48096, 29544,
       18864, 33530, 28650, 14216,  6518,  1708,  6187,  3582, 49340,
       31955, 42190, 29620, 45438,  4523, 11304, 32056, 25905, 47931,
       47645, 23746, 12542, 33386, 26081, 42844, 28068, 20743, 34568,
       28467, 38190, 49375,  7985, 39311, 40012, 11721, 17994,  3585,
       17154, 41030,  4096, 33711,  6090, 35028, 14958, 33062, 38580,
         332, 30888, 18763, 47271, 36007, 31246, 48059, 49992,  5441,
       45182, 37712,  2959, 48378, 30467, 38334, 30909, 48677,  1187,
       23984, 10326, 41879,  9058, 27080, 33831, 24713, 49346,  5331,
       34705, 28515, 26342, 48554,  8242, 37467, 34900,  2716, 37787,
       14143, 24026, 38142, 18997,   470, 36806, 20884, 49277,   948,
       12751, 36829, 40942, 42909, 11323, 27374, 28184, 44101, 16074,
       24267, 39717, 38225, 43066,  8966, 17913, 49809, 16719, 42184,
        5114, 14818, 21109,  6850,  9778, 17095,  6283, 47092, 27565,
       25742, 41347, 33096, 19250, 32920, 40120, 23290, 11046,  9071,
         318,  1165, 35284, 26494, 11393, 29718, 10927, 36897, 20550,
        9795, 43617, 20436, 32568, 46671, 46719, 27242, 17951,  6821,
       43435, 40684, 47923, 10226, 17494,  2612, 19418, 24782, 10685,
        9560, 22730, 12980, 43229, 24025, 35986,  5307, 13147, 23577,
       22207, 37190, 26397, 49610, 21100, 24970, 42651, 37131, 47880,
       31800, 26821, 17499,  4396, 48343,  8664, 39262, 35815, 30036,
       49229, 34273, 47835, 23356,  6460, 34191, 24371, 39261, 47438,
       35972, 26749, 27015, 36597, 31262, 23034, 18874, 29481, 41780,
       15853, 22291, 14603,  2814, 41370,  9239, 12838, 44911,  7482,
       19883, 32514, 49454, 45929,  6695, 43906, 38512, 43528, 41584,
       10884, 13272, 14191,  8745, 39799, 35421, 32888, 33949, 38155,
       26811,  6072, 49037, 30278, 47856,   688,  4555, 11284, 44081,
       37637, 25197, 15472,   575, 44115, 45376,   329, 26067, 46691,
       13540, 39155, 35962, 47169, 32354, 33884, 27078, 39017, 39720,
        7895, 19997, 20369, 10598, 23490, 11502, 13136, 34113, 43830,
       33349, 33636, 46996,  4903, 33155, 10106, 24954, 30972, 44286,
         746, 36034, 11102, 21329, 15551, 32688, 23880,   362, 43305,
        8764,  7717, 43671, 18037, 48029, 43782,  8643, 24088,  9768,
       23835, 23842,  1857, 17872, 12844,  5798, 33045, 18881, 12848,
       47590, 43242, 34826,  6653, 46834,  1395,   294, 23331, 33908,
       46982, 11260, 20099, 25304,  2000, 36037, 30410, 23778, 42251,
       49010, 12402,  4513, 22723, 40585, 26702, 33556, 34528, 33139,
        1046, 37497,  6696, 21899, 31939, 20120, 30915, 19936, 24889,
       13412,  4231, 19033, 26717,  1855, 43756, 33221, 28795, 37124,
       37778, 41500, 42403, 38455, 20253,  6465, 49669, 13834, 48529,
       46398, 47300,  9907, 30794, 25371, 45598, 24513, 37465, 41219,
       23247,  5259,  6785,  3668, 34410, 27897, 16383, 49421, 25114,
       10641,   130, 43714, 11573, 26062, 29070, 47482, 19822,  3252,
       38794, 34840, 35432, 39734, 42072, 24548,  1388, 47796, 14244,
       47415,  4297, 23279, 13023, 46339, 32088, 10640, 15909, 22949,
       13882, 30917, 36466, 33352, 35868, 26830, 21788, 22727, 14858,
       11967, 10706, 39169, 41453, 23748, 39573,  5940, 27074, 40707,
       14943, 18553, 14036, 17083, 42809,  9749, 17199,  1639,  9506,
       30942, 10979, 30435, 30911, 20306, 28100,  7368, 43306, 32533,
       43203, 29098, 43499, 15625, 24171, 11994, 16294, 42040, 44563,
       16241,  3314, 32989, 22288, 23512, 27247,  3288,  7052,  9615,
       48179,  1018, 45136, 21980, 22740,  4695, 28913, 21676,  1968,
       16433, 14111, 31305,  3522, 15596, 17958, 40770, 17412, 46241,
       36646, 20108, 16865, 37438, 28489, 30094, 32069, 17671, 23394,
       46164, 11209, 38787, 43327, 31767, 20325, 20407,  5285, 47139,
       44851, 24040, 27489, 45801, 36420, 29849, 44572, 34331, 10851,
       21666, 39945, 39344, 48738, 34589, 14256,  3112,  3360, 21708,
       42640, 30970, 37442, 18043, 31476, 34450, 46891, 47855, 21176,
       15582, 41246, 19657, 34082, 29420, 35135, 32518, 40301, 38593,
       15377, 42695,  5173, 41337, 22886, 27276, 22241, 40665, 48616,
       27180, 44040, 17420, 31627, 14181, 27017,  4429,  3761,  5495,
        8631, 35563, 15339, 14827, 16659, 13269, 28793,  9888, 22509,
       13764,  6378, 26586, 25147, 20060, 40071, 34051, 40793,  3176,
       49243, 11757, 38070, 38953,  6562, 19426, 47975, 48716, 16891,
       15256, 18418,  5788, 12022, 33129, 17719, 29024, 46255, 21629,
       14051]), [5, 0, 7, 4]), (array([25450, 26156, 12543, 48247,  1236,  2881, 16730, 20375, 23858,
        2417, 35995, 12400, 19402, 18221, 37437, 34655, 14767, 25987,
       27370,  4245,  9117,  2979, 29747, 19696, 40389, 47021, 15161,
        1770, 17416, 31714, 35315, 18691, 45697, 30087, 48469,  6632,
       40784, 16042, 14427, 10098, 48133, 43807, 19123, 27281, 16381,
       33626, 31879, 46392, 21477, 43662, 15260, 19603, 38867, 32896,
       40368, 10606, 46736,  5648, 45138, 32447,  5985, 18623, 32463,
       20668, 21383, 39981, 42467, 30770, 16607, 30835, 45168,  9960,
       43423,  4670, 32728, 17799,  2929, 22696, 28450, 42497, 40691,
         418, 27398, 33372, 12418,  9076, 16512, 24456, 46926, 43899,
        7112,  7387, 24787, 47114, 46175,  6337, 43001, 43883,  1479,
       41070, 48445, 28006, 23818, 12725, 40671, 22440, 49372, 41585,
       37318, 40881, 48698,  2489, 48794, 22374, 46938,  3130, 28284,
       25096, 20812, 17617, 19374, 23294, 26437, 28205, 13266,  5278,
       12650,  2820, 20405, 31692,  8489, 39838, 43344, 25756, 21257,
       48552, 32327, 42300, 42203, 12258, 21249, 45025, 40789,  8270,
        9572, 43540,  9669,  4502, 16692, 45354, 26195, 17519, 19167,
       42840,  8961, 26328, 43119,  7561, 21312, 17348, 45309, 34171,
       35173, 19720, 41674, 14211, 16483, 36229, 32207, 41008, 44937,
       37557, 29056,  4602,  9972, 42826,   562, 35864,  8211, 29968,
       37858, 43755,  5941,  6139, 27640, 49166,  9599, 13694, 32246,
       39895, 17007, 13139, 39802,  9651, 42965,  4713, 40495, 25951,
       29476, 34212, 16727,  2483, 38689,  8022,   222, 22649, 13886,
        8831, 32465, 32138, 48475, 35588, 16861, 16084, 49253, 24629,
       26694, 23172, 36742, 34893, 23085, 29929, 26254, 46735, 43895,
       33287, 49625, 25902,  4723, 27056, 29877, 19314, 21565, 48125,
       22682, 36803, 39621, 49916, 49553, 38469, 45853, 38809, 42763,
       25629, 16049,   244, 41298, 10191, 31317, 28800, 24652, 20638,
       33525, 16289,  1104, 34160,  4542, 35769, 49408, 33917,  3679,
       19830, 30995, 33007, 24733, 19101,  3429, 15313, 19578, 31400,
       44621, 18414,  4954,  7906, 48296, 13541, 14047, 25633,  5190,
       24696, 24751, 31463,  1461, 43490, 32732,  9794, 36351,  9164,
       11409, 42504, 22891,  1070,  7651,  8710,  6482,  8101, 26688,
       30586,  1010, 22011, 29523,  9298, 26224, 12039, 38068, 10506,
       34199, 11564, 14466, 28330, 22239, 13530, 30839, 19327, 49975,
       25307, 15122, 39021, 14993, 37187, 14161, 32405,  2141,  4151,
       27608, 20301, 22850, 40249,  7006, 19922, 21299, 36230,  6849,
        8330,  7050, 32702,   266,  4402, 44987,  2165, 46094,  9006,
        8028, 33280, 36440, 41713, 33964, 41269, 14574,  9700, 11357,
       22905, 24099, 34661, 35937,  7680, 44173, 20639, 35715, 23094,
       32139, 11386, 19221, 16731, 35772, 22411, 38364,  9008,  3997,
       24174, 44943, 42738, 35229, 42736, 34889,  2412, 29442, 34710,
       46425, 24434, 16851, 33600, 20625, 14187, 40973, 11902, 43827,
       31414, 43784,  4221,  6883, 30774, 49319, 15640, 16495, 36772,
       30346, 44381,   639, 16561, 20326, 32085, 42953, 20577,   922,
       17920,  5592, 29654,  3355, 25769, 44576, 11268,  5945, 49140,
       29464,  8408, 29960, 10017,  3571, 38647, 46347, 43737,  1265,
        2127, 47062, 44226, 34999, 44768, 31542, 42837, 17828, 46023,
       45670, 15080,  8246, 18520, 41271, 10931, 20882, 48346, 14253,
        7200, 25471, 13862, 41910, 20330, 36212,   857, 10414, 16646,
       43440, 27274, 37965, 20685,  9343, 40232,  9897, 43744, 17074,
       45402, 34185, 16271, 13824, 26900, 47838,  8790, 24148, 23372,
       29050, 39778, 10634, 10869, 27906, 15224, 42496,  2892, 14239,
       10066,  8041, 48772,  6724, 49918, 16341, 37370, 39158, 12517,
       38215, 47654, 26907,  5744, 48376, 12147, 17155, 38006,   207,
       13235, 27475, 47717, 28880,  8105, 30265,  1135, 26931, 10384,
       46045,  5620, 42063,  5995, 49843, 38176, 31336,  7817, 39107,
       32068, 10988, 45103, 28520, 32487,  7023, 26810, 34260, 43436,
       22601, 40646,  7756, 34511, 27600, 10295, 16557, 23344, 33681,
       39395,  6326, 43791, 33773, 43144, 27573, 44935, 40694, 23889,
       38595, 12722, 43091, 27824,  3611, 46211, 14614, 13132, 39400,
       27423, 45353, 25741,  7940, 12229, 46383, 29824, 10333, 20879,
       39304, 32440, 38121, 29195, 44076, 31818, 31671, 13732, 36407,
       39345, 17040, 36805, 48548, 19542, 24311,  2139, 15579, 47082,
       44784, 18481, 37636,  2807, 37302, 26767, 27400,  9486,  7682,
        4820, 29719, 29441, 29588,  3434, 36239, 17464, 41185, 22501,
       16399,  6966, 36959, 27043, 45343, 40709, 49470,  9891,  2244,
        1226, 28553, 27189, 42421, 44292,  1758, 27568, 30199,  6481,
       36256, 27226, 44363, 24505,  2113, 14222,  2750, 31432,  2278,
       21601, 21367, 33191, 40497, 43579,  4886, 10823, 20900, 13430,
       10216, 22901, 42052,  5579, 26916, 34308, 13974, 43309, 25131,
       18519, 25343, 26895, 24439, 39797, 12466, 13224, 40916, 47003,
       40295,  5200, 32211, 15979, 35982, 39111, 47828, 43940,   739,
       37055, 34955, 41700,  3279, 38539, 35698, 18405,  3914, 27748,
       33722, 10121, 32700, 15317, 31389,  3996,  8646, 46154, 48714,
       16184, 12901, 31331, 48307,  7016,  3623, 12163,  2776, 46517,
       20614,  9741, 10916, 47572, 28951, 40266, 44356, 15881, 23794,
       20158, 39700, 39276, 10621, 35003, 43215, 38732, 30066,  8946,
       39767,  5368, 47890, 25876, 41205, 47575, 14837, 16348, 12565,
        5863, 26633, 36136, 44222, 10712, 25116, 44135,  3150, 20960,
       22312, 35511, 29320,   722, 40837,  3699, 37670, 18828, 19361,
       13790, 40897, 42353, 17001,  2119, 26983,  3213,  9726, 33176,
       26104,  3933, 11329, 36417,  9672, 35153, 27036, 35002, 23330,
       37405, 37001, 22403, 36740, 41382, 41958, 17077,  2995,  4707,
       49488,  7933,  6314, 26838, 27790, 10888, 14186, 30559, 39501,
       22617,  1903, 37928, 31902, 32384, 31143,   831, 35522, 47155,
       28881,  2515, 10055, 37731, 14618, 42982, 34604, 31860, 44974,
       39413, 49352,  2210, 40277, 12213, 20778, 44656, 14310,  6168,
       23826, 18977, 44465, 47147, 26048, 35915,  5909,  7283, 24866,
        1762, 10015, 10179, 17513, 44900, 12417, 16240,  9638, 40943,
       31799,  3182, 19585, 18142, 31227,  5225, 14560, 20409, 18523,
        5815, 43758, 45586, 48046, 42795, 40195, 25014,  1569, 14082,
       27135, 19569, 20192, 43983, 41878, 49718,  3556, 17350, 43139,
       32100, 45623, 37530, 13310, 30306, 35186, 33674, 45397, 28696,
        7100, 10059, 38242, 22318, 42544, 30206, 45279, 41623,  4106,
        3397, 48235, 32654, 37123, 37960, 28796, 19006, 40067,  6720,
       46076, 27519,  6266, 26434, 42285, 48876, 32169, 13636, 45812,
       16155,  8331, 32643, 25703, 47348, 31306, 36755,  3322, 30841,
        2488, 31446, 46036, 30674, 45121, 30228,  3952, 22820, 23393,
        8913, 15525, 22210,  3189, 45749, 33027, 27613, 27199,  9153,
       27029, 25576, 47908, 13984, 46778, 39036, 27853, 35151,  4725,
       46011, 45688, 30242, 42603, 27100, 30425, 39576,  1303,  8447,
       46947, 40219, 29377, 43950, 39459, 43196,  9791,  5488, 31290,
        7636, 26380,  2175, 10196, 14635, 33401, 40088, 18829, 22443,
       26307, 27624, 39146, 49252,  2293, 30061, 26844,  2088, 29830,
       27227, 13803, 31053, 43765, 15744, 45712, 20978, 14914, 28658,
       47605,  1937, 47408,  5264, 28191, 40085,  2703, 33057, 33947,
        7984, 10765,  6894, 16136, 33657, 19170, 32373, 36694, 19548,
       44360, 37075, 10570, 34298, 38660,  1889, 28603, 20303, 35743,
       29582, 25863, 15917,  8361, 19627, 33979, 36105, 13427, 22513,
       30131, 21033,  7207, 14415, 33334, 36142,  5193, 32018, 35443,
       48223]), [8, 3, 7, 4]), (array([46298, 15758,  3526, 26668,  4622, 30615,   147, 28019,  5228,
        1504, 30038, 26888, 45235, 30578, 14599, 22252,  9683,  6643,
       46611,  4675,  1384, 28353, 21159, 32563,  3895, 27837,  9229,
       30973, 28358, 24107,  3237, 10405, 29356, 27815, 41419, 48140,
       18642, 20944, 41244,   278,  6293,   768, 17163, 25308, 28337,
       38426, 36321,  2767, 47641, 21300, 49249, 26394, 34515, 18260,
       36280,   657,  1231, 23410, 49287, 24638, 29529, 17139, 40875,
       29697, 32163,  3199, 39699, 30764, 39806, 17600, 18963, 33671,
       13782, 32838, 41765, 47765, 36521, 43619,  6158, 39319,  8841,
       43406, 23305, 14071, 47340, 45405, 39917, 31870, 30223, 29258,
       24550, 14205,  1332, 29193, 33248, 26945,  2158, 42414, 39117,
       18344, 37017,  2303, 26136, 32040, 17896, 20765, 26021, 47187,
        2585, 18539, 28656, 30503, 32072, 43141,  8402, 32156, 34409,
       26229, 30934, 47660, 31077, 44052,  8303,  7652, 13780, 43673,
       32552, 34077, 16267, 19670, 43614,  6934,  1390, 20129,  7752,
       35545, 12902, 27957, 30903,  3086, 38357, 19620, 11096, 27668,
       43539, 19369, 26518,     1, 32192, 11552, 34502,  5394, 29753,
        9501, 18430, 27791, 36508, 24133, 26506, 45984,  4618, 42102,
       24391,  8323, 36549, 29221, 48839, 29585, 33620, 12262, 21907,
        2838, 36288, 47896, 33676, 42362, 45871, 12011, 36887, 45673,
        3755, 33483,   360, 26600,   541, 21696, 24880,  7943,  9434,
       47875, 45684, 35218, 23075, 27965, 36504, 48751,  9513, 33224,
       41976, 13662, 36668,  2129, 19055, 44716, 33672, 14813, 49035,
       25483, 49266, 14626, 15616,  5133, 18666, 12186, 32245, 13380,
       24407, 23359, 26530, 10144, 44969, 42556,  1045, 30180, 17903,
        3977, 31176,  3298, 17716, 40387, 35423, 23856, 35184, 10622,
       38402, 11814, 28879, 22652, 40412, 15725, 42420, 37538, 48308,
       43591, 40693, 46723,  5138, 41580,  5300, 44280, 21189,  7410,
       19806, 34959, 41556, 42542, 18163, 14180,  7633, 35575, 45953,
       39687, 15118, 19969,  6315,  3193, 24210, 42489, 47563,  3454,
       36710, 16439, 13081,  9093,  8881, 40189, 34600, 13794, 10457,
       34433, 16357, 17114, 32361, 12720,  3520, 31074, 38170,    47,
       37515, 24500, 48504,  4525, 15977, 27685, 25268, 32470, 14500,
       29625, 40487, 34555, 21316, 33282, 27204,  1921, 22012, 38565,
        5634,  8789,  7340, 46633, 42155, 33564,  3161, 25379, 40444,
        1207,  3847, 48870, 27738, 29650, 34609, 37990, 20570,  1812,
       44658,  9989, 34866, 24994, 34398, 11500, 28689, 26146, 39339,
       49800,   646,  5087, 28426,  6477, 17344, 16742, 26956, 20226,
       31307, 31486, 37353,  1884, 21031,  2948, 42536, 19343, 25222,
       46967, 45277, 18457,  2533, 47940, 20660, 25628, 18964, 29902,
       16372, 49159, 15697, 47172, 39928, 13757, 36972, 13985, 15088,
        8037,  6073, 38129,  3944, 12043, 26651, 44070, 15430, 19899,
       26213, 30513,  2372, 22279, 42120, 42990, 23843, 18573, 16593,
       31475, 46405, 47632, 31333, 28452, 26990, 18269, 48498, 31636,
       19964,  5836, 46800,  5576,  9144, 16683, 46014, 46179, 44879,
       14588, 41407, 46258,  9952,  7074, 24610,  4811, 37166, 28227,
       44008, 27046,  2833, 48092, 27542, 31308,  8092, 48146, 26893,
        9408, 37811, 42995, 39935,  8121, 33531, 14172, 43441, 34088,
       35486, 48973, 30333,  9558, 30746, 48480, 16569, 47500, 44739,
       45803, 32582, 48733, 49397, 35420, 30371, 47360, 24719, 21210,
       31285, 10512, 16462, 42889, 33503,  1024,  2678,  5127, 35723,
       25095,   630, 16757, 41401, 10725, 45218, 18720, 36152, 29126,
       42588, 26760, 38638, 17339, 21063, 32706, 45021,  2354, 33834,
       23222, 21039,   957, 35947, 33403, 45335, 43367, 10018,  7258,
        3164, 35063, 21090,  5189, 20051,  5618, 35834, 18888, 42427,
       10594, 17188, 19918, 14583, 44181, 21301, 26276, 11867, 40955,
       15567, 18712, 24596, 17648, 19058,    43, 45708, 36513, 10552,
       26451,  2541, 46301,  6752, 43536,  8238, 49737, 34228, 42117,
       39030, 47406, 22405,  6939, 45610, 30484, 26165, 46792, 28048,
       23435, 24812, 16618, 49350, 40563,  1927, 45747, 48453, 12925,
       30150, 28502, 31789, 36691, 42386,  8052, 34412, 26826, 49396,
       28759, 32414,  1276,  6839, 46117, 15916, 35922, 43768,  7672,
       11469,  2616,  7565, 17720,  7463, 43902,  8465, 34329, 36562,
        7613, 20423,  5846, 25917, 13566, 48727, 42959,  8927, 24370,
       10957, 33150, 23364, 10348, 43532, 28195, 49796, 18292, 19600,
       24568, 48099, 30996, 40730, 27255, 25724, 13622, 32296, 33299,
        4582, 20707, 22854, 28771,  5403, 38500, 40025,  3711,  5116,
       17177,  7400, 36987,  8924, 17840, 13547, 25617, 44958, 42598,
       25778, 16025, 11621, 45281, 49774, 21317, 15550,  6726,  7141,
       11742, 17549, 48586, 10779, 48578, 24013, 40541, 23547,  3151,
       29018, 24995,  6827,  8984, 34788, 41473,  6335, 30890, 42812,
        8751, 23784, 38109, 26208, 29592, 25182, 14202, 35008, 28453,
       18124,  4078, 42951, 17989, 37935, 28429, 47039, 47665, 32053,
        6291, 15413, 43150,  7564, 33450, 47246, 25592,  4401, 49592,
       44762,  7101, 19064, 43322, 23486, 23909, 23609,  5761, 20314,
        4241, 33660, 13597, 37319,  8604, 37902, 45945, 33775, 27969,
       10034,  6641, 45150, 41390, 25836, 24233, 30556, 31529, 19744,
       18339, 45336, 44560,  7836,  2370, 19599,  9369, 11581, 29975,
       11414, 46857,  6513, 28965, 31224, 34469, 47422, 38325, 32329,
       22624,  8580, 22806, 21324,  9279, 31217, 22309, 17678, 41846,
        8109, 12627, 10041,  6680,  3733, 26752, 38965, 46167,   647,
       18139,  6979,  6864, 17159,  5996, 14431, 24090, 25226,   994,
       18914, 26411, 49850, 36144, 38284, 43168, 22687, 28911, 12888,
       27915, 25269, 24715, 30822, 45689, 12580, 18912, 29756, 48399,
       47205,  1435, 22205, 22149, 21251, 23853, 20276, 34793, 46521,
       16908,  9777, 23532, 49237, 37719, 45441, 25898, 27193, 35276,
       41766, 35624, 12510, 43335, 15656,  1347, 37875, 41737, 30528,
       12179, 33209, 43473, 32947,  9074, 16173, 46245, 40162, 14022,
       43632, 38115, 23655,  1804, 43304, 34808, 18161, 29218, 17507,
       13231, 10350, 14440, 13903,  5826, 13978,  6298,  9776,  1407,
       33744, 45867, 38720, 11586, 41934, 34852, 14283, 14316, 37591,
       29787, 39067,  4058, 37850,  2976, 48478,  4173,  1349, 25403,
       33501, 27390, 10561, 45517,  3587, 35811, 37777, 33796,  6603,
       36327, 21382, 16636,  3664, 17550, 15777, 39377, 41688, 22133,
       20698, 21252, 42910,  9655,  3560, 32285,  3757, 41590, 44703,
        7855,  2990, 46531, 41005, 41249,   725,  2830, 22990, 15538,
       23739, 30857, 18403, 30967, 26706,  3879, 28260, 31160, 48087,
       48484,   528, 35648, 14087, 12261, 28076,  6274,  3519, 22153,
       25299,  8379, 48985, 48447,  1654, 48937, 42938, 41451, 36690,
        8423, 39866, 19180, 29161,  5029, 43529, 48807,  3913, 28596,
       27282, 38995, 17179,  9155, 44846, 44095,  9389, 47399,  3695,
        6594, 22202, 44586, 17707, 49475,  8263,  6636,  6569, 49564,
        3690, 26713, 17845,  4978,  6652,  8271, 41186, 20446, 42675,
       14744, 19857, 39698, 42313,  6535, 11081, 24421, 48367, 21229,
       25860,  4280, 13428,  4642, 33740, 39891, 35175, 35077,  9019,
        3377, 45605, 16640, 18628, 28903, 13038, 41302, 32757, 18185,
       20826, 25776, 30291, 19882, 48008, 35185,  3326,  7043, 38103,
       17728, 21659, 40842, 45311,  6770, 38810, 43637, 43694, 21453,
       45221, 18434, 41777, 25401,  5742, 45396, 42769, 34352,  1324,
       12324, 44517, 34816,  8208, 10904, 25292, 39772, 18054, 34882,
       19980, 35797, 37193, 25099,  5358, 12331,  5813, 36247, 46174,
       48521]), [9, 2, 7, 4])]
Competition
DC 0, val_set_size=1000, COIs=[6, 1, 7, 4], M=tensor([6, 1, 7, 4], device='cuda:0'), Initial Performance: (0.25, 0.04443984019756317)
DC 1, val_set_size=1000, COIs=[5, 0, 7, 4], M=tensor([5, 0, 7, 4], device='cuda:0'), Initial Performance: (0.262, 0.04436455535888672)
DC 2, val_set_size=1000, COIs=[8, 3, 7, 4], M=tensor([8, 3, 7, 4], device='cuda:0'), Initial Performance: (0.275, 0.04432411301136017)
DC 3, val_set_size=1000, COIs=[9, 2, 7, 4], M=tensor([9, 2, 7, 4], device='cuda:0'), Initial Performance: (0.225, 0.044532188177108765)
D00: 1000 samples from classes {4, 7}
D01: 1000 samples from classes {4, 7}
D02: 1000 samples from classes {4, 7}
D03: 1000 samples from classes {4, 7}
D04: 1000 samples from classes {4, 7}
D05: 1000 samples from classes {4, 7}
D06: 1000 samples from classes {1, 6}
D07: 1000 samples from classes {1, 6}
D08: 1000 samples from classes {1, 6}
D09: 1000 samples from classes {1, 6}
D010: 1000 samples from classes {1, 6}
D011: 1000 samples from classes {1, 6}
D012: 1000 samples from classes {0, 5}
D013: 1000 samples from classes {0, 5}
D014: 1000 samples from classes {0, 5}
D015: 1000 samples from classes {0, 5}
D016: 1000 samples from classes {0, 5}
D017: 1000 samples from classes {0, 5}
D018: 1000 samples from classes {8, 3}
D019: 1000 samples from classes {8, 3}
D020: 1000 samples from classes {8, 3}
D021: 1000 samples from classes {8, 3}
D022: 1000 samples from classes {8, 3}
D023: 1000 samples from classes {8, 3}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.07179966375231743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.062282080620527265) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.0866992349922657) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.10071668189764023) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.09029780164361) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.0706805988252163) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.09509955850243569) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.269, 0.12162477773427963) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.10274842362105846) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.0916736244559288) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.12655268445611) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.14859290887415408) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.404, 0.1201973113194108) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.11129003369808196) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.17445171085000039) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.18057729303836823) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.13035795422643423) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.451, 0.13108988019824028) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2068759422376752) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2312651706188917) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.1425810459293425) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.158301618501544) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2383120637461543) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.24315960404276848) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.1451124506518245) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.457, 0.15763706554472445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.2743811624888331) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.2798636357784271) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.1760368304941803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.17171229308471084) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.3013828208167106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.29754252534732223) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.1761757607832551) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.17199157116003336) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.3272228060346097) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.30223537323623895) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.19416647404991091) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.448, 0.17022600414045155) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.3739811360947788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.2888700136058033) --> New model used: True
FL ROUND 11
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO0', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19031314682215453) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.18340951904281974) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.39412847183458505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.31200646061077714) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.466, 0.23359228840842844) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.19952387613989414) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.3568888035826385) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.372750877273269) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.24853772978670896) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.19810874040052295) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.3588982521984726) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3774753681756556) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.23987051874306053) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.19682635425776243) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.4179078217633069) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3871815587184392) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.2560490776579827) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.20588201360590755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.405184631831944) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.4176318839257583) --> New model used: True
FL ROUND 16
DC-DO Matching
DC 0 --> ['(DO5', '(DO1']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO4']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.24797167168557643) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.18197959163412453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.392299519777298) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.4460220302087255) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.26933135152049364) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.1987727743126452) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.4197673295494169) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.38738691445346923) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.257529443513602) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.465, 0.21154149946710094) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.44488343491777776) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.422484582982026) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.26897268440853805) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.1936933790287003) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.44400890224799516) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.4262213265541941) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.26143405995704233) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.21120805268269033) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.39630526142008604) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3875679081510752) --> New model used: True
FL ROUND 21
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.27730985713843254) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.22078587534371763) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.4346873934213072) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.42509507326432505) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.29785239784559236) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.21508490075543524) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.45723216447699816) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.42265893540903926) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.30975258057238536) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.2070809772927314) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.4771266522407532) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.4257280303693842) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.30714385375566783) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.2016122925132513) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.5335402210932225) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.4290971884019673) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.2897059484217316) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.18902729897387327) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.43263471900299194) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.42132977709011177) --> New model used: True
FL ROUND 26
DC-DO Matching
DC 0 --> ['(DO2', '(DO1']
DC 1 --> ['(DO5', '(DO4']
DC 2 --> ['(DO3']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.3081491091153584) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.22008365234266966) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.32985423524305224) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.4391816114441026) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.27069710143934933) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.19902973206341268) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.4209800578514114) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.4357658193800598) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.473, 0.30785034110583365) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.20836921497248112) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.4067387063149363) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.3940096089811996) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.2779790239296854) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.2341244389312342) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.3983031134530902) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.38981720944400877) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.2902744578444399) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.2126141618564725) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.3269210476130247) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.39345605088351293) --> New model used: True
FL ROUND 31
DC-DO Matching
DC 0 --> ['(DO5', '(DO0']
DC 1 --> ['(DO3', '(DO2']
DC 2 --> ['(DO4']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.2869791607414372) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.22326710271742195) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.3493642883300781) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.40686495129903777) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.32723339351150205) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.20778521680645645) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.4061717757890001) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.418085280040279) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.3186496263500303) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.19875626358296722) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.41553028019051996) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.3774628175578546) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.3342643886816222) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.18456906443089247) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.5039027050472796) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3918181165526621) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.31877425403613596) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.18396772016584872) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.4670669603087008) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.44260569497488905) --> New model used: True
FL ROUND 36
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.479, 0.3149605361316353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.21970205821562558) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.5465845178710297) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.41533528557559474) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.33078371370676907) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.2230215194565244) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.40806395720038563) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.4376663875374943) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.33853215783694757) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.19617282200511546) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.5392231391752139) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3928335584732704) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.3405666625434533) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.1957190043386072) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.4484367260080762) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.421746231660014) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.3323875864655711) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.22983123861812055) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.5296295073926449) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.43163970318064093) --> New model used: True
FL ROUND 41
DC-DO Matching
DC 0 --> ['(DO0', '(DO1']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO4']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.31697051545698196) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.2136323872767389) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.4470313948635012) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.4433435222278349) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.283095558215864) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.21005859114602207) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.463792155681178) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3976218298450112) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.3241201109974645) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.23445590479671954) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.4466324753444642) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.4332173044206575) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.33736033764248713) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.20136212190706282) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.4233497650329955) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.375194832874462) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.33868450005352496) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.20408052855730058) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.3691657473631203) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.34721781459078194) --> New model used: True
FL ROUND 46
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO4']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.3466339791943319) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.20269220560602844) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.5092368347449228) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.37023184619820676) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.29148514559632166) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.18523026861064135) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.3529745602030307) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.4057163640658837) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.28170552419358863) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.18732002821937205) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.5036418668236583) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.40728156697610396) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.2930214057969861) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.19115647030621766) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.40033292466774584) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.3766026977954898) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.3066453105141409) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.22205244495347143) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.42716957023367286) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.36057418382680045) --> New model used: True
Results: Competition
Competition_DC_0
VAL: 
[(0.25, 0.04443984019756317), (0.25, 0.07179966375231743), (0.294, 0.09029780164361), (0.405, 0.10274842362105846), (0.404, 0.1201973113194108), (0.451, 0.13035795422643423), (0.451, 0.1425810459293425), (0.461, 0.1451124506518245), (0.459, 0.1760368304941803), (0.468, 0.1761757607832551), (0.465, 0.19416647404991091), (0.471, 0.19031314682215453), (0.466, 0.23359228840842844), (0.46, 0.24853772978670896), (0.474, 0.23987051874306053), (0.473, 0.2560490776579827), (0.478, 0.24797167168557643), (0.474, 0.26933135152049364), (0.474, 0.257529443513602), (0.469, 0.26897268440853805), (0.473, 0.26143405995704233), (0.47, 0.27730985713843254), (0.474, 0.29785239784559236), (0.467, 0.30975258057238536), (0.467, 0.30714385375566783), (0.476, 0.2897059484217316), (0.474, 0.3081491091153584), (0.471, 0.27069710143934933), (0.473, 0.30785034110583365), (0.478, 0.2779790239296854), (0.478, 0.2902744578444399), (0.48, 0.2869791607414372), (0.477, 0.32723339351150205), (0.476, 0.3186496263500303), (0.477, 0.3342643886816222), (0.48, 0.31877425403613596), (0.479, 0.3149605361316353), (0.475, 0.33078371370676907), (0.477, 0.33853215783694757), (0.477, 0.3405666625434533), (0.48, 0.3323875864655711), (0.477, 0.31697051545698196), (0.476, 0.283095558215864), (0.477, 0.3241201109974645), (0.48, 0.33736033764248713), (0.478, 0.33868450005352496), (0.476, 0.3466339791943319), (0.478, 0.29148514559632166), (0.482, 0.28170552419358863), (0.48, 0.2930214057969861), (0.482, 0.3066453105141409)]
TEST: 
[(0.26325, 0.043459017485380176), (0.25, 0.06910176756978036), (0.28625, 0.08651740956306457), (0.405, 0.09793887826800346), (0.40425, 0.11450791484117508), (0.45, 0.12330296683311462), (0.4495, 0.13521357721090316), (0.46975, 0.1373603984117508), (0.46, 0.1669597494006157), (0.47575, 0.16677658635377884), (0.46975, 0.18435369455814363), (0.47775, 0.1797192513346672), (0.469, 0.21988757091760636), (0.46, 0.23485506695508956), (0.47475, 0.22657945477962493), (0.474, 0.2424580945968628), (0.4785, 0.23353283184766768), (0.47675, 0.25571918046474457), (0.47675, 0.24169841611385345), (0.47075, 0.2541197488307953), (0.47725, 0.24638587540388107), (0.47225, 0.2605398112535477), (0.47825, 0.27963412475585936), (0.4715, 0.2903943541049957), (0.4705, 0.28771201813220976), (0.47825, 0.2710885579586029), (0.477, 0.288154088973999), (0.476, 0.2538822474479675), (0.47475, 0.2902655737400055), (0.477, 0.2597942441701889), (0.47775, 0.2730162570476532), (0.479, 0.2699487645626068), (0.47825, 0.3084003908634186), (0.4775, 0.30087159967422483), (0.47675, 0.31591342508792875), (0.478, 0.300921457529068), (0.47975, 0.2968024431467056), (0.48025, 0.31165725326538085), (0.47825, 0.3171494326591492), (0.47625, 0.3204759931564331), (0.4775, 0.3136351497173309), (0.479, 0.29859300708770753), (0.47825, 0.26499234783649445), (0.47925, 0.3026061178445816), (0.481, 0.3165654433965683), (0.48025, 0.3166141829490662), (0.47775, 0.3246188986301422), (0.48125, 0.2746976325511932), (0.483, 0.2641925941705704), (0.47825, 0.27611818993091586), (0.47975, 0.28774472284317015)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.81      0.94      0.87      1000
           4       0.00      0.00      0.00      1000
           6       0.34      0.98      0.51      1000
           7       0.00      0.00      0.00      1000

    accuracy                           0.48      4000
   macro avg       0.29      0.48      0.35      4000
weighted avg       0.29      0.48      0.35      4000

Competition_DC_1
VAL: 
[(0.262, 0.04436455535888672), (0.251, 0.062282080620527265), (0.373, 0.0706805988252163), (0.463, 0.0916736244559288), (0.459, 0.11129003369808196), (0.451, 0.13108988019824028), (0.443, 0.158301618501544), (0.457, 0.15763706554472445), (0.461, 0.17171229308471084), (0.446, 0.17199157116003336), (0.448, 0.17022600414045155), (0.46, 0.18340951904281974), (0.46, 0.19952387613989414), (0.463, 0.19810874040052295), (0.463, 0.19682635425776243), (0.469, 0.20588201360590755), (0.464, 0.18197959163412453), (0.463, 0.1987727743126452), (0.465, 0.21154149946710094), (0.46, 0.1936933790287003), (0.467, 0.21120805268269033), (0.47, 0.22078587534371763), (0.473, 0.21508490075543524), (0.468, 0.2070809772927314), (0.476, 0.2016122925132513), (0.469, 0.18902729897387327), (0.472, 0.22008365234266966), (0.471, 0.19902973206341268), (0.468, 0.20836921497248112), (0.47, 0.2341244389312342), (0.47, 0.2126141618564725), (0.469, 0.22326710271742195), (0.469, 0.20778521680645645), (0.469, 0.19875626358296722), (0.468, 0.18456906443089247), (0.472, 0.18396772016584872), (0.464, 0.21970205821562558), (0.469, 0.2230215194565244), (0.463, 0.19617282200511546), (0.467, 0.1957190043386072), (0.471, 0.22983123861812055), (0.469, 0.2136323872767389), (0.476, 0.21005859114602207), (0.473, 0.23445590479671954), (0.471, 0.20136212190706282), (0.472, 0.20408052855730058), (0.472, 0.20269220560602844), (0.47, 0.18523026861064135), (0.472, 0.18732002821937205), (0.472, 0.19115647030621766), (0.474, 0.22205244495347143)]
TEST: 
[(0.25325, 0.0434291261434555), (0.25125, 0.06006230038404465), (0.37275, 0.067833630412817), (0.46325, 0.08763139560818672), (0.46075, 0.1062627123594284), (0.45225, 0.12552101576328278), (0.4415, 0.1515355521440506), (0.457, 0.1510611606836319), (0.45775, 0.1647727227807045), (0.44675, 0.16469195330142974), (0.45125, 0.16312419641017914), (0.46025, 0.1764462519288063), (0.46025, 0.192179447889328), (0.4595, 0.18977122634649277), (0.4635, 0.18893464815616606), (0.46375, 0.19780298042297365), (0.46775, 0.17517882585525513), (0.462, 0.19050717252492905), (0.46225, 0.20474802827835084), (0.461, 0.18680650556087494), (0.4645, 0.2029718016386032), (0.465, 0.21182054209709167), (0.46575, 0.20758626115322112), (0.45825, 0.20014669382572173), (0.471, 0.19629893708229065), (0.46575, 0.1839549123644829), (0.46625, 0.21433466112613678), (0.4675, 0.19337100595235823), (0.4665, 0.20217051219940185), (0.4695, 0.22624042308330536), (0.4675, 0.20477064299583436), (0.4685, 0.21520957684516906), (0.46975, 0.20031351447105408), (0.46425, 0.19134246110916137), (0.469, 0.17669090002775192), (0.47075, 0.1766527361869812), (0.46625, 0.2099319919347763), (0.467, 0.21354493749141693), (0.46225, 0.18928850471973419), (0.4655, 0.18890198957920074), (0.46875, 0.2213033893108368), (0.47075, 0.20503232264518736), (0.468, 0.20158011794090272), (0.47175, 0.2247683267593384), (0.46625, 0.19424899113178254), (0.4685, 0.19587226063013077), (0.47175, 0.19448763686418533), (0.4725, 0.17799041831493378), (0.4705, 0.17950106984376907), (0.4745, 0.18344136971235275), (0.471, 0.2136627917289734)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.81      0.91      0.86      1000
           4       0.00      0.00      0.00      1000
           5       0.34      0.98      0.50      1000
           7       0.00      0.00      0.00      1000

    accuracy                           0.47      4000
   macro avg       0.29      0.47      0.34      4000
weighted avg       0.29      0.47      0.34      4000

Competition_DC_2
VAL: 
[(0.275, 0.04432411301136017), (0.387, 0.0866992349922657), (0.451, 0.09509955850243569), (0.448, 0.12655268445611), (0.451, 0.17445171085000039), (0.464, 0.2068759422376752), (0.464, 0.2383120637461543), (0.47, 0.2743811624888331), (0.468, 0.3013828208167106), (0.467, 0.3272228060346097), (0.47, 0.3739811360947788), (0.466, 0.39412847183458505), (0.471, 0.3568888035826385), (0.471, 0.3588982521984726), (0.47, 0.4179078217633069), (0.472, 0.405184631831944), (0.47, 0.392299519777298), (0.466, 0.4197673295494169), (0.471, 0.44488343491777776), (0.468, 0.44400890224799516), (0.462, 0.39630526142008604), (0.464, 0.4346873934213072), (0.465, 0.45723216447699816), (0.466, 0.4771266522407532), (0.469, 0.5335402210932225), (0.467, 0.43263471900299194), (0.465, 0.32985423524305224), (0.47, 0.4209800578514114), (0.472, 0.4067387063149363), (0.472, 0.3983031134530902), (0.469, 0.3269210476130247), (0.471, 0.3493642883300781), (0.473, 0.4061717757890001), (0.471, 0.41553028019051996), (0.475, 0.5039027050472796), (0.475, 0.4670669603087008), (0.47, 0.5465845178710297), (0.475, 0.40806395720038563), (0.478, 0.5392231391752139), (0.476, 0.4484367260080762), (0.473, 0.5296295073926449), (0.474, 0.4470313948635012), (0.473, 0.463792155681178), (0.473, 0.4466324753444642), (0.474, 0.4233497650329955), (0.469, 0.3691657473631203), (0.469, 0.5092368347449228), (0.466, 0.3529745602030307), (0.473, 0.5036418668236583), (0.469, 0.40033292466774584), (0.474, 0.42716957023367286)]
TEST: 
[(0.27825, 0.043275490760803226), (0.38925, 0.08313762971758842), (0.45525, 0.09075938367843628), (0.461, 0.11947788047790528), (0.466, 0.16523100256919862), (0.4715, 0.1961016225218773), (0.4705, 0.22676054126024245), (0.47375, 0.25869140833616255), (0.4735, 0.28333983933925627), (0.47325, 0.3097177770137787), (0.47275, 0.3541496697664261), (0.473, 0.37362668466567994), (0.4725, 0.33961908757686615), (0.475, 0.3412060295343399), (0.47575, 0.399239386677742), (0.47425, 0.3868744459152222), (0.47225, 0.37392539298534394), (0.472, 0.3954271866083145), (0.47575, 0.4230076699256897), (0.4745, 0.4338935190439224), (0.47075, 0.3767064176797867), (0.47225, 0.4151171454191208), (0.467, 0.435840500831604), (0.47425, 0.4510060040950775), (0.475, 0.5113092031478882), (0.4735, 0.40316533970832824), (0.4755, 0.3111808317899704), (0.47275, 0.3953493844270706), (0.474, 0.38176835680007937), (0.4745, 0.3759157844781876), (0.47125, 0.3065530194044113), (0.4725, 0.33106331717967985), (0.47375, 0.38181337547302246), (0.4715, 0.3928920849561691), (0.47375, 0.489808878660202), (0.47625, 0.44740571367740634), (0.473, 0.5310418137311935), (0.47425, 0.38938676071166994), (0.47625, 0.5035672135353089), (0.47425, 0.4158856283426285), (0.4735, 0.4911271599531174), (0.47625, 0.41735250735282897), (0.475, 0.42739433991909026), (0.475, 0.414151818394661), (0.4765, 0.40218696546554566), (0.47675, 0.3481228632926941), (0.47625, 0.4795124890804291), (0.47325, 0.3339594570398331), (0.47275, 0.46541076302528384), (0.4755, 0.37800980579853055), (0.474, 0.40478985571861265)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.34      0.96      0.50      1000
           4       0.00      0.00      0.00      1000
           7       0.00      0.00      0.00      1000
           8       0.82      0.94      0.88      1000

    accuracy                           0.47      4000
   macro avg       0.29      0.47      0.34      4000
weighted avg       0.29      0.47      0.34      4000

Competition_DC_3
VAL: 
[(0.225, 0.044532188177108765), (0.25, 0.10071668189764023), (0.269, 0.12162477773427963), (0.416, 0.14859290887415408), (0.441, 0.18057729303836823), (0.446, 0.2312651706188917), (0.463, 0.24315960404276848), (0.455, 0.2798636357784271), (0.467, 0.29754252534732223), (0.469, 0.30223537323623895), (0.468, 0.2888700136058033), (0.473, 0.31200646061077714), (0.473, 0.372750877273269), (0.473, 0.3774753681756556), (0.472, 0.3871815587184392), (0.468, 0.4176318839257583), (0.472, 0.4460220302087255), (0.468, 0.38738691445346923), (0.463, 0.422484582982026), (0.463, 0.4262213265541941), (0.472, 0.3875679081510752), (0.463, 0.42509507326432505), (0.473, 0.42265893540903926), (0.47, 0.4257280303693842), (0.474, 0.4290971884019673), (0.472, 0.42132977709011177), (0.468, 0.4391816114441026), (0.473, 0.4357658193800598), (0.469, 0.3940096089811996), (0.468, 0.38981720944400877), (0.469, 0.39345605088351293), (0.468, 0.40686495129903777), (0.467, 0.418085280040279), (0.47, 0.3774628175578546), (0.472, 0.3918181165526621), (0.467, 0.44260569497488905), (0.467, 0.41533528557559474), (0.465, 0.4376663875374943), (0.473, 0.3928335584732704), (0.47, 0.421746231660014), (0.47, 0.43163970318064093), (0.472, 0.4433435222278349), (0.472, 0.3976218298450112), (0.471, 0.4332173044206575), (0.471, 0.375194832874462), (0.472, 0.34721781459078194), (0.466, 0.37023184619820676), (0.47, 0.4057163640658837), (0.466, 0.40728156697610396), (0.469, 0.3766026977954898), (0.47, 0.36057418382680045)]
TEST: 
[(0.23075, 0.04349978512525558), (0.25, 0.09680754977464676), (0.27325, 0.11629943352937698), (0.42225, 0.14138725328445434), (0.44, 0.17147793173789977), (0.4455, 0.21885153269767763), (0.4645, 0.2289725457429886), (0.4565, 0.2658118067979813), (0.46975, 0.28245870041847226), (0.472, 0.28643305253982543), (0.475, 0.2728337438106537), (0.47425, 0.29480820667743685), (0.476, 0.3523068776130676), (0.471, 0.35561001670360565), (0.46925, 0.36763006043434143), (0.472, 0.3980542761087418), (0.47175, 0.4241338584423065), (0.47125, 0.3682322759628296), (0.4695, 0.40374961388111114), (0.4715, 0.4060871324539185), (0.47575, 0.36844826543331144), (0.468, 0.40480338406562805), (0.4765, 0.40512750720977786), (0.47025, 0.4018600302934647), (0.47325, 0.4084382860660553), (0.475, 0.40100753462314603), (0.4715, 0.4207399640083313), (0.47475, 0.4129907282590866), (0.47125, 0.37595517611503604), (0.4725, 0.37159390389919283), (0.471, 0.37438466370105744), (0.474, 0.3891755686998367), (0.47075, 0.39577223551273344), (0.4745, 0.36101003992557523), (0.47125, 0.373710262298584), (0.46725, 0.42359389185905455), (0.4745, 0.39519196343421936), (0.4715, 0.41907194793224334), (0.47225, 0.3765739185810089), (0.474, 0.4043478704690933), (0.4725, 0.4162473647594452), (0.47375, 0.4216956560611725), (0.474, 0.38118028235435486), (0.472, 0.4154240732192993), (0.472, 0.3610659152269363), (0.472, 0.3327484468221664), (0.4715, 0.3547508548498154), (0.47125, 0.3882248033285141), (0.47175, 0.39119005274772645), (0.4735, 0.3606393626928329), (0.47125, 0.3437466071844101)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.33      0.98      0.50      1000
           4       0.00      0.00      0.00      1000
           7       0.00      0.00      0.00      1000
           9       0.86      0.91      0.88      1000

    accuracy                           0.47      4000
   macro avg       0.30      0.47      0.34      4000
weighted avg       0.30      0.47      0.34      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [145]
name: alliance-2-dcs-145
score_metric: contrloss
aggregation: <function fed_avg at 0x719d5af6ec10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=145
Partitioning data
[[0, 3, 6, 8], [1, 4, 6, 8], [9, 7, 6, 8], [5, 2, 6, 8]]
[(array([33451, 31000,  3114, 10896,  4060, 19644, 44165, 35381, 42784,
       22062, 10061, 13549, 40945, 48788,  6015,  4859, 39301,  7347,
       13833, 47754, 38289,  3570, 44806, 46113, 42829, 15677,  1338,
       12492, 41880,  4273, 13712, 17232, 13627, 44568, 19945, 26018,
       25242, 36106, 19444, 10954, 21869, 26049,  3713, 35996, 38185,
       30267, 36163, 49050, 33058, 48847, 19778, 26060, 27080, 18572,
       24534, 32538, 42931, 12461, 28068, 31207, 15870, 32062, 42630,
       18211, 22129, 34856, 34017, 32376, 16380, 45186, 16763, 12659,
        4498, 24700, 20267, 44970, 24887, 26724,  6512, 15309, 37328,
       47473, 38618, 37022, 45344, 40635, 32078, 41055, 35269, 22674,
        3204, 26846,  6687,  4079, 46872, 34168, 31837, 47243, 30517,
       32885, 20015, 27415, 39236, 27280, 11812, 38222,   115, 36007,
        4940, 44668, 11734, 38433,  2720, 28080,  4619, 16596, 30029,
       16316, 19015, 17486, 48359, 34434, 11768,  9868, 48986, 16553,
       32479, 13401,  7504,  2365, 25027, 32233, 10326, 32967,  8249,
       38796, 13861, 44443, 18953, 42992, 30913, 14341, 44875, 45347,
       15735, 12911, 14482, 45102,  8858, 22844, 43777, 29295, 12178,
       24349, 33419,  9755, 37729, 25082, 17357, 17866, 26632, 11263,
        5994,  7655, 12149,  1147,  5194, 38545,  7320, 21881, 25362,
       43992, 12499, 34944, 14116,  7970, 12257, 24111, 35973, 38566,
       13714, 29860, 20217, 43757, 40211, 44580, 27768,  7299, 46367,
       33603,   564,  3441, 34917, 38466, 24837, 30895,   695, 17093,
       17994, 23581, 49717, 36052, 20398,  9257,  1522, 36737, 24735,
       30233, 20924, 22684, 25995, 47520,  3136, 14732, 41490, 48147,
        5589,  5828,  9671, 43951, 36149, 30963,  3582, 13368, 25178,
       20813, 16305, 41164, 41851, 21144,  7925, 47324,   405, 39329,
       48924,  4869,  1493, 31284,   392,  9096, 43542, 23746,  9287,
       48958, 15102, 40022, 20454,  5858, 46987,   527,  7603, 39211,
       37554, 16438,  3351, 22761, 37429, 41601, 27304, 17024,  4266,
       30346, 20784, 35960, 10085, 32844, 30318, 30305, 38093,  5507,
       41853,  5741, 29074, 38304, 39761,  1109, 25215, 23465, 12391,
       24733, 23974,  2738,  3365, 15247,  4025,  9034, 39289,  5586,
       33717, 37236, 19808, 47654, 21147, 42233, 13593, 33098, 48742,
       15421, 36249, 20252,  9830,  1951, 37194, 39543, 46349, 33582,
       20017,  7469,  3818,  5006, 18366, 14171, 35007, 45948, 43836,
       38982, 28237, 33633, 41053, 19577,  4558, 28996, 20496, 18188,
        8943,  8105, 22523, 18348, 41159, 46423,  6872, 13064, 21913,
       13581, 30839, 43110,  6205,  8101, 48088, 28513,  7049, 17530,
       38761, 20648, 16301,  5086, 11476, 39045, 14018, 34984, 24276,
        3756, 46977, 10463, 48019, 45117, 23695, 26485,  9394,  5578,
       48062, 25997, 38127, 17567, 47788, 27585, 15383, 47830, 31242,
       10565, 25062, 21373, 26556, 42633, 20625, 17920, 16714,  4472,
        1573,   878,  6716, 14536,    80,  9052, 46529, 27906,  1919,
       41521, 23700, 13688, 49386,  3749, 49233, 14924, 21873, 34647,
        3514, 36990, 21652, 37513, 32145, 35130, 27796, 12155, 38699,
       22008, 20338,  9817, 47997, 48402, 36511, 47603, 28835, 10439,
       43065,  6089, 15298, 36828, 41910, 19871, 49582, 37514, 34581,
       11434, 49574, 30159,  3524, 31632, 41578, 35923, 34822,   287,
        6724,  9345,  3110, 26064, 12904, 32264, 37105, 29109, 29096,
       44093, 48132, 17478, 26482, 34741, 31415, 46855, 35759, 41285,
       23278,  8067, 21832,  7224, 39235, 26036, 18442, 15614, 41845,
       11738, 39921, 13053, 10948, 23541, 11894, 38100, 17251, 33874,
       29865, 28050, 48849, 27673,  7343, 45217, 35323,  2353, 36269,
       42738,  3125, 42445,  3222,  8381, 37912, 23295,  2425, 43013,
       11294, 40651, 45590, 48346, 44456, 36946, 28725, 20965, 24384,
       26160, 14706,  3758, 46341, 42189,  8069, 40039,  1327, 45135,
        5398,  5964, 32918, 15338, 45169, 27346,  3780, 37378, 11392,
       13753, 40261, 21580, 14476, 17680, 46073, 22550, 17788, 31601,
       36903, 14906, 47161, 11651, 17566, 38268, 24354,  3122, 44654,
        9941, 23582, 48400, 26618,  2571, 43264,  6262, 41870,  9928,
       14342,  6506, 34552, 18103, 21497, 16387, 19275, 37013,  4217,
       12396, 35332,  3724, 14212, 14131,  2288, 11675, 20788, 42791,
       20972, 23708,  8991,  2112, 32694,  3892, 12168, 22926, 37481,
        8891, 22085, 35340, 23780,  1192,  3593, 10938, 12508,  8436,
       28690, 13120, 29767, 30323, 46307, 11917, 48977, 14608, 23135,
       46320, 20586, 35831, 25622, 48888, 23008, 15534, 31353, 24478,
        6938,  6949,  7825, 19139, 40202, 27648, 26228, 46473, 23208,
       36310, 21097, 15497, 48619, 39343, 41491,  3111, 25469,   899,
       41668, 45730, 49771, 15351, 26211, 10026,  2684, 36712, 40514,
       18736, 48532, 43392, 33696, 49406, 34965,   143, 19239, 34035,
       28277, 15650, 44012, 41603,  4391, 41309,  1563, 32869, 22889,
       22640, 42447, 31112, 36943, 24728, 49265, 15185,   248, 23086,
       42108, 45862,  5000,  5019, 15402, 17936, 37579, 40153, 19959,
       24000, 29560, 13683, 17172,   368,  5474,  9575, 26566, 30626,
       40357,  8612, 39913, 46322,  4082, 27450, 23516, 38751, 38508,
        2503, 43298, 32793, 25841,  8206,   819, 20934, 16539, 17857,
       49952,  9924, 31447, 18282, 21037, 22044, 12003, 30982, 21588,
        4421, 21192, 13635, 15424, 41549, 43074, 35900, 18992, 20141,
        8309, 11044, 48446,   117, 42099, 40264, 31616, 30625, 25442,
       15651, 25291, 39148, 46612, 10600, 15971, 39068, 35123, 34264,
       18126, 37956,  9681, 23596, 49829, 47115, 34804, 34643, 16822,
       17204, 33069, 46550, 22082, 15231, 37748, 13077, 36625, 24401,
        5095, 16927, 37822,  5921,  7064,  9762, 25255, 44428, 12632,
       10733, 21970, 49962,  5644,  8926, 10234, 31594, 13633, 39336,
       16430,  6250, 10442, 32464,  1914, 41732,  4092, 17442, 23845,
       36146, 46888, 31386, 42050, 46821, 15328,  4233, 16512, 16473,
       45318,  2923, 29746, 12418, 24811, 40109,  3885, 35013, 48943,
        2070, 26109, 25002, 29477, 12452, 40983, 21427, 28153, 33240,
       30299, 20954, 30438,  2462, 11591, 37042,  2586,  5675,  4397,
       28239, 46142, 14693, 13425, 36526, 12481, 17007, 30273, 27271,
        8006, 39502,  8123,  5716, 12457,   627, 21414, 11993,  8512,
       16435,   485, 47708, 10682, 11948, 13109, 31919, 49782, 48213,
       38453,  1250, 30406, 32915, 12605, 11244, 46714, 13729, 37147,
       14132, 11005, 33942, 35687, 22776, 28807, 41041, 18397, 13241,
       41689,  1383, 31526,  5487, 15635,  3614, 27820, 28125,  6989,
       14642, 18645, 16156, 40008,  1141, 39578, 23440,  8854, 43109,
        8574, 49291,  1967, 29340, 21579,  7443, 10680,   888,  8483,
       25902, 44458,  3215,  6219, 37994, 29147, 49182, 39069, 47770,
        6176, 26055,  8668,  2285, 14753, 35300, 30357, 10956,   546,
       32987,  5078,   909, 37308,  6679, 49778, 19502, 31832, 43872,
       23893, 49011, 23219, 40939,  2754, 21246,  9282, 42699, 27661,
       32619,  3319, 27854, 33356, 38008, 14934, 17277, 34910,  5910,
       44213, 47877, 11577,  2732,  8406, 22888,  1439, 16049, 41335,
       12194, 39577, 29493, 25111, 33407, 18141, 34892, 25047, 38105,
       43725,  5999,  7496, 45372,  9708, 37653, 27561, 32281, 31697,
       40429, 37587, 12801, 30807,  1080, 31778, 35661, 32567, 29027,
       15746, 37002,  6627,  6862, 28687, 21565, 16125, 34577, 25557,
       35268,  3800, 37642, 15795, 36940, 40641, 12442, 29175, 49827,
        2065, 31121, 19132, 24451, 33888, 48253, 12861, 37318, 28599,
       16856, 40483, 17210, 39032,  3169,  4723, 33869, 45442,  7013,
       29850, 20913,  6192, 47960, 48838,  8358, 18349, 45375, 28103,
        9960]), [0, 3, 6, 8]), (array([28634, 35692,  6709, 46330, 11418, 23607, 40152, 35799, 48975,
       35004, 29536, 28526, 13032, 37570, 20336, 45210,  4591,  9809,
       49561, 38257, 24191, 23213, 34263, 16736, 35174, 48631, 10153,
       47314, 29828, 36158, 11308, 42294, 18223,  3366, 30902, 49503,
        3822, 41061, 10271, 36033, 25071, 30676, 19310, 39405, 26905,
       18932, 21154,  1251, 49491, 25799, 24807, 49871, 33650, 41248,
       29121, 41607, 29963,  8350, 43711, 28615, 42281, 42637, 17539,
       17397, 47616, 13317, 24484,   644, 49376, 49269,  2455, 25105,
       22900, 23357,  4696, 40145, 29120, 27073, 21540, 22349, 22013,
       33870, 35357, 41654, 15299,  8825,  9251, 47190,  4379, 25827,
       18066, 19565, 32235, 28183,  3735,   119, 14717, 41739, 46421,
       12676,  2727, 11242, 45979, 30208,  2390,  7709, 42287, 22659,
       40178,  4624,  4909, 36101, 32736, 38047,  9095,  4557, 32099,
         493, 30804, 19574,  2180,  7352, 43145,   617, 39800, 32603,
       10714, 42684, 26241, 43935, 43207, 14101, 36991, 38157, 10248,
       45810, 17534, 21398, 49314, 30492, 46649, 36376, 41257, 18063,
       41576, 29791, 23601, 49416,  5356, 19500, 48806, 36872, 13402,
       39352, 37989, 39192, 49354, 12169, 25975, 20494, 40013, 47839,
       19667, 28127, 12049, 46359, 23058, 42410, 31147, 30716,  6922,
       28018, 34349, 34519, 29049, 48528, 30712, 44472, 21528, 10895,
       44965, 13763, 44359, 24263, 43006, 15710, 37401,  6623, 47606,
       38682, 16128, 31985, 17111, 13774, 14040, 39892, 24446,  9480,
       33392, 47151, 30019, 36679, 18134, 44097, 35805, 36334, 17772,
       32472, 28105, 48675, 30049, 14851, 16638, 47026,  4840, 27404,
       44309, 24280, 17312, 39240, 28896, 46966, 49795, 22867, 12628,
       46547, 42318, 28326, 35041, 40220, 10868, 26180, 37702, 10522,
       28248, 47836, 19046, 49933, 12412, 27814, 21877,  9565, 25479,
       10420, 33808, 44411, 12424, 17474, 26362,  9442, 39177, 43897,
       43934, 11794, 30474,   145, 41766, 18834, 11567, 27044,  5256,
       49645, 36345, 49740, 48787, 19197, 40228, 38247,  1953,  3217,
       24548, 30950, 45881, 15170, 33357, 22886, 20147, 33272, 27303,
       40375, 34574, 10766, 20730, 32533, 48429,  1471, 14036, 38386,
       41302, 22247, 33610,  2951,  2363, 15917, 44752, 18434, 32754,
       46947, 25414, 14816, 37775, 38660, 32930, 44978, 32882,  8068,
       14555, 42863,    89, 37996, 15723, 48591, 19045,  4413, 43758,
       24067, 41625, 20406, 25781, 33037, 27028, 40530, 36896, 33073,
       35839, 25474, 28658,  1385, 41270, 21048, 37757, 47573,  2216,
       11765, 46884, 38805, 34752, 48513, 22981, 22480, 14535,  8792,
       14974, 47149, 18455, 11565, 49352, 33222, 21949,  2673,  6378,
       38701, 28084, 22385, 14303,  6387, 43420, 49719, 37853, 47141,
       30794, 17083,  3695, 25292, 44002,  9010, 37563, 13828, 23994,
       21260, 37903, 27214, 47399, 49981, 31588, 43304, 47155,  6612,
        3908,   660, 43923, 36420,  5212, 25672, 39451,  2515, 18114,
        9141, 21607, 10015, 19704, 43928, 19170, 30044, 22606, 42303,
         581, 43202,  2442,  6940, 33002, 48689, 17770, 10629, 40835,
       45022, 22098, 43502, 15024, 24524, 44900, 13134, 17634, 48478,
       23187, 19980, 39932, 28160, 38296,  3688, 32066, 13391, 27342,
       31735, 28797, 20828, 11190,  5462, 40642, 27014, 22202,  2691,
       29353, 16669, 26118, 28561, 31699, 22173, 22284, 22902,  9550,
       27964, 37886, 40798, 46526, 46452, 48016, 27074, 49727, 30153,
       47922, 45555,  6331, 13995, 44994, 49027, 32119,  8405, 48228,
       18912, 14893, 29922,  9776, 10913, 32450, 17800,  8289, 30156,
        4400, 28803,  8404, 10888, 36665, 46077, 33743, 41777, 14382,
       11033, 48985, 16640, 15530,  4317, 16481, 49355, 33862, 37639,
       24498, 29172, 20142, 29265,  4642, 10536, 17260, 22360,  1659,
       38395,  1663, 18821,  3866,  7984, 32063, 19092,  6391, 40823,
       33623, 26383, 39120, 27660, 25000, 16508, 30724, 11875, 20310,
       28188, 26569, 16177, 20802, 32709, 49404, 11719, 49279, 10108,
       36525, 12666, 42261, 36060, 10327, 13403, 45140, 31128, 26748,
       30496, 15994, 15479, 10985, 11657,  3712,  4440, 48771, 17751,
       37654,  8617, 30302, 39994, 32338, 28943, 22113, 28644, 31405,
       45900, 49655, 30953,  6095, 22947,   234,  9278, 17925, 37351,
        4201, 38723, 43624, 13600, 38572, 29339, 46263, 18330, 30222,
       28194, 36984, 41652, 29408, 36816, 28481, 10400, 26872, 19707,
        1584, 41466, 45978,  6745, 13719, 40627,  1628, 21956, 38959,
        4415,  4810, 22347, 44959, 17766, 22157,  5785, 45266,  3942,
       44492, 31315, 17776, 13101,  6804, 29680, 32359, 46342, 18610,
        3683, 16429,  9855, 34942, 10345, 43482, 33528, 37625, 45944,
       11394,  3991, 47874, 15986,  7720, 48325,  6722, 32739, 32759,
       45420, 30193,  2631, 29360, 24424, 39889,  6454, 42013, 44258,
       12356, 21846, 33244,  1956,  8002, 24552,   807, 19834, 33420,
       38310, 28171, 47567, 40364, 40031,  8008, 16287, 23066,   862,
       33953, 39057,  3912,  9045, 11468, 29506, 12221, 34754, 45823,
       20800,  8466, 26771, 36666, 33405, 39823, 43611, 23412, 31270,
       15179,  2625, 39007, 21560, 33543,  1761, 28325, 42778, 30130,
       34644, 49853, 49516,  9877, 34031, 39680, 47240, 20268,    19,
        7232, 15064, 27803, 35672, 14973, 24510, 15810, 11310, 18494,
       33366, 29889, 15612, 17663,  4911, 48947, 26921, 14660,  9406,
       43210, 23686, 33411, 32768, 44675, 34823, 42888,  6749, 16442,
        9850,   680, 12879, 28722, 14951,  9785, 15039, 29973, 41707,
       32904, 44597, 28506, 26727, 31629, 48463, 16687,   770, 36296,
        7143,  4118, 18338, 22910,  3742, 31118,  4637, 43166, 13162,
        1176, 39678, 44602,  6046, 39639, 34694, 45355, 39637, 44393,
       30741, 26240, 45208,  8027, 47214, 11808,  2441, 40988, 37384,
       16246, 13591, 40137,  6508, 36949, 26603, 13958, 25386, 43755,
       46431, 38724, 44570, 36739, 16902, 41954, 41980, 33752, 44981,
       38133,  5906,  5378, 36151, 29479, 11148, 47206, 24878, 33410,
        4446, 27842, 16395, 13404, 18132, 31530, 19706, 27339, 10876,
        1862, 39707, 18996,  6775, 38301, 47894, 14158, 27857, 45095,
       33029, 41256, 12600, 10102, 30844, 47758, 40691, 26220, 37064,
       31372,  6474, 49330, 44057, 49257, 40491,  9387,  3268, 31695,
       25258, 45017, 22661, 47119, 46008,  6403, 24692, 40276,  8879,
       37420, 48602, 25238, 29919, 37798, 49025, 41678, 22712, 22649,
        1138,  2212, 34359, 22214, 47723, 22255, 12762,  7118,  4652,
        5297,  4313, 20239,  4848,  2803,  8222, 26463, 26898, 19238,
       49220, 46313, 15727, 38443, 11593, 49712, 45486, 10848,  5245,
       39334, 43705, 42032, 29782, 27982, 20000, 46015, 10763, 31628,
       20650, 34208, 42325, 38120, 14684, 43334, 15323, 23149, 29877,
       10346, 27976, 39383, 49446, 40136, 41207, 49960, 49144, 13695,
       17922, 27955,  2749, 34135,  6825,  8214, 45161, 15405, 13868,
       35540, 48805,   291,  9786,  4191,  9490, 15759, 16381, 11965,
       47342, 15429, 49042,  5463,  2663, 27536, 43409, 43161, 23342,
        8250,  3330, 31313, 32259, 42265, 40019, 35619, 36267, 18548,
       40805, 42749, 32611, 38237, 18822,  1617, 36579, 19622, 23617,
       38362, 40493, 32077, 29734, 18112, 36966, 48286,  4611, 43855,
       36295, 12207, 30388, 37832, 45839, 39106, 41957, 27243, 26703,
       48757, 42004,  5630, 22862, 41753, 17482,  1151,  4794, 29674,
        2666, 35177,  2109,  3740, 36933,  6139, 16792, 47000, 41543,
       23723, 43678, 14123, 28295, 35912, 45892, 15046, 29078, 43662,
       23538,  1702, 26156,  7507,  7108, 30123, 23438, 45853, 42024,
       48541, 22635, 41676,  1357, 17628,  4602, 22455,  6964,  2454,
       41555]), [1, 4, 6, 8]), (array([36549,  8103, 41592, 46390, 14059, 48503, 20804, 21775, 41996,
       19199, 15690, 30525,  6571,  9911, 39672,  4364, 11549, 22475,
       15337,  3097, 26585, 46221, 23358, 27589, 12315, 25476,  3006,
       47437, 35558, 39434, 46775, 24343, 48084, 40168, 28949, 36683,
         615, 20693, 49193, 20825, 13711,  7657,  7583, 28379, 46599,
        3755, 26150, 43566, 35749, 12483, 31324, 23819, 27569, 14545,
       31983,   438, 38651,  1893, 46848, 17905,  6111, 38328,  1504,
        4665, 34049,  6480, 37201, 37536, 18019, 20683, 16592,  7854,
       12008, 14467, 40257, 38414, 39830,  1049, 21470, 46296, 39440,
       21159,   672, 25094,  9382, 11173,  5591,  6129, 49880, 39884,
       11332,  1868, 24712, 41994, 19670, 49123,  2526,  3941, 43560,
       47175, 37058, 27675, 47641, 14326, 15616,  8295, 47086,  4768,
        2585, 41477, 14077, 24149, 12588, 20851, 19111, 40540, 38226,
       44678, 29186, 48292, 23383, 21520, 19076, 19432, 24987,  8728,
       35203, 32724, 10806,  3677, 22663, 44182, 39174, 26347, 16205,
       22497, 32273, 19976,  1263, 47036, 17747,  5650, 15159, 30785,
        3014, 18937, 29580, 24211, 35807, 41357, 35197,  5009, 33729,
       49670, 45146, 41329, 32133, 37780, 33856, 25648,   881, 22233,
       27287, 49370,  8323, 28709, 38789, 48654, 43882,  8561, 23141,
       46788, 43008, 46059, 18749, 12279, 41044,  2818, 40052, 40237,
       15993, 21289, 43901, 14083, 17163, 33547, 44871, 45844, 21010,
       34774, 15637,  6275, 27959,  4416, 13138,  2985, 20187, 17108,
       30161, 24494, 37040, 36059, 42101, 27058, 41316,  5532, 20613,
       14205, 37126, 11504, 24718, 39418, 43218, 15252, 19687, 46346,
       17109, 17709,  1706,  9715, 43696, 31905, 17435, 42154, 49087,
       40387,  7494, 42385, 29651, 15221, 28871,  1437, 38277, 39115,
       41181, 40229, 46744, 16488, 43141, 38280, 29349, 44901, 28141,
       23633, 43558, 48353, 21516, 45008, 15976,  2961, 49310, 14254,
       23310, 49162,   163, 20063, 15881,  9462, 33605, 21113, 43833,
       15539,  1927, 42951, 27824, 23773, 30066, 20971, 24090,  9581,
       23356, 47321, 46694, 47027, 32660,  7604, 20618, 36303,  4883,
       42386, 15999, 46283, 37679, 40426, 37159, 45630, 39354, 12365,
       11877, 25868, 36256, 30348,  9454, 49640,  4241, 10368, 28296,
       40837, 16175,  6748, 19089, 33374, 15324, 19691, 32073, 37884,
       36452, 21086, 30845, 24536, 25572, 10879, 13975, 47387, 18144,
        6119, 18443, 36936, 33936,   289, 23331, 29904, 29618, 23506,
       23330, 18975, 20879, 44278, 31073, 49058, 27913, 18620, 39246,
       34357, 32382, 16600, 17377, 26861,  9805,  7546, 20773, 28249,
       39168, 46455, 38616, 34440,  6600, 37474, 37660, 14556,  2370,
       45493, 31082, 22647, 45725, 10162, 14044, 38611,  3544, 35341,
       22965, 36977, 19037, 37573, 44115, 47856, 41755, 38196, 17499,
       18041, 26788, 25164, 46025, 17462, 22198, 31653, 35589,  6695,
        7385, 12925, 15290,  9294,  9891, 11781,  9329, 41397, 42366,
       17119,  3844, 31777, 29644, 35284, 14897, 26699, 45459, 32354,
       35290, 21268,  7544, 20010, 28812, 45939, 39307, 13482,  6301,
         916, 34518, 32973, 18022, 37019,   237,  6864, 23501,  4871,
       14641, 17436,  1908, 31461, 18997, 18350, 15047, 49695, 30163,
       16251, 26692, 13347, 32148, 22625, 33678, 34228, 12527, 13124,
       31597, 16633, 26086, 40579, 42895, 37211, 14983, 35812,  8481,
       25896, 18079,   152,  6051, 22612, 20166, 29669, 21493,  6792,
       49183, 36815,  2563,  6278, 47607,  4485, 31718, 38920,  5771,
       32545, 13514, 17649, 44471, 27672,  1197, 33341,  4928,  3251,
       44353, 49037,  5168,  5542, 19306, 23484, 25227, 21660, 28156,
       49903, 25957,  7482, 18313, 35056,  7314, 42368, 23456, 37100,
       13703, 12547, 36205,  7611, 22130, 49809, 31746, 20738, 22934,
       19600, 42485,  4386, 30670,  3157, 14236, 13796,  5796, 24897,
       20098,  7638, 27087,  3841,  2369, 21941, 11363, 36176, 32170,
         921, 39808, 40313,  2972, 46306,  6179, 29931, 19421,  3341,
       22266, 28061, 36716, 27610,  5723,  1453, 24989, 46850, 13441,
       24156, 10920, 20628, 38769, 39759,  4430, 44077, 18265,  7968,
       24215,  6521,  9245,  8983, 15631,  8171,  1819, 12550, 22290,
       38401,  2596, 49119,  1760, 32240, 17272, 11233,  1393, 19984,
       27872, 45836, 44244, 31794,  9482, 31421, 16675, 36061, 48079,
       31984, 38409, 46385, 13365, 12004, 12739,  8950, 44314, 40016,
       31142, 46193, 10256, 45863,  8253, 19588, 49776,  2179, 38397,
       21030,  3563,  7481, 12996,  5679, 42079, 33309,  6395, 37356,
       46273, 37682, 13261, 40927, 27611, 26340, 46492, 36913, 44068,
       45474,  1259, 16440,  2831, 23054, 38088, 21091,  2638, 23745,
       40436, 39969, 34304, 20986, 28042, 29456, 33551, 22525, 27367,
       13634, 27473, 11638, 14828,  2100, 34157, 47545, 29538,  5599,
        1550, 29958,  9468, 31133, 42527,  1246,  4605,  5177, 16606,
       39922, 46885,  4077, 14163, 49854, 19482,  6107, 42391, 15026,
       26701,  6962,  4786, 27577, 25612, 42358,  4606, 27503, 25935,
        5878, 46915, 42614, 25340, 36348, 25569, 41684, 47135, 37796,
       41399, 13619, 15048, 41350, 28859,  8633, 26122, 38391,  9974,
       26247, 18033, 22670, 40675, 34953,  7092, 24950, 40438, 26619,
        7315, 40047,  7001,  1741, 32856, 15458,   210, 19189, 26413,
          23, 16242, 31312, 31181, 33018,   721, 24213, 44011,   242,
       20254, 18050, 21146,  9339,  7267, 48526, 40568, 24895, 29935,
       14395, 39000,  9219, 43661, 29279, 12850, 43155, 30644, 12302,
       28776, 32339, 21438,  7348, 16639, 22068, 23030,  3403, 45385,
       32385, 38845,  2517, 20008, 38721, 45582,  1766, 25723,  4946,
       38137, 38227, 35098, 49459, 16059, 49804,  1837,  6768, 43538,
       32812, 20561, 35342, 30916, 46670, 22600, 36229, 49043, 44444,
       13582,  9818, 44927, 49551, 35468, 37258, 19124,  3881, 21755,
       26195, 16388, 42963,  5648, 30660,  2987, 30071, 43400, 17894,
       15698, 27515, 15268, 37329, 45536, 36985,  7834, 45743,  7131,
       15803, 44504, 36911,  4272, 11616,  4300, 39462, 48273, 43014,
       36650, 29030, 11091, 17531,  7366, 47094, 26643, 41165,  7996,
       30262,  3860, 47791, 22504, 30948, 18312, 16700, 46413, 19557,
       48311, 44528, 17933, 39367, 31981,  7440,   602, 20508, 47646,
       25749, 42897, 28115,  6238, 48158, 14821,  4955, 20081, 21298,
        3829,  6995,  5253, 29934, 20530, 13565, 13015, 40919, 40881,
       21671, 24716, 23031, 35419, 43159, 25669, 39419, 34893, 40292,
       11906, 48965, 46152,  5251, 44304, 35388, 48475, 23294,  8007,
        7886,  4761, 45024, 38881, 23281, 43540, 15022, 24286, 11207,
       32861, 27978,  2426, 49631, 39838, 26639, 35169, 14369,  8273,
       28855, 23045,  8952, 47713,  3165, 10245, 43381,  9378, 31625,
       19713, 23148, 21931, 49378, 38255, 13606, 48320, 38045, 20946,
        6337, 46012, 27858, 34212, 13504, 41264, 36443, 27699, 34171,
        5891, 27783,  9192, 36013, 13674, 42870, 21664, 48107,  7716,
       12585, 11149, 39089, 31917, 22518, 47596,   566, 41835, 40409,
       31795, 22462, 35496,  1964, 40470, 18461, 17405,  8852, 37643,
        9975, 26967,  9883, 47827, 35321, 33418, 35181, 47245, 25508,
       46028, 30018, 10120, 36097, 25042,  4283, 35295,  8672,  3384,
       48737, 15491, 20290, 32183, 15241,  2640, 46938, 16090, 22696,
        8468, 45309, 32783, 18444, 44681, 28928, 42933, 18892, 49542,
       46892, 19725, 32897, 44691,  6182, 48120, 24578, 45138, 45994,
       13131, 21596, 27507, 49372, 23728, 16942, 18795, 12951,    92,
       14464, 11461, 33104, 40575,  2176,  8211, 38382, 27252, 22616,
       47609, 24039, 26579, 20100, 20089, 38158, 19472, 34636, 49677,
       38214]), [9, 7, 6, 8]), (array([ 2260,   993, 34763, 37911, 39124, 49703, 43397, 20416,   681,
       21586, 13411, 43886, 11735, 13679, 44223,  1486, 10033, 41400,
       33552, 47211, 20083, 13103, 41587, 15403,  9174, 18552, 43330,
       18732,  3810, 36540, 22470, 45331,  6094, 44268, 15987, 25722,
       25670, 26479, 32255, 45260, 24899, 33136,  7072, 45991,  2657,
       10175,  5049, 12832, 35748, 27188, 20076, 20809, 35438, 45903,
       36952, 42537, 49548, 31303, 42538,  3008, 21889, 49399, 24663,
       39694, 31243, 25065, 38272,  5755, 49327, 43466,  4960, 18685,
       46017, 10897, 14405, 33423, 24226, 37790, 43063, 44312, 10709,
       30684, 32137, 37537, 42924,  5431, 30543, 26738, 39869,  5372,
       37740, 36587,  4319, 15849, 12567, 11953,  2532, 30356, 11495,
       37046, 15686, 12205, 11153, 24440, 19965, 10484,  9295, 23162,
        9729, 20477, 39539,  1691, 22478, 35640, 12480, 48566,  9450,
       30567, 14221, 48139, 41210, 17811, 22742, 49826, 48754, 49288,
        7518, 12716,  7873, 49859,  7699, 14477, 20922, 23233, 19786,
       34252,  2305, 48991, 49431, 23207, 26313, 30723, 19900,  9987,
        4572, 22830, 38997, 49579, 27701, 19437, 24287, 27844, 11314,
       27671,  8223, 24532,  4362,  5208, 39920, 27225,  9815, 38975,
        4153, 28090,  2242,  5780, 34634, 19735, 42686,  5905,  6982,
       13551, 46621, 21465, 15591, 20394,  7545, 31949, 36629, 31155,
       37151,  8730, 34941, 48181,  5962, 30028, 21352,  9337,   624,
       18038, 40014, 40721, 34303, 26028,  1134, 13930, 36246, 13345,
       38437, 19319, 12532, 16610, 39647,  3660,  7773, 19515, 11396,
       31190, 15861, 44740, 43796, 33707, 26728, 46720,  3616, 12135,
        5960,  3966, 35879, 29321, 25658, 18856, 25127, 46246, 21286,
        3249, 17293,  6778, 35410,   534, 39973, 48645, 21598, 21726,
       37486, 24714, 34074, 33313, 27462, 30105, 31781, 39446, 24952,
       44899, 12728, 11347, 15985,  8851, 16934, 47621, 42648, 22966,
       20556, 40288,  8186, 42986, 38741, 38287, 33771,  1812, 46642,
       49956, 21711, 29637, 42345, 36510, 13433, 35126, 17826, 12536,
       13777, 27546, 26630,  7807, 36293, 33829, 37305, 28343, 19337,
       24739, 46516, 25587, 10433, 20695,  1129, 37353, 47133, 31026,
        6976, 27430, 41208, 31999, 18485, 17239, 43122, 22726, 41856,
       33030, 46895, 13655,  8333, 12589, 29243,  4431, 13415,  7878,
       39887, 32850, 19163, 35157, 46214,  4471, 33580,  9446, 31721,
       38007, 41970, 26279, 15326, 44736,  6542, 49549,  7853,  2550,
       45692,  2983, 42602, 48014, 14017,  5707, 16232,  9144, 46527,
       44344,  3569, 35880, 24789,  3118,  7194, 33621,  6744, 17682,
       33932, 22797, 11766, 41890, 24313, 13757,  6488, 14768,   538,
        8264, 18204,  8534,  2377, 43118, 49099,  1500, 38451, 38806,
       26572, 30524, 22734, 26837, 18756, 20880, 19110, 24097, 23347,
        9989, 21316, 12978, 14915, 12669, 35320, 26234,  2291, 47508,
       15940,  7480, 33513,  6477, 31813, 12703, 25939, 40917, 47255,
       14655, 25830, 37578, 39935, 16352,  4375,  1067, 43450,  2091,
       16202, 22667, 35022, 49536, 34149, 31307, 32711, 44186, 43127,
       24643, 36095, 49020,  9350, 37821, 16327,   963, 36184,  7397,
       25650, 41273, 25149,  3423, 24272, 28708, 47903,  6317, 18564,
       21399, 42094, 10440, 36496, 21650, 25195, 36137, 17363, 36355,
       35477,  5781, 37840, 26956, 49274,  7477, 40716, 30493, 21145,
       32693, 37754, 28095, 23209, 24919, 15584, 23181, 17267, 12533,
       15977, 41498,  2033, 42891,  9933, 30791,  2626, 26629, 27764,
       22045,  2467,  8065, 39928, 37361, 26265,  1800,    54, 41599,
       22275, 16102, 39976, 16176, 29942, 42993, 35360, 24399, 44622,
       42532, 33929, 16165, 36242,  6106, 12161, 44652, 39983, 35255,
        1288,  1139, 12607, 31275, 41538,  5012, 31035, 42746, 31020,
       19153,  7929, 43421, 48885,  3476, 24717,  4824, 29253, 37284,
        1402,  4678, 13410, 44405, 11412, 15343, 23517, 45149, 28692,
       49364,  8431, 31342, 23230, 41320,  7137,  1194, 23764, 24176,
       25829, 13389,   347, 19000, 42509, 16008, 29802, 10203, 45453,
       21311, 33704,  3098, 37574,  9360, 48167, 22986, 25638, 33848,
       33536, 32606, 43258, 30526, 24864, 36188, 24743, 21264, 25519,
       11733, 43353, 49061, 27332, 36561, 20930, 21763, 34861, 14830,
       49200, 39200,  3976, 37077, 49747, 18653, 40519, 42291, 12014,
        4875, 40640, 23413, 24282, 27385, 30976, 19456, 18826, 48542,
       31685, 11303, 11429, 16097, 31598, 20830, 27464, 38738, 38747,
       30001,  8009,  4126, 40544, 15211, 31639, 31413, 37970, 49834,
       18571,  7795, 17697, 26139,     0, 20921, 15812, 21747, 12906,
       44163, 15038, 14345,  7073, 39630, 26387, 31808, 41476, 43459,
       26841, 46921, 23762, 46317, 36023,  2915, 15833, 42436, 34242,
       48203, 41064, 34045,  7226, 41621, 25681, 19184,  3290, 17555,
       21200, 19608,  6582,  1085,   755, 33466, 25526, 40437, 44999,
       28088, 46404, 33785, 33402, 13806,   361,  3986, 14027, 18965,
       28588, 33060, 10053,  1131, 28956, 48658, 37759,  1447, 43958,
       12034, 12218, 32949, 48072, 41326, 48047, 12031, 27229,  8589,
       25139, 44772, 12374, 30212, 42484, 36638, 24423, 19420, 34139,
       43986, 23734, 44014, 48022, 45958, 29327, 25815, 18542,  1248,
       41140, 34539,  6252, 20641, 15107, 30666, 27928, 25985, 11358,
       23375, 44128, 47653, 25795, 46066, 17075,  8108, 33924, 14133,
       35506,  5508, 23323, 18794, 33436, 31577, 23927,  7422,  1917,
       49928, 25493,  5122, 16489,  1637,  7393,  7321, 13184, 34716,
        8699, 42562, 44274,  3804, 31798, 32035, 49698, 40850,  7070,
       28490, 33541, 23876, 15489,    22,  2810, 48373, 35600, 37992,
       37933,  7967, 13916, 32628, 24397, 33381, 23563,  6246, 45759,
       33759, 22282, 49361, 16108, 17962,  7281,  1763, 46540, 21699,
       24085, 49090, 15619, 35150, 22010,  5255, 14208, 13350,  8270,
       33398, 12189, 13967, 41647, 25721, 13810, 11866, 20228,  7112,
       17854, 47999, 36636, 24608, 37681, 20467, 46048, 18295, 48125,
       12125, 44807, 40415, 18274, 31565, 18045,  5916, 32038,  3121,
       20395, 26214, 16558, 10572,  9722, 49625, 38239, 11117, 23858,
       41251, 29537, 20749, 35678,  5452, 48920, 16342, 47112, 15617,
       48872, 13738,  2658, 18367, 31187,   793, 38458, 10068, 34404,
       49108, 34834, 26563, 36555, 25780, 36024, 32092, 32356, 21953,
       10354, 17824, 49216, 10228,  9545, 36708, 12258, 10708, 12866,
        5222, 42822, 10395,  4514, 32370, 31553,  3333,  7646, 43913,
       16676,  5667,  4380,  3324, 43191,  2974, 34419,  3269, 11042,
       24631,  5280, 15835, 29762, 35035,  9154, 43035, 33158, 20082,
       21434,  6788,  6631, 27669, 38524,  9913,  1853, 29371, 10688,
       46668, 27640,  1404, 23509,  8401, 31489, 18327, 31066, 13545,
       27747,  1014, 34310, 27902,  8717, 43175, 32234, 16042, 44438,
        9306, 46707, 16830, 19618, 47842, 48124, 18675, 22589, 46292,
       47236, 36744,  5977,  1325, 26927,  1775, 20118, 37808, 35489,
       32275, 25072, 43985, 14281, 34295, 21140, 11200, 14211, 11674,
        2344, 36804, 49180,  4807, 35621, 43101,  6667, 34021,  4863,
        2471, 37060, 18530,   736, 34527, 21884,  5687, 24688, 48794,
       44362,  7046,  4974, 40547, 31052,  6977, 42449, 11798, 30770,
       32501,  1673, 42842, 26239, 14804, 42847, 48374, 10817, 23707,
       13388, 26306, 35808, 20500, 45461,  1468, 18853,  2847, 15688,
       42763, 11141, 43807, 47992,  6632, 18728, 27864, 22000, 49746,
       38374, 41447,  5777, 39015, 27490, 33815, 39098,  3353, 12426,
       47814, 15334, 23838, 19131, 29530, 22471, 40139, 20052, 27386,
       47506,  3950, 46764, 14260, 46854, 18104, 17246, 18501, 31196,
       27988]), [5, 2, 6, 8])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 3, 6, 8], M=tensor([0, 3, 6, 8], device='cuda:0'), Initial Performance: (0.259, 0.04455009317398071)
DC 1, val_set_size=1000, COIs=[1, 4, 6, 8], M=tensor([1, 4, 6, 8], device='cuda:0'), Initial Performance: (0.222, 0.04445403122901916)
DC 2, val_set_size=1000, COIs=[9, 7, 6, 8], M=tensor([9, 7, 6, 8], device='cuda:0'), Initial Performance: (0.222, 0.04478240823745728)
DC 3, val_set_size=1000, COIs=[5, 2, 6, 8], M=tensor([5, 2, 6, 8], device='cuda:0'), Initial Performance: (0.331, 0.04419129967689514)
D00: 1000 samples from classes {8, 6}
D01: 1000 samples from classes {8, 6}
D02: 1000 samples from classes {8, 6}
D03: 1000 samples from classes {8, 6}
D04: 1000 samples from classes {8, 6}
D05: 1000 samples from classes {8, 6}
D06: 1000 samples from classes {0, 3}
D07: 1000 samples from classes {0, 3}
D08: 1000 samples from classes {0, 3}
D09: 1000 samples from classes {0, 3}
D010: 1000 samples from classes {0, 3}
D011: 1000 samples from classes {0, 3}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {9, 7}
D019: 1000 samples from classes {9, 7}
D020: 1000 samples from classes {9, 7}
D021: 1000 samples from classes {9, 7}
D022: 1000 samples from classes {9, 7}
D023: 1000 samples from classes {9, 7}
D024: 1000 samples from classes {2, 5}
D025: 1000 samples from classes {2, 5}
D026: 1000 samples from classes {2, 5}
D027: 1000 samples from classes {2, 5}
D028: 1000 samples from classes {2, 5}
D029: 1000 samples from classes {2, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO1']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.06674954384565353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06847400909662246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.08236931937932968) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09157281869649887) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.445, 0.06579716289043426) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.08552074305713177) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.08714920091629029) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.307, 0.12057992538809777) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.08181295055150986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.10531247013807296) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.10465574797987938) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.16387051099538802) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.45, 0.11633282428979874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.385, 0.123072186216712) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.1520442082285881) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.16512881445884706) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.14339558928459883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.415, 0.1408000374212861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.1840640387907624) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.20105758833885193) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO3']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.441, 0.16513536177948118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.414, 0.16343627533689142) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.2134882289879024) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2007472256422043) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.16899262822791933) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.1821278923470527) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.2760615418329835) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.20005470670759679) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.19086242514848709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.17739180225878953) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.30164285308122635) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.2237542724609375) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.2003168370425701) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.19552786941453815) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.34175975008215753) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.2044250428378582) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.454, 0.20181888235360385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.18588235468231143) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.350616532756947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.258475079447031) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[8, 6], M=tensor([0, 1, 3, 4, 6, 8], device='cuda:0'), Initial Performance: (0.0, 0.3465498647689819)
DC Expert-0, val_set_size=500, COIs=[0, 3], M=tensor([0, 3, 6, 8], device='cuda:0'), Initial Performance: (0.908, 0.0079437797665596)
DC Expert-1, val_set_size=500, COIs=[1, 4], M=tensor([1, 4, 6, 8], device='cuda:0'), Initial Performance: (0.926, 0.006831320371478796)
SUPER-DC 0, val_set_size=1000, COIs=[0, 3, 6, 8], M=tensor([0, 3, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[1, 4, 6, 8], M=tensor([1, 4, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x719d404ae250>, <fl_market.actors.data_consumer.DataConsumer object at 0x719d401d4d30>, <fl_market.actors.data_consumer.DataConsumer object at 0x719d4016f3a0>, <fl_market.actors.data_consumer.DataConsumer object at 0x719d242805e0>, <fl_market.actors.data_consumer.DataConsumer object at 0x719d4057daf0>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO4', '(DO1']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.006270945239812135) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.003230337374843657) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.28020865077711643) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.24654460524022578) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.005478003907948732) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.06741818032413721) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.57, 0.05895584548264742) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.006033585112541914) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.003591684078797698) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.3048307999256067) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.2715361794233322) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.006250564580783248) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.589, 0.03706048777699471) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.689, 0.0290220784842968) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.006434533841907978) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0032966788187623022) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.2768591152674053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.418, 0.2747449993789196) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.004975917101139203) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.546, 0.053957928292453286) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.702, 0.032036394506692885) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.006910632779821754) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.0038925338564440607) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.25791762896068393) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.412, 0.27797043427824975) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.00974100866727531) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.06866519701853395) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.674, 0.038580646380782124) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.007023983523249626) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.004028671892359853) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.25331740658450874) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.418, 0.2715687876343727) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.006701513921259903) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.06764975260011852) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.671, 0.04726056236028671) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO1', '(DO2']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.006802521551027894) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002479879975318909) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.26225797361321745) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.28152289140224457) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.962, 0.00503164699440822) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.544, 0.0800246926676482) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.65, 0.057637104127556085) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.008121587350033224) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.004093506894540042) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.2465230761137791) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.29919145756959914) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.006606829602213111) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.0690899160169065) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.657, 0.044634803503751756) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.006342438391409814) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.003871191210579127) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.25771293781138954) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.30247270204126836) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.006404130211099982) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.526, 0.07129316823929548) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.635, 0.053522639356553554) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.009299655556678771) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.0052568085275124755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2886114768590778) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.30603532058000565) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.012311690022324911) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.07324527788721025) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.634, 0.0508977230861783) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.916, 0.009800588107667863) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.00356528477743268) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.27194322925410236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.30194901512563227) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.0075451071462593975) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.07401385981962084) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.666, 0.05339674556255341) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO2', '(DO3']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.007724599746987223) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.004489825875149109) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.282459774736315) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.31088745161890985) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.0076849752888083454) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.06830756122432649) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.688, 0.038936904773116114) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.00878340083360672) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.0064358040797524154) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.25747348501579836) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.31922538807988166) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.943, 0.008036756713408977) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.05678805434703827) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.629, 0.052007714450359344) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.008137361685745418) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.004834280246053822) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.2638437492516823) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.3150491363704205) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.011103339760098606) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.07128452064469457) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.67, 0.04482427218556404) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.007473599125631154) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0028479494424536824) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.2788447575182654) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.3057040803581476) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.011003306635306217) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.06128343039751053) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.691, 0.045059922456741335) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.0075152547750622035) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.0032893175173085184) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.2531890823082067) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.3143017274439335) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.935, 0.01200125696315081) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.06859206781350076) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.679, 0.04236184386909008) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO0', '(DO2']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.007941946683451533) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.00399834388284944) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2771493708671769) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.3308884386718273) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.00555651500276872) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.56, 0.07221573635935784) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.643, 0.06403424339927732) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.009235610520234331) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0023156378543935717) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.26852880955906583) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.30044407910108567) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.010165548556386057) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.06832895345240832) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.676, 0.04510835607349872) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.008303606245666742) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.0044272163610439745) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.26997780830063856) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.412, 0.30461472845077514) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.006797047547996044) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.0634929006099701) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.63, 0.05547994533181191) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.007877347100526094) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003116193080088124) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2955666782634798) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.3066281044781208) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.939, 0.009665641607483849) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.57, 0.07937047130707651) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.615, 0.08574337002635002) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.008764628535136581) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003382272113347426) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.263844259865873) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.29439707365632056) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.012519920475487993) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.09583156207343563) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.624, 0.09747507287329063) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO4', '(DO0']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.008501678330823778) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.0037233227270189674) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.242269280025037) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.27195010355114935) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.005828590048709884) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.589, 0.06637251377105713) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.659, 0.056459280744194984) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.00863369015941862) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.003877204319811426) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.2244085344960913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.3151666951328516) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.955, 0.005479959454620257) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.0813212116882205) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.66, 0.06583760844916105) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.008175453907810151) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003406265830155462) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.24325731633650138) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.42, 0.2878619722127914) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.959, 0.009218188873172039) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.545, 0.07846467215940356) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.615, 0.08068177054822445) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.008180632616160437) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.003765155316912569) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.2352374476226978) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.412, 0.29509967014193533) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.01158980546222665) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.09678126954403705) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.09243772572278977) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.00809845457645133) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002792167745763436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.216470230627805) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2891772848665714) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.007815220097312704) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.564, 0.07602286547422409) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.599, 0.0707937866449356) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO4', '(DO0']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.008131996497511864) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003609248654101975) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.23372176728677005) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.27942187175154687) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.005569529457192402) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.55, 0.08792381220310927) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.628, 0.06541822531819344) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.007432435771916062) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003328215674468083) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2226940331691876) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.29701414141058924) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.005977522562199738) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.09796800614334643) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.641, 0.05771708972938359) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.00921662615207606) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.00444795401423471) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.249840152235236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.2860190576314926) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.008688287659533672) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.10051617136970162) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.08313806597422808) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.008904132054449293) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.004266041348979343) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2355048130750656) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.2550739328861237) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.012612513467043755) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.08710712559893727) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.08334707205928862) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.007897867453168146) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003947858173516579) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.22431820527277888) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.27815730091929436) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.009594041712447507) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.0940469550974667) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.552, 0.08463358693942427) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO4', '(DO0']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.007887771920650266) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002673548872815445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.25532234156876804) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.2603359959721565) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.962, 0.0040143723349901845) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.0860591539144516) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.639, 0.08202110432833433) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.009383616990875452) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.986, 0.0019482257636263968) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.24793194771464913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.25818903183937075) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.0054283531516739454) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.519, 0.11658773569809273) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.59, 0.0961134145539254) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.006995193576440215) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.002992695170920342) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.23142958235321567) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.3100135363340378) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.961, 0.005910610600905784) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.11576500287931413) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.551, 0.09349785692989826) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.008397809887770563) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0030090688400086947) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.268267447674647) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.28400745104253294) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.005777800635725725) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.10073621253669261) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.08268897621333599) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.916, 0.010177599318209104) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002784507642034441) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.22636541045771447) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.2722024999558926) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.007111114713653904) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.10318958892766386) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.553, 0.12097778828442096) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO4', '(DO2']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.009416839199489914) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002502480634488165) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.24269453431107105) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.2733414165973663) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.004808110280893743) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.525, 0.10957088106125593) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.09101207292638719) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.912, 0.009024247691733763) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002859181016450748) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2330112534853397) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.32599559499323366) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.008147476056330106) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.11025004424829968) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.551, 0.08961581681575626) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.008582265319884754) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0021687673102132976) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.20873825071682223) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.29058274388313293) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.961, 0.005342278546115267) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.11628954168129713) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.593, 0.08319953763484955) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.010141265398531687) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0027413815369363875) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.23331507899658754) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.28339945524930954) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.006482244938262738) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.11412898198375479) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.569, 0.10321408914588391) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.007224570753984153) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002339308365015313) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.26314924177271315) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.2616522587835789) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.005956597207987215) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.10927909166924656) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.588, 0.08472752713039518) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.259, 0.04455009317398071), (0.269, 0.06674954384565353), (0.445, 0.06579716289043426), (0.451, 0.08181295055150986), (0.45, 0.11633282428979874), (0.447, 0.14339558928459883), (0.441, 0.16513536177948118), (0.452, 0.16899262822791933), (0.453, 0.19086242514848709), (0.448, 0.2003168370425701), (0.454, 0.20181888235360385), (0.488, 0.06741818032413721), (0.589, 0.03706048777699471), (0.546, 0.053957928292453286), (0.558, 0.06866519701853395), (0.539, 0.06764975260011852), (0.544, 0.0800246926676482), (0.547, 0.0690899160169065), (0.526, 0.07129316823929548), (0.536, 0.07324527788721025), (0.539, 0.07401385981962084), (0.534, 0.06830756122432649), (0.554, 0.05678805434703827), (0.539, 0.07128452064469457), (0.535, 0.06128343039751053), (0.536, 0.06859206781350076), (0.56, 0.07221573635935784), (0.555, 0.06832895345240832), (0.542, 0.0634929006099701), (0.57, 0.07937047130707651), (0.54, 0.09583156207343563), (0.589, 0.06637251377105713), (0.549, 0.0813212116882205), (0.545, 0.07846467215940356), (0.528, 0.09678126954403705), (0.564, 0.07602286547422409), (0.55, 0.08792381220310927), (0.536, 0.09796800614334643), (0.547, 0.10051617136970162), (0.531, 0.08710712559893727), (0.542, 0.0940469550974667), (0.558, 0.0860591539144516), (0.519, 0.11658773569809273), (0.533, 0.11576500287931413), (0.533, 0.10073621253669261), (0.542, 0.10318958892766386), (0.525, 0.10957088106125593), (0.517, 0.11025004424829968), (0.54, 0.11628954168129713), (0.531, 0.11412898198375479), (0.547, 0.10927909166924656)]
TEST: 
[(0.26825, 0.04347268170118332), (0.26725, 0.06421797105669975), (0.4395, 0.06346339270472527), (0.4565, 0.07869957354664803), (0.4475, 0.11173120200634003), (0.457, 0.1374445474743843), (0.4525, 0.15964710181951522), (0.458, 0.1622095382809639), (0.46325, 0.18300087195634843), (0.46175, 0.19399894100427628), (0.45875, 0.1961120474934578), (0.494, 0.07006506815552711), (0.58825, 0.03724230942130089), (0.55625, 0.051405404910445214), (0.5465, 0.06834665352106094), (0.5335, 0.06664466696977615), (0.531, 0.07916052454710007), (0.5595, 0.06741604027152061), (0.532, 0.06859034579992294), (0.525, 0.07266055038571358), (0.53475, 0.07190771707892418), (0.5445, 0.06434284004569053), (0.579, 0.05441133907437325), (0.54625, 0.06912917491793633), (0.55125, 0.05922710785269737), (0.5365, 0.06832760362327099), (0.5655, 0.06988290128111839), (0.553, 0.06892521232366562), (0.547, 0.062451994091272355), (0.5705, 0.07804589235782623), (0.534, 0.09738729816675186), (0.595, 0.06302823662757874), (0.54775, 0.08234091833233834), (0.54325, 0.07863081485033036), (0.527, 0.09635849142074585), (0.56375, 0.0764755799472332), (0.5345, 0.0884619303047657), (0.52775, 0.09870040026307106), (0.5395, 0.09639159989356995), (0.543, 0.0856336295902729), (0.52875, 0.09251982173323631), (0.569, 0.08115402689576148), (0.51125, 0.11102214750647545), (0.53375, 0.10948595988750458), (0.5445, 0.0916514012813568), (0.54425, 0.09991246607899666), (0.51225, 0.11033127367496491), (0.508, 0.11043367213010788), (0.53075, 0.11702274584770203), (0.52925, 0.1096280300617218), (0.527, 0.11084877708554268)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.88      0.09      0.15      1000
           3       0.75      0.15      0.25      1000
           6       0.52      0.93      0.67      1000
           8       0.49      0.95      0.65      1000

    accuracy                           0.53      4000
   macro avg       0.66      0.53      0.43      4000
weighted avg       0.66      0.53      0.43      4000

Collaboration_DC_1
VAL: 
[(0.222, 0.04445403122901916), (0.25, 0.06847400909662246), (0.256, 0.08552074305713177), (0.33, 0.10531247013807296), (0.385, 0.123072186216712), (0.415, 0.1408000374212861), (0.414, 0.16343627533689142), (0.442, 0.1821278923470527), (0.462, 0.17739180225878953), (0.471, 0.19552786941453815), (0.463, 0.18588235468231143), (0.57, 0.05895584548264742), (0.689, 0.0290220784842968), (0.702, 0.032036394506692885), (0.674, 0.038580646380782124), (0.671, 0.04726056236028671), (0.65, 0.057637104127556085), (0.657, 0.044634803503751756), (0.635, 0.053522639356553554), (0.634, 0.0508977230861783), (0.666, 0.05339674556255341), (0.688, 0.038936904773116114), (0.629, 0.052007714450359344), (0.67, 0.04482427218556404), (0.691, 0.045059922456741335), (0.679, 0.04236184386909008), (0.643, 0.06403424339927732), (0.676, 0.04510835607349872), (0.63, 0.05547994533181191), (0.615, 0.08574337002635002), (0.624, 0.09747507287329063), (0.659, 0.056459280744194984), (0.66, 0.06583760844916105), (0.615, 0.08068177054822445), (0.565, 0.09243772572278977), (0.599, 0.0707937866449356), (0.628, 0.06541822531819344), (0.641, 0.05771708972938359), (0.584, 0.08313806597422808), (0.557, 0.08334707205928862), (0.552, 0.08463358693942427), (0.639, 0.08202110432833433), (0.59, 0.0961134145539254), (0.551, 0.09349785692989826), (0.6, 0.08268897621333599), (0.553, 0.12097778828442096), (0.558, 0.09101207292638719), (0.551, 0.08961581681575626), (0.593, 0.08319953763484955), (0.569, 0.10321408914588391), (0.588, 0.08472752713039518)]
TEST: 
[(0.218, 0.04341068071126938), (0.25, 0.06590802815556526), (0.252, 0.08200803625583648), (0.3275, 0.10096185243129731), (0.392, 0.1177846822142601), (0.42225, 0.1349279899597168), (0.42825, 0.15713550812005997), (0.44525, 0.175697412610054), (0.46675, 0.1712297403216362), (0.47075, 0.19024631458520888), (0.4625, 0.18031305837631226), (0.56525, 0.057673829540610316), (0.67375, 0.028440850779414176), (0.68875, 0.03293888088315725), (0.659, 0.040226990059018135), (0.65025, 0.05178048418462276), (0.644, 0.062088836789131165), (0.6465, 0.04791041113436222), (0.62525, 0.058596130043268205), (0.61675, 0.05348241649568081), (0.65, 0.057894969537854196), (0.67975, 0.04153894194960594), (0.6245, 0.05369040276110172), (0.655, 0.044450898855924605), (0.667, 0.05045933753252029), (0.66525, 0.0458656007796526), (0.63475, 0.06643975023925304), (0.67075, 0.0477916819602251), (0.62425, 0.05797768498957157), (0.631, 0.0889120838046074), (0.621, 0.10058917072415352), (0.6635, 0.05864910934865475), (0.65575, 0.06809743525087833), (0.61075, 0.08759916907548905), (0.56, 0.09830386278033257), (0.5885, 0.075536142796278), (0.61075, 0.07176829645037651), (0.6185, 0.06380870823562146), (0.5655, 0.08927133083343505), (0.55625, 0.09559910428524017), (0.54875, 0.08847390893101692), (0.65225, 0.0873329484462738), (0.603, 0.0983945247232914), (0.5785, 0.09541650685667992), (0.61875, 0.08457970634102821), (0.5505, 0.12328320848941803), (0.55525, 0.09402048620581627), (0.54525, 0.09695957323908806), (0.57925, 0.08965279293060303), (0.57375, 0.10785959723591805), (0.588, 0.0870328942835331)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.91      0.32      0.47      1000
           4       0.66      0.23      0.34      1000
           6       0.52      0.89      0.65      1000
           8       0.58      0.92      0.71      1000

    accuracy                           0.59      4000
   macro avg       0.67      0.59      0.54      4000
weighted avg       0.67      0.59      0.54      4000

Collaboration_DC_2
VAL: 
[(0.222, 0.04478240823745728), (0.442, 0.08236931937932968), (0.397, 0.08714920091629029), (0.462, 0.10465574797987938), (0.472, 0.1520442082285881), (0.475, 0.1840640387907624), (0.472, 0.2134882289879024), (0.468, 0.2760615418329835), (0.469, 0.30164285308122635), (0.479, 0.34175975008215753), (0.474, 0.350616532756947), (0.475, 0.28020865077711643), (0.467, 0.3048307999256067), (0.476, 0.2768591152674053), (0.473, 0.25791762896068393), (0.475, 0.25331740658450874), (0.478, 0.26225797361321745), (0.472, 0.2465230761137791), (0.476, 0.25771293781138954), (0.478, 0.2886114768590778), (0.469, 0.27194322925410236), (0.475, 0.282459774736315), (0.474, 0.25747348501579836), (0.473, 0.2638437492516823), (0.474, 0.2788447575182654), (0.48, 0.2531890823082067), (0.477, 0.2771493708671769), (0.477, 0.26852880955906583), (0.471, 0.26997780830063856), (0.477, 0.2955666782634798), (0.475, 0.263844259865873), (0.476, 0.242269280025037), (0.48, 0.2244085344960913), (0.478, 0.24325731633650138), (0.476, 0.2352374476226978), (0.478, 0.216470230627805), (0.474, 0.23372176728677005), (0.478, 0.2226940331691876), (0.475, 0.249840152235236), (0.477, 0.2355048130750656), (0.468, 0.22431820527277888), (0.476, 0.25532234156876804), (0.476, 0.24793194771464913), (0.479, 0.23142958235321567), (0.479, 0.268267447674647), (0.478, 0.22636541045771447), (0.481, 0.24269453431107105), (0.477, 0.2330112534853397), (0.476, 0.20873825071682223), (0.478, 0.23331507899658754), (0.477, 0.26314924177271315)]
TEST: 
[(0.23925, 0.04380939894914627), (0.4305, 0.07938512188196183), (0.4055, 0.0837293835580349), (0.463, 0.1001882000863552), (0.472, 0.1443197346329689), (0.477, 0.17273092937469484), (0.4735, 0.20166757559776305), (0.474, 0.25924735152721406), (0.47175, 0.28262863409519196), (0.47725, 0.32329213321208955), (0.47725, 0.3337722965478897), (0.478, 0.26433087611198425), (0.46975, 0.28752060782909394), (0.47625, 0.2630842341184616), (0.4735, 0.248845254778862), (0.47675, 0.24072655749320984), (0.47575, 0.24593956899642944), (0.47525, 0.23290653985738755), (0.476, 0.2425903096795082), (0.47775, 0.2735694625377655), (0.4745, 0.2569385726451874), (0.4785, 0.268111855506897), (0.4785, 0.2422487506866455), (0.47625, 0.24868472635746003), (0.475, 0.2646848523616791), (0.479, 0.23943641924858095), (0.47775, 0.26177433037757875), (0.4775, 0.2552733508348465), (0.47775, 0.25231998300552366), (0.482, 0.282255234003067), (0.478, 0.24778404915332794), (0.477, 0.23021366769075394), (0.4785, 0.2118906522989273), (0.481, 0.23031844186782838), (0.47925, 0.222595172226429), (0.47925, 0.20362289083003998), (0.4775, 0.2201413064599037), (0.4805, 0.20975354886054992), (0.478, 0.23356379956007003), (0.4815, 0.22032689565420152), (0.47575, 0.21154328674077988), (0.481, 0.23813832092285156), (0.47925, 0.2304442983865738), (0.48075, 0.21908437913656234), (0.47725, 0.2527128982543945), (0.47925, 0.21562527477741242), (0.4805, 0.22760259646177292), (0.47925, 0.22062476485967636), (0.47775, 0.1965591031908989), (0.4775, 0.22327313143014907), (0.47875, 0.24649513506889342)]
DETAILED: 
              precision    recall  f1-score   support

           6       0.00      0.00      0.00      1000
           7       0.62      0.95      0.75      1000
           8       0.00      0.00      0.00      1000
           9       0.39      0.97      0.56      1000

    accuracy                           0.48      4000
   macro avg       0.25      0.48      0.33      4000
weighted avg       0.25      0.48      0.33      4000

Collaboration_DC_3
VAL: 
[(0.331, 0.04419129967689514), (0.257, 0.09157281869649887), (0.307, 0.12057992538809777), (0.398, 0.16387051099538802), (0.394, 0.16512881445884706), (0.405, 0.20105758833885193), (0.408, 0.2007472256422043), (0.413, 0.20005470670759679), (0.419, 0.2237542724609375), (0.413, 0.2044250428378582), (0.404, 0.258475079447031), (0.411, 0.24654460524022578), (0.414, 0.2715361794233322), (0.418, 0.2747449993789196), (0.412, 0.27797043427824975), (0.418, 0.2715687876343727), (0.411, 0.28152289140224457), (0.413, 0.29919145756959914), (0.404, 0.30247270204126836), (0.406, 0.30603532058000565), (0.415, 0.30194901512563227), (0.407, 0.31088745161890985), (0.416, 0.31922538807988166), (0.407, 0.3150491363704205), (0.413, 0.3057040803581476), (0.406, 0.3143017274439335), (0.413, 0.3308884386718273), (0.41, 0.30044407910108567), (0.412, 0.30461472845077514), (0.419, 0.3066281044781208), (0.408, 0.29439707365632056), (0.404, 0.27195010355114935), (0.413, 0.3151666951328516), (0.42, 0.2878619722127914), (0.412, 0.29509967014193533), (0.408, 0.2891772848665714), (0.406, 0.27942187175154687), (0.413, 0.29701414141058924), (0.413, 0.2860190576314926), (0.416, 0.2550739328861237), (0.401, 0.27815730091929436), (0.405, 0.2603359959721565), (0.408, 0.25818903183937075), (0.408, 0.3100135363340378), (0.411, 0.28400745104253294), (0.419, 0.2722024999558926), (0.411, 0.2733414165973663), (0.419, 0.32599559499323366), (0.415, 0.29058274388313293), (0.414, 0.28339945524930954), (0.416, 0.2616522587835789)]
TEST: 
[(0.3205, 0.04325020852684975), (0.2555, 0.08785385882854461), (0.29475, 0.11601749670505523), (0.39875, 0.15733955764770508), (0.3965, 0.1590762510895729), (0.403, 0.1939288192987442), (0.40575, 0.19325960141420365), (0.40525, 0.19291542088985444), (0.412, 0.21559283995628356), (0.412, 0.19505343741178513), (0.411, 0.25018797516822816), (0.4105, 0.23389805912971495), (0.4165, 0.26093123602867124), (0.41475, 0.26470288932323455), (0.411, 0.269813453912735), (0.413, 0.26347255825996396), (0.416, 0.27032153224945066), (0.418, 0.2889888037443161), (0.4135, 0.29163798928260803), (0.418, 0.2918181782960892), (0.414, 0.28980129420757295), (0.41075, 0.3012294281721115), (0.41725, 0.3120628606081009), (0.42025, 0.3033766635656357), (0.41325, 0.29764521980285646), (0.4155, 0.3020893695354462), (0.4165, 0.31996466720104216), (0.41375, 0.2943339775800705), (0.42, 0.29422170579433443), (0.416, 0.29530579805374146), (0.414, 0.28736821365356446), (0.417, 0.26782876932621), (0.418, 0.30493638503551485), (0.419, 0.28067510747909546), (0.419, 0.29027269542217254), (0.4135, 0.2791092346906662), (0.41625, 0.27052845478057863), (0.41575, 0.2868787190914154), (0.41775, 0.276146867275238), (0.4145, 0.24751640820503235), (0.41225, 0.2670456211566925), (0.42075, 0.2527993482351303), (0.416, 0.24909430134296418), (0.417, 0.29727146196365356), (0.4155, 0.2746617238521576), (0.42075, 0.26743385434150696), (0.4215, 0.2654264853000641), (0.42, 0.31535434103012083), (0.4195, 0.2841790118217468), (0.41675, 0.2762845678329468), (0.41925, 0.25550774550437927)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.34      0.87      0.49      1000
           5       0.55      0.81      0.65      1000
           6       0.00      0.00      0.00      1000
           8       0.00      0.00      0.00      1000

    accuracy                           0.42      4000
   macro avg       0.22      0.42      0.29      4000
weighted avg       0.22      0.42      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [104]
name: alliance-2-dcs-104
score_metric: contrloss
aggregation: <function fed_avg at 0x7e8055a58c10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=104
Partitioning data
[[5, 9, 0, 3], [4, 1, 0, 3], [6, 2, 0, 3], [7, 8, 0, 3]]
[(array([29961, 11298,  5755, 34458, 42818,  8742, 20483, 38411, 20171,
         767, 11024, 14802, 11949, 32795, 30095, 15575, 40417, 30072,
       43979, 47688, 40721, 25662, 30618, 14004,  7435, 10429, 35189,
       38600,  2071, 35606,  6017, 37958, 20615, 33853, 13316, 35905,
       16623, 30098, 27844, 37451,  3495, 35602, 35712, 15606, 41318,
       16358, 43338, 22445, 24648, 18626, 23392, 11269, 35053, 38437,
        5853, 35652, 23623, 47167,  7926, 34183, 21561, 45581, 48303,
        8929, 37874, 26503, 46901, 23146,  9284, 17778, 18845, 38369,
       23385, 48701, 26747, 47024, 49638,  8851, 21610, 47555, 24819,
       11740, 21586,  9383,  7601,  7417, 41942,  1717, 43495, 11928,
       16282,  9286, 35534,  9594,  2410, 32468, 24981, 46606, 12239,
       15685,  3229,  7090, 25262,  5606, 40490,  3471, 45698, 35501,
       15849, 32919, 43387, 33138, 48454, 40561, 32926, 46684, 34008,
       44371, 27679, 43759, 30260, 26095, 43878, 34048, 43852, 47810,
       49472, 28460, 45728, 29896,  4002, 48730, 48139, 15130, 14677,
       45184,  1911, 25981, 33818, 39002, 30063, 28845,  3794, 37680,
       13624, 18913, 35510, 44435, 46074, 11405, 48750, 24196, 31145,
       19150, 28652, 30757, 21543, 27345, 16626, 41376, 40044, 12177,
       20122, 22679,  2877, 21910, 23234, 47988, 18167, 23724, 44904,
       36035, 46925,  3653, 35993, 46267, 22999, 37471, 47920, 44094,
       48537, 49927, 41082,  5736, 31791, 45206, 44106, 30085, 45503,
       13960,  8243,   607, 47054, 37366, 18764, 18319, 20207, 10703,
       29110, 23614, 17332, 42745,   988, 41062, 25248, 36132, 38202,
       49920, 42323, 47258, 31031, 19735, 49848,  2361, 48632, 11725,
       42533, 36653, 24295, 28736, 40765, 22390, 34763, 27359, 49052,
       22061, 34941, 27885, 20460, 26589,  5970, 30134, 31222, 40140,
        4494, 49428, 17773, 27956, 45583, 36952, 15775, 49431,  5288,
       19201, 46187, 44150, 47485, 36439,  1662, 18230, 33778, 15785,
       10375, 20172, 37050, 41580, 26822, 25010, 49779, 22366,  3032,
       18860,  2758, 32595, 23798,  8826, 13906, 38331,  8364, 32489,
       10622, 29458, 24537, 31605, 13078, 37547, 41169, 15142, 49369,
       22415, 42627,  6178, 18749, 23877, 29431, 21273, 40508, 13669,
       12793,  9425, 28907, 44714,  5801, 38378, 35057,  6150, 49360,
        2208, 38820, 49793, 38827, 41976, 18213,  5591, 37094, 19824,
       40040,  1893,  7441, 44009, 14008, 42591, 44088,   360,  5244,
       33432, 29115,   664, 27237, 33784, 19505, 12152, 39510, 34943,
       39025, 22959, 43997, 29551, 12680, 33224, 32266, 36031, 40931,
       24609, 42159,  3479, 19766, 28713, 24731, 40215,  6643,  5621,
        2302, 47511, 26289, 34502,  1078, 27837, 32635, 49112, 43695,
       13364,  7556, 23651,  4289, 45876, 17709, 30392, 43839, 40735,
       26393, 16533, 19934, 21537,  6357,  2477, 39041,  4749, 34049,
       48544, 13767, 28850, 39280, 39801, 14458, 24987, 13074, 26113,
       39731, 16818, 10231, 44410, 35203, 47426, 34147, 29672, 38314,
       23225,  3915, 28915,  9466, 16614, 46269, 23770, 32745, 17544,
       40722,  3061, 28899, 30178, 10751, 42704, 24683, 39557, 30175,
       29418, 47076,  7698, 15096, 21237, 15586, 29651, 24603, 36329,
       45735, 23129, 41054, 38561, 15901, 10523, 45463,  1572,  8716,
       24960, 34129, 44947,   512, 26447, 12116,   406, 39870, 32838,
       21688,  4675, 22815, 10161, 24042,   763, 18430, 46195, 47025,
       25965, 26880, 30503, 39572, 15566, 23359,  5881, 19474, 30771,
       32902, 47469,  2609, 23091, 41683, 31453, 13421, 47985,  5622,
        9947, 45791, 44996, 48751,  1115, 13491, 32701, 26852, 44332,
        7652, 27287, 40915, 34640, 26600,  1906, 47709, 40037, 36401,
       45048, 34485, 27899,  6201, 22346, 39894, 33928, 15252, 40693,
       35746,  5234,  6268, 16708, 29529, 19380, 21506, 32575, 48345,
       30758, 37242,  2880,  9990, 28931, 24581, 25624, 35750, 17196,
       47916, 19390, 33960, 24404, 45666, 31726, 11913, 44771, 43324,
       30114,  5719, 10197,  2341, 28972, 19286, 16238, 25920, 10911,
       45297, 27487,   185, 37867, 12634, 18893, 12764, 34783, 16825,
       43989, 23002, 39170, 23796, 16219, 35796, 29307,  7788,  3842,
       10847,  8741,  3292, 46576,   527, 31728, 21878, 27047, 45067,
        9819,  7731, 17082, 37962, 49359, 24356, 43289, 18763, 34568,
       29578, 38041,  6954, 11325, 32471, 24257, 24587, 35731,  3329,
       38044, 29927, 48986, 28724, 41762, 21469, 28272, 42711, 44645,
       10650, 16391, 43657, 46523, 10130,  8857, 20686, 28467, 38043,
        3789,  3689, 28570, 41855, 40703, 22557, 37358, 41146, 17352,
        5414, 13419, 30378, 22299,  1243, 12916, 41222, 48324, 31471,
       27301, 13260, 13966, 15258, 12922, 28833,  6833, 28515, 47651,
       13660, 26644, 10499, 10159, 18768,  5770, 41435, 38963, 39329,
       10275,  6242, 32905, 11372, 47525, 40802, 34259, 47906, 22167,
         557,  3277,  8079,  8018, 34722, 20454, 25212,  8759, 21439,
       41258,  1306, 13368, 41797, 42992, 26704, 13174, 35369, 34234,
         695, 44864, 44624, 27460, 15827, 41274, 24395, 48852,  2053,
       31600, 25748, 15483, 48147,  9193, 29053, 16048, 15304, 10954,
        1319, 32631,  2145, 11122, 15848, 39392,  9697, 17432, 41038,
       21092,    77, 28092, 49633, 10545, 41040,   555, 41745, 18358,
       41907, 49219, 24072, 19492, 49837, 35089, 31412, 36464, 17257,
       19294, 21288, 36955, 40432, 24365, 12944, 37068, 16225, 10782,
       31863, 30968,  4721,  2451, 35658, 21143, 31391, 38478, 49308,
       31976, 22970, 15542, 22449, 47925, 37049, 16552, 27855, 38828,
       44296, 33128, 11768, 21275, 22628, 42309, 21087, 16196, 17947,
       19664, 14867, 11975, 35039, 12430, 39574, 39763, 13401, 36606,
       40851, 23528, 34109, 20287, 30271, 48671, 35690, 19974, 37102,
       23851, 24764,   940,  7389, 12145, 38040, 41021, 42382, 15210,
       15444, 38442, 40378, 34204, 10293,  6496, 39211, 11205,  4558,
       18371, 21373, 23715, 34536, 47995, 45949, 21687, 47284, 38048,
       40651,  5493,  9904, 43551, 19922, 13329, 45552, 40587,  8380,
       36948, 36930, 25380, 33049, 21728, 23389,  9183,  1963,  4166,
       38095,  8356, 47146,  1405, 45885,  7426, 26964, 46094, 43813,
       34011, 37966, 20139, 30347, 49322, 16680,  6482, 44399, 42122,
       45631, 34967, 34517, 20630, 48200, 40249, 11628, 42800, 32172,
       15595, 28020, 27796,  7997, 31148, 40127, 24077, 43598, 46817,
       27734,   416, 46286, 15383, 41330, 25019, 21366, 37863, 33841,
       32180,  2337, 44530,  5329, 31698, 43402,  5478, 44084,   377,
       20050, 48306, 42357, 10135, 20639, 25947,  3340, 32520, 41016,
       32219, 18769, 12500, 20204, 32379, 34155, 21783, 20770, 45908,
       11067,  1895,    91, 10653, 33894,  4294, 43784, 35832, 30461,
        6061, 43336,     9, 25882, 34742, 16714, 11366, 48039, 20271,
       41372, 22611, 28373, 16817, 39151, 37217, 16405, 20529, 37408,
       17930, 18127, 26716, 16209,  4435, 27424, 10869,  5209, 14032,
       35715, 34997, 41705, 24733, 23270, 41364, 29423, 32304, 32725,
       12988, 32917, 27683, 45250, 48629, 24434, 15127, 42233,  5007,
       49190, 36951, 12971, 24522,  3190, 32621, 23590, 41159, 45508,
        3763, 21835, 44868, 24364,  5423, 15417, 13205, 24335,  1150,
       44029, 15640, 32293, 28731, 34317, 49982, 33525, 14064, 19617,
       20410, 17451, 41057, 47478, 22302, 17644, 39341,  6837, 39543,
       35931, 22759, 42752, 37910, 42482, 16872, 40690, 39384, 39158,
       48097, 21672,  2197, 37415,  8876, 38012,  9120,  4481, 29273,
       34822, 26564, 49384, 43704, 30753, 25492,  6732, 31567, 46882,
       44093,  5260,  8105, 12973, 34061, 33582, 32914,  1265, 20784,
       25235, 30817, 46013, 42134, 35935, 34055, 34649,  7493, 20631,
       41853]), [5, 9, 0, 3]), (array([39470, 41148, 10032, 35811, 15140, 44629, 29163, 13803,  6022,
       23743,    89, 12313, 43632, 11116, 43746, 13453, 41246,  7634,
       34410, 41688,  6298, 28577, 40718, 30970, 29138, 41337, 48860,
       28097, 20906, 10904, 41852, 23134, 14600, 15152, 27105, 15790,
         310, 27579, 45274,  9502, 47951, 40529, 16967, 45441,  9685,
       12591, 28414, 21676, 18300, 49154,  3587, 23252, 22491, 33099,
       14060, 11125, 24933, 12106, 36533,  4527, 13636, 10925, 36646,
        6879, 35791, 29787, 25636, 40848,  3219, 48513, 25216, 18450,
        2418,  5902, 22902,  4750, 24290,   520, 15434, 12084, 46803,
        7459,  9780,  4328, 11860, 43049, 25474,  6605,  1889, 21419,
       20450,  1904, 22246, 30153, 13303, 11626,  2691, 42418, 45100,
       45645, 40277, 28151, 41788,  3533,  1303, 42907,  6415,  4725,
       32280, 32491, 31306, 43044,  7818, 47622, 32634, 34306, 47922,
        2155, 39125, 38755, 31061, 24804, 49981,  1866, 25423,  7078,
        1851, 44095,  7295, 21946, 47312,  2999, 32446, 46528, 43386,
        7382,  3577,  7099, 10890, 44974, 31860,  7811, 28904, 36225,
       27847, 24900, 37075, 48309, 23354, 49480, 45595,  5029, 11020,
       40299,  4227, 19471,  3664, 30942, 30354, 10982, 41836, 38730,
       24967, 33827, 12590, 14105, 35429, 49690, 39803, 16672,   856,
       16961, 15909, 36755, 13145, 26943, 43627, 29770, 31256, 13646,
       34418, 26512, 44846, 18818, 16294, 33296, 43682, 22468,  9872,
       43188, 40088, 35304, 22606, 35023, 38776, 11226, 12826,  3089,
       29086,  7154, 11573, 15482, 45764, 40862, 46220, 14474, 34671,
       47796, 41648, 37639, 29830, 36466,  1315,  1333, 17038, 39209,
       47305, 27247, 20243, 18476, 23445, 33735, 33290, 22053, 14576,
       16514,   904, 13134, 49390, 22825, 11769,  3776, 39576, 40067,
        9441, 11279, 18821, 15643,  3695,  3112, 19458,  7536, 11423,
       25099,  5119,  4381, 48338,  6110, 49950, 27914, 44726, 44936,
       42318, 38841,  7607, 28045, 36844, 41445, 20162, 48845, 17244,
       37264,  2394, 19382, 28127, 31854, 10644, 34064, 15710, 15748,
       21151, 17752, 24446, 48656, 17694, 21407, 36546, 35011, 44671,
       38278, 40967, 49151, 27024, 45535, 44138, 24110, 33063, 25544,
        3452, 12373, 46866,  8189, 26491, 35453, 48584, 18438,  7884,
       34911, 43854, 42221, 11532,  8001, 34128,  4591, 31352, 38610,
       20988, 34246, 11558, 35605, 10795, 34530,  6226, 12977, 22818,
       22829, 19079, 14358,  4492, 11075,  1380, 12494, 43217, 22675,
       18654, 41627, 33951, 35357, 41217, 32988, 26008, 20307, 11571,
       36473,  4101,  2428, 12210, 11920, 34063, 22697, 37551, 26292,
       43297, 19939, 24678, 22490, 48574, 39455, 25331, 48806,  4200,
       46135,   834, 26375, 29536, 42085, 12231, 10049, 40075,  9263,
       14740, 17856, 47035, 24288, 25280,  7349, 34913, 26065, 45001,
       44526, 17861, 35051, 18890, 45569, 41742, 25975, 18927,  3414,
       10110, 25558, 41519, 18206, 12741, 39638, 23895, 38919, 24762,
       21422,  6429,  9407, 44499, 27794,  1578, 49121, 48915, 19500,
       17031, 23500, 34637, 11407, 21848, 46173, 37480,  6786, 42121,
        9717, 18702, 19031, 47568,  2849, 35644, 11971, 28532, 35086,
       26789,  2067, 44824,  8869, 22379, 43206, 10035, 40423, 30245,
       33230, 13402,  4858, 40520, 26080, 27402, 37260,  7706, 34162,
       33188, 47540, 15740, 42443,  9411, 16471, 42060, 11403, 42684,
       38630, 41849, 21871, 21118, 42113, 44674, 24975,  4696, 35567,
       26267, 41209, 25974,   840, 48441, 23020, 44960, 41579, 40518,
       20746, 33808, 43893, 46177, 22636, 14201, 16745, 43865, 24979,
       33196, 41092, 49438, 26405, 19226, 46993,  5627, 23709, 31101,
       33271, 14659,  9534, 25278, 24240, 43668,  6411, 39353,  9524,
       35827, 13468, 26509, 48269,  2511, 24003, 48263, 15069, 45483,
       25488,  2752,  3840,  9400, 37136, 17838,  3574, 39995, 20105,
        4651, 30695, 43700, 19066, 14778, 31693, 14909, 20682,  8392,
       35326, 45800, 41220,  2662, 32884, 20831, 42073, 16010, 18298,
       44884, 41677,  8882, 26592, 45737, 35446, 35279,  4165,  5341,
       24793, 44468, 30895, 40343, 14848, 15439, 13031,  9714, 11080,
       48778, 20537, 19639, 32592, 16567, 46380, 49387, 48119, 34831,
       25489, 18979, 13860, 13181, 25605, 49375,  7490,  8849, 35758,
       29732, 17383, 41774, 41806, 39236, 18302, 36119, 48136, 13223,
       45143,  2598, 26003, 32661,  6328, 20976,  3548, 30971,  4524,
        2365,  2171,  6190, 46409, 29468,  9934,  6512,  3337, 18900,
       19216, 31947,  3113, 15949, 21458,  3066,  6085, 47326, 36106,
       16100,  7674, 25337,  2027, 43002, 36786, 48856, 49660, 48470,
        3024, 27986, 11424, 45089,  1338, 28231, 12420,  1142, 11224,
       18231, 31648, 21564, 28897, 28035, 17884, 15900, 32091, 15509,
       34280, 38547, 35403, 39179, 46480,  2066, 20275,  9292, 24367,
       39196, 16733, 34746, 17764, 23478, 46137, 39589,  9119, 26840,
        4311, 25422, 30730, 32247,  3900,  1711, 20790,  7660,  5566,
       45546,  5477, 11671, 32460, 29497, 37729,  4030,  6354, 35286,
        8249, 43998, 32131,  1759, 37177, 36386, 23056, 38566, 32058,
       47865, 14407, 31481, 17763, 32016,  9920, 31048, 35281, 13902,
       38345, 32670, 13861, 18659, 18849, 38205, 44067, 33336, 31654,
       10526, 46987,  5924, 43664, 49276,  5858,   405, 35079, 32398,
       12344,  4571, 34828, 26027, 14329,  1424, 43771,  3644, 32968,
       49863, 43368,   417, 11078, 23972, 39121,  4192, 36389, 21338,
       27609,  5549, 18315,  6904, 33514, 34123, 42621,  2459, 16423,
       41800, 38419, 42272, 30101, 26083, 35529,  6929,  6124, 20566,
       27147, 44742, 35059, 23924, 14138, 33864, 38656, 36038, 27278,
        7002, 18704, 26913,  3852, 30605, 11937, 38344, 16370,  1477,
        9895, 41651, 30152, 29561, 13189,  9300, 21736,  9343, 32702,
        1002, 25307, 16536, 12598, 21909, 20522, 26064, 37924, 36970,
       24348,  1271, 43487,  6162, 41725, 25008, 26613, 26372,  6309,
       38880, 38894, 19221, 13941, 46659, 33914, 43554,  3795, 18111,
       11564, 27652, 22517, 34835, 32732, 11131, 17322, 41734, 44313,
       28091, 32098, 19871, 18169, 23120, 49368, 49336, 40788,  7584,
       37546, 37998,  1363, 22933, 49713, 30838, 22406, 38423, 30022,
       12289, 10383,  4177, 38127, 25256, 47526, 12475, 18222, 49481,
       24135, 36366, 36054, 14225, 36448, 18180,  4799, 30960, 34159,
       36027,  3785, 26336, 35083, 24174, 20215,  5741, 32963, 13857,
       34581, 49015, 23935, 29810, 18662, 31198, 33511, 15706,  2042,
        8330, 35583, 20151, 21739, 28991, 12681,  7835, 17904, 47892,
       34116, 18862, 41815, 40668, 32449,  1057, 19780, 41134, 33355,
       42056, 25055, 10784, 35428, 28529, 15341, 16628, 33642, 22223,
       10770, 34170, 16583, 15397, 24871,  4437, 47819,  8781, 45921,
       13986, 38761,  1803, 40954, 27761, 46085, 33714, 12105,  9166,
       11477,  3447, 24100, 16335, 14108, 13925, 20593, 20121, 31632,
       34345, 10611, 38781, 15739, 30756, 27698, 15753, 42153, 41271,
       19060, 33168, 43685, 38976, 36809, 36979, 40399, 22453,  8722,
       24384, 45251, 37176, 12966, 21326, 21851,  9700, 38879, 39300,
       33601,  8122, 22209,  8760, 10418, 34016, 32325, 36743, 15877,
         479, 43730, 20159, 47553,  2383, 34461, 14373,  2594, 33236,
        6466,  6669, 10244,  4669, 38066, 18072, 24551,  5425,  3110,
       36800,  8686,  1196, 35629, 33955, 24022,   691, 32844, 28523,
       24751, 49298, 31141,  7467, 32021, 28149,  4672, 48045, 33031,
       36216, 15345, 45439,  3043, 28513, 37204,  5797,  4310, 23995,
        9914, 16231,  2223, 29950, 49430, 49944, 22826,  6914,  5106,
       41102, 31261,   774,  9371, 37765, 13811, 33972, 24598, 22161,
       21287]), [4, 1, 0, 3]), (array([20177, 36044,   854,  9901, 32063, 44492, 31408, 40153, 32074,
       45741, 19281, 35219, 30128, 44023,  3504, 18722, 41297, 27687,
        3991, 33948, 11638, 31660, 35136, 46791, 34020, 14476, 30938,
       31356,  5723, 46377, 15986, 37887,  3341, 13188,  3098, 34142,
       45925,  8256, 42584, 23821, 24556,  8633, 10013, 30813,  3716,
       38061, 49082, 25815, 10961,  1191, 49708, 16478, 42881, 16760,
       10738, 10111, 30001, 29976, 46951,  1176, 15823, 37917,  5728,
        9275, 30910, 23606, 45585, 45863, 26469, 41969, 17235,  9625,
       42196, 30685, 22337, 29506, 17427, 30610, 10786, 24879, 26685,
       29108, 13018, 39203, 21941, 23945, 45978, 21612, 49041, 38665,
       13495, 47396, 13804, 39457, 38738, 14043, 46979, 38461,  1959,
       33733, 26243,   937, 49343, 42525, 29141, 17574, 13120, 31342,
        7092, 49412, 23087,  7881, 47552, 16475, 35830, 28254, 15513,
       11204, 43977,  4167, 11181, 32904, 23193,   355, 45878, 35117,
       49200, 27022, 47649, 44259, 44405, 37992, 16455, 32345, 40565,
       34612, 48886, 39122, 23208, 44028, 16945, 22256, 43268, 13866,
       10889, 35747, 40142, 20527, 42167, 39161, 18909, 20766, 40206,
       30682, 27177, 37021, 47821,  6941, 14345, 23542, 34092, 44417,
       41350, 30976,   409,  6517, 37033, 47499, 43869,  6035, 23144,
       28986, 38993,  1837, 14838,  2831, 16539, 24928,  4095, 27229,
        7084, 10600,  5402,  9727, 40710, 43155, 14236,  5381, 48723,
       29233, 25999, 10108, 25829, 20501, 42836, 18655, 28533,  6935,
       34740, 37609, 18868, 49771,  3943, 38707, 43481, 42975, 47150,
       47361, 49212, 32454, 46602, 18085, 38372, 18050, 14679, 34058,
       11903, 18618, 43666,  3311,  7252, 17204, 42762, 28668, 29931,
        9237, 35435,  9681, 34542, 18254, 34563, 31465, 34686, 35370,
       18420,  8063, 45524, 20254, 37219,  7927,  8603, 26619, 17536,
       11110, 22631, 14163,  8069,  8212, 32493, 45155, 42694,  8750,
        2291, 28399, 34488, 39540, 13639, 29612, 38352,  9798, 28397,
       13985,  9463, 33300,  7850, 36441, 22602, 47450, 12584,   271,
        1614, 14918, 37838, 14559,  5773, 10391, 48427, 14423, 25366,
        9168, 17954, 38546,  5506, 29135,  9900,  5535, 28705,  2080,
       22974, 37374, 25263, 13477, 12940, 29912, 22672, 18184, 25312,
       12473, 45023,  5550,  3455, 30021, 36006,  1492, 44193,  8676,
        6073, 38991,  6230, 25757, 44910, 13321, 49020, 13036, 43106,
        6836, 46652, 42014, 18485, 31401, 46877, 49870,  2567, 21486,
        1139, 24617, 24500, 29658,  7714, 12720,  5746,  7136, 34373,
         281, 24789, 11471, 40221, 36598, 44256, 41619,  4679, 37560,
       29988, 30108,  1523, 13934,  8037, 22280,    47,  9217, 31802,
       19130, 17695, 37586,  4014,  4109, 22104, 45550, 13071, 16028,
        6901, 15310,   288,  5012,   108, 14314, 24980,  7890, 27764,
       17980,  1067, 31704,   673, 40860, 36994, 43503,  6927, 33449,
       15895,  4719, 44730, 48270,  5118, 21590,  2812, 18027, 27049,
       46365, 40087, 46405,  6816, 42450, 25276, 33350, 20671, 34149,
       22441, 36920, 31063,  8776, 19944, 28890, 48498, 40687, 24699,
       42271,  5150,  2744, 35126, 30102, 32387, 42193,   648, 20797,
       35656,  4418, 41894, 36268,  3416,  6041, 31884,  7576, 43484,
       38556, 25295, 11478, 45451, 34698, 10398, 20323, 14195, 22603,
       37781, 18328,   800, 39389, 14793, 42472, 46299, 15109, 10217,
       38072, 44652, 22331,   483, 47674,  6317, 10833, 10725, 17325,
       40908, 30694, 42098, 41362, 22843,  1777, 33977, 47360, 33412,
       42185,  3982, 38981,  4012, 22486, 29058, 26893, 45864,  1354,
       24927,  1677, 36385, 43823, 30003, 19899, 30224, 11961, 35360,
        3372, 38705, 41662,   335, 32730, 37208, 13770, 26110,  7688,
       12945, 42803,   864, 49357, 23343, 39426, 21076,    24, 43116,
       45021, 37515,  5281, 11221, 48833, 37178, 44668, 24721, 42399,
       40680,  7328, 39628, 26776, 49869,  4490,  2504, 28363,  2574,
        4941, 27444,   165, 32403, 39650, 24169, 22924, 44464, 11989,
       20834, 12534, 15893, 39281, 16704, 46324, 11642,  8813, 23805,
       26656, 28866, 47201, 19127,  9542,  6374, 19444,  8365, 20523,
       19662, 25976, 32483, 45085, 44141, 32455, 40649,  3332, 22648,
       25668, 30487, 39716, 42755,  1466, 25419, 20039,  1999,  5657,
       37558, 22418,  2460, 41900, 19541,   989, 22329, 24476, 40479,
       25616, 33328, 18591, 20097, 28625, 32025, 48145, 34460, 25930,
        9983, 42678,  7444,  6400, 18706, 11297, 23847, 15622, 17026,
       44794, 13030, 21023, 33148, 15707, 46763, 26842, 12178, 30820,
        1381, 23684, 19277, 48209, 45176,   905,  1185, 31435, 37355,
       38479, 30324, 35418, 36993, 38235, 29881, 26824,  8774, 42952,
       21308, 42945, 35782, 31201, 38551,  3214, 32873, 14615, 21248,
       31700,  8155,  6054,  2932, 32194, 33380, 21297, 28619, 34131,
        4229, 32316, 16596, 48318, 12660, 27440, 37276, 48891, 21881,
       45827, 16521, 41352, 29257, 12133, 11944, 41664, 29944,  1664,
       34758, 22408,  8558, 42573, 34711, 32617,  3609,  5346, 39904,
        1926, 42010, 45373, 47328,   189,  9533, 44568, 17355, 17520,
       25191,  5428, 23024, 24445, 35121,  6711,  3204, 35996, 37279,
        2675, 26045, 33087, 34601, 22858, 13011, 13483, 37712, 20539,
        8098, 32002, 37674, 40648,  8810, 34919, 10893,  8912, 29884,
       44476, 26258,  9801,  8118, 16375, 32444, 20169,  5028, 38847,
       25789, 22026, 27686, 31521, 36558, 10989, 33654, 33076, 25807,
       46625,  8190, 48643, 33926,   129, 39883, 20449, 48635, 21869,
       48024, 28650, 38580, 22967, 46935, 17154,  9616, 42630,  1755,
       31105, 49490, 10215, 14056, 21467, 25820, 47691,   199, 10365,
        6687, 28487, 28921, 37189, 35269, 11451, 33416, 46873, 29940,
        5494,  3515, 25318,  7290, 36781, 29324, 30571, 11271, 21898,
       43626, 23295, 17530, 49970,  9163, 16014, 37135, 26688, 35794,
       48722, 22866, 48062,  9489, 40246, 35549, 41285, 17441, 34545,
        3905, 20017, 34756, 25922, 46204, 25378, 39482,  3490,  5800,
       17051, 25062, 48331, 26711, 21882, 26689, 47413, 46561, 14089,
       44381, 47643, 30896,  3016, 31606, 22914, 16070, 30687, 38626,
       40226, 22011, 47075, 25627,   174, 31886, 43254, 22555, 39166,
       42757, 15247, 33560, 23145, 48326,  6602, 11458, 26609, 43716,
       34361, 32312, 46950, 26210,  7734, 37209, 24707, 20153,  9034,
       25825,  7909, 11199, 39420, 21655, 28067, 33801,  4720, 19895,
       39979, 36074, 45454, 22922, 33952, 41910, 37105, 23483, 48144,
       37180,  3178,  9052, 38552, 33301, 32358, 43034, 30305, 25771,
       41013, 49901,  5183, 11236, 49054, 39822, 18393,  1487, 45479,
       36063, 16339,  9357, 23963, 43922, 17698, 20917,  3910, 30862,
       44550, 48377,   801, 17195, 36124,  9307,  6658, 40369, 43084,
        7765, 12147,  2738, 43500,  2770, 45152,  5720, 24991, 44543,
       46595, 12558, 44234, 32607, 21703, 40326, 27433,    26,  3218,
       45670, 44127, 32006, 20131, 46596, 31295, 35245, 44267,  9345,
       25673, 24839, 26097,   494, 43233, 36518, 48999,  8101,  3807,
       47313, 32264,  6402, 37697, 43283, 34579, 11294, 40097, 44110,
        1127,  3371,  6199, 44376,  9332, 37967, 40584,  5221, 34984,
       37675, 23461, 14558,   776, 13584, 15937, 24611, 26260, 12783,
       32583, 30986, 46712, 17413, 23314, 10070, 22007, 49146, 17835,
        6590, 42647,  6945, 18497, 22994, 46157, 21351, 12647, 17661,
       27753,  5870, 27759, 36230, 19518, 26907, 23278,  5389,  6674,
       12707, 11864, 42021,  7205, 24949, 24749, 37038, 20691, 41540,
       46229, 33915, 36969, 45850, 27263, 46232, 29050, 19381, 36714,
       20338, 29960, 10949, 15226,  1109, 39609,  2505, 26862, 40459,
       14942]), [6, 2, 0, 3]), (array([37228, 15148, 28398, 26615,  5771, 18916, 23880, 46397, 40005,
       45365, 28815, 21340, 47665, 13445, 10874, 45459, 24820, 37482,
          85, 10073, 12844, 24159, 19244, 20094, 24892, 47575, 35767,
       39667, 30939,  3071, 25568,  6734, 22723, 38575, 49335,  3988,
       48066, 14742, 16265, 31212,  5403, 38358, 43617, 46797,  4248,
       42366, 22806, 33191, 16706,  6655, 29535, 31321, 44353, 43150,
        2405,  9590, 40374,  1386, 19997, 29886, 11581, 14531, 11391,
       39081, 13935,  5325,  5976, 39164, 44081,  1583, 37489, 16995,
       27657,  5220, 33821, 40977, 43929, 28015, 15364, 22126, 26056,
       29614, 17720, 12271, 20618, 34904, 12633,  5307, 35120, 17913,
       26916, 49377, 42959, 41239, 26702, 41437,  1902, 10754, 21751,
       39304,  3817, 38104, 27451, 17398, 45292, 16344,  2969, 46154,
        2397, 46465, 38195,  1489, 42555, 43848, 41805, 37293,  3835,
       25283,  3637, 35008, 19159,  1483, 39400, 13799, 35462, 15067,
       32640, 37971,   662, 44578, 46954, 29789, 12794, 33330, 47924,
       44892, 13026, 10156,  6157, 20429, 29859,  9939, 49456,   727,
       25865, 42018, 16096, 46167, 27929,  7141, 33388, 32490, 48307,
       38367, 43556, 45682, 46027, 41959, 46266, 34165, 21060, 48767,
        7185, 11321, 25791, 16879, 27985, 19206, 28206, 21314, 29910,
       46549, 36718, 47657, 15332, 44935, 12397,  7197, 10496, 41347,
       30484, 39351, 45657,   652, 41526, 48732,  4427, 49048, 14066,
        7420, 27217, 29962, 19779, 34733, 13566, 15400, 15842,  1549,
       46672, 15932, 47817, 24930, 43036, 48166, 34584,  6827, 16031,
       44123, 25724, 40243, 44366, 42268, 26314, 18417, 49116, 26692,
       11553,  6335, 44086, 18620, 10818, 39246, 36270, 16654, 16138,
       12941,  3824, 22782, 24371, 33935,  8887,  3782, 47673, 12535,
       16996, 43305, 31389, 42453, 37159, 31529, 20223,  7829, 31653,
        4736,  6713, 41517,  6307, 37948, 37862, 48055, 45327, 18104,
       30433, 15964, 49445, 22440, 26951, 47527, 40491, 32243,  8972,
       13054,  9972, 14010, 32574, 48076,   766, 29414, 19195, 42794,
       19147, 46248, 22462, 20400, 13131, 19175, 38469, 39621,  5348,
       37703, 21312,  5701, 20455, 26337,  4891, 36733, 15721, 21377,
       17116, 25823, 37207, 44199, 20405, 35864,  8241, 40702, 33093,
        7334, 49571, 19851, 33562, 40252, 21830, 22920, 25207, 37583,
       14123, 29868, 29199, 40493, 48959, 13674, 30018,  1770, 32543,
         193, 39802, 44877, 11993, 20652, 18593, 11652,  8401, 41616,
       27864, 43149, 23436, 21253,  1383, 47854,  7118, 44205,  4287,
       44382, 24841, 39298, 49393, 36799,  7455, 26603, 22262, 47206,
       10682,  1250, 12115, 48690, 34152, 31104, 36534, 47791, 19733,
       26246, 34910, 23973, 30510, 12543, 34248, 36013, 48311,  3497,
       19416, 44304, 20089,  1794,  6995, 28384,  9418, 10475,  6418,
       27932, 32915, 26977,  8073, 40939, 16479, 42617, 43899, 24444,
       21936, 46552, 23766, 31170, 23839, 47978, 38525, 35139, 38949,
         442, 12041, 22584, 44458,  3159, 29762, 26714, 48678, 23258,
       35115,  2160, 28435,  5185, 40429,  7104,  3670, 15198, 24386,
       37416, 11977, 40598, 20540, 18171, 33287, 45619, 40458, 19479,
       13672, 46151, 20237, 18853,  3144, 40408, 41988, 19472, 48871,
       30808, 26094, 30411, 29371, 42050, 42842, 18623, 30380, 31423,
       32567, 40778,  6397, 31966, 10095, 34419, 30328, 32463, 33335,
       17345, 28728, 31121, 13355, 19887, 19141,  8526, 35562, 20006,
       19768, 16084, 24350,   880,  1702,  1357, 38276, 30844, 26988,
       49905, 16118, 25258, 21172, 25794, 48602, 23153, 10606, 45890,
        3584, 23215, 13406,  3353,  6508, 42676, 42388, 43278, 12233,
       10272,  2923, 45240, 41570, 34560,  1967, 10445, 23583, 11842,
       11501, 11010, 38315,  8852, 13967,  2395, 17013, 37335, 28874,
       23282, 15043, 45776, 28461, 42182,  1144, 48212, 33000, 27934,
        2169, 16632, 29060, 18508, 37083, 26427, 41588, 29487, 22962,
       44749, 11721, 29295, 33933, 21983, 38433,  1270, 15466, 22190,
       32251,  8549,  3665, 24700, 45615, 15288, 38112,  9171, 41300,
       35594,  9221, 46458, 18525, 27940, 19370,  7747, 32505, 39149,
       11178, 25144,  1329,   497, 19004,  3180, 31459, 21105,  2718,
       17088, 38335, 40644, 48105, 49091, 17620, 26300, 41960, 36363,
       28033, 22881,  3263, 15436, 48280,  7360,  9296,  3466, 21777,
       13850, 10855, 40589,  5984, 30829, 26879, 23157, 43965, 11475,
        7826,  2780, 10266, 15678,  8200, 43750, 24639,  8478, 20434,
       22200, 30667, 17237, 47379, 34022, 32190, 13751, 48508, 14450,
       43659, 38198, 28449, 31674,   663, 29274, 23419, 41574, 34537,
        7808, 42934,  4368, 26488, 13238, 24939, 36149,  8437,  9512,
        2258, 42298, 10353,  6665, 14708, 12785,  1044, 49656, 37265,
       12818, 45113, 22799, 23182, 19527, 11431, 16460,  7504, 38003,
       43184, 27323, 10607, 45936, 48246, 12079, 21227,  8072, 30811,
       22973, 27004, 35991, 30652, 23610, 46831, 24179, 16316, 10513,
       46303, 16554, 12083, 16230, 19814, 48850,  8695,  4314, 48729,
        5484, 11919, 46807,  1816,  5010,  2659, 22276, 47220, 29941,
       25160, 14080, 30002, 36446, 17133, 43219, 41818, 33438,  9004,
        2714, 46655, 33242, 37165, 36623, 30830, 11682, 13106,  8355,
       24881, 30158, 16146,  9375, 29977, 49723, 37181, 13668, 31358,
       43814, 11691, 30078,  4079, 35685,  6629, 42597, 24642, 24291,
       39530, 12836, 19084,   436, 45621, 24325, 47143, 19812,  3441,
       27981, 33732, 49734, 18192, 23738, 19804, 16002,  7792, 39380,
       26723, 29336, 40504, 30304, 34964, 27562, 24600, 46964, 38326,
       18614, 21413, 18100, 10323,  8444, 14509, 36909, 12181, 11029,
       47317, 26761, 21892, 42722,  2345,  6640, 16015, 34125, 14591,
       25070,  7203, 33453, 32139,  2559, 36870, 11188, 49983, 42825,
       25934,  9760, 37163, 25547, 26160, 43735, 43773, 48058, 18798,
       49500, 25971, 31649, 31603, 24330, 42084,  1098, 37817,  8599,
       16869, 15057, 34437, 37453, 38364, 14574, 13061,  3514, 11976,
         949, 31254, 39761, 11257, 11293, 20034, 28738, 14527, 37700,
        6060,  8164, 25745,   550, 27298, 23122, 14861, 18387, 46653,
        2164,  8161, 28838, 28028, 19787,  9737,  1554, 20961, 43742,
       12050,    21, 11938, 16366, 23417, 22778,  5848, 44926,  4288,
       49819, 29692,  9667, 26883,  9982, 44456, 17414, 18865, 34623,
       36418, 29839, 22364, 28433, 33964, 30634, 46841, 23027, 31788,
       23239, 37973, 23465, 44802,  3756,  7107, 27156, 32686, 19838,
         101, 42789,  9421, 28808, 27742, 44142, 29460,  1685, 13064,
       21103, 40511, 39689, 48540, 30624,  9222,  3003,  3376, 17144,
       31094, 14340, 28819, 38171, 39533,  4402, 10011, 25077, 39608,
        8067, 30386, 36273, 43800, 20877, 42997, 38699, 49599, 14148,
       35449,  4392, 17430,  9399, 41511,  8246,  8635, 11231, 25443,
       26906, 13463, 18771,  8422, 25006, 48849, 47973, 32524, 28813,
       35595,  3818, 11578, 25986,  2518, 30779, 28983, 25688,    74,
       33211, 13914, 49499, 28029, 23998, 30573,  6186, 18422, 46962,
       13313,  1778,  8381, 20601, 27467,  6980,  6394, 26302, 41269,
       17909, 31825, 33925, 17098, 21962, 25219, 45091, 22664, 12821,
       32541, 10958,  4791, 32982, 33393, 28034, 44562, 45053, 46347,
       11743, 30839,  5062,  1461, 22638, 29979, 42411, 44409, 34952,
        5915, 31676, 20007,  2359,  3758, 25782, 12631, 32113,  1427,
        6883, 15856, 38932, 37156, 16814,  4083, 25706, 15083, 43105,
       31366, 18954, 39789, 23763, 11040, 34590, 28984,  6343,  8822,
        9943, 18389, 47539, 38951, 43533, 27212, 31161, 15080, 38377,
       21510, 32350, 37773, 20849, 38897, 11409, 37304,  6724, 46198,
       39171]), [7, 8, 0, 3])]
Collaboration
DC 0, val_set_size=1000, COIs=[5, 9, 0, 3], M=tensor([5, 9, 0, 3], device='cuda:0'), Initial Performance: (0.25, 0.045359280467033386)
DC 1, val_set_size=1000, COIs=[4, 1, 0, 3], M=tensor([4, 1, 0, 3], device='cuda:0'), Initial Performance: (0.274, 0.04425249779224396)
DC 2, val_set_size=1000, COIs=[6, 2, 0, 3], M=tensor([6, 2, 0, 3], device='cuda:0'), Initial Performance: (0.254, 0.04541688692569733)
DC 3, val_set_size=1000, COIs=[7, 8, 0, 3], M=tensor([7, 8, 0, 3], device='cuda:0'), Initial Performance: (0.255, 0.044730136394500734)
D00: 1000 samples from classes {0, 3}
D01: 1000 samples from classes {0, 3}
D02: 1000 samples from classes {0, 3}
D03: 1000 samples from classes {0, 3}
D04: 1000 samples from classes {0, 3}
D05: 1000 samples from classes {0, 3}
D06: 1000 samples from classes {9, 5}
D07: 1000 samples from classes {9, 5}
D08: 1000 samples from classes {9, 5}
D09: 1000 samples from classes {9, 5}
D010: 1000 samples from classes {9, 5}
D011: 1000 samples from classes {9, 5}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {2, 6}
D019: 1000 samples from classes {2, 6}
D020: 1000 samples from classes {2, 6}
D021: 1000 samples from classes {2, 6}
D022: 1000 samples from classes {2, 6}
D023: 1000 samples from classes {2, 6}
D024: 1000 samples from classes {8, 7}
D025: 1000 samples from classes {8, 7}
D026: 1000 samples from classes {8, 7}
D027: 1000 samples from classes {8, 7}
D028: 1000 samples from classes {8, 7}
D029: 1000 samples from classes {8, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.321, 0.059396352589130404) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.0762955961972475) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.08767266011238098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.0864397220313549) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.06267942577600479) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.09520264573395253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.333, 0.12743896949291228) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.09229815077781678) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.07932132732868194) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.11468889297544957) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.15861733889579774) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1226446467190981) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.10350485199689866) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13766499603539706) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.17524761104583741) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1584977867305279) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.425, 0.12845924731343986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.15613945835083723) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.18912637266516685) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.19356412656605243) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.14465451043099165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.17646908596530556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.19739552516490222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.22081087283417583) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.1671894628610462) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1882399409338832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.18544992347434164) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.23137943388335408) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.1650114922504872) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.1933495959304273) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.22099166601337492) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2418985731303692) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.1857669574674219) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.21793028167169542) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.268153380241245) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.272056638228707) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.432, 0.21375495231803507) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.23726580816414208) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.27721165637299416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.26264312120061367) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[0, 3], M=tensor([0, 1, 3, 4, 5, 9], device='cuda:0'), Initial Performance: (0.0, 0.3653028163909912)
DC Expert-0, val_set_size=500, COIs=[9, 5], M=tensor([5, 9, 0, 3], device='cuda:0'), Initial Performance: (0.864, 0.014043234599754215)
DC Expert-1, val_set_size=500, COIs=[1, 4], M=tensor([4, 1, 0, 3], device='cuda:0'), Initial Performance: (0.924, 0.007941799638792872)
SUPER-DC 0, val_set_size=1000, COIs=[5, 9, 0, 3], M=tensor([5, 9, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[4, 1, 0, 3], M=tensor([4, 1, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7e80480561f0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7e8038025d00>, <fl_market.actors.data_consumer.DataConsumer object at 0x7e80480360a0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7e803071ddc0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7e80480ffa90>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO0', '(DO5']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.007488905884325504) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.003501298187300563) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.19007089081779122) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.482, 0.2944188005393371) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.898, 0.009474406588822603) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.508, 0.06276759320124983) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.07165833412110806) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.006047449848614633) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.0044313889760524035) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.1916154723763466) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.30957024714211) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.862, 0.012674216404557228) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.595, 0.039874509274959564) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.603, 0.0444775161743164) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.004961958454921841) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.004173000985756517) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.1682098748702556) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.29623782690428196) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.013636592827737331) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.03823779046535492) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.64, 0.03669719186425209) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005684545554220676) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003045516555197537) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.17753619533590972) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.33085318251885476) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.898, 0.015186706016771495) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.587, 0.05224229922890663) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.65, 0.039122896909713746) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.006907616772688925) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004278622119687498) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.168191300611943) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2929052599389106) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.862, 0.019582527324557304) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.505, 0.06290086860954762) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.605, 0.0499937362074852) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO2', '(DO4']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.008231029875576496) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004077361914329231) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.14569616655632853) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2862556876782328) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.887, 0.016047231689095498) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.526, 0.07174159848690033) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.582, 0.053546564042568204) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.006672046524472535) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003880434467922896) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.15280052544549108) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.3046746424487792) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.866, 0.019309601590037345) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.07256708431243897) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.63, 0.046885191202163695) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005492866242770106) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003996259118663147) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.1484529738407582) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.483, 0.2952754307272844) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.889, 0.018664011130109428) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.08443696753680706) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.05462068379670382) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.007657619441859424) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003079035478644073) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.14506863360106945) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.28259778969362376) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.894, 0.015366092154756188) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.07238533770292997) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.617, 0.054520710671320555) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.007120299002155662) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002977590200025588) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.12034614564292133) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.483, 0.2995556022464298) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.017056969803757965) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.06483349107205867) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.667, 0.04286374567449093) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO4', '(DO5']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.0052035758225247265) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002985589366639033) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.15749525277875365) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.26955703370226547) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.903, 0.013910102128982544) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.569, 0.062467166900634766) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.667, 0.03972887876629829) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005497155879624188) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003965606773737818) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.14863044795952737) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.27026765277329834) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.89, 0.010757127672433853) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.571, 0.06141727995872497) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.61, 0.04676275712251663) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.0058855028729885815) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.004126197523670271) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.13340905020385982) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.25375326273590326) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.904, 0.019130895966372918) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.514, 0.09643180863559246) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.0696448533423245) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007514033243525773) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002651366788893938) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.12400686652213334) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.482, 0.2668396622276632) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.885, 0.016832196775823833) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.483, 0.08619514313340187) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.624, 0.052916023954749106) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006859736800426617) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003970026229624637) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.1269433961585164) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.27046509309951217) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.881, 0.018177795320225412) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.08714202500879764) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.647, 0.052427079372107985) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO5', '(DO2']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.007800518288742751) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.004202745073125697) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.12880928766448052) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2561888306851033) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.010376577912829817) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.08533451851457358) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.649, 0.04647506728768349) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00746926519786939) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0037235867159906774) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.11899526837095618) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.28590692841639975) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.904, 0.012068727385252715) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.08776727584004403) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.595, 0.06854239976406097) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.008204643263365142) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0038908107738243416) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.13756472665071487) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.24841907145967707) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.915, 0.011491751382127404) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.09924796240031719) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.586, 0.07253751781582832) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.00890363673819229) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.00278516600234434) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.127149362815544) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.2867338064447977) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.908, 0.017461211958900096) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.10513575113750995) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.607, 0.06989993924088776) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005658467038534581) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0029478345112875105) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.1317097545452416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.30846444813429846) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.907, 0.01417114487872459) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.565, 0.08837051342800259) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.07041197068989277) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO1', '(DO4']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007388303550193086) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0025879213977605103) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.11545161516964436) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.22741691055963748) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.914, 0.011244352359557524) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.10620912795793265) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.591, 0.07750038022827357) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005994475677143782) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003520922129333485) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.1243274373151362) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2734421260749223) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.913, 0.012516292087733746) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.07663011875748635) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.603, 0.0586222004070878) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.007263257483486086) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004635119164478965) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.11295107477158307) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.27212514989008196) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.911, 0.016300240292781383) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.559, 0.0903406987618655) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.581, 0.07111988029442727) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005729320430196821) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0028708371462416835) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.12545403330214322) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.2726662370691774) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.898, 0.01812007695098873) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.572, 0.07836019889265299) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.062492230143398046) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006160272345645353) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0035370535940746775) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.12410292216017842) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.284060621180397) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.894, 0.02316719800591818) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.1015200627129525) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.07945984322577715) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO4', '(DO1']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007689155066735111) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003659797580796294) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.11703885355964302) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.484, 0.28851999365817754) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.907, 0.010246388574596494) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.595, 0.0691948719844222) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.0701461627818644) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006786848008225206) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0033143458554986864) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.12630984785500915) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3057359377421672) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.015636557537131012) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.557, 0.07756127398461103) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.548, 0.07773784086108208) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.007905131851672196) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.004276386180164991) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.11945295379124582) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.31454061074194034) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.909, 0.015858515739208087) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.557, 0.07502446910738945) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.10122805102355778) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007495217632036656) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0038547663654899225) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.13442073375172914) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2722050162440282) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.899, 0.01944584192156617) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.526, 0.0869915326833725) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.1037449069917202) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007663165813544765) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003457502771401778) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.1347788644693792) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.3103650416455057) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.014635284699885233) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.08098561202734708) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.599, 0.07312347355671227) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO3', '(DO2']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.006675733118318022) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0044457701871579046) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.12598139364458621) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.30618507498688996) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.009263684869743884) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.596, 0.06901354083418847) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.568, 0.07117755633592605) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.011036991569941165) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.003001096205262002) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.12978571209311485) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.26465610005822965) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.906, 0.014307554655941203) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.557, 0.0910030248016119) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.09006005968898535) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.00619459437765181) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002244486324954778) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.1522485567741096) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.33495537763106403) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.907, 0.016379008555726614) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.59, 0.08727236690372228) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.552, 0.08601824310421943) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005244077345356345) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004334128418122418) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.15549187941104173) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.27206729354267006) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.914, 0.01616213235828036) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.08460419134795666) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.09852314292266964) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.008715445599285885) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0034576320569904054) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.13647901137359442) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.2803167977553094) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.013167142477817834) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.573, 0.08606133256852627) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.08101992694288493) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO3', '(DO5']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006970225708559156) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002540867214673199) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.1392017645277083) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.32047755639159003) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.913, 0.008938796422909945) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.08295253252983094) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.0905731084086001) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.008489171934779733) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0030744065504404714) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.14819689301773906) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2504834563981276) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.927, 0.011089987782761455) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.574, 0.08842905451357365) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.549, 0.0904759514182806) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007325722463428974) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.001744141448289156) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.15792521795257927) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.483, 0.3071470771608874) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.918, 0.012668049726635218) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.584, 0.08820976198092104) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.538, 0.10030853682756424) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007877390338457189) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002107608071062714) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.1577737809829414) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.3207766140225867) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.01591005756543018) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.572, 0.09763600851595401) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.08704354610480368) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.006087187984958291) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0019256460480391979) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.1610343178138137) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.484, 0.30778514794283546) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.01743157563917339) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.09698522854596377) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.08825996056199074) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.045359280467033386), (0.321, 0.059396352589130404), (0.405, 0.06267942577600479), (0.434, 0.07932132732868194), (0.459, 0.10350485199689866), (0.425, 0.12845924731343986), (0.434, 0.14465451043099165), (0.452, 0.1671894628610462), (0.465, 0.1650114922504872), (0.456, 0.1857669574674219), (0.432, 0.21375495231803507), (0.508, 0.06276759320124983), (0.595, 0.039874509274959564), (0.577, 0.03823779046535492), (0.587, 0.05224229922890663), (0.505, 0.06290086860954762), (0.526, 0.07174159848690033), (0.527, 0.07256708431243897), (0.547, 0.08443696753680706), (0.533, 0.07238533770292997), (0.536, 0.06483349107205867), (0.569, 0.062467166900634766), (0.571, 0.06141727995872497), (0.514, 0.09643180863559246), (0.483, 0.08619514313340187), (0.547, 0.08714202500879764), (0.549, 0.08533451851457358), (0.555, 0.08776727584004403), (0.549, 0.09924796240031719), (0.558, 0.10513575113750995), (0.565, 0.08837051342800259), (0.543, 0.10620912795793265), (0.583, 0.07663011875748635), (0.559, 0.0903406987618655), (0.572, 0.07836019889265299), (0.528, 0.1015200627129525), (0.595, 0.0691948719844222), (0.557, 0.07756127398461103), (0.557, 0.07502446910738945), (0.526, 0.0869915326833725), (0.609, 0.08098561202734708), (0.596, 0.06901354083418847), (0.557, 0.0910030248016119), (0.59, 0.08727236690372228), (0.586, 0.08460419134795666), (0.573, 0.08606133256852627), (0.606, 0.08295253252983094), (0.574, 0.08842905451357365), (0.584, 0.08820976198092104), (0.572, 0.09763600851595401), (0.549, 0.09698522854596377)]
TEST: 
[(0.25, 0.044425843864679335), (0.315, 0.05723254606127739), (0.4005, 0.06019690066576004), (0.43425, 0.07564766851067543), (0.455, 0.09838825124502182), (0.426, 0.12078447866439819), (0.4355, 0.13630374038219453), (0.44775, 0.15698001247644425), (0.46175, 0.15570431262254714), (0.45275, 0.1750297738313675), (0.43225, 0.19910654628276825), (0.513, 0.058287806421518325), (0.60575, 0.03819352720677853), (0.592, 0.03807192000001669), (0.606, 0.04994837898015976), (0.52975, 0.061039287954568866), (0.51775, 0.07010589334368705), (0.55025, 0.06989688476920128), (0.541, 0.0824729415178299), (0.54175, 0.07209002116322517), (0.55675, 0.06207382620871067), (0.57775, 0.06119139650464058), (0.56875, 0.059784800320863724), (0.52225, 0.09597592607140541), (0.50975, 0.08484662365913391), (0.57275, 0.08258931338787079), (0.56375, 0.0786759372651577), (0.568, 0.08326042500138282), (0.5565, 0.09390639850497245), (0.552, 0.10176826539635658), (0.57, 0.08340377500653266), (0.5515, 0.10154570057988167), (0.58825, 0.07089409019052982), (0.564, 0.08818721386790275), (0.578, 0.07599580124020576), (0.53, 0.09756723073124886), (0.5965, 0.06526249977946282), (0.55225, 0.07673833641409875), (0.56125, 0.07235175311565399), (0.533, 0.08493039548397065), (0.60075, 0.07692987459897994), (0.59325, 0.06712599048018456), (0.5655, 0.08608122491836548), (0.595, 0.0859387609064579), (0.599, 0.08156143356859684), (0.5705, 0.08695495527982712), (0.6035, 0.08062488034367561), (0.5725, 0.08927790969610214), (0.58025, 0.08674543178081512), (0.5685, 0.09671615961194038), (0.53325, 0.0966722437441349)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.65      0.89      0.75      1000
           3       0.40      0.85      0.54      1000
           5       0.41      0.06      0.10      1000
           9       0.96      0.34      0.50      1000

    accuracy                           0.53      4000
   macro avg       0.60      0.53      0.47      4000
weighted avg       0.60      0.53      0.47      4000

Collaboration_DC_1
VAL: 
[(0.274, 0.04425249779224396), (0.25, 0.0762955961972475), (0.251, 0.09520264573395253), (0.301, 0.11468889297544957), (0.395, 0.13766499603539706), (0.39, 0.15613945835083723), (0.431, 0.17646908596530556), (0.459, 0.1882399409338832), (0.462, 0.1933495959304273), (0.466, 0.21793028167169542), (0.462, 0.23726580816414208), (0.527, 0.07165833412110806), (0.603, 0.0444775161743164), (0.64, 0.03669719186425209), (0.65, 0.039122896909713746), (0.605, 0.0499937362074852), (0.582, 0.053546564042568204), (0.63, 0.046885191202163695), (0.6, 0.05462068379670382), (0.617, 0.054520710671320555), (0.667, 0.04286374567449093), (0.667, 0.03972887876629829), (0.61, 0.04676275712251663), (0.558, 0.0696448533423245), (0.624, 0.052916023954749106), (0.647, 0.052427079372107985), (0.649, 0.04647506728768349), (0.595, 0.06854239976406097), (0.586, 0.07253751781582832), (0.607, 0.06989993924088776), (0.6, 0.07041197068989277), (0.591, 0.07750038022827357), (0.603, 0.0586222004070878), (0.581, 0.07111988029442727), (0.576, 0.062492230143398046), (0.545, 0.07945984322577715), (0.576, 0.0701461627818644), (0.548, 0.07773784086108208), (0.508, 0.10122805102355778), (0.505, 0.1037449069917202), (0.599, 0.07312347355671227), (0.568, 0.07117755633592605), (0.528, 0.09006005968898535), (0.552, 0.08601824310421943), (0.527, 0.09852314292266964), (0.565, 0.08101992694288493), (0.561, 0.0905731084086001), (0.549, 0.0904759514182806), (0.538, 0.10030853682756424), (0.561, 0.08704354610480368), (0.556, 0.08825996056199074)]
TEST: 
[(0.27025, 0.043206459641456606), (0.25, 0.07365380853414535), (0.25025, 0.09170785903930664), (0.305, 0.110004026055336), (0.39225, 0.13228247344493865), (0.3865, 0.14950004374980927), (0.42625, 0.1695541403889656), (0.4605, 0.1806327423453331), (0.45975, 0.18581818532943725), (0.4595, 0.20804015803337098), (0.4505, 0.22729678165912628), (0.50425, 0.07001800128817558), (0.62525, 0.041209489002823826), (0.6425, 0.03481127519160509), (0.66025, 0.03852414658665657), (0.60375, 0.04787168091535568), (0.59475, 0.04988904429972172), (0.631, 0.04353687836229801), (0.618, 0.05122947037220001), (0.61725, 0.05159402030706406), (0.66125, 0.041064761966466905), (0.6745, 0.03769706582278013), (0.626, 0.043400629907846454), (0.565, 0.0674021396934986), (0.61975, 0.05025985008478165), (0.63725, 0.049865994811058044), (0.63625, 0.047660806819796564), (0.59875, 0.06605039738118648), (0.57, 0.07342681261897087), (0.58925, 0.0723718481361866), (0.594, 0.0696080026179552), (0.58875, 0.07720801761746407), (0.5975, 0.059122710943222044), (0.5695, 0.07354974865913391), (0.59475, 0.05877831715345383), (0.55375, 0.07560515764355659), (0.596, 0.06649025605618954), (0.5405, 0.07411694791913033), (0.528, 0.09635862523317337), (0.516, 0.09945661962032318), (0.60575, 0.07229569232463837), (0.59175, 0.06597698432207108), (0.54075, 0.08561531507968903), (0.57375, 0.08341439360380173), (0.5305, 0.09806800445914268), (0.571, 0.07433045016229153), (0.5645, 0.08891337183117866), (0.5455, 0.08700456646084785), (0.54425, 0.09352665767073631), (0.534, 0.08996665957570076), (0.532, 0.08764254227280617)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.57      0.87      0.69      1000
           1       0.92      0.31      0.47      1000
           3       0.44      0.83      0.58      1000
           4       0.45      0.11      0.18      1000

    accuracy                           0.53      4000
   macro avg       0.60      0.53      0.48      4000
weighted avg       0.60      0.53      0.48      4000

Collaboration_DC_2
VAL: 
[(0.254, 0.04541688692569733), (0.317, 0.08767266011238098), (0.333, 0.12743896949291228), (0.39, 0.15861733889579774), (0.423, 0.17524761104583741), (0.423, 0.18912637266516685), (0.413, 0.19739552516490222), (0.415, 0.18544992347434164), (0.428, 0.22099166601337492), (0.418, 0.268153380241245), (0.431, 0.27721165637299416), (0.408, 0.19007089081779122), (0.417, 0.1916154723763466), (0.417, 0.1682098748702556), (0.425, 0.17753619533590972), (0.425, 0.168191300611943), (0.432, 0.14569616655632853), (0.417, 0.15280052544549108), (0.42, 0.1484529738407582), (0.43, 0.14506863360106945), (0.426, 0.12034614564292133), (0.435, 0.15749525277875365), (0.424, 0.14863044795952737), (0.431, 0.13340905020385982), (0.424, 0.12400686652213334), (0.429, 0.1269433961585164), (0.428, 0.12880928766448052), (0.429, 0.11899526837095618), (0.426, 0.13756472665071487), (0.418, 0.127149362815544), (0.431, 0.1317097545452416), (0.422, 0.11545161516964436), (0.435, 0.1243274373151362), (0.429, 0.11295107477158307), (0.426, 0.12545403330214322), (0.43, 0.12410292216017842), (0.431, 0.11703885355964302), (0.425, 0.12630984785500915), (0.433, 0.11945295379124582), (0.419, 0.13442073375172914), (0.424, 0.1347788644693792), (0.418, 0.12598139364458621), (0.432, 0.12978571209311485), (0.432, 0.1522485567741096), (0.426, 0.15549187941104173), (0.422, 0.13647901137359442), (0.426, 0.1392017645277083), (0.424, 0.14819689301773906), (0.434, 0.15792521795257927), (0.429, 0.1577737809829414), (0.43, 0.1610343178138137)]
TEST: 
[(0.25275, 0.04452108883857727), (0.3225, 0.08423771148920059), (0.348, 0.12218378227949142), (0.39725, 0.15200227892398835), (0.41825, 0.16702659863233565), (0.416, 0.18177365124225617), (0.3995, 0.19000737953186037), (0.40175, 0.17578768062591552), (0.42075, 0.21132051086425782), (0.41375, 0.25228228414058684), (0.42475, 0.2621416676044464), (0.40175, 0.1794213000535965), (0.4105, 0.18378646665811538), (0.41525, 0.160983438372612), (0.42525, 0.16748728597164153), (0.4285, 0.15850664907693862), (0.4245, 0.13789439189434052), (0.41225, 0.1395623933672905), (0.42175, 0.14097350281476975), (0.42775, 0.13937981802225113), (0.42775, 0.11426614445447922), (0.43525, 0.14886640322208405), (0.43125, 0.13895762026309966), (0.43425, 0.12409417921304702), (0.42975, 0.11565224838256837), (0.4305, 0.11820443487167358), (0.43125, 0.11948294979333877), (0.42875, 0.11427518129348754), (0.4315, 0.12637819558382035), (0.41975, 0.11772247371077538), (0.43125, 0.11992042100429536), (0.42725, 0.10694858968257905), (0.4345, 0.1157277227640152), (0.42925, 0.10534641569852829), (0.4305, 0.11603998023271561), (0.435, 0.11410287201404572), (0.4295, 0.11056812703609467), (0.42975, 0.12015212363004685), (0.4325, 0.11399015474319459), (0.42375, 0.12818691205978394), (0.4255, 0.12979782861471176), (0.4195, 0.1207790983915329), (0.4275, 0.12265902692079544), (0.43675, 0.14286070996522904), (0.431, 0.14784234988689424), (0.43125, 0.12943084263801574), (0.429, 0.13317867654561996), (0.43275, 0.140205839574337), (0.43575, 0.15159968757629394), (0.432, 0.15122806084156037), (0.43325, 0.15526186949014664)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           2       0.41      0.78      0.54      1000
           3       0.00      0.00      0.00      1000
           6       0.45      0.95      0.62      1000

    accuracy                           0.43      4000
   macro avg       0.22      0.43      0.29      4000
weighted avg       0.22      0.43      0.29      4000

Collaboration_DC_3
VAL: 
[(0.255, 0.044730136394500734), (0.45, 0.0864397220313549), (0.465, 0.09229815077781678), (0.477, 0.1226446467190981), (0.477, 0.1584977867305279), (0.481, 0.19356412656605243), (0.477, 0.22081087283417583), (0.479, 0.23137943388335408), (0.48, 0.2418985731303692), (0.479, 0.272056638228707), (0.48, 0.26264312120061367), (0.482, 0.2944188005393371), (0.479, 0.30957024714211), (0.481, 0.29623782690428196), (0.48, 0.33085318251885476), (0.48, 0.2929052599389106), (0.48, 0.2862556876782328), (0.481, 0.3046746424487792), (0.483, 0.2952754307272844), (0.481, 0.28259778969362376), (0.483, 0.2995556022464298), (0.476, 0.26955703370226547), (0.477, 0.27026765277329834), (0.473, 0.25375326273590326), (0.482, 0.2668396622276632), (0.479, 0.27046509309951217), (0.48, 0.2561888306851033), (0.478, 0.28590692841639975), (0.479, 0.24841907145967707), (0.479, 0.2867338064447977), (0.48, 0.30846444813429846), (0.48, 0.22741691055963748), (0.48, 0.2734421260749223), (0.478, 0.27212514989008196), (0.479, 0.2726662370691774), (0.48, 0.284060621180397), (0.484, 0.28851999365817754), (0.479, 0.3057359377421672), (0.48, 0.31454061074194034), (0.48, 0.2722050162440282), (0.478, 0.3103650416455057), (0.479, 0.30618507498688996), (0.479, 0.26465610005822965), (0.481, 0.33495537763106403), (0.479, 0.27206729354267006), (0.477, 0.2803167977553094), (0.48, 0.32047755639159003), (0.48, 0.2504834563981276), (0.483, 0.3071470771608874), (0.481, 0.3207766140225867), (0.484, 0.30778514794283546)]
TEST: 
[(0.2535, 0.04368370050191879), (0.4385, 0.08277494287490844), (0.46125, 0.08807962861657143), (0.47575, 0.11717280966043472), (0.48025, 0.15088662540912628), (0.47625, 0.1845778265595436), (0.48075, 0.20741392266750336), (0.48275, 0.2172055230140686), (0.48175, 0.22952548241615295), (0.481, 0.25865569376945496), (0.47825, 0.2520137227773666), (0.482, 0.2849697543382645), (0.481, 0.2955157477855682), (0.4785, 0.2878939779996872), (0.48025, 0.3175454691648483), (0.4845, 0.28054094564914706), (0.48125, 0.27558190023899076), (0.48325, 0.2896598912477493), (0.4825, 0.28278799045085906), (0.481, 0.2723062419891357), (0.48475, 0.28480437004566195), (0.48125, 0.2555406354665756), (0.4845, 0.25686412370204925), (0.4765, 0.2391361665725708), (0.48325, 0.255457505941391), (0.48475, 0.25727881050109863), (0.48225, 0.23915355026721954), (0.485, 0.26896396148204804), (0.48475, 0.23625653755664824), (0.48275, 0.2717379746437073), (0.48125, 0.29430006778240203), (0.484, 0.21633311927318574), (0.48325, 0.2639649866819382), (0.48525, 0.2587325454950333), (0.4835, 0.26771376574039457), (0.48425, 0.27195253801345826), (0.48475, 0.27719060838222503), (0.48225, 0.2919961951971054), (0.48425, 0.3056658253669739), (0.483, 0.2577540426254272), (0.48175, 0.29874016892910005), (0.48275, 0.29452008867263796), (0.4825, 0.25137613475322723), (0.484, 0.3207351449728012), (0.48375, 0.2567809089422226), (0.4845, 0.26610978710651395), (0.48125, 0.30214607834815976), (0.4815, 0.23640595734119416), (0.48375, 0.29145409440994263), (0.4815, 0.30719572710990906), (0.4835, 0.2946901397705078)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           3       0.00      0.00      0.00      1000
           7       0.52      0.95      0.67      1000
           8       0.46      0.98      0.62      1000

    accuracy                           0.48      4000
   macro avg       0.24      0.48      0.32      4000
weighted avg       0.24      0.48      0.32      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [95]
name: alliance-2-dcs-95
score_metric: contrloss
aggregation: <function fed_avg at 0x7b2c24edbc10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=95
Partitioning data
[[2, 3, 8, 9], [0, 1, 8, 9], [4, 6, 8, 9], [5, 7, 8, 9]]
[(array([46451,  2561, 21623, 18088, 24694, 25512, 35140, 16671, 31785,
       37613, 18952, 26960, 37745, 26051,  8019, 41312, 44181, 19057,
       28834, 28377, 33205,  4293, 43484, 25415, 24165, 33699, 13141,
       33849, 36141,  7008, 21765,  2993,  4008, 26765, 29673, 35157,
       11882, 33204, 43422, 29048,  1670, 35466,  4426, 17883, 12140,
       30849, 25626, 31704, 18477, 27191,  4109, 30690, 37972, 19039,
       25982, 12992, 22618, 17188,  9629, 18496, 12782,   288,  6628,
        8234,  4888, 31677, 48501, 20285, 47632, 47846, 31809,  5686,
       28934, 37247, 21859, 47563, 43545,  9386, 41362,  2702, 48404,
        3673, 22307, 29779, 11385,  5384, 39594, 13071, 41763, 19950,
       12871, 18586, 30680, 34608, 22864, 37280, 27484,  6230,  3068,
       20864, 12346, 16906, 44340, 36883, 24199, 36726, 32257, 19694,
        2193, 44053,  3331, 14926, 46907, 32106, 39339,  6583, 16747,
       11089, 43476, 43503, 15589, 44542, 33621, 45359, 31428, 28443,
        6836,  9038, 19359,  2393,  2693, 19811, 30239, 18888, 37811,
       23642, 43961,  8871,  2033, 40716, 30466, 36450, 42328, 48784,
       24773, 38434, 33535,  8485, 20490, 21168, 27131, 21446, 18101,
        6944, 11486, 30572,  3786, 11171,  8165, 47727,  3253, 19163,
        9827, 21819, 38565, 37015,  7363, 34130, 40965, 42605, 22931,
         849, 42894, 22275,  7178,  3814, 30110, 45533, 33851,  2535,
       13300, 46457,  7009, 39675, 17329,  3338, 30096, 26265, 20563,
        2093, 36469, 34219, 28281, 27973, 32956, 26730, 41604,  8219,
       32457, 12287, 21746, 40849,  9740, 36622, 18138,  5219,  3791,
         586, 46887, 10767,  1676, 17273, 20570,  6069, 37198, 29354,
       46837,    18,  5012, 30454,  4604, 14472, 16593, 37848, 40583,
        7410, 49700,  6305,  2041, 47273,  9896, 13985, 45131, 26001,
       18234, 40115, 36860, 18485, 27499, 46516, 28158, 49725,  5151,
       43738, 39051,  9678, 40480,   924, 17267,  1050, 30013,  2521,
       36431, 26797,  1553, 41117, 44322, 49819, 11960, 29478,   861,
       31676, 34334, 38449,  5945, 30754,  6135,    21, 35525, 18942,
       32215, 13204,  8104, 22273, 42504, 38068, 21103, 13167, 46496,
       44635, 29607,  6847, 36990,  1496, 26485, 26126, 22914, 26675,
       41446, 46561, 46109, 47075, 32813,  6492, 14032, 34042, 17661,
       42133,    91,   806, 48957, 40852, 37410,  4628, 40086, 34379,
       20593, 17831,  6208, 42646,  8255,  7065, 43017, 36772, 28808,
       31460, 37399, 41116, 32371, 20704, 43773,  9564,   774,  9152,
       47410, 14941, 31380, 17047,  5733, 22278, 20425, 48614,  4010,
       47067,  7584, 37095,  1070, 41323, 32914,  7075, 28423, 49982,
       27525, 46290,  7872, 16904, 40226, 47439, 41993, 11359, 39095,
       44267,  9305, 47217, 20770, 25649,  3081, 19580,  7128, 10575,
        4982, 30476,  6556, 30571, 39552, 49233,  5445, 24022, 15154,
       27090, 10135, 25705, 12639, 16131, 20639, 27628,  7213, 33964,
        7180,  6782, 20330, 25218, 15836, 15388,  9107, 42354,  2930,
       48798, 45850, 48561, 17155, 41511, 28712, 24903, 35215, 25492,
       30122,  2562, 39543, 34649, 32520,  5423,    26, 10137, 49443,
       23417, 20163, 49563,  5601, 45053, 42783, 17545, 41493,  2903,
       17912,  1965, 16932, 39325, 14264, 29082, 49463, 42192,  7086,
        9930, 35307, 26598, 34710, 25953, 44973, 18259, 30270, 38066,
       24775, 11564, 48262, 16635, 24949,  6185, 38444, 43438, 13093,
        5534, 21712, 22226, 44530, 19577, 12241,  2525,    33, 38679,
       41053, 15411, 12155, 12076,   494, 21373, 13813,  8288, 44276,
       19727, 42445, 15967, 32379, 35293, 16754, 47014, 17430,  2011,
       46121, 47970, 27250, 23763, 42122, 40985, 33642, 30960, 33915,
       16308, 31974, 44868,  2581,  6433, 48494,   342, 12758, 31204,
        1864,  2565, 37204, 43257,  7389, 25443, 38149, 14232, 49835,
       14419, 37562, 18233, 46424, 15974, 15618, 22682, 38835, 23736,
       10848, 47380,  4292, 19768, 28782, 14369, 29647, 10876, 15237,
       43408, 14804, 13633, 21165,  8120,  8057,  2732, 16820, 16676,
       47430, 27728, 49852, 38754, 22696, 38666, 20650, 32887,  3516,
       47635, 49446, 10245, 31340, 38239, 25374,  1087, 16304,  5807,
       35115, 29371, 30745, 11676,  1957, 11149, 14464, 15835, 30990,
       25096, 47894, 40389, 43344, 34387, 34743,  2325, 19131, 42050,
       48552, 24085, 17418, 11180, 47259, 47202, 16473, 32421, 47094,
        5073,  8919, 26925, 27958, 22492,  7508, 34093, 14969, 35361,
       32644,  8429, 19979,  8892, 31183, 35671, 34641, 44373,  6909,
       33005, 29674, 35730,  6948,  4380,  9188, 38443, 36803, 38162,
       40780, 15046, 23405, 18892, 39114, 36560, 11693, 31332, 36405,
        2229, 28435, 49679, 24608, 13641, 36670, 32051, 42515, 44458,
        5949, 10445, 16457,  6397, 43100, 14497, 47971, 21919,  3980,
       42135, 14790, 44681, 10088, 26188,   410, 34471,  9651, 42371,
       32876, 45045, 41753, 23960,  2817,  2018, 11886, 31732,  9743,
        5297, 42203, 16257, 32760, 37953, 30438,  2534,  3832, 12418,
        2798, 23672, 21451, 14010,  3380, 40990, 20461, 46737,  6773,
       25093, 34814, 33869, 11309, 44236, 47291, 30164, 37072, 27842,
       17653, 48393, 42236, 24878,  9684, 42389,   192,  3359, 20620,
       35995, 26981, 12125, 11345, 29903,  8471, 21172, 21812, 13565,
       39627, 44649, 38881,  6830, 45718, 21457, 42693, 26105, 38817,
       18122,  1688, 24185, 36985, 11266, 33992, 30018,  5732, 29476,
       37641,  7876, 34296, 19081, 26579, 17962, 45830, 12951,  8031,
       35540, 11042,  2743, 19648,  2918, 23534, 14832, 40500, 29316,
       46386, 15239, 38436, 20228, 22920,  9527,  2298, 26149, 18660,
        8526, 48562,  3310, 32676, 15780, 40136, 36742,  3753, 47181,
       20596, 31097,  2754, 39278, 26510,  7149, 48611, 24787, 49897,
       23258, 34893, 13874, 31490, 24343,   369, 15752, 33563, 29352,
       35262,  7698, 21690, 26489, 28709, 37139, 37730, 39934, 34943,
        4321, 39856, 24188, 38494, 49997, 44996, 43039, 26384, 44188,
       15652,  1829,  5861, 30056,  9869, 18361, 11677, 49502, 45740,
       12385, 38928, 28065, 17974, 15817, 43910, 32605, 30796, 19625,
       23770, 44871, 33486,  1633, 37780,   706, 10559, 34587, 15857,
        9097, 18789, 38740, 40381, 22614,  2020, 39879, 23397,  9382,
       27791, 34903, 16910, 23569, 26610, 43376,   798, 26875, 33432,
       37499, 20799,  8650, 27237,  2192, 41363, 42638, 48435, 31911,
        5938, 21658,  3179, 27040,  5662, 13669, 16078, 27168, 10118,
       16174, 10309, 13199, 42025, 19674, 18423, 33248, 47700, 17716,
       25009, 12700,  2935, 29334, 24603, 34502, 18778, 40229, 11828,
       37444, 26942,  5591,  1075, 17941, 11388, 39583, 15142, 12718,
       14599, 39984, 39713, 41495,  2845, 34078,  1353,  8743, 48713,
       12948,  9026, 29721, 34811, 23726,  8245, 27420, 38405, 23299,
       41245, 47765, 34586,  7414, 25604,  2761, 15750, 24331, 37314,
       49057, 29820, 14179, 24214,  3381, 26769, 40819, 25897,   668,
       31742, 11111,  6984, 13000, 18860,  8807, 28560, 43341, 18785,
       17968, 21645, 17470, 17139, 35085,  4215, 43619, 11931,  7708,
       46039, 25091, 31039, 21766, 49634,  2495,  9432, 44829, 18337,
       21209, 39876, 15150, 21121, 49601, 28907, 49417, 41267,  4735,
       19353,  6571, 31106, 29186, 26021,  3672,  5310, 47159, 26511,
        3312, 42632,  1987, 17163, 47419, 44155, 38370,  9213, 29148,
       13665, 38644,   768, 38561, 28591, 22196,   188,  1796, 41282,
       44669, 24114, 24182, 44469, 13343,  1262, 19640, 11485,  1652,
       27570, 49072,  3316, 16454,  5275, 31862, 20430, 13919, 23243,
       41503, 21632,  6644, 47084, 30404, 16735, 33593, 12775,  1201,
       32221, 31324, 11678, 22995, 47636, 29043, 25433, 23291, 27695,
       34290]), [2, 3, 8, 9]), (array([32290, 12297, 10399, 24605, 13194, 11963, 30605, 31086, 32220,
       45004, 33249,  1831, 19630, 17456, 47253, 36924, 32190, 39601,
       28515, 33732, 16297,   116, 26412, 14898, 37324, 12294, 20454,
       40649,  9287, 10776, 32703, 43996, 28080, 39396, 20265, 27128,
       40343, 19066, 47220, 49799, 18745,  7444, 34964,  3304,  4746,
       40906, 48257, 12820, 38185, 47404, 22112, 31991,  9696, 28179,
       14486, 10867,  8086, 33919, 40328, 16316, 11509,  1715, 24802,
       28921, 17178, 32857,  4906, 36007, 25615, 40401, 22167, 37841,
       10989, 34279,   983, 48054, 15707, 13382,  5642, 22190,  1147,
       25616, 40418, 20398,  9657,  4411, 40542, 41324, 17395, 47689,
       46046, 22118, 21903, 12139,  8733, 37758, 30837, 45349,  5472,
       26662,  4141, 34123, 21467,  9157, 21000,  6240, 34009, 35758,
       24447, 26495, 19942,  5924, 38581, 17080, 38748, 28653,  9004,
       27431, 24639, 44946, 48059, 41609,  7731,  3842, 20574, 25016,
        6479, 44925, 22724, 44742, 28791, 11643, 18368, 49050, 43564,
        4192, 40890, 49735,  6425, 41266, 35763, 38791, 38326, 24641,
       12037,  4348, 28650, 40830,  1144, 48430, 38683, 11958, 23822,
       40319, 27749, 10787, 20257,  6165,  7801, 29917, 30888, 45088,
       14056, 40746,  1811, 23924, 15949, 24349,  5174, 40245, 42073,
       30609,  4271, 29893, 46137, 46343, 27155, 17432, 11937,  3066,
       35524, 15620, 34537, 13420,   822,  1514, 17303,  6682, 34855,
       37962, 47267, 27375, 49245, 15594,  9373, 41179, 16392, 37265,
       34125, 20816, 22449, 25644, 28216, 13467, 48355, 23854,  8759,
       44975, 28408, 35042,  3918, 44031,  7592,  7965, 24488, 16160,
       10637, 42175, 39012, 34173, 25820, 32263, 44319,  4060,  6008,
       26846, 33999, 11229, 42945, 44875, 45484, 39875,  1338, 41258,
        2435,  9390, 22123,  1270, 16011, 37881, 36415, 30595, 10990,
       31412, 24549,  9619, 40190,   115, 18854, 26311, 33207, 38037,
       25852,  3992,  6515, 42954,  7289,  7019, 27509,  4360, 30103,
       25915,  7102, 19382,  2994, 17534,  2597, 27404, 35070, 38958,
       46993, 18413, 25910, 26637, 18716, 23463, 48322, 33097,  2569,
       14247, 40283, 39828, 29536, 13722, 13852, 23391, 23248, 43847,
       42282, 14048, 17479, 21080,   275, 44818, 41241,  4496,  3851,
       23425, 31251, 11037, 41568, 31079, 17553, 43655,  7442, 17701,
       16813,  2582, 17539, 24068, 23901, 43015, 24762, 46956, 42976,
       11831, 19351, 12150,  6922,  9668, 10276, 18160, 28245, 26467,
       49589, 30486, 42143, 22983, 31338, 44185, 35344, 28822, 20731,
         354, 33939, 36086,   815,  8232, 46140, 31102, 30166,  6895,
       47362, 46213,  3212,   676, 49411, 34720, 31445,  2084, 34249,
        2318, 29765,  3327,  8197, 23198, 26008, 14851, 38333, 47824,
       28098, 25463, 24269, 45016, 26471, 41254, 42758, 28175, 42318,
       28192, 45268, 15867,  8350,  2180,   330, 13156, 34301, 25105,
       30119, 21593, 21540,  9043, 46173, 12348, 32319, 21992, 24725,
       42697, 33237, 38691, 21003, 28220, 35335, 39405, 11776, 24044,
       12303, 37145,  4464, 29791,  2185, 12523, 16774,  2067, 28810,
        6411,  9889, 23263, 40000, 17205,  9535, 12516, 49435, 47035,
       38086, 40595, 27814,  1377, 24041,  4332, 15779, 31523, 38515,
       22859,  2841, 32226,  8566,  5822,  6356, 40024, 13509, 34721,
       43453, 48791, 46649, 39195, 18583, 46238, 16850, 42834, 39404,
       42307,  8221, 43711, 46731, 31539, 18639, 39805, 29546, 19181,
       32886, 41398, 33252,  4385, 28089,  1574, 19450, 47745, 13088,
       20856, 35531,  5572, 42294, 43859, 32994, 22659, 46240, 10535,
       18890, 35134,  7778, 33569,  4185,  8189, 36542, 43132,  9318,
        3907, 28383, 29143, 29555,  9380, 47453, 29675, 17802, 29252,
       20897, 46461, 12047,  8735, 40394, 23789, 18961,  7352, 15710,
       21270, 44824, 37512, 49354, 42305, 41403, 48168, 39915, 25580,
       42870, 10475,  2464, 17624, 46854,  4848, 40671,  9324, 36267,
       15648, 24039, 19720,   139, 35388, 26863,  7941, 26328, 11141,
       19309, 23031,  6044, 42280,  6379,  2857, 41889, 19413,  4758,
       14891, 24905, 48877,   602,  3039, 40169, 49851, 38322, 46582,
       34094,  9648, 40191, 35416,  8348,  1673, 36579, 48774, 12442,
       25501, 26170,  5360, 21842, 12024, 35345, 11598, 23319, 23267,
       32574, 10171,  4812, 10572, 30035, 26977, 24131, 45062, 39999,
       11577, 19887, 37104, 42521, 31313, 32178, 43913, 31950, 21267,
       39658, 45907, 11529,  7045,  8952, 26369, 31121, 48225, 21596,
        9055, 44888, 18378, 49976, 43400, 24140, 13562, 22849,  9109,
        4304, 36882, 20491, 21830, 21042, 10962, 18840,  3430, 45461,
       45243, 46944,  8544, 13582,  8982,   460, 15011, 16942, 29786,
       36628, 27241, 11674, 29055, 30472, 39590, 42810, 40192,  3518,
       25002,  1671, 32896, 27056,  9326, 25176, 17168, 44205, 20530,
       11010, 32322, 43409, 49967,  4575, 28918, 42162, 13388, 15914,
        5794, 48821,  3898, 21157, 39207, 37659, 20182, 10620, 36892,
       40926,  6004, 15007, 11576,  5948,  3469, 30294,  1391, 21377,
       35864, 21484, 38809, 29287, 46128,  6160,  5787, 47886, 25888,
       34522, 38421, 21036, 27261, 28115, 23399, 35938, 34712, 46048,
       49789, 11265, 14872,  9469, 31644,  9645, 13810, 41727, 46615,
       36874, 26131, 15346, 22064, 37858, 21295,  3170, 43858, 18327,
       20509, 25669, 36636,  5339,  6250, 13015, 23307, 39605, 10483,
       31498, 47628, 49757, 25124, 19547, 21741, 17696,   567, 41042,
       17516,  6192, 49378, 16167,  3067,  7737, 17601, 17327, 41867,
       48286,  7468, 37524, 26856, 36262, 32014, 15276,  9488,  7104,
         860, 34894, 19020, 21608, 35917,  4066, 27692, 31456, 41059,
       12277, 17705, 21823, 33078, 35894, 25364, 39298, 47376, 26694,
       40368, 35736, 18236,   915,  4603, 10829, 30276, 40735, 27524,
       38777, 21883, 15901, 29169,  7793, 36456, 34796, 26152, 17240,
       13609,  4371, 46629, 47663, 25960, 48949, 24234, 28285, 47511,
        6178, 39453,  5519, 11717, 23444, 17765, 19601, 23229,  5801,
        3407, 21537, 33786, 23749, 34538, 18943, 24615, 33930, 17435,
       48827,  2529, 46833, 43121,  3671, 13586,  5013,   225, 13024,
        5332, 18947, 27622, 26597, 22152, 12182, 37993, 32595,  4665,
       34483, 21866, 39801,  8428,  8517,  8575, 29514, 14695, 20187,
       42681, 22296,  7457,  4701, 19377,  9511,  1223, 41905, 43281,
       40068, 23287, 20943, 41348,  6152, 19593, 13705, 16744, 14100,
       28046, 26447, 48051,  7691, 26508,  5671, 37141, 30412,  7042,
       33900, 29094, 41640, 35498, 19661, 22433, 13577,  4289, 37737,
       47690, 21065, 14077, 29013,  9106,  2379, 38056, 17731, 19289,
        8811, 27439,  6286, 19410, 46345, 29609,  1034, 31638, 27198,
       25883, 32841, 37490, 33668, 10421, 32778, 49238, 40157, 11104,
        1706, 38602, 29335, 42387, 17201, 33671, 21555, 11213,  8951,
         360, 30073, 44157, 10071,  1767, 35330, 38516, 32717, 24152,
        6038, 27566, 31869, 32163, 46298,   657, 40742, 37538, 48743,
       28959, 10533, 15027, 48868, 42278,  7746, 13148, 41634, 32158,
       33856, 43377, 35206, 35961, 10859, 37454, 17406, 21648, 29349,
       35047,   666,  3023, 49694, 37540, 46774, 27675,  3923,  4587,
       47637, 37031,  3697, 36401,  3097, 35657, 20472,  2351, 34158,
       30179, 33639, 40083,  9739,  8159, 29938,  5978, 47944,  9244,
       27588, 48734, 14626, 27589,  9461,  4704, 33839, 13904, 36288,
       35138, 20925,  1774,  8644,  4252, 46656, 16909, 25001, 30678,
       12383, 11533, 37535, 16347, 28166, 40722,   884, 21583, 45860,
       47083, 27997, 17021, 11373, 46781,  6171, 42458, 30215,  3643,
       33982, 39522, 37600, 47448,  2437, 10252, 13889, 48751,  8126,
       31115]), [0, 1, 8, 9]), (array([24658,  3862,  9010,   693, 29770,  6021, 16461, 25898, 18340,
        6527, 37075,  6586, 21753,  2691, 41953, 40195, 39146, 39656,
       46415, 21838, 41770, 14996, 18559, 33434, 39009, 49727, 39772,
       31349,  5940, 15656, 14815, 23455, 42403, 29663,  3587, 42512,
       35624, 25949, 29466, 17033, 45773,  6924,  4053, 43683, 14415,
       30815, 48478, 15503, 41023,  7574, 28489, 23029, 39740, 26653,
       18099, 16254,  5289, 17585, 25441, 38995, 12179, 27853, 20575,
       41518, 36142, 46153, 39878, 40510, 27923,  7407, 38353, 35113,
       28226, 15582, 44473,  9041, 33215, 36960, 25017, 30907, 30911,
        2712, 12103, 26602, 13186, 22536, 20810, 33027, 29629, 25803,
       17937, 33223,  3690, 46526, 20409,  4296, 41305, 33080, 18699,
       25529, 28040,   930, 33735, 23420, 36225, 42303, 21978, 15737,
       24769, 17632,  2327, 38123, 39177, 22750, 20798, 36003, 18279,
         434, 28803, 28654,  3252, 12334, 43222,  1256, 23460, 41003,
       25992, 13412, 18690, 15802, 15428, 42544, 20782,   951, 27455,
       39209, 33501, 45571, 12416,  1158, 18293, 41822,  3089,  3687,
       42695,  3776, 47126, 28334, 20608,  7404, 19627, 44780, 24967,
       22318, 10150, 23057,  4763, 10661, 47716, 44558, 14704, 21341,
       49595, 34365, 27466, 26894, 37827, 31290, 18734,  7775,  4660,
       41699,  2830, 44760, 35634, 43793, 40028, 35250, 13025, 22018,
       34051, 30522, 14181, 32786, 30790, 48812,  1348, 19098, 38660,
       27860, 16195,  5619, 27193, 22361, 18566, 38852, 31143, 30499,
        7382, 25785, 26844, 14937, 44978, 45512, 18836, 25941, 45586,
       38386, 10350, 38103,  5491,  5347, 28100, 17728, 24883, 40665,
       24265,  8523, 37281, 37124, 35180,  8404, 19833, 49597, 36923,
         677,  2216, 17613,  4043,  6475,   543, 25834, 10561, 10970,
       25620,  2155, 23743, 31470,  8131, 25147, 46644,   925, 33266,
       11586, 46416, 29102, 35429, 30225, 31840,  6844, 21155, 31301,
       24242, 38698, 49412, 33953,  8474, 28925, 32859, 31792, 29751,
       16367, 24531, 31270, 19511,  3048, 40041, 31984, 33630,   854,
       21435, 17562, 39680, 19308, 42679, 26197, 28607, 43269, 27346,
         204, 45918,  1342, 36922, 16687, 38771, 16685, 49771, 36913,
       15830, 18282, 35535, 37270, 38061, 31135, 10792,  7025, 32880,
       37940, 11602, 31672, 46271, 21072, 36069, 32869,  8603, 48897,
       31570, 29284,   645, 36602,  9256, 18988, 14864,  9978,  2774,
       42391, 20146, 41916,  6582,  7034, 25892,   488, 45443, 38508,
        3262, 22746,  4824, 37380, 29217, 40396, 31080, 31928, 15497,
       17847, 21146, 23154, 48619, 17427, 41969,  7524, 14065, 49285,
       44590, 10704, 32648, 35891,  3266, 38738, 14188, 11751, 22290,
         103, 44194, 49364, 28776, 26190, 43026,  4926, 33967, 27022,
       38475,  9029, 20293,  9521,  1842, 25964, 23780,  8873, 23050,
        8564, 26387,  4751, 15962, 31508, 25511, 14443, 48400, 34033,
       31312, 14984, 35455,  9381, 33425, 49129, 18396,  4550, 36164,
       10649, 49798, 36232, 44018, 48401, 23230, 26335, 41668,   680,
       30350, 14638,   132, 43182, 47792, 35225, 36373, 16789, 26192,
       39415, 46991, 34805, 16389, 36638, 19870, 13619, 33733, 27688,
        8460, 24298, 22082, 11634, 29009,   655, 31852, 16998,  9953,
       48277,  7264, 26415, 34926, 16287, 38583, 49928, 12459, 23896,
       20853, 24732, 28663, 10600, 19665, 19281,  9924, 46622,  2650,
       19420, 40313, 20025, 29327, 13878, 24424, 23917, 36515, 16562,
       41558, 23216, 14513, 11911, 30089, 14411, 38665, 42641, 23539,
       18224, 41272, 49061, 39544, 17342, 21264, 18090, 27830,  7422,
       42145, 42724,  8956, 28490, 46995, 25835, 13291, 15039,  5443,
        4570, 32958, 23135, 38773, 15498,   437, 20687,  4752,  2925,
        7959,  5474,  9323, 37457, 49404,  3698, 10117, 43074,  9706,
       21719, 10444, 27450,  3567, 21192, 21972, 31652, 44984, 22471,
       18693, 24414, 47021, 29446, 44685, 12426,  2670, 20727, 36371,
       48911, 29175,  2923, 15795, 36390, 49358, 44659, 15676,   888,
        5245, 14823, 46636, 49712, 29530,  1300, 12867, 30639, 19817,
       16435, 33145, 33029, 49389, 13684, 24860, 19618,  1822, 16572,
       24286,  4507,  1174,  1032,  7484, 16147, 10810,  4892, 28295,
       49543, 48164, 29344, 33537, 10864,  9693, 10923, 46029, 19123,
       13115, 23321, 36856, 20298, 35078, 42571, 21546,   870, 27621,
       12206, 27057,  9674, 41314, 19167, 46787,  7534, 47960, 19658,
        9483, 15241, 26214, 18959, 40344, 48127, 34582,   987,  1138,
       48281, 46008, 44438, 28537,  7012, 31872, 18141, 48133, 21164,
        1375, 19219, 44807,  4611, 43612, 29661, 12041, 33372, 11591,
       42202, 34683, 31745, 40408, 49778, 31085, 21515, 30647, 39776,
       16902, 21679, 27124, 43623,  2471, 10524, 39702, 13197, 18125,
       43713, 28287, 18797, 43357, 10511, 23080, 47694, 46985, 22583,
        5643,   892, 47562,  3765, 10068, 40429, 16156, 32611, 37044,
       36949, 17935, 14123, 21427, 19733, 35171,  1236, 33756, 39072,
       21312, 30504, 36942, 32726, 14901, 38625, 17783,   244, 36214,
       26004, 26402, 32351, 15416,  7614, 19605, 28706, 22760, 16084,
        3330,  4210, 38274, 41002, 34212, 28204, 21611, 47854, 12207,
       32265, 15022,  9913, 38619,  4469, 19214, 26938, 35912,  7329,
       49501, 34104,  7310, 41507, 41674,  7281,  2220, 12725, 10975,
        9160, 40110,  4988, 31069, 17685, 23567, 32037,  8766, 12258,
        4916, 26454, 15617, 26608,  6418, 17020, 26239, 28642,  2272,
        2974, 48905,   170, 39956, 23669, 34004, 10817, 25399, 49568,
       18700,  1014, 35536, 12487, 47019, 20786,   795, 22551,  8926,
       38089, 10997, 14046, 10016, 39094, 32234,  1732, 17891, 16532,
       34815, 40179, 16620, 19464, 38008, 42677, 13139, 23765,  9796,
       28012, 30529, 32622, 26209, 24408, 29990, 17404, 29565, 14458,
       46955, 18000, 22898, 48140, 43946, 14510, 32851, 37126, 31014,
       27146, 16302, 13963, 22925, 20765,  7461, 12816,   756, 39330,
       44511, 16317, 24066,  4136,  9358,  1378, 43787, 47876, 44919,
       29972, 15694, 10312, 29926, 12085, 33028, 49793, 21738, 35625,
        8838, 21562, 16601,  2508, 46190, 33806, 33896, 22637, 13358,
       31845, 44410, 30733,  4340, 46269, 33149, 43137, 42764,  2853,
       12008,  2956, 47124,  4131, 19801, 17918, 21219, 41128,  3090,
       44723, 28318,  4169,  1744, 28601, 33208, 18786, 46687, 41409,
       39743,  6784, 15085, 37427, 24372, 19424, 16255, 12835, 37433,
       15337, 37793,  4115,  2199, 24055, 38300,  3129, 16480,  9911,
       10351, 37071, 17798, 42560, 27380, 21129,  5203, 44380, 12902,
        3876, 44056, 18244,  5637, 12405, 46852, 23284, 45993, 49931,
       25051,  5172, 40507,  4541, 40315, 31089, 46861, 29472,   749,
        8850, 49065, 36381,  8539, 30823, 44332, 30392, 11232, 15096,
       16362,  1277, 49662, 21221, 35521, 40750, 29073, 41396,  3479,
        2548, 41395, 36837, 23341, 45755, 16373, 16923, 36875,  3477,
       22150, 23550, 17313, 29418, 35974,    14, 28534, 33858, 22220,
       46090, 22694, 28572,  1638, 25991, 19824, 13767, 45431, 20995,
       19709, 23647,  7920, 42554, 24133,  9211, 49461, 17042,  3705,
       10144, 28879, 19122, 33794,  2001, 41906, 27365, 41126,  2767,
        4111, 10257, 46186, 34645, 10827, 48442, 39894, 26627,  6834,
       14083, 37201, 43942, 36640,  9036, 49945, 42406, 16092, 40885,
       42343, 24731, 47296, 38687, 36866, 23471, 38153, 28781, 14170,
       38324, 16725, 17176, 16917, 21817, 44289, 43696, 45183,  4028,
       17869, 48461,  3793, 30615, 30985, 39183, 49520, 22346,  4223,
       37363, 48178, 14220, 23303, 24391,  8762, 42349, 23403, 15690,
       42213, 39955, 29384, 18488, 20462, 48033, 37132, 17973, 10208,
       35618]), [4, 6, 8, 9]), (array([18202, 17830, 41371,  4579, 25845, 41499, 38106,  6723, 19999,
       10715, 11826, 47524, 17931, 39475, 35543,  5108, 37364, 34846,
       18921, 30290, 27892,  4002, 47659, 18245, 30698, 29077, 21318,
       25606, 39381, 16436, 10597,  1072, 37632, 25886, 33259,  3060,
       38860, 49541, 29615, 40657, 36350, 47977, 24909, 46422, 20809,
       19071, 26700, 33878,  7750,  8756, 15527, 17375,  1274,  8591,
        9999, 49118, 31409,  2305, 34188, 39229,  2013, 43330,  4009,
       19024, 42745,  7406, 33800, 46393, 19362, 11185, 18521, 24585,
       49508, 41680, 11328, 30099, 45417,  4463,  4692, 41034,  1152,
       47158,  1993, 12198, 29321, 10242, 31038, 22470, 18985, 25065,
       14184, 21879, 27947, 14246, 22225, 12728, 44756, 24380, 47783,
       24663, 33629, 19096, 18626, 31059, 29040, 21179, 39403, 46903,
        5100, 48882, 42293, 48736, 18059, 21077, 34790, 24703, 22021,
       20484, 10549,  1588, 26560, 24309, 17921, 17354,  8407,  1111,
       32427, 21678, 39456, 46289, 38807, 11043, 26735, 32877, 48673,
       12839, 27828, 25284, 11145, 13190, 41475, 22671,  5613, 15488,
       45440, 47090, 35155, 20747, 38305,  1662, 44388, 22430, 15549,
       40044, 30140, 33989, 34444, 18721, 46980, 35547, 18598, 45674,
       27835, 27804, 44223, 30684, 37568, 44284, 32862, 40538, 29251,
        9772, 13142, 33172, 32835, 26917, 47485, 19925, 25919,  5175,
       27805, 16798, 15312, 20315, 30085, 44337, 32764, 22590, 40014,
        8928, 38839, 41817, 44377, 18685,  4316, 43543, 49267,  4662,
        2059,  9132, 27401, 19595, 10831, 19757, 18212, 47160, 36532,
        3794, 11507,  3714, 30718, 35510, 26010, 15301, 31129, 46716,
       27885, 45831, 32935, 37396, 29890, 20207, 35819, 44496, 44792,
       44676, 45181, 18966, 39718, 36471, 22174, 11165, 10104,    83,
        2231, 44988, 26313, 11603, 21502, 39647,  3022, 11610, 20348,
       14391, 12716, 38172, 35652,  9235, 37795, 21233, 22647, 44098,
       45147, 47308, 10384, 22626, 39767,  8187, 23988,  3279, 35623,
        2201, 42955, 34900, 32343,  1896, 38890,  5295, 36303, 14503,
        2397, 20099, 45596, 35005,  3272, 21100, 39717, 40026, 22115,
       34733, 26562, 33585, 26702, 35698, 42078, 36270, 14400, 11278,
       28152, 18823, 20565, 42126,  1931, 11697, 31871, 34010, 38252,
       18258, 13409, 29886,  1582,  3261,  9560, 10905, 24237, 40912,
       40744,   492,  6271,  7874,  9719, 46117, 17929, 34412, 41390,
       37800,  6808, 17503,  4396, 49048, 20903, 21244, 29733, 38780,
       19391,  5047,  2140, 29727, 30617,  4480, 36608, 19064, 27161,
       28884, 43360,   113, 48837, 18869, 37601,  2008, 20998, 25617,
       28553, 44076, 40426,  1149, 45945, 32487, 16355,  4264, 18914,
        2813, 34291,   739, 29622, 32399, 27381, 17804, 19119, 48823,
       48741,  1891, 26358, 32815,  5241, 47923,  5564, 24853, 10735,
         191, 45561, 49310,  6821, 10496, 45058, 47246, 15543, 49912,
       25968,  5126,  9184, 14837, 45540,  4736, 47864, 10994, 22198,
        8115, 47346,  9443, 24439,  8038, 46671, 12759, 16198, 38142,
        8897, 44839, 33652, 22595, 33605, 18219,  9689, 36936, 28735,
       23621,  7613,  4325, 35483, 19263, 21113, 12985, 19940, 31839,
       23873, 25804, 20738,  4898,  9486, 19956, 32579, 17643, 10459,
       30925, 47286,  6460, 22267, 47285, 44664, 42500, 29326, 20707,
       40692, 43066, 29778, 42996, 43544, 21410, 43844,  7611,  7577,
       22739,  3104, 40107, 33636, 34599, 22612, 17944, 16764, 42137,
        6680, 34339, 28240, 38597,  9958, 18524, 10778, 47323,  9082,
       44353, 21599, 46167, 40866, 28977, 28206,  3573, 45610, 27998,
       17398, 34868, 35284, 15889,  8277, 18916, 25811, 27682,  6164,
        7831,  1716, 28914, 33254, 37423, 43587, 17467,  4820, 41416,
       37467, 29137,   595,  2195,  5958, 37190, 48055, 35020, 41973,
       38962, 16881, 45874, 21101, 14356,  7437, 12646, 25513,   106,
       22132, 26623,  8235, 48125,  5185, 19706, 25906,  9975, 34983,
       14011,  5546, 48156, 27612, 43048, 49144,  6176, 48462, 26112,
        1255, 45917, 42917, 30664, 17162, 31903, 18397, 39653, 39317,
       40019, 33190, 32897, 27643, 38607, 12172,  8455, 17519, 36766,
        5365,  9206, 35210, 42265,  8830, 32663, 14624,  3094,  3244,
       24127, 31018,  2483, 27252, 25258, 38904, 43198,  6880, 21257,
       46926, 30941, 19283, 14149, 17799, 11244, 17813, 44357, 31692,
       18171, 26098, 26172, 27068,  9023, 14045, 46102,   901, 49681,
       24740, 19748,  4303, 25566, 24444, 13893, 13623, 18076, 31725,
       14826, 32448, 39462, 28599,  1970, 35150, 20089, 18950, 36484,
       17022, 29556, 44490,  3614, 35586, 28829, 26182, 27618, 37420,
       45784, 35169, 28929, 41732, 40147,   503, 25088, 11977, 48597,
        4823, 15429,  8180,  2954, 48737, 37534, 46395, 25987,  6598,
       44577, 19028, 25728, 32560, 47784, 18312,  7678, 42367, 12490,
       30123, 32765, 10003,  3121, 10408, 16430, 47646, 30444, 27061,
       44569,  8940, 38689, 15135,  1821, 17933, 26948, 29014, 39015,
       40415,   943, 23827, 36733, 11001,  9159, 13355, 47814,  9567,
       30788, 23731,  3250, 46477, 12422, 33571, 24855, 41413, 21943,
       33661, 17120, 39210,  1782, 16330, 42770, 24588, 22177, 25744,
       28323, 35562, 13095, 33356,  4580,   627, 11461, 23890,  5987,
        4828, 28153,  9844,   609, 36678,  9951, 19342, 16729, 29479,
        2658, 44414, 49955, 25072, 47881, 24629, 38371, 12452,  1506,
       17630, 22010, 27861, 32578,  7334, 28103, 28066,  2721, 39454,
       46699,  2454,  2523, 32506, 35035, 22524, 38901, 40609, 13923,
       30252, 29690, 36253, 22584, 43807, 26703, 38969, 16847,  5283,
       38158, 30514, 20479, 15422, 46028, 25040, 20662, 21483, 48175,
       39142,  3541, 35617, 47793, 41163, 24347,  4029,  4742, 44382,
       44927, 14866, 46068, 27395, 49405, 44010, 18968, 27975, 37606,
       18467, 36309,  9787, 32249, 41920, 45735, 26506, 39993, 34730,
       11457, 28846, 17977, 20248,  7694, 48123, 25036, 40082, 39586,
       25507, 14278, 18166,  1940, 11547, 39746, 49158, 40923, 18030,
       45465, 39427, 23091, 34772,  6288, 40059, 20176, 29909, 31662,
       32273,    76, 27005, 45829, 36118, 18792, 34573, 27603, 24537,
       19563, 46280, 21954, 30343, 26650, 12653, 39841, 13717, 25988,
       44066, 36521, 49637, 43199,  8567, 41765, 29401, 42454, 45197,
       33739, 10306,   214, 45388, 49398, 32146,  3006, 21507, 47656,
        8402, 46024, 34251, 14564, 10246,  9261, 42452,  8788,  6708,
       30247, 11984, 17296, 15159, 47002, 27575, 17371, 31056, 44212,
       21592, 25994,  5071,  5352, 46243, 45241, 31815, 34175, 36475,
       25285, 12531, 49278, 49244,  1773, 49385, 40237, 26013, 40063,
        3345, 21273, 31637, 15420,  9513, 20441,  8565, 11275, 33656,
       38393, 21300, 22233, 25393, 31162,  8075, 48455, 28159, 46830,
        2757, 39781, 10448, 45405, 32115, 34753, 49300, 13125, 22915,
       23464, 39564, 49453, 34077, 45167, 31483, 21182, 36266,  2477,
        2218, 13531,  5191, 26970, 20069, 32040, 32940, 41786,   541,
       34957, 21010, 30178, 14610, 16293,  8896,  1026, 19248,   306,
       33316, 21542, 45577,  3597,  9683, 42556, 47592,  8821, 22815,
       28888, 29574, 29529, 43325, 18642,  5056, 49125, 35288, 25506,
       46788,   219, 35178, 17052, 27784, 25338,  9971, 49074, 15597,
        7703, 16051, 20013, 40222, 28656,  1602, 20600, 43882, 17336,
       43900,  8394,  1456, 26218, 44810, 23961,  3216, 16107, 42140,
       18394, 16728, 39546, 18252, 16631, 46302, 36818, 25907,  8012,
       11720,  4454, 31905,   883, 18989, 31205,  6149, 37751,  4826,
       41415, 15031, 35280, 42714,  3685, 16234, 17061, 29825,  3719,
       49284, 20250,  3526, 10658, 11311, 48766, 24027, 18846, 10187,
       43634]), [5, 7, 8, 9])]
Collaboration
DC 0, val_set_size=1000, COIs=[2, 3, 8, 9], M=tensor([2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.226, 0.04464438664913178)
DC 1, val_set_size=1000, COIs=[0, 1, 8, 9], M=tensor([0, 1, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04425914561748505)
DC 2, val_set_size=1000, COIs=[4, 6, 8, 9], M=tensor([4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.244, 0.04420206964015961)
DC 3, val_set_size=1000, COIs=[5, 7, 8, 9], M=tensor([5, 7, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04452222609519958)
D00: 1000 samples from classes {8, 9}
D01: 1000 samples from classes {8, 9}
D02: 1000 samples from classes {8, 9}
D03: 1000 samples from classes {8, 9}
D04: 1000 samples from classes {8, 9}
D05: 1000 samples from classes {8, 9}
D06: 1000 samples from classes {2, 3}
D07: 1000 samples from classes {2, 3}
D08: 1000 samples from classes {2, 3}
D09: 1000 samples from classes {2, 3}
D010: 1000 samples from classes {2, 3}
D011: 1000 samples from classes {2, 3}
D012: 1000 samples from classes {0, 1}
D013: 1000 samples from classes {0, 1}
D014: 1000 samples from classes {0, 1}
D015: 1000 samples from classes {0, 1}
D016: 1000 samples from classes {0, 1}
D017: 1000 samples from classes {0, 1}
D018: 1000 samples from classes {4, 6}
D019: 1000 samples from classes {4, 6}
D020: 1000 samples from classes {4, 6}
D021: 1000 samples from classes {4, 6}
D022: 1000 samples from classes {4, 6}
D023: 1000 samples from classes {4, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.0756532991528511) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.07162128928303718) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.346, 0.10060960775613785) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.262, 0.09178792214393616) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.08844312906265259) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.08061573424935341) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.407, 0.13195230773091315) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.319, 0.1186123097538948) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.373, 0.09776843130588532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.428, 0.0912200716137886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.15404716596007348) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.14389754155278206) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.382, 0.116108429312706) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.1028368262052536) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2008201377093792) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.19386132979393006) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.383, 0.11486499130725861) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.12005862887203693) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.2020772139430046) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.1812336931824684) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.396, 0.12786139059066773) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.12526907271891832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.411, 0.2141107092946768) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.188261329382658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.391, 0.12239557676017285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1390012750029564) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.363, 0.259074219936505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.19005238442867994) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.397, 0.1381242759525776) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.1493007049560547) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.21411252587661148) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.2096376870945096) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.393, 0.1398952288478613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.16985845598578453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.241391664955765) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.22949583377689123) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.15190104061365128) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.1669399950876832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2810998134780675) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.2314559705518186) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[8, 9], M=tensor([0, 1, 2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.0, 0.3157558107376099)
DC Expert-0, val_set_size=500, COIs=[2, 3], M=tensor([2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.822, 0.014769307374954223)
DC Expert-1, val_set_size=500, COIs=[0, 1], M=tensor([0, 1, 8, 9], device='cuda:0'), Initial Performance: (0.922, 0.006159715548157692)
SUPER-DC 0, val_set_size=1000, COIs=[2, 3, 8, 9], M=tensor([2, 3, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[0, 1, 8, 9], M=tensor([0, 1, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7b2c0846e1c0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b2c08194cd0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b2c14087ac0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b2c081ee490>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b2c08597b50>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO2', '(DO0']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.015144353300333023) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.0060443423837423325) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2520921399043873) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.23817335342988372) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.892, 0.009544476984068751) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.037132113963365555) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.553, 0.04083089232444763) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.01523147639632225) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.004911109991371632) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.406, 0.23947266311384738) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.23134526911750436) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.898, 0.009378171794116498) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.575, 0.045515879690647125) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.601, 0.04090243119001388) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.013801711559295654) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.92, 0.006857586289756) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.405, 0.25086985405161977) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.2697769595272839) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.88, 0.014590404748916626) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.06466335919499397) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.05860705643892288) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.015499027818441391) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.005399913223460316) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.20674306693673133) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.2663009179290384) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.859, 0.02089803721103817) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.457, 0.09528089845925569) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.0831944510191679) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.01548401415348053) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.005852301225066185) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.19640120197832583) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.2816378436665982) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.905, 0.015387668061070144) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.10619064901024103) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.08800161688774824) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO1', '(DO0']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.014842849612236022) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.007693145086988807) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.2242064044876024) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.23541474130377174) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.89, 0.009853978896513581) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.512, 0.07225667342543601) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.59, 0.04805659759044647) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.016816523790359496) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.92, 0.008051470885053277) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.20986750034801663) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.2621549252066761) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.907, 0.012284287401940673) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.513, 0.07575820463895798) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.547, 0.06258057963848114) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.82, 0.016643803983926772) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.0065830485709011555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.20351063143275677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.2899568679425865) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.908, 0.00802864294499159) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.06170579059422016) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.551, 0.06557248632609844) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.016930884420871734) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.0061853115353733305) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.19545632499456406) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.31485017242841423) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.01119418615452014) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.492, 0.07533481937646866) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.07094934549182653) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.017192771196365357) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006825849862769246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.20447598098125308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.3172620241250843) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.894, 0.01370917480252683) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.513, 0.06802148224413394) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.06103265295177698) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO4', '(DO1']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.016742925480008126) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006467538267374039) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.21340117949061096) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.2998739472404122) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.919, 0.008201894977129996) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.06146591679751873) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.055786843731999394) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.01615571229159832) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.006912014707922936) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.18694957618042826) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.28809490899555384) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.926, 0.008542277410626412) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.07579111576080322) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.574, 0.057834170449525116) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.017398926340043544) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.005892789177596569) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.1759221824519336) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.2931449057944119) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.911, 0.00999163651606068) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.05959178051352501) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.06830006148666143) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.016400505065917968) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.00739931657910347) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.15110598314367235) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.2846662008073181) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.926, 0.01017987921711756) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.567, 0.0632849200218916) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.564, 0.05712612149119377) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.017818023175001144) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.007307058783248067) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.1655116061354056) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.29564410248957573) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.01042844662477728) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.513, 0.07167283584177495) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.07795644132792949) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO3', '(DO5']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.015923425555229187) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.007248214459978044) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.17667653117515147) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.3483126039337367) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.917, 0.009752926997840404) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.06229613601416349) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.551, 0.060549796417355535) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.018529125794768335) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.009144438398070633) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.17784963866882025) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.31303659817017615) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.012640004804357886) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.08306973376870155) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.09169621632993222) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.017663048803806303) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.007718028403818607) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.15791983966343104) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.2988231377489865) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.011441310905291175) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.075019327275455) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.06918115130811929) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.852, 0.018981559574604033) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.007100797152146697) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.453, 0.16542487598769368) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.3536569001888856) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.01343706511729397) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.056985786072909834) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.533, 0.07092875014990568) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.01833895939588547) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006856323413318023) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.16853917601844295) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.2836633053310215) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.01244808516924968) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.0705766683742404) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.543, 0.07058578219264745) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO0', '(DO4']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.01763900426030159) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005965392334386707) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.17363469420932234) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2857837231531739) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.013152373211611121) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.07472203504107892) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.553, 0.06841898662038147) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.848, 0.018115203380584716) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006591026485897601) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.457, 0.15358865891397) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.30744594299420713) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.014725800367305055) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.553, 0.06745120941847563) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.07388751229643822) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.01618254363536835) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.926, 0.008017345638480038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.17121548350900412) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.2858867790773511) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.888, 0.02135528276019613) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.08025405514240265) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.497, 0.0875028768070042) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.019568678498268127) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.007542435361072421) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.18081441479362548) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.31430505708325657) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.913, 0.012382607920328154) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.582, 0.05273273066431284) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.0866216700077057) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.824, 0.020585530504584313) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.008602775682345964) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.1703469977742061) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.273521457593888) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.907, 0.016985175709442045) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.07076559517532587) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.08065795569680631) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO4', '(DO3']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.021277313590049744) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.008505368802929297) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.463, 0.17414285383373498) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.25823979334719477) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.008573279961710796) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.584, 0.06128604394197464) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.06489925673604012) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.017726612135767938) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.007988713714759796) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.17787731209304183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.3010638317335397) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.906, 0.012294424610387069) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.519, 0.08213130626827478) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.554, 0.06761339297890663) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.018943056151270867) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.009200733430101537) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.18755087356921285) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.30339931179955604) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.914, 0.015313402711268282) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.522, 0.07365597642958165) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.51, 0.07993363883160055) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.018847413644194604) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.009137278829759453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.453, 0.19932784629054368) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.438, 0.28110258232802154) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.918, 0.013755674561740306) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.538, 0.07310447270795703) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.513, 0.09425023585511372) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.01828037616610527) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.007935529092501383) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.20067279726359993) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.32142629026249053) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.897, 0.0183925656368956) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.57, 0.0654707334190607) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.52, 0.07641808614134789) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO2', '(DO4']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.01844456845521927) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.007349091229494661) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.19718933962937443) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.2886843992471695) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.923, 0.011757730539969998) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.559, 0.06774396796524525) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.07599517586175353) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.01878514428436756) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.00786116292886436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.19193145788274704) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.2796178508959711) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.911, 0.011289766419693478) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.59, 0.05900130346417427) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.06340832441300154) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.019928676560521125) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.00706982942391187) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.21196454599360004) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.26792488930374386) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.010502550560086093) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.050416979745030406) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.55, 0.06649295445065945) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.019944945484399797) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.007733939129859209) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.23919919840432705) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.3004286370575428) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.917, 0.012371910558547824) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.06613279958814383) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.06227832178771496) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.020838528737425803) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.00706869044341147) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.2122972808321938) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.3074445856567472) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.898, 0.011697497297078372) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.573, 0.061204913288354874) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.06939143896102905) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO0', '(DO3']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.019870537996292115) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.007398028804454952) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.22247048138361425) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.2859950211942196) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.008509754046797753) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.06203100446611643) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.574, 0.05685606206953526) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.824, 0.017904309272766112) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.007399853619746864) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.22991794631397353) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.29370421512797473) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.011197129662614316) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.08634514001384377) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.553, 0.07106614655256271) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.018961640015244485) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.008180732432054355) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.21442821273766458) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.266909276669845) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.011944471186958253) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.522, 0.08966402271389962) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.08888933509588241) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.018187622174620627) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.007138037156313658) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.24935085910838098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.27470721554383637) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.01034141904162243) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.07401830980181694) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.05801641938835383) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.824, 0.018684277668595314) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.007025142829399556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.22318239524122327) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.438, 0.29843128147348763) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.921, 0.013259710304439069) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.509, 0.0869245560504496) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.519, 0.08534577307105064) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.226, 0.04464438664913178), (0.25, 0.0756532991528511), (0.338, 0.08844312906265259), (0.373, 0.09776843130588532), (0.382, 0.116108429312706), (0.383, 0.11486499130725861), (0.396, 0.12786139059066773), (0.391, 0.12239557676017285), (0.397, 0.1381242759525776), (0.393, 0.1398952288478613), (0.411, 0.15190104061365128), (0.558, 0.037132113963365555), (0.575, 0.045515879690647125), (0.518, 0.06466335919499397), (0.457, 0.09528089845925569), (0.464, 0.10619064901024103), (0.512, 0.07225667342543601), (0.513, 0.07575820463895798), (0.555, 0.06170579059422016), (0.492, 0.07533481937646866), (0.513, 0.06802148224413394), (0.531, 0.06146591679751873), (0.52, 0.07579111576080322), (0.547, 0.05959178051352501), (0.567, 0.0632849200218916), (0.513, 0.07167283584177495), (0.554, 0.06229613601416349), (0.521, 0.08306973376870155), (0.531, 0.075019327275455), (0.555, 0.056985786072909834), (0.531, 0.0705766683742404), (0.54, 0.07472203504107892), (0.553, 0.06745120941847563), (0.528, 0.08025405514240265), (0.582, 0.05273273066431284), (0.539, 0.07076559517532587), (0.584, 0.06128604394197464), (0.519, 0.08213130626827478), (0.522, 0.07365597642958165), (0.538, 0.07310447270795703), (0.57, 0.0654707334190607), (0.559, 0.06774396796524525), (0.59, 0.05900130346417427), (0.592, 0.050416979745030406), (0.563, 0.06613279958814383), (0.573, 0.061204913288354874), (0.549, 0.06203100446611643), (0.517, 0.08634514001384377), (0.522, 0.08966402271389962), (0.533, 0.07401830980181694), (0.509, 0.0869245560504496)]
TEST: 
[(0.236, 0.04354009944200516), (0.25025, 0.07285457700490952), (0.325, 0.08505733188986778), (0.3635, 0.09408129823207856), (0.36725, 0.11148307979106903), (0.381, 0.10974129223823548), (0.39125, 0.12281110650300979), (0.38925, 0.11663438200950622), (0.393, 0.1320787268280983), (0.38825, 0.133778289437294), (0.39625, 0.14531275349855424), (0.5245, 0.03852608820796013), (0.551, 0.04661246295273304), (0.4885, 0.06551904249191284), (0.44575, 0.09763756629824638), (0.4575, 0.1089039272069931), (0.49175, 0.07832258343696594), (0.49725, 0.0786066763550043), (0.545, 0.06322256645560265), (0.487, 0.07925215923786164), (0.51825, 0.06774030248820782), (0.5205, 0.06553325346112251), (0.49875, 0.07933210349082946), (0.5335, 0.06191119369864464), (0.5255, 0.06866648361086845), (0.5035, 0.07204988957941533), (0.53175, 0.06721348048746587), (0.50075, 0.08852734026312828), (0.5215, 0.08028977164626122), (0.545, 0.0622642013579607), (0.523, 0.07345110088586808), (0.52025, 0.07918213468790054), (0.54, 0.0671462287902832), (0.491, 0.08415806835889816), (0.57175, 0.05706138999760151), (0.523, 0.07430216583609581), (0.555, 0.06741017958521843), (0.498, 0.09034256911277772), (0.50375, 0.08206102439761162), (0.5245, 0.07884147027134895), (0.5395, 0.06888663758337497), (0.52425, 0.07316350054740905), (0.55275, 0.062210545435547826), (0.5705, 0.05565408369898796), (0.54775, 0.06850938433408738), (0.53725, 0.06394897638261318), (0.539, 0.06520066767930985), (0.4915, 0.09295650613307953), (0.50575, 0.0971282348036766), (0.519, 0.0778063308596611), (0.498, 0.0952287583053112)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.56      0.16      0.25      1000
           3       0.81      0.10      0.18      1000
           8       0.55      0.85      0.67      1000
           9       0.43      0.88      0.58      1000

    accuracy                           0.50      4000
   macro avg       0.59      0.50      0.42      4000
weighted avg       0.59      0.50      0.42      4000

Collaboration_DC_1
VAL: 
[(0.25, 0.04425914561748505), (0.265, 0.07162128928303718), (0.304, 0.08061573424935341), (0.428, 0.0912200716137886), (0.443, 0.1028368262052536), (0.447, 0.12005862887203693), (0.446, 0.12526907271891832), (0.459, 0.1390012750029564), (0.456, 0.1493007049560547), (0.464, 0.16985845598578453), (0.461, 0.1669399950876832), (0.553, 0.04083089232444763), (0.601, 0.04090243119001388), (0.499, 0.05860705643892288), (0.486, 0.0831944510191679), (0.528, 0.08800161688774824), (0.59, 0.04805659759044647), (0.547, 0.06258057963848114), (0.551, 0.06557248632609844), (0.525, 0.07094934549182653), (0.557, 0.06103265295177698), (0.578, 0.055786843731999394), (0.574, 0.057834170449525116), (0.546, 0.06830006148666143), (0.564, 0.05712612149119377), (0.516, 0.07795644132792949), (0.551, 0.060549796417355535), (0.486, 0.09169621632993222), (0.526, 0.06918115130811929), (0.533, 0.07092875014990568), (0.543, 0.07058578219264745), (0.553, 0.06841898662038147), (0.556, 0.07388751229643822), (0.497, 0.0875028768070042), (0.507, 0.0866216700077057), (0.524, 0.08065795569680631), (0.561, 0.06489925673604012), (0.554, 0.06761339297890663), (0.51, 0.07993363883160055), (0.513, 0.09425023585511372), (0.52, 0.07641808614134789), (0.536, 0.07599517586175353), (0.58, 0.06340832441300154), (0.55, 0.06649295445065945), (0.561, 0.06227832178771496), (0.541, 0.06939143896102905), (0.574, 0.05685606206953526), (0.553, 0.07106614655256271), (0.525, 0.08888933509588241), (0.598, 0.05801641938835383), (0.519, 0.08534577307105064)]
TEST: 
[(0.25, 0.04342002448439598), (0.266, 0.06890167579054833), (0.31225, 0.07728723338246346), (0.42825, 0.08714401468634606), (0.44875, 0.09824884018301963), (0.45125, 0.11498481222987175), (0.44725, 0.11927160546183586), (0.4605, 0.13327452754974364), (0.4625, 0.14331845676898955), (0.4655, 0.1620286877155304), (0.467, 0.16024554711580277), (0.54775, 0.04007662329077721), (0.586, 0.04203880949318409), (0.50375, 0.059629151210188865), (0.478, 0.08438434156775475), (0.5005, 0.0902270196378231), (0.56825, 0.05140506978332996), (0.536, 0.06549999782443047), (0.5455, 0.06803385338187218), (0.51975, 0.07170128962397575), (0.541, 0.06504067128896714), (0.54725, 0.06054491868615151), (0.534, 0.06498155602812768), (0.53175, 0.07302093407511712), (0.556, 0.06420277833938598), (0.50425, 0.08058193680644035), (0.55325, 0.06194983577728271), (0.49125, 0.08966758745908737), (0.53575, 0.06963444527983666), (0.529, 0.07343417128920555), (0.5385, 0.07109653490781784), (0.53725, 0.07120807036757469), (0.533, 0.07613620474934578), (0.5025, 0.09056354638934136), (0.49525, 0.0909643549323082), (0.52, 0.08499245393276214), (0.5405, 0.07158671888709069), (0.53275, 0.07271100920438767), (0.49575, 0.08408102652430534), (0.49725, 0.098336192548275), (0.52225, 0.08047590577602387), (0.53, 0.0770315742790699), (0.5655, 0.06530168294906616), (0.56475, 0.0686086343228817), (0.571, 0.06193002989888191), (0.529, 0.07134293138980866), (0.55875, 0.06324916067719459), (0.539, 0.07642800706624985), (0.51925, 0.08928897082805634), (0.574, 0.06412294527888299), (0.52, 0.08925691995024682)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.68      0.23      0.35      1000
           1       0.80      0.14      0.23      1000
           8       0.53      0.84      0.65      1000
           9       0.46      0.88      0.60      1000

    accuracy                           0.52      4000
   macro avg       0.62      0.52      0.46      4000
weighted avg       0.62      0.52      0.46      4000

Collaboration_DC_2
VAL: 
[(0.244, 0.04420206964015961), (0.346, 0.10060960775613785), (0.407, 0.13195230773091315), (0.432, 0.15404716596007348), (0.437, 0.2008201377093792), (0.371, 0.2020772139430046), (0.411, 0.2141107092946768), (0.363, 0.259074219936505), (0.442, 0.21411252587661148), (0.431, 0.241391664955765), (0.437, 0.2810998134780675), (0.404, 0.2520921399043873), (0.406, 0.23947266311384738), (0.405, 0.25086985405161977), (0.449, 0.20674306693673133), (0.451, 0.19640120197832583), (0.44, 0.2242064044876024), (0.454, 0.20986750034801663), (0.449, 0.20351063143275677), (0.416, 0.19545632499456406), (0.455, 0.20447598098125308), (0.438, 0.21340117949061096), (0.445, 0.18694957618042826), (0.456, 0.1759221824519336), (0.454, 0.15110598314367235), (0.454, 0.1655116061354056), (0.451, 0.17667653117515147), (0.464, 0.17784963866882025), (0.443, 0.15791983966343104), (0.453, 0.16542487598769368), (0.455, 0.16853917601844295), (0.452, 0.17363469420932234), (0.457, 0.15358865891397), (0.461, 0.17121548350900412), (0.44, 0.18081441479362548), (0.45, 0.1703469977742061), (0.463, 0.17414285383373498), (0.443, 0.17787731209304183), (0.445, 0.18755087356921285), (0.453, 0.19932784629054368), (0.455, 0.20067279726359993), (0.448, 0.19718933962937443), (0.447, 0.19193145788274704), (0.449, 0.21196454599360004), (0.444, 0.23919919840432705), (0.446, 0.2122972808321938), (0.449, 0.22247048138361425), (0.447, 0.22991794631397353), (0.447, 0.21442821273766458), (0.447, 0.24935085910838098), (0.443, 0.22318239524122327)]
TEST: 
[(0.2445, 0.04316330614686012), (0.35275, 0.09631808972358703), (0.40675, 0.12594314509630203), (0.4285, 0.14750396132469176), (0.43075, 0.19383546972274782), (0.37875, 0.19488197696208953), (0.4085, 0.20684573304653167), (0.3695, 0.2504873937368393), (0.435, 0.20733263194561005), (0.43075, 0.2325922714471817), (0.43575, 0.27226708149909973), (0.3955, 0.242617280960083), (0.40525, 0.23135315304994583), (0.4095, 0.24534145557880402), (0.44075, 0.20242267978191375), (0.44175, 0.19193326669931413), (0.43725, 0.21601950627565383), (0.44425, 0.20206510549783707), (0.44375, 0.19678362089395524), (0.4195, 0.18953744333982467), (0.4475, 0.19371430945396423), (0.43375, 0.20819357407093048), (0.4375, 0.17996395534276963), (0.449, 0.16782040405273438), (0.44425, 0.14665542250871658), (0.44375, 0.16243783003091813), (0.44775, 0.16675085157155992), (0.4485, 0.1725034966468811), (0.4385, 0.15564610975980758), (0.43975, 0.16077156990766525), (0.44775, 0.16112751775979997), (0.44825, 0.16723437184095383), (0.4435, 0.14895028644800187), (0.45125, 0.16641975408792495), (0.435, 0.1733369207382202), (0.44125, 0.16564975589513778), (0.45225, 0.17072172915935516), (0.433, 0.17495846444368363), (0.4445, 0.18223951190710067), (0.44675, 0.19373660743236543), (0.44925, 0.1953766924738884), (0.43925, 0.19177217239141464), (0.44225, 0.18766573637723924), (0.4435, 0.2046375632882118), (0.43925, 0.23227009201049806), (0.44125, 0.2064401734471321), (0.43825, 0.21643216454982758), (0.44175, 0.22190549975633622), (0.4415, 0.20924126654863356), (0.4365, 0.2399252325296402), (0.4385, 0.21610355335474013)]
DETAILED: 
              precision    recall  f1-score   support

           4       0.34      0.96      0.50      1000
           6       0.69      0.80      0.74      1000
           8       0.00      0.00      0.00      1000
           9       0.00      0.00      0.00      1000

    accuracy                           0.44      4000
   macro avg       0.26      0.44      0.31      4000
weighted avg       0.26      0.44      0.31      4000

Collaboration_DC_3
VAL: 
[(0.25, 0.04452222609519958), (0.262, 0.09178792214393616), (0.319, 0.1186123097538948), (0.397, 0.14389754155278206), (0.436, 0.19386132979393006), (0.439, 0.1812336931824684), (0.441, 0.188261329382658), (0.443, 0.19005238442867994), (0.44, 0.2096376870945096), (0.444, 0.22949583377689123), (0.447, 0.2314559705518186), (0.448, 0.23817335342988372), (0.445, 0.23134526911750436), (0.437, 0.2697769595272839), (0.432, 0.2663009179290384), (0.444, 0.2816378436665982), (0.443, 0.23541474130377174), (0.441, 0.2621549252066761), (0.444, 0.2899568679425865), (0.446, 0.31485017242841423), (0.448, 0.3172620241250843), (0.443, 0.2998739472404122), (0.444, 0.28809490899555384), (0.442, 0.2931449057944119), (0.437, 0.2846662008073181), (0.445, 0.29564410248957573), (0.441, 0.3483126039337367), (0.447, 0.31303659817017615), (0.449, 0.2988231377489865), (0.441, 0.3536569001888856), (0.447, 0.2836633053310215), (0.446, 0.2857837231531739), (0.445, 0.30744594299420713), (0.443, 0.2858867790773511), (0.444, 0.31430505708325657), (0.441, 0.273521457593888), (0.436, 0.25823979334719477), (0.441, 0.3010638317335397), (0.443, 0.30339931179955604), (0.438, 0.28110258232802154), (0.443, 0.32142629026249053), (0.439, 0.2886843992471695), (0.445, 0.2796178508959711), (0.439, 0.26792488930374386), (0.437, 0.3004286370575428), (0.44, 0.3074445856567472), (0.449, 0.2859950211942196), (0.444, 0.29370421512797473), (0.444, 0.266909276669845), (0.444, 0.27470721554383637), (0.438, 0.29843128147348763)]
TEST: 
[(0.25, 0.04352640879154206), (0.2605, 0.0880089095234871), (0.3135, 0.11392097294330597), (0.38675, 0.1381194160580635), (0.41975, 0.18566381484270095), (0.4255, 0.17218308329582213), (0.42525, 0.18001394498348236), (0.42625, 0.17942561841011048), (0.4315, 0.19832437139749526), (0.43375, 0.21836428129673005), (0.42825, 0.22052164554595946), (0.4315, 0.22578497111797333), (0.4275, 0.21563107013702393), (0.42975, 0.24759928834438324), (0.4215, 0.2508199963569641), (0.43375, 0.26843926560878756), (0.432, 0.22277371472120286), (0.4345, 0.2516734759807587), (0.43375, 0.27393142008781435), (0.43825, 0.29893453085422517), (0.4365, 0.30095717871189115), (0.43525, 0.2866172034740448), (0.43325, 0.27518767154216767), (0.44, 0.28273363316059114), (0.4285, 0.2702295323610306), (0.43525, 0.28835180830955504), (0.436, 0.33499854171276094), (0.43775, 0.3051101622581482), (0.4375, 0.292645059466362), (0.4345, 0.3381444642543793), (0.4355, 0.26756572210788726), (0.43325, 0.27328012347221375), (0.43475, 0.2968149555921555), (0.43775, 0.27522570168972016), (0.43775, 0.2981491461992264), (0.433, 0.2620278569459915), (0.43425, 0.24653816962242125), (0.43575, 0.2901065313816071), (0.43775, 0.2935021362304687), (0.4365, 0.2706914083957672), (0.43775, 0.3102780307531357), (0.43325, 0.2745264191627502), (0.437, 0.26497200393676756), (0.44025, 0.2640200309753418), (0.43625, 0.2925124821662903), (0.43575, 0.2912810077667236), (0.43675, 0.2727101277112961), (0.43875, 0.27716378808021547), (0.43225, 0.2516723465919495), (0.435, 0.2700299409627914), (0.43675, 0.28883771800994873)]
DETAILED: 
              precision    recall  f1-score   support

           5       0.40      0.94      0.56      1000
           7       0.49      0.81      0.61      1000
           8       0.00      0.00      0.00      1000
           9       0.00      0.00      0.00      1000

    accuracy                           0.44      4000
   macro avg       0.22      0.44      0.29      4000
weighted avg       0.22      0.44      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [89]
name: alliance-2-dcs-89
score_metric: contrloss
aggregation: <function fed_avg at 0x71c9819f2c10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=89
Partitioning data
[[3, 9, 1, 4], [0, 2, 1, 4], [8, 6, 1, 4], [5, 7, 1, 4]]
[(array([13581, 11353, 30571, 14064, 46596, 26097, 45402,  7200, 17497,
        3134,  2665, 24099, 41137, 17655, 33371, 39773, 48088, 45634,
       31198, 16586, 19455,  8179, 40587, 39868,  3000, 45217,  1966,
       26036,  8408, 29979,  6556,   367, 25315, 33443, 44913, 10209,
       21106, 41513,  7075, 23885, 32075, 21687, 40651, 18814, 45216,
       41725, 43262, 47902, 11805, 33980,  1625,  6198, 39135, 35154,
       19496, 13371, 24847, 43254,  2947, 15753, 27722, 13062, 27735,
       37370, 15706,  5876, 24538, 43744, 42590, 43843, 20932,   806,
       19913, 20469, 39287, 48131, 20770, 26022, 14779, 39980, 13253,
       14091, 12076, 15467,   685, 24348,  6422, 39865,  9332,  8162,
        2565, 19726, 37924, 39187, 45502, 21712, 34689, 43017, 17897,
       39165, 42970, 34999, 13205, 39101, 38879, 20371, 44856, 39398,
       43237, 32541, 36113, 35071, 15080,  1164, 38897, 41951, 39158,
       38444, 43428,  5725, 32404, 36159, 47666,  8434,   995, 13686,
       37546, 10101, 17155, 12872, 30809, 44110, 11047, 40433, 14936,
       20488, 43034, 17333, 49430, 22244,  4197,  7869, 18570, 48592,
        3927, 25640, 23526,  7259,   639, 16190, 27683, 19617, 39461,
       40097, 37057, 21551,  7997, 32938, 26132,  6014, 16364, 18400,
       22505, 19422, 12370, 14580, 13302,  7665, 39289, 14993, 31861,
       24922, 45250,  6368, 42997, 38891, 41639,  4672, 49143, 19631,
       36219, 28237, 25259, 26707,  5493, 14938,  6343, 46155, 40809,
       36124, 26357, 30213,  9024, 10534, 24606, 14232, 46595,  5553,
       47196, 18414, 11370, 43726, 45914, 45456,  1696, 40399, 49078,
       20534, 38390, 39885, 24082, 20058, 11359, 47062, 28786, 28554,
       11623,   314,  3910,  8988, 42198,  9300, 27673,  9429, 31449,
       35449, 26558,  8105, 46423, 33422, 27658, 21731,  3875, 12007,
       45901, 25257, 25563, 42646, 41705,  5502,  8386, 35427, 22226,
       24704, 15335, 39689, 26665, 26900, 22523, 42682, 42407, 19380,
       44809, 39319, 38402,  2506, 11918, 12086, 36029, 49360, 13889,
       10533, 12541,  5009, 10138, 24173,  3477, 37536, 36967, 30127,
       14099, 39572, 37857, 44646, 48769, 23990,  7838, 31744, 26597,
       47066, 47687, 24436,  2632, 49031, 42224, 49662, 11901, 41631,
       15141, 33248, 35504, 48431, 10373, 34034, 29148, 41611, 19601,
       26832, 21138, 28932, 40267, 28376, 26908,  6002, 34666, 36050,
        7040, 39993, 42739, 48126,  3974, 15252, 34772, 31918, 38601,
       39699, 38663, 14571, 44606, 42278, 33025, 35064, 35749, 36475,
       45465,  9701,  6158, 34006, 28495,  5079, 23169, 18439, 14572,
       38328, 27807, 39870, 30525, 40030, 46243, 45101, 33053,  6984,
        3929, 34957, 29105, 31637, 35852, 25973, 13346, 12040,  1000,
       36705, 38380, 40254, 45284,  6360,  3479, 27168, 32164, 20013,
        2199,  1115, 40082,  3316,  9302, 32439,   205, 41904, 34480,
       15817, 31378, 27957, 21571, 49029, 49405, 41284, 32879, 40559,
       33281, 16163, 40895, 26531,  8532, 17099, 36590, 18648, 28949,
       32902, 34232, 46698, 35607,  7302, 49074, 19674, 46651, 27752,
       12011, 20799, 14880, 26252, 11111, 17452,   316,   188,  6480,
       33886, 34903, 20787,  4407, 38324, 39611, 30080, 12288, 43171,
        6457,  3700,  3633, 17875, 11517, 18183, 49403, 40693, 10523,
        3876, 43522,  3023, 29628, 10056, 36741, 41535, 23783, 37126,
       19199, 44009,   269, 39331, 40742, 34474, 23657,  4587, 28259,
       32841, 17171, 13128, 48451, 10252, 36109, 49634, 41508, 36859,
        8841, 28304, 41395, 13642, 42885, 24048, 43614, 36165, 24731,
       47303, 43104, 26945, 30580, 10717, 10118, 23341, 41683, 46844,
       15993, 10078,   613, 17607, 24246, 34843, 41746, 34549, 41541,
       19805, 44280, 30733, 33024, 37349, 30678, 34423, 33679, 24893,
       28915, 37968,  4256,  1767,  6610, 46227, 20675, 10859, 26013,
       26674, 15586, 27821, 42452, 24320, 47365, 32937, 37849,  6241,
       14507, 34810, 12929, 10714, 13002, 36868, 20667, 20155, 44725,
       25459, 42976,  4528, 25689, 17275, 33261, 43932, 15386, 40822,
       19354, 31890, 20619, 47470, 29429,  5568, 48128,  6733,  9318,
       46679, 40145, 45126, 31352,   997, 21938,  3379, 29986, 42793,
       29963, 30198,  2731, 18456, 22060, 25046, 20368, 35106, 28001,
       35850, 30782, 23662,  5344, 16986, 12586, 10035,  6383, 19526,
        7992,  3734, 11708,  2259, 16310, 47279, 11804,  9327, 42282,
       19773, 46396,  7861, 35863, 38078, 29318, 45077, 22437, 17279,
       35567, 44541, 29874, 24119, 27215, 11522, 15905, 41472, 45613,
       49933, 22479, 35692, 35021, 38223, 38892, 27868, 46461, 42347,
       41502, 47900, 47178,  9411, 11971, 47588, 44341, 39422,  7723,
       34420, 28861, 29019, 36611, 42081, 10122, 40000, 46162, 20307,
       18752, 14007, 49594, 44767, 16066, 31720, 32736, 48595, 34629,
        7430, 31747,  4047, 15286, 13166, 31348, 31583, 26950, 17534,
       14061, 38175, 33748, 41289, 21407, 18654, 33624, 19046, 16745,
       15738, 26470, 18671, 47377, 41262, 27792, 12169, 35516, 22472,
       41520, 19769, 41723,  7145, 12185, 42163,  7683, 23929, 19324,
       23020,  6608, 18069, 32410,  4553, 13366, 13922, 20463, 37422,
       31282,  4011, 15934, 10655, 18160,  6623, 38577, 13296, 25323,
       29875,  1907, 44472, 24161,  6703,  9380, 47680, 30457, 36331,
       11824, 10079, 16992, 15583,  2469, 42085, 20796, 39906,  8346,
       31203, 34906, 14964, 20937, 22349, 31607, 18526, 48086, 17032,
       45316, 17718, 26740,  6831,  9232, 13108, 33866, 18961, 11558,
       21796, 19485,  6798, 38725, 15897, 44869, 19164, 45212,  2023,
       42007, 40258, 41215, 29023, 13417, 21153, 23203, 13849, 33561,
        2390, 49643, 46357, 28476,  9668, 31523, 26465, 34993, 14878,
       43711, 11782,  4565, 42802, 45704, 48539,  7130, 46514, 36251,
       15114, 46177, 32235, 48807,  6485, 18912,  2363,  3948, 48613,
       25232, 46061, 19568, 13834, 17632,   345, 28372, 36047,  9061,
        3802,  2300, 48351, 19118, 42313, 34562, 21733, 12737, 35915,
       28577, 40757, 24996,  7317, 17878, 14635,  3464, 46526,   505,
         449, 30585, 28407,   887, 11138, 21912, 10447, 15170, 19643,
       23367, 38964, 28040, 23792, 47239, 40642, 40309, 21187, 23022,
       32518, 42700, 37389, 44032, 36383, 25977, 44397, 19475, 19340,
       43139, 10736, 48605, 16216, 41814, 38351, 42750,  3533,  2842,
        6924,  6891, 36817, 25954, 18060, 41162,   632, 19952, 36566,
       40794, 29419, 37923, 36896, 33095, 47837, 43595, 14956, 26860,
       49027, 10865, 36976, 23755, 26892, 23211,  7296, 14036, 24265,
       28319, 25355,  8987, 46133,  7787, 23057,  7851, 42544, 42729,
       12026, 41301, 33796, 48478, 10808, 48824, 38598, 22259, 39656,
        6877,  4043,  9615, 44517,  1695, 19543, 22246,  3027, 49608,
       32494, 27535, 29835,  4305, 30630, 14670, 29737, 13466, 36160,
       47257, 15718,  8396, 23954,  1315, 42910, 48997, 42640, 35877,
       36652, 43032, 28614, 14777, 35180,  4158, 43203, 18043, 15267,
       44200, 38454, 24630, 15036, 13427, 36090, 30353,   669, 35573,
       27259, 26838, 35714, 30900,  4687, 32643,  2905,  9149, 43650,
       10765, 45775, 34217, 41451, 13691, 45644,  1791, 43860, 28600,
       29070, 18289, 42229, 38776,  5940, 41691, 31003, 27247, 29129,
       21676, 27322,   563, 15152, 39127, 26929, 25346, 14879, 41530,
       29184, 22990, 18300, 29827, 46778,  1516,  8784, 44729,  7382,
       49213, 22798, 22698, 39348,  3236,  9771, 22509, 14474,  1350,
       11957,  2178, 47120, 16033, 16897,  6270,  7743, 10633,  3522,
       12301, 33013, 12075,  2703, 36777, 37089,  9507, 14255,  4527,
        9713, 10900, 31143, 30499,  8137, 28799, 37996, 27891, 45234,
       35954, 41186, 41669,  1934, 24778,  5495, 27596, 15508, 31840,
       22580]), [3, 9, 1, 4]), (array([44167, 49992, 38943, 14754, 19493, 22993, 23796, 41606,  9059,
       38238, 10847,  2365, 33117, 47981, 47328, 16460, 30321,  3304,
       41745, 45412, 30160, 49303, 11424, 35788, 43829, 43012, 27434,
       26607, 37178, 21087, 33943, 44143,  7573, 14945, 38039, 34017,
       28354, 18271, 10637, 35121, 30709, 40125, 37022, 13324, 26922,
       18073,    49, 47486, 19041, 32602, 35858, 11963,  8971, 49490,
       42931,  2617, 47834, 41300,  6743, 28794, 44925, 27911, 37160,
       38036, 46738,  7643,  6276, 42476, 26776, 32206, 48254, 32056,
       41324, 46725, 14116, 22973,  3731, 29225, 14698, 26391,  1381,
       36858, 16137, 10690, 27278, 39329, 24113,  3335, 11734,  2720,
         871,  1935, 45163, 41355, 38345, 11356, 24720, 43875,  2714,
       10205,  4721, 37517,  1524, 17677,  4155, 30383, 44967, 36464,
       42446, 29003, 42456, 28972, 42183,  2574,  6190, 40796,  1950,
       34944, 24079, 41907,  4081, 47468, 21974, 26060, 25942, 17917,
       43649, 17762, 40639, 42628, 10340, 41235, 36353, 33258, 48556,
        5864, 48926, 15304,   733,  3789, 36106, 25212, 45344, 10275,
       32369,  8553, 10990, 17717,  2633, 42332,  1473, 25288, 38613,
       16531, 35286, 25190,  1926, 49517,  2962, 24096, 30428,  2401,
       28665,  4854, 34018, 11372, 28456, 29412, 27137, 20891, 49557,
       10577,  8199, 10301, 36357,   115, 12083, 47105,  5457, 32670,
       16186, 48246, 15077, 34083,  3815, 28921, 23825, 49656,    93,
       15946, 38478,  4653, 48327, 38043,  7093, 44175, 26361, 32107,
       10148, 27042,  4353,  2107, 25236, 25616, 30078, 14153, 26296,
       11224, 46945, 39606, 12420, 46287, 24939, 11709, 18080, 43479,
       32934,  3906,  4477, 40485,   965, 24644,  4552, 30172, 38499,
       24750, 40679, 47771, 41600, 27487,  8062, 20813, 20602,  1187,
       26783, 38179,  1759,  3184, 19778, 47865,  8535, 17115, 34036,
       14413, 39650,  4952,  2413, 11612, 40652, 29364,  2393, 11380,
       39646, 11873, 40348, 37878, 26128,  1527, 43781, 10567, 47826,
        3901, 37713, 13848, 10651, 29742, 36481, 16402, 43898, 49457,
       23918,   933, 27484, 36025, 39882, 26386, 10628,  6097, 23537,
       14946,  9597, 19868, 18040,  2626, 47932, 32792,  6488, 36213,
       28933, 35792,  6369, 37298, 38032, 42889, 36377, 25148, 38745,
       30665, 35031, 25929, 21867, 30333, 30746, 22281,  3193, 20216,
       32697,   522, 38459, 17739, 38054, 25381, 11934,  4525,  9503,
       42588, 13179, 20038, 33396, 46564, 39237,  1798, 35486, 21746,
       40527, 44995, 44427,  2927,  2202,  9151, 32071, 46261, 38001,
       40463, 39847, 48090, 46088,  4549,   513, 40333, 46943, 42774,
       14811, 21452,   403,  7761, 36759, 37645, 13270, 33644, 26176,
       13814, 32908, 39407, 13195, 42600,  8032, 17702, 10833, 34196,
       44658,  6843, 13130, 49730, 20700, 24706, 45460, 42928, 46326,
       40282, 35096,  4518, 20547,  9712, 45131, 19056,  6757,  7662,
       20656, 44335, 19932,    24, 31705, 33361, 11730, 17834, 20183,
       17086,  9249,   218, 31501, 37581,    47, 19815, 26795, 45533,
       46509, 14314, 16288, 49093, 12361, 42990, 12589, 26864, 25989,
        5871, 11348,  4370,  5889, 35616,  3259, 32406, 11962, 25695,
       12067, 13901, 21135, 36170,   196, 36726, 13770, 10742, 29625,
       38129, 27097, 10461, 37446, 49784, 15139, 29379,  7664, 49693,
       41925, 44008, 26999, 43523, 48101, 24409,  6946,   724,  7658,
       43747, 32419,  1139, 32067, 26572, 41775, 40087, 48735,  2126,
       18027,  8081, 10756, 42450, 34925,  9011,  9940,  3349, 42581,
       34734, 15065, 37315,  9790, 12148, 27980,  7477, 31007, 10204,
       31707, 42564,  3944, 27877, 11548,  4930, 35104, 41721, 21145,
       26442, 48951, 14088, 29907,  2136, 43041,  6772, 46957, 46584,
        3321, 42158, 19899, 10940, 36994, 14646, 27356, 13934, 10968,
       29637, 13819, 36581,  1789, 27746, 47939, 43853, 12276, 19202,
       34336, 32564,  9549, 32472, 35344,  1574, 43214, 29252, 22298,
        2727, 18048, 39652, 38756, 33207,  2173, 15363, 24385, 14502,
       42921, 26077, 23313, 18890, 27942, 39338, 29777, 17268, 21150,
       31019, 41529, 48820, 16471, 23332, 45769, 29188,  7198, 18078,
       30407, 18844, 27465, 10229, 45675,  1037, 10795, 35965, 20047,
       37702, 34364,  8547, 34850,  9898, 44841, 15885,  5171,  9476,
       37826, 16385, 41710, 45834,  2587, 21152, 45157, 12773,  8529,
       14379,  6814, 32258, 44524, 16077,  4492, 28643,  9717, 44109,
       10039,  7288, 45475, 31253, 12923, 16638, 30313, 21593, 10446,
        3346, 38086, 20168,  9878, 29044, 38420, 42475, 10953, 41248,
       22089,  6074, 42734, 23260, 14376,  7338, 46827, 26550, 19939,
       46544, 49381, 46859,  4100, 22553, 20725,  4132, 49045, 26930,
       28655, 13852, 34163,  8965, 16947, 16594, 48709, 40971,  5811,
       42448, 46900, 33951, 33126, 47529, 24446,  2038, 42239, 29146,
       27447, 33611, 39195, 49348, 47088,  9857, 46993, 40518,  7326,
       49629, 29226, 41706, 26787, 48667,    79,  5422,   840, 46866,
         261, 11631, 44204, 34128, 17112, 29901, 14859, 47310, 31957,
        5157, 28626, 48234, 24284,  6226, 40449, 28127, 21309, 34118,
        7261, 30804, 19185, 45972, 27089, 31959,  8944, 48681, 24228,
       40152, 25993, 23127, 39996, 37666, 10074, 33742,  9400, 12080,
        2072, 45011, 24151, 35635,  4265, 16777,  6349,  7349, 14549,
       15756,  8450, 29828,  1901, 37743, 29512,  4496, 43274,  3414,
       31079, 44591, 11696, 25050,  7605, 12100, 31288, 32462, 28298,
        4993, 42710, 43480, 36400, 12620, 23275, 36663, 10479, 12025,
       28420,  2200, 23213,  8475, 38908, 29029, 23210, 44423, 26295,
       48639, 33314,  6813, 19989, 18527, 36813, 42413,  8127, 24484,
       41963, 24008,  1464, 19571, 30062, 26870, 44537, 15984, 41209,
       27262, 19181, 32743, 20206, 27282,  8913,   272, 47975, 20340,
       42072, 30307, 14305, 38033,  1268, 11582, 17197, 41219, 31446,
       10661, 45293, 20515, 12313, 36317, 46076, 29852, 28823, 15012,
       34350, 28289,  9650,  8405, 16155,  9816, 26844, 14388, 10951,
        2695,  7059, 31733, 40188, 21781, 23013,   399, 37577, 29111,
       18404, 12997, 28084, 28903, 32969, 39413, 25579, 46947, 43228,
       35806, 31953, 29698,  5491, 46531, 31804, 38668,  6879, 31998,
       12417, 27029, 48884, 17953, 48447, 27220, 27390, 19331, 11860,
         660,  8049, 30881, 47139, 49669, 34477, 39772, 25098,  9141,
       41766, 19345, 12489, 40116, 37172, 34114, 48801, 42603, 16880,
       41270,  8184,  7811, 24810,  2647, 18365, 38603,  9568, 29525,
       31549, 48202, 35185, 17039,  1804, 39377, 21629, 16672, 32803,
        7110, 43758,  7269, 14584, 40027, 16749, 32818, 36420, 13510,
       10282, 10966,  3684, 18560, 27946, 23739, 49685,  4987, 43810,
       48902,  6911, 35432,  1904, 28830, 43231, 19264, 40949,  4642,
        3326, 41337, 41056, 22443, 22617, 13764, 16772, 25403, 24594,
       27489, 10154, 24846, 21659,  8930, 36442, 20343,  2155, 33279,
       44760, 42979, 46998, 28809, 15744, 31277, 38995, 11872, 31709,
       25931, 46650,  1952, 14810,  6339, 33482,  1992, 49114, 27630,
       36514, 41103, 28755, 28751, 22949, 36731,  3784, 42907, 25871,
       14284,  2673, 20011, 19943, 19704, 10264,  4297, 14051,  4447,
       22018, 20409, 42982, 18813, 37853, 44614, 31932, 32533, 33226,
        5067, 27455, 10134, 45199,  1007, 23200, 49318, 11021,  1425,
       27221, 36740, 42418, 30449, 17725, 24084, 32413, 24785, 34956,
       21194, 20342, 34328,  6605,  6128, 47079,  7383, 27649, 14492,
       44007, 16890, 39177, 24804,  2225, 20608, 43584, 10904, 33266,
       34182, 21597, 12846, 24776, 22144,  2672, 45114, 39467, 17483,
       11249,  8114, 45400, 15624, 42636, 26631, 19407,  9260, 20521,
       10131]), [0, 2, 1, 4]), (array([48247, 20762, 49551, 29868, 44236, 29075, 15260, 16466, 14891,
       30357, 24692, 25521, 34712, 33441, 16422, 38587, 39802, 23438,
        6651, 34491, 38686,  4283,  9599, 13227, 46892, 31423, 27507,
       39094, 13425, 20455, 49985,  9265, 45024, 28396, 23408, 15743,
       45062, 38416,  1162, 46488, 34473, 10856,  9675, 48757, 19979,
       42333,   716,  4298,  4823, 13582,  1512, 39184, 26055,  8222,
       24554, 46033,  1763, 39254, 49681, 20491,  1751, 34441, 41761,
       49861, 19851, 36221, 26463, 10923,  4955, 34369, 49389, 20650,
       29782, 12426, 19748, 28599, 44384,  8329, 16132,  8495, 39507,
         460,  9722, 42469, 32562,  4204,  1370, 48541,  4780, 49899,
       15250, 42693, 18565, 19509, 17588, 21157, 37318, 27864, 43913,
       40391,  8138, 25501, 31489, 26131, 49388, 16727, 23205, 15046,
       39336, 19887, 45779, 31456, 20822, 39715, 17894,  9192, 22268,
       11327, 42713, 30026, 48984, 13874,  8384, 44180, 21036, 41629,
       33398, 18444,  6153, 37643,  2434, 24435, 47597, 39702, 42254,
       35593, 40345, 29371, 47342, 11180, 13702,  4758, 19666,  8830,
        8248, 12513, 10443, 26147, 38460, 14872, 25855, 38221, 38925,
       20617, 47395, 43492, 37633, 47506, 38065, 31778,  6825, 19605,
       16724, 45104, 20479, 13992,  6773, 27458,  9023, 26784,  5283,
       45912, 13788, 20288, 48905, 15811, 29056, 19991, 45620,  3215,
       33815,  3518, 13386, 28491, 24876, 43109,  2979, 13576, 10836,
       34210, 45447, 19836, 35224, 14642, 10822, 25869, 39317, 45240,
       40702, 30955, 16086, 34656, 27715, 35367, 45868, 15049,  3832,
       12319, 10956, 30844,  5730, 10234, 42933, 23312, 31725, 36390,
       27615, 14693,  6846, 46938, 19398, 46888, 13004,  6789, 31841,
       43183, 16178, 12861, 25618, 17394, 17220, 40062,  8526,  4652,
       26004, 16044, 17335, 19720, 38694, 49851, 27842, 21499,  9346,
       12646, 32213, 27988, 43982,  6912, 11674,  2918,   164, 41870,
       14162, 17035,  7041, 21424,  2267,  2939,  2019, 28678, 30130,
        6918, 47347, 43070, 22534, 45970,  5318, 31382, 43426, 23520,
       19128, 10113, 16507, 24418, 13123, 37574, 48201, 38197, 26151,
        7392, 13796, 32637, 36360, 31616, 27927, 49289, 18801, 46134,
       38061, 26243, 35099, 40653, 39874, 30176, 35114, 44078, 18816,
        3976, 43452, 34652, 34432, 37992, 22654,  7137, 20324, 19511,
       48298, 38865,  4208, 20098, 44612, 17248, 24184, 36192, 41184,
       47734, 26324,  2494, 35499, 25985, 31558, 14039, 41004, 12214,
       47071, 48400,  7215, 36586, 17680,  1031, 26066, 15497, 28378,
        9653, 40274, 36808, 15651, 43488, 44194, 39007, 30427, 16546,
       12321,  5745, 30367, 38438, 27450, 37796,  5679, 46124, 28139,
       12765, 30626,  6572, 11644, 35270, 45585, 10792, 26429, 34804,
       35900, 23821, 49804, 23025, 27134, 33425,  3655, 33012, 32793,
       34362, 25396, 48723,  6367,  6395,  6810, 11319, 34322, 38537,
       48460,   588, 14830, 19834, 37926, 21917, 25417,  5907, 28990,
       39252, 41448, 34505,  3018, 20767, 28986, 31465, 18851,  1766,
       17282, 48229, 12445, 31933,  2729, 20993, 25729, 43439, 16760,
       40811, 17000, 24117, 25085, 48710, 17075, 11150,  7143, 17952,
       30100, 16404,  9303, 40929,  3716, 47464,  5025, 44708, 40164,
       46743, 40357, 45019, 31099, 48932, 44566, 12122, 38993, 28956,
       15294, 45958, 28021, 44518, 49892, 43645, 20930, 47752, 29223,
       47728, 13797,  9468, 24198, 35703, 49305, 20293, 41909,  9808,
       49311,  3070, 41809, 46913, 22100, 26122, 49708, 18336, 45383,
       22345, 17788, 37481, 25067, 41074, 45823, 15220,  1374, 23881,
        7675, 43538, 24882, 49776, 37487, 25747, 36089, 44696,  2965,
       13683, 43298, 20561, 19924, 30741, 38606, 49210, 35208, 46180,
       29677, 43577, 48665, 18407, 22101, 23912,  7976, 31228, 16129,
       41593, 21200, 43481, 32534, 28374,  2851, 21104, 17970,  4858,
       45628, 31935, 43145, 12893, 49416,  3937, 40279, 37844, 48740,
        4099, 24466, 20856, 48182, 18744, 15748, 42438,  7005, 14204,
       27419, 45500, 47749,  3062, 22984, 40537,  1578, 49383, 13996,
       13505, 30136, 23837, 46173, 47616, 19609, 48728, 14048, 10387,
        7529, 10912, 22829, 37494,  9538, 22306, 32580, 16985, 47845,
       11850, 22884, 21675,  6899, 48917, 34442, 29693,  3452, 40815,
       38646, 20376, 28163, 24137, 47035,   206, 35048, 18263, 37512,
       12071, 36641, 42237, 17815, 44889, 34701,   942, 49658,  7777,
       33963, 35364, 17662,  2445, 38822, 13144, 14110, 22227, 33164,
       21218, 47987, 30783, 45535, 39737,  5917, 33762,  8693, 36158,
       36237, 30083, 15299, 24986, 26074,  4459, 34909, 14904,  5144,
       16588, 18425, 28417, 14005,  5197,  4557,  5816, 34426, 15183,
       49616, 45099, 45899, 18178, 18388, 49932, 28532, 25926, 41964,
       36298, 15609, 20931, 32438, 35081,  2656, 15988, 32474, 39811,
       42287, 32418,  9781, 12280, 33392, 49494, 44309,  4533, 22788,
       44308, 17106, 17856, 14068, 27716, 17334, 40453, 19898, 15855,
       47362, 18246,   364, 13234, 35953, 24774,  8576, 13858,  1551,
       42709,  6353, 41627, 38192, 29953, 35666,  2802, 10248, 19351,
       44411,  1021, 35366, 45013, 19490, 10453,  3781, 41307, 20444,
       47633,  8977, 38028, 44395, 39277, 26794, 15832,  2883,  1694,
       15496, 34376, 39745, 32681, 14512, 19265, 26115,   325, 16682,
       19313, 49080, 46359, 14209, 48669, 19598, 32011, 21181,  5635,
       37239, 30553, 13952,  2268, 40341, 43167, 10188, 20746, 28175,
       18518, 33838, 22037, 45210, 27178, 32941, 43684,  1605, 48954,
       14288, 39912, 37213, 42341,  1020, 25377,  4949, 16223,  9095,
       39109, 32083, 34791, 49716, 17454,  9012, 21180, 35508, 37533,
       34233, 16855,   250, 30949,  3444, 21870, 40034, 12800,  7286,
       32042, 44601, 44225, 12805, 30681, 29489,  9202,  9074, 10615,
       38787,  5754, 46153, 14667, 34431, 22149,  5756, 46823, 43746,
        2056, 40121, 24108,  2911, 22183,  7043, 33666,  6894, 39005,
       13464, 27003, 33369, 16240, 13472, 29396, 44857, 10207, 27394,
       30268, 38511, 20192, 35563, 13145,  4438, 13498,  7100, 35517,
       26653,  3577, 19852, 12803, 24674, 49622, 38354,  3635, 35843,
       40665, 21949,  5909, 38805, 32776, 32095,  2123, 33480, 49077,
       33370, 21853, 31579, 11967,  8279,   951, 18065,  5063, 17338,
        3145, 34950, 31755,  5237,  6039, 49002, 44324, 40848, 27572,
       47928, 36915, 12179,  9967,   764, 45618, 42160, 46968, 43049,
       27935, 37719, 20904, 12645, 45397, 37977, 10067, 11481, 12590,
       45408, 14853, 20108, 24997, 26706, 13927, 27790,  3296,  2787,
       43172,  9782, 28913, 38762, 40562, 41810, 38755, 20435,  9155,
       14905,  3288, 10604, 32179, 40793, 47576, 45111, 41665, 14935,
       45340,  2088, 25620, 17877, 15990, 40842, 47664, 36167, 41573,
       12723,  1882, 36536, 25402, 38213, 13469,  7625, 31939, 22683,
       43974, 24419, 48878, 34699, 14152, 41417, 20363, 15737, 19441,
       38517, 49751, 16587,  3776, 46119, 19032, 39701,   162, 35228,
       12453,  2144,  3509,  8236, 28174, 12858, 35522, 45076,   816,
       23445, 21222, 28414, 19098, 11759, 47295, 28654, 27888, 22205,
        9153,  1069, 17556, 41382, 34038, 21041, 17077, 25250, 36096,
       41191,  6387, 11997, 16007,  3377, 49207, 47312,  8688, 34384,
       37741, 32960,  9761, 42312, 14649, 29939,  5180, 43683,  7642,
       40259, 17049, 37688, 12224, 46470,  4763, 45935, 11626, 40473,
        3587, 49984, 20659, 14104, 21176, 32535, 30061, 37342, 26638,
       19702, 19569,  8447, 27313,  6780,  8254,  5624,  6844, 23937,
       33015,  3189, 10105, 10136, 37294, 35956, 45170, 44172, 14377,
        6710,  4781, 43509, 25371, 21929, 22837, 14737, 47570, 27206,
       12416]), [8, 6, 1, 4]), (array([41098, 10258,   686, 36782, 17797, 34541,  4850,  1847, 40471,
       24160, 16797, 47065, 27893, 47289, 30393, 25230, 29432, 45728,
        6873,  6259, 19856, 36851, 44738,   426, 18346, 25465,  9301,
       11852, 13911, 33311, 47544,  3002, 22473, 16812, 48245, 10632,
        7160, 28877,  2794, 31264, 13257, 18764, 11314, 28661, 12326,
       26987, 14965, 46264, 18230, 26125, 33116, 48625, 21128, 25698,
       31047, 26024, 27771, 46720,  6958, 46842, 37252, 36556,  3017,
       11819, 18082,  7476, 30016, 28902, 27879, 14632,  4529, 44260,
       44781, 33295, 18729,  6760, 10397,  3421, 49327, 49680, 38529,
       43467, 18170, 44899, 45436, 23119, 30349,    70, 48645, 21278,
       49797, 47218, 20836, 33702,  8325, 31949, 20445, 34095,  2314,
         359, 11519,  1545,  6131, 17855, 37486,  5962, 31129, 49980,
       39002, 11770, 38695, 33435, 44706, 30622,  9196, 49753, 29110,
       31901, 36956, 22893, 34803, 11185, 12573, 22427, 18748, 13067,
       32511, 41499, 21780, 38854, 25311, 48673, 30662, 20292,   875,
        7094, 14227, 11662, 32184, 36224, 23335, 21077, 48383, 46112,
       45991, 44700, 29304, 29474, 46246, 47496, 28218, 45503, 11365,
       27598, 46494,  1656, 22483,  5393, 27292, 40351,  1849, 36603,
       24650, 41583, 25418, 37096, 36171, 48083,  5305, 23947, 12701,
        7472, 30235, 39228,  7157, 23976, 18061, 43703, 48883, 32530,
       22158, 20075, 45469, 22321, 24006, 29061, 25870, 36899,  5681,
       35165, 41046, 41069, 23146,  1786, 33306, 17346, 29558, 39929,
        8198, 30735, 23150, 44551, 21046, 28693, 39103, 27397, 44290,
       16653, 14875, 43999, 48573, 28442, 23949, 30684, 42337, 25057,
       43071, 39660,  1519, 11201, 33873, 27679, 16436,  2984,  7181,
       27945, 45553, 40091, 32719, 37946, 44555,  5500, 49720, 39587,
       15207, 35660, 49938,  6436,  5379, 21767, 41793, 20020, 21494,
       25610, 28995, 12912, 37813, 36669, 18985, 47625,  4686,  9201,
       11334, 21083, 19346, 29196, 16031, 42284, 16537,  6449, 13525,
       12099,   570,  2045, 11410, 21927, 30167, 42612, 20741, 25283,
         514,  6808,  3146, 31822, 10666,  3398, 29350, 33495, 15539,
       28715, 15081,  3272, 42593, 39425,  1071, 15283, 30030,  7819,
       13749, 10127, 43128,  8311, 32354, 22901, 38104, 11391, 37449,
       29026, 40837, 21967, 30447, 14947,  9082, 28132,  5774, 23880,
       37836, 40892, 42751, 37671, 37624, 41973,  5126, 26752, 49377,
       28150, 18095,  9789, 15679, 17989, 45861,  9795, 10887, 25553,
       40821, 44286, 47856,  8516, 35982, 33755, 32307, 11338, 17129,
       37902, 11846, 28965,   913, 34308,  1915, 37159,   318,   320,
       22725, 10734, 48467, 17997, 26991, 10730,   797, 22128, 43322,
       22934, 39395, 28945, 27568, 24916,  3955, 42668, 37496,  3715,
       45365, 18139, 45061,  8984, 22324, 16666, 15317,  5525, 37935,
       46211,  1135, 27036, 23928, 36364, 21718, 39107, 24091,  7372,
       17886, 16876, 24129, 16198,  4706, 23196, 18556, 10184, 20699,
       12925, 12994,  6752,  2616, 10695, 11796, 45874, 28853, 37458,
       12864, 47777,  9214, 29957, 39498, 11088, 32875, 19973,  7506,
       29727, 31719, 16843, 23547, 16149,  6699,  7609, 12838,  4903,
       12320,   329, 36352,  7939, 30996, 22526, 36136, 25724, 34782,
       28201, 25464, 16040, 18528, 12437, 17254, 43305, 48948, 17636,
       44101, 28243,  3637, 22198, 16022, 46391, 10269, 47372, 47673,
       36589, 46348,    11,   131, 26163, 15136, 38798, 18828, 10481,
       35158, 31036, 38500,  7023, 16589, 39485, 19250, 19361, 38955,
       21773, 32208,  7185, 48041, 34260, 36306, 10538, 43512, 33773,
       42936,  4809, 44151, 48490, 45087, 44694,   152,  3649, 23842,
       26291, 19200, 42909, 14398,  8904, 19794, 45758,  6759, 10034,
       14758,  9496, 37482, 36677, 31807, 40295, 26032, 37472,  4253,
       36691,  8202, 47039, 25598, 14537, 37327, 46213, 33112, 23895,
       47349, 22054, 29473, 25697, 10902, 21706, 49554, 49121, 16332,
        2286,  6928, 33686, 34595, 33114, 42666, 34097, 18698, 35401,
       31737, 43786, 33615, 12592, 36774, 19575, 43641, 42065,  9080,
       44993, 46649, 26652,  2339, 12894, 29409,  8480, 21234, 12560,
       39771, 34043, 44474, 48141, 10569,   396, 41254, 46956,  8119,
       20736, 35530, 38858, 16829, 31251, 20761,  8135, 29485,  1559,
       29494, 23936, 26915, 45602, 34263,  3078, 18331, 37468, 14764,
       25718, 29283, 45485,  1090, 22833, 30446,  7673, 38493,  4696,
       44936, 17126, 43178, 42860, 33693, 41579, 26127, 27268, 46889,
       21529,  4423, 41550, 40248, 34090, 15749, 28381, 47779, 16989,
        8191, 20866, 12903,  2455, 41563, 22217, 18594, 33250, 16412,
       22419,  9766, 39474, 11707, 22165, 31655, 41338,  9263, 38347,
       16549, 42925, 10022,  8771,  3827, 11109, 18649, 21020, 46463,
       35691, 15702, 42912, 42758, 15506, 21959, 18228,  7048,  7722,
       19018, 37309, 37385, 31830,  5207, 14378, 30234, 35338, 34081,
        3315, 49795, 18551,  4200, 28096, 35705,  6121, 18117, 12247,
       24247,  8794,   257,  3085, 23006, 17479, 33640, 28539, 36085,
       34493,  2184,  8884,  6068,  1052, 35674, 18093, 32121,   432,
       15632, 48138, 13899,  3020, 36343, 29230, 27627, 12038, 29203,
        6020, 19869, 36975, 40880, 31945, 20042, 36872, 30458, 40972,
       25619, 25910,  1304,  9809, 46099, 15557,  5810, 23500, 46893,
       30104, 24923, 28876, 47213, 44272, 11645, 19143, 11243,  3366,
       46547, 40713,  2039, 11075, 34068, 18625, 29974, 31138, 20840,
       35651,  3273, 37012,  1394, 43416, 19565, 13122, 23714, 30530,
       34349, 18303,  8432, 22372,  7284, 29633,   262, 49186,  2511,
        6639, 45977, 31599, 46078, 49849, 45317, 10362, 42318, 33970,
       32584, 46406, 45487, 38491, 19628, 18639, 25614,  7214, 27994,
        9442, 18333,    99,  9000, 27547, 32284, 28100, 31290, 37987,
       35538, 41286, 20137, 27050, 48892, 41897, 20730, 10888, 48367,
       47441, 27135, 31769, 27494,  1925,  1001, 46150, 30300, 33027,
       31775, 14397, 10766, 21188, 27631, 21327,   712, 29743, 32253,
       26417,   520, 44834, 27812, 23421, 45820,  2654, 46331, 18161,
       48211, 23072, 31954, 18637, 44850, 23187, 42080, 45782, 38146,
        8338, 27500, 38228, 15155, 35415, 40631,  3354, 24920, 17402,
       18384, 32553, 18056, 32634, 44963, 34751,  4104, 21382,  9147,
       43335, 41675, 44641, 21021,  7563, 21626, 27613, 19462,   489,
        5434, 24582,  5173, 40974, 44587, 37903,  9724, 35526, 23354,
       33129, 47534,  9887, 19256, 19178, 28260, 20906,  4741, 44942,
        9550, 47149, 10303, 36968, 43299,  3519, 33004,  7404, 17713,
       45773, 47644, 40531, 10179,  7295,  6168, 12810,  3217, 27718,
       47611, 37193,  7619, 14937,  9937, 11982, 13210, 30734, 20655,
       15909,  6603, 47907, 17199,  5931, 27469, 45515, 28892,  2106,
       38728, 29101, 25879, 13416,  4773, 39217,  3695, 21946,  7207,
       16527,  9529, 11116, 23252, 14359, 18493, 22137,  5581, 18130,
       48591, 18606,  8792, 47571, 20778, 31281, 44565, 44202,  4195,
       43044, 47738, 32018, 35276, 11994, 11250, 32094, 31515, 26586,
       17727, 45957,  2835, 15203, 19180, 32620, 36302, 43294, 32026,
       18734, 31053,  4695, 37778, 39768, 26819, 45962, 35450, 45496,
       15004, 17623, 27429, 23362, 48738, 33232, 46382, 24829,  2216,
        9962,  4998, 42315, 33971, 13513, 37963, 18011, 18110, 14521,
       24823,  7536, 48889, 43529, 17500,  4359, 17852,  3141, 26715,
       46446,  5298, 43337, 32590, 18374, 30801, 25798, 33401, 42662,
        1903,  4156, 30627, 37234,  7712, 47374, 29071, 43870, 17819,
       14726,  6298,  8131, 16073, 35514, 40705, 16328, 34936, 11757,
       44125, 23869, 30044, 29922, 10890, 26023, 42296, 33523, 15454,
       14704]), [5, 7, 1, 4])]
Collaboration
DC 0, val_set_size=1000, COIs=[3, 9, 1, 4], M=tensor([3, 9, 1, 4], device='cuda:0'), Initial Performance: (0.25, 0.04431799054145813)
DC 1, val_set_size=1000, COIs=[0, 2, 1, 4], M=tensor([0, 2, 1, 4], device='cuda:0'), Initial Performance: (0.252, 0.044353610038757325)
DC 2, val_set_size=1000, COIs=[8, 6, 1, 4], M=tensor([8, 6, 1, 4], device='cuda:0'), Initial Performance: (0.276, 0.04447247278690338)
DC 3, val_set_size=1000, COIs=[5, 7, 1, 4], M=tensor([5, 7, 1, 4], device='cuda:0'), Initial Performance: (0.218, 0.04446711611747742)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {9, 3}
D07: 1000 samples from classes {9, 3}
D08: 1000 samples from classes {9, 3}
D09: 1000 samples from classes {9, 3}
D010: 1000 samples from classes {9, 3}
D011: 1000 samples from classes {9, 3}
D012: 1000 samples from classes {0, 2}
D013: 1000 samples from classes {0, 2}
D014: 1000 samples from classes {0, 2}
D015: 1000 samples from classes {0, 2}
D016: 1000 samples from classes {0, 2}
D017: 1000 samples from classes {0, 2}
D018: 1000 samples from classes {8, 6}
D019: 1000 samples from classes {8, 6}
D020: 1000 samples from classes {8, 6}
D021: 1000 samples from classes {8, 6}
D022: 1000 samples from classes {8, 6}
D023: 1000 samples from classes {8, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO2']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.375, 0.06283778327703476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07227671495079994) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.07495411774516106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.349, 0.08856414425373077) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.284, 0.06856693160533905) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.08170339134335518) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.08742304396629333) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.09984482651948928) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.08577430075407028) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.357, 0.09504514583945274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.11441597338020802) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.10870000609755516) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.435, 0.11007066434621811) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.11734175483882427) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.16358603571355343) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.13449031338095666) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1351322904229164) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13958969017863274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.17408120495080948) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.1502700263261795) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO4']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.15643265687674285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.419, 0.1466355331838131) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2705200811526738) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.1623449475169182) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.1696251774523407) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.432, 0.15292382569611074) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2703526836093515) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.190651739038527) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19223802105896176) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.425, 0.16991872733086347) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3325524968125392) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.1890639338158071) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19927251303382218) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.418, 0.17540518701449037) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3120443267803639) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.2167533227801323) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.21156952209956945) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.424, 0.16317588014900683) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.3092682206723839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.2174901111423969) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[1, 4], M=tensor([0, 1, 2, 3, 4, 9], device='cuda:0'), Initial Performance: (0.0, 0.3779596796035767)
DC Expert-0, val_set_size=500, COIs=[9, 3], M=tensor([3, 9, 1, 4], device='cuda:0'), Initial Performance: (0.94, 0.005162857327610254)
DC Expert-1, val_set_size=500, COIs=[0, 2], M=tensor([0, 2, 1, 4], device='cuda:0'), Initial Performance: (0.848, 0.010477451875805855)
SUPER-DC 0, val_set_size=1000, COIs=[3, 9, 1, 4], M=tensor([3, 9, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[0, 2, 1, 4], M=tensor([0, 2, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x71c9607711c0>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c90c74dcd0>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c90c4d5ee0>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c90c528520>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c9746326a0>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO5', '(DO1']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.004691212682053447) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.009114856667816638) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.2905787508720532) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.2288923665173352) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.004810410596430302) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.05925917188078165) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.04190209773182869) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004426035630516708) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.010347857784479856) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.26237700392236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.22874133194237947) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.008045314067974686) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.572, 0.04087113991379738) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.501, 0.05879997279495001) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.003950650453567505) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.011358545109629631) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2551430446449085) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.426, 0.23922158036381005) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.004857556101400405) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.04157237386703491) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.06752537213265895) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0043992875730618835) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.906, 0.010208921119570733) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.24125349630980053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2568849093317985) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.00394035432767123) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.046138653486967086) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.058869879826903344) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.004436644520610571) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.009984974019229412) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.2188989610444987) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.2810688246190548) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.926, 0.007454558815807104) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.05671131333708763) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.0735227660164237) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO3', '(DO2']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.00519136414444074) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.009619934476912022) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.485, 0.20385369112668558) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.435, 0.2763240528255701) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.008470671396702528) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.523, 0.07500792333483695) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.08402937849611045) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.003969127969350666) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.009984306395053863) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.1869539916277863) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.28909522835537793) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.00999700640896117) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.507, 0.08017064135149121) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.10973101943265647) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.007817347285337745) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.904, 0.010843087449669837) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.1966088991361903) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.421, 0.3004858526550233) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.006376179987564683) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.07849225649237633) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.559, 0.06843662911653518) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005694955013226718) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.012585482202470303) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.485, 0.1826811176305637) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.424, 0.3147787351384759) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.004981726860627532) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.623, 0.049733171582221986) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.588, 0.07008283373713493) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.0068704773737117645) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.012263249298557639) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.18289113834616727) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.3099942891150713) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.00503899432788603) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.55, 0.07293332076072692) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.06952183987386525) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO5', '(DO1']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004006885829847306) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.904, 0.011296049565076828) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.19054251021961682) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.32181952918320894) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.004932633264688775) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.06721400199830532) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.07936977070569992) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004878819026518613) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.011163540616631508) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.19437545802770181) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.28343770196661355) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.004553568385366816) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.07681803411245346) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.539, 0.10214133582077921) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004595744061749428) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.01327752124145627) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.17215078497090144) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.2644650461152196) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.961, 0.006645588969546225) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.09298652433324606) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.10603591647185386) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.006720599023392424) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.013649252001196146) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.18903057567734505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.32515233564376833) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.962, 0.006388539284307626) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.503, 0.08846567318588495) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.095710483700037) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0049516841939184816) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.012445987176150083) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.1848380478931067) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.326269022449851) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.963, 0.007000722943799957) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.515, 0.09881086400151252) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.10128068303782493) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO5', '(DO2']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004538463046774268) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.012400957357138396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.18713063718535705) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.3078265748769045) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.004581466357361933) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.07237036386877298) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.08981219331175089) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005631650159368291) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.012672219794243574) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.17678774490783689) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.2985080732377246) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.005276707545774115) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.541, 0.0752366793397814) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.533, 0.10766842316277325) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005708043130813166) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.902, 0.012327017087489367) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.1758796933213598) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.2866444644443691) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.005248364947419759) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.501, 0.09618840028624982) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.5, 0.11927978489175439) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005351522678742185) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.012370243929326534) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.18667796396449557) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.31123794815316796) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.0049244916999959965) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.512, 0.0953982037594542) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.512, 0.11991778997611255) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004310836763121188) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.010751810647547246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.21047517040651292) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.27565173198282716) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.967, 0.0034079352840781212) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.05932373483479023) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.10089599004387856) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO2', '(DO4']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004447636436671018) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.011788986183702947) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.19966621486961958) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.2911319706216455) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.963, 0.0053732148183044046) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.525, 0.08186794218420983) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.0948446912933141) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.0060034357169643044) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.012133572414517403) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.485, 0.1671998210058664) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.3050692562283948) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.005259083589728107) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.541, 0.08002036074921488) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.531, 0.11591791678126902) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006846598126692698) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.011603521645069122) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.485, 0.18214610518998234) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.2931141526401043) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.962, 0.006682988968590507) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.532, 0.10103609627485276) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.1139929278884083) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005303867717739195) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.013889627926051617) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.485, 0.17522202765173278) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.28657235980778933) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.967, 0.004764003711636178) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.07088321696035564) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.548, 0.08459112322330475) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004703653998440131) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.01315025083720684) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.483, 0.19453826058632695) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.31629382963851094) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.006652714652637769) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.07170919942483306) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.09374874632433057) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO0', '(DO3']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005171339268330485) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.012967125292867423) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.19534987440996338) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.426, 0.28896864630281927) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.00891107480565539) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.485, 0.10736596358194947) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.11087734150979668) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005813176764640957) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.012552256520837546) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.20045600920834114) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.3289101923033595) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.00659195580213418) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.511, 0.09145935396105051) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.10887365350965411) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005622729533351958) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.902, 0.012499608075246214) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.2080001062163792) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.2826531979739666) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.012907808971445774) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.07326784607768058) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.52, 0.08142100427299738) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0052402727548032995) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.0142502047047019) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.483, 0.2002442039544112) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2830949171036482) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.009538045623987274) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.07260326154902577) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.08332890688069165) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006685378229361959) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.0131668038405478) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.20183354685644736) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.31199558871984484) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.010912695602346275) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.506, 0.09519547527795658) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.12753689050726824) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO0', '(DO2']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.0066702776865568015) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.013745926203206181) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.188366348808544) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.3001628960371017) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.966, 0.005204938613518607) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.584, 0.04998781548440456) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.09235820435732603) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005920602762838826) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.014219838384538889) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.2205403304110223) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.2997458920478821) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.008527704745953088) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.532, 0.07640856688749045) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.10262756193429232) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.00738138703815639) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.01436328212916851) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.21786126274781417) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.25438299534469844) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.007313333481630252) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.07477112846821546) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.06721398084238171) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0049613364138640466) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.013243014983832836) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.483, 0.20506503567111212) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.435, 0.26929220093786715) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.0077167306453557105) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.07782497011497616) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.07298569629341364) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005931413350743242) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.014016640700399876) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.22546315783161844) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.435, 0.2973505354374647) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.961, 0.007763873974243324) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.568, 0.05229009596630931) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.06111613709479571) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO1', '(DO0']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007110032574273646) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.012592201460152865) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2065259869727306) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.2473675927147269) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.974, 0.003397278068572632) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.05707247393205762) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.543, 0.0757502583898604) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0057540314910002055) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.010321697533130645) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.22321085164802207) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.2486401267051697) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.004038757189264288) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.07273663422465325) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.11439149584434927) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0060981276808306575) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.9, 0.012690059993416072) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.21491347349900752) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.426, 0.2498540355861187) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.967, 0.005860889215979114) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.538, 0.07043216078542174) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.08076739898696542) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0057119268279057) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.012229299575090408) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.2215430060806102) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.2505981601867825) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.973, 0.005651311823278775) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.064411870053038) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.537, 0.07846526216343046) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005492442491929978) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.013844836000353099) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.2318352054738425) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.426, 0.2400800924897194) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.007510908211387004) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.10657332429813687) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.513, 0.13513966970017646) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04431799054145813), (0.375, 0.06283778327703476), (0.284, 0.06856693160533905), (0.394, 0.08577430075407028), (0.435, 0.11007066434621811), (0.458, 0.1351322904229164), (0.452, 0.15643265687674285), (0.463, 0.1696251774523407), (0.471, 0.19223802105896176), (0.471, 0.19927251303382218), (0.47, 0.21156952209956945), (0.524, 0.05925917188078165), (0.572, 0.04087113991379738), (0.618, 0.04157237386703491), (0.629, 0.046138653486967086), (0.531, 0.05671131333708763), (0.523, 0.07500792333483695), (0.507, 0.08017064135149121), (0.529, 0.07849225649237633), (0.623, 0.049733171582221986), (0.55, 0.07293332076072692), (0.536, 0.06721400199830532), (0.516, 0.07681803411245346), (0.517, 0.09298652433324606), (0.503, 0.08846567318588495), (0.515, 0.09881086400151252), (0.537, 0.07237036386877298), (0.541, 0.0752366793397814), (0.501, 0.09618840028624982), (0.512, 0.0953982037594542), (0.586, 0.05932373483479023), (0.525, 0.08186794218420983), (0.541, 0.08002036074921488), (0.532, 0.10103609627485276), (0.537, 0.07088321696035564), (0.547, 0.07170919942483306), (0.485, 0.10736596358194947), (0.511, 0.09145935396105051), (0.535, 0.07326784607768058), (0.524, 0.07260326154902577), (0.506, 0.09519547527795658), (0.584, 0.04998781548440456), (0.532, 0.07640856688749045), (0.537, 0.07477112846821546), (0.529, 0.07782497011497616), (0.568, 0.05229009596630931), (0.562, 0.05707247393205762), (0.547, 0.07273663422465325), (0.538, 0.07043216078542174), (0.562, 0.064411870053038), (0.52, 0.10657332429813687)]
TEST: 
[(0.254, 0.043263681530952454), (0.37075, 0.060598327726125716), (0.28775, 0.06587039145827293), (0.39, 0.08190187042951584), (0.42925, 0.10396925833821297), (0.45375, 0.12755469673871994), (0.44575, 0.1482144956588745), (0.45525, 0.16008145165443421), (0.46375, 0.17392877840995788), (0.4635, 0.18666653883457185), (0.46075, 0.1964863825440407), (0.51175, 0.054857018277049065), (0.5835, 0.03905948352813721), (0.607, 0.04286541078239679), (0.601, 0.04920833560824394), (0.56425, 0.055526064068078994), (0.54425, 0.07361212676763534), (0.51225, 0.08508697652816773), (0.53225, 0.07989148454368114), (0.59625, 0.05450844718515873), (0.541, 0.0751375679075718), (0.54575, 0.06879891481995583), (0.532, 0.07615075786411762), (0.51425, 0.09632765525579452), (0.51575, 0.08851587608456611), (0.52, 0.101678038418293), (0.527, 0.07525788906216621), (0.5485, 0.07725491143763065), (0.50775, 0.09779092645645142), (0.51075, 0.09670851400494576), (0.5775, 0.06240917073190212), (0.54575, 0.08043163391947747), (0.53725, 0.08298707696795464), (0.53075, 0.10119955432415008), (0.54725, 0.07086918833851814), (0.5445, 0.07130905163288116), (0.501, 0.10834784796833992), (0.51975, 0.0917260570526123), (0.53675, 0.07443436029553413), (0.5395, 0.07331148111820221), (0.5145, 0.09672098144888878), (0.56975, 0.053548068434000014), (0.53325, 0.0787611600458622), (0.53775, 0.0794060685634613), (0.52675, 0.08494250532984733), (0.55725, 0.05648462341725826), (0.56325, 0.06056050279736519), (0.54675, 0.07932510381937027), (0.532, 0.07219325375556945), (0.556, 0.06665360835194588), (0.50825, 0.11182413810491562)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.52      0.91      0.66      1000
           3       0.68      0.06      0.12      1000
           4       0.48      0.98      0.64      1000
           9       0.74      0.09      0.15      1000

    accuracy                           0.51      4000
   macro avg       0.61      0.51      0.39      4000
weighted avg       0.61      0.51      0.39      4000

Collaboration_DC_1
VAL: 
[(0.252, 0.044353610038757325), (0.25, 0.07227671495079994), (0.293, 0.08170339134335518), (0.357, 0.09504514583945274), (0.416, 0.11734175483882427), (0.395, 0.13958969017863274), (0.419, 0.1466355331838131), (0.432, 0.15292382569611074), (0.425, 0.16991872733086347), (0.418, 0.17540518701449037), (0.424, 0.16317588014900683), (0.511, 0.04190209773182869), (0.501, 0.05879997279495001), (0.524, 0.06752537213265895), (0.565, 0.058869879826903344), (0.536, 0.0735227660164237), (0.511, 0.08402937849611045), (0.508, 0.10973101943265647), (0.559, 0.06843662911653518), (0.588, 0.07008283373713493), (0.576, 0.06952183987386525), (0.54, 0.07936977070569992), (0.539, 0.10214133582077921), (0.523, 0.10603591647185386), (0.524, 0.095710483700037), (0.525, 0.10128068303782493), (0.546, 0.08981219331175089), (0.533, 0.10766842316277325), (0.5, 0.11927978489175439), (0.512, 0.11991778997611255), (0.54, 0.10089599004387856), (0.541, 0.0948446912933141), (0.531, 0.11591791678126902), (0.529, 0.1139929278884083), (0.548, 0.08459112322330475), (0.535, 0.09374874632433057), (0.529, 0.11087734150979668), (0.541, 0.10887365350965411), (0.52, 0.08142100427299738), (0.507, 0.08332890688069165), (0.49, 0.12753689050726824), (0.529, 0.09235820435732603), (0.527, 0.10262756193429232), (0.546, 0.06721398084238171), (0.536, 0.07298569629341364), (0.532, 0.06111613709479571), (0.543, 0.0757502583898604), (0.527, 0.11439149584434927), (0.523, 0.08076739898696542), (0.537, 0.07846526216343046), (0.513, 0.13513966970017646)]
TEST: 
[(0.267, 0.04317044323682785), (0.25, 0.06941601613163947), (0.3085, 0.07784869140386581), (0.37125, 0.09040846940875054), (0.41275, 0.11196270486712456), (0.39575, 0.1334228486418724), (0.41875, 0.14155897158384323), (0.43075, 0.1480341517329216), (0.4325, 0.16426597940921783), (0.4275, 0.16917636078596116), (0.4355, 0.1590513065457344), (0.51275, 0.0423591708689928), (0.51425, 0.0573318327665329), (0.5375, 0.06395750749111176), (0.57725, 0.05551974794268608), (0.5575, 0.07384679144620895), (0.52, 0.08239130336046219), (0.51575, 0.10808970096707345), (0.56425, 0.06952416491508484), (0.59625, 0.07097694349288941), (0.57225, 0.07105314666032791), (0.54625, 0.07793892547488213), (0.53675, 0.1019065306186676), (0.5255, 0.1067631945014), (0.538, 0.09583549538254738), (0.53, 0.09900624644756317), (0.5305, 0.09055387100577354), (0.5455, 0.10607391729950905), (0.508, 0.11695737671852112), (0.5145, 0.11789551216363907), (0.54775, 0.09872866663336753), (0.5435, 0.09241484102606773), (0.54125, 0.11476962605118751), (0.5315, 0.11207189065217972), (0.55925, 0.08245446962118148), (0.531, 0.09378272029757499), (0.52175, 0.11152114233374595), (0.54825, 0.11140153756737708), (0.52625, 0.07804599750041961), (0.52325, 0.08127888479828835), (0.51325, 0.1267383229136467), (0.525, 0.091062539935112), (0.52725, 0.10288596579432488), (0.556, 0.06386441293358802), (0.557, 0.06949011436104774), (0.5355, 0.05773640860617161), (0.5375, 0.07430271166563034), (0.5295, 0.11288841217756271), (0.524, 0.07873586469888687), (0.54025, 0.07532961931824685), (0.52275, 0.12927641052007674)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.83      0.15      0.25      1000
           1       0.76      0.92      0.84      1000
           2       0.72      0.05      0.09      1000
           4       0.38      0.97      0.55      1000

    accuracy                           0.52      4000
   macro avg       0.67      0.52      0.43      4000
weighted avg       0.67      0.52      0.43      4000

Collaboration_DC_2
VAL: 
[(0.276, 0.04447247278690338), (0.436, 0.07495411774516106), (0.452, 0.08742304396629333), (0.473, 0.11441597338020802), (0.472, 0.16358603571355343), (0.459, 0.17408120495080948), (0.478, 0.2705200811526738), (0.477, 0.2703526836093515), (0.474, 0.3325524968125392), (0.474, 0.3120443267803639), (0.475, 0.3092682206723839), (0.473, 0.2905787508720532), (0.482, 0.26237700392236), (0.478, 0.2551430446449085), (0.478, 0.24125349630980053), (0.487, 0.2188989610444987), (0.485, 0.20385369112668558), (0.486, 0.1869539916277863), (0.481, 0.1966088991361903), (0.485, 0.1826811176305637), (0.484, 0.18289113834616727), (0.486, 0.19054251021961682), (0.488, 0.19437545802770181), (0.482, 0.17215078497090144), (0.48, 0.18903057567734505), (0.481, 0.1848380478931067), (0.486, 0.18713063718535705), (0.484, 0.17678774490783689), (0.484, 0.1758796933213598), (0.489, 0.18667796396449557), (0.486, 0.21047517040651292), (0.484, 0.19966621486961958), (0.485, 0.1671998210058664), (0.485, 0.18214610518998234), (0.485, 0.17522202765173278), (0.483, 0.19453826058632695), (0.487, 0.19534987440996338), (0.484, 0.20045600920834114), (0.482, 0.2080001062163792), (0.483, 0.2002442039544112), (0.481, 0.20183354685644736), (0.482, 0.188366348808544), (0.481, 0.2205403304110223), (0.482, 0.21786126274781417), (0.483, 0.20506503567111212), (0.484, 0.22546315783161844), (0.478, 0.2065259869727306), (0.475, 0.22321085164802207), (0.475, 0.21491347349900752), (0.479, 0.2215430060806102), (0.481, 0.2318352054738425)]
TEST: 
[(0.27075, 0.04346053540706635), (0.4375, 0.07203164887428283), (0.44975, 0.083502028465271), (0.4725, 0.10839265322685242), (0.47325, 0.15421055006980897), (0.45525, 0.16559282863140107), (0.48025, 0.25170764434337617), (0.478, 0.2528487293720245), (0.475, 0.3110364720821381), (0.478, 0.291153617978096), (0.48075, 0.29257084810733797), (0.4675, 0.27151850748062134), (0.48325, 0.24684937036037444), (0.47575, 0.24082189851999283), (0.476, 0.22677797412872314), (0.4835, 0.20588127845525742), (0.48325, 0.191612422645092), (0.48325, 0.17657097911834715), (0.47775, 0.1864932178258896), (0.482, 0.17358258777856828), (0.48275, 0.17279843533039094), (0.48225, 0.17990777122974397), (0.48225, 0.18272061419486998), (0.48275, 0.1610881029367447), (0.47875, 0.1776358433365822), (0.479, 0.1747632364630699), (0.4815, 0.17671753734350204), (0.48075, 0.16680385649204255), (0.48, 0.16606811743974687), (0.479, 0.1774495911002159), (0.48275, 0.1992003393769264), (0.483, 0.18919264680147171), (0.4805, 0.1583361475467682), (0.48225, 0.17356543445587158), (0.481, 0.16501406049728393), (0.48175, 0.18453200501203537), (0.48425, 0.18453346574306487), (0.4785, 0.18901778304576874), (0.47925, 0.19680064064264297), (0.47775, 0.19019860398769378), (0.48025, 0.19174154424667358), (0.47775, 0.17865070199966432), (0.47675, 0.20723701232671737), (0.4775, 0.20699714213609696), (0.4785, 0.1964244042634964), (0.47625, 0.21708209043741225), (0.47875, 0.19729234135150908), (0.47075, 0.2130081031322479), (0.47475, 0.20319443166255952), (0.47925, 0.2104391998052597), (0.478, 0.21879107719659804)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           6       0.39      0.99      0.56      1000
           8       0.63      0.92      0.75      1000

    accuracy                           0.48      4000
   macro avg       0.26      0.48      0.33      4000
weighted avg       0.26      0.48      0.33      4000

Collaboration_DC_3
VAL: 
[(0.218, 0.04446711611747742), (0.349, 0.08856414425373077), (0.393, 0.09984482651948928), (0.407, 0.10870000609755516), (0.407, 0.13449031338095666), (0.415, 0.1502700263261795), (0.416, 0.1623449475169182), (0.416, 0.190651739038527), (0.406, 0.1890639338158071), (0.414, 0.2167533227801323), (0.43, 0.2174901111423969), (0.41, 0.2288923665173352), (0.419, 0.22874133194237947), (0.426, 0.23922158036381005), (0.429, 0.2568849093317985), (0.431, 0.2810688246190548), (0.435, 0.2763240528255701), (0.43, 0.28909522835537793), (0.421, 0.3004858526550233), (0.424, 0.3147787351384759), (0.432, 0.3099942891150713), (0.43, 0.32181952918320894), (0.427, 0.28343770196661355), (0.427, 0.2644650461152196), (0.429, 0.32515233564376833), (0.429, 0.326269022449851), (0.428, 0.3078265748769045), (0.437, 0.2985080732377246), (0.433, 0.2866444644443691), (0.429, 0.31123794815316796), (0.434, 0.27565173198282716), (0.43, 0.2911319706216455), (0.433, 0.3050692562283948), (0.433, 0.2931141526401043), (0.432, 0.28657235980778933), (0.429, 0.31629382963851094), (0.426, 0.28896864630281927), (0.425, 0.3289101923033595), (0.427, 0.2826531979739666), (0.429, 0.2830949171036482), (0.429, 0.31199558871984484), (0.432, 0.3001628960371017), (0.433, 0.2997458920478821), (0.434, 0.25438299534469844), (0.435, 0.26929220093786715), (0.435, 0.2973505354374647), (0.43, 0.2473675927147269), (0.432, 0.2486401267051697), (0.426, 0.2498540355861187), (0.433, 0.2505981601867825), (0.426, 0.2400800924897194)]
TEST: 
[(0.2305, 0.0433762149810791), (0.34325, 0.08504497039318085), (0.39575, 0.09558765774965286), (0.4115, 0.10409454986453057), (0.40925, 0.12879934746026994), (0.41825, 0.14445116937160493), (0.41375, 0.15593986171483992), (0.4215, 0.18018277913331984), (0.41, 0.17879825294017793), (0.424, 0.20379313588142395), (0.42875, 0.2091999084353447), (0.417, 0.22234842777252198), (0.4255, 0.22261157780885696), (0.43175, 0.23078928554058076), (0.43325, 0.24882063853740694), (0.43175, 0.2728913333415985), (0.436, 0.26471710216999056), (0.43575, 0.2771452513933182), (0.42975, 0.29026478362083435), (0.43175, 0.298292954325676), (0.437, 0.30013892459869385), (0.439, 0.30714428615570066), (0.43375, 0.2755620776414871), (0.43325, 0.25254097270965575), (0.43525, 0.3147801604270935), (0.43975, 0.31063690459728244), (0.43525, 0.2918150017261505), (0.43175, 0.2851661231517792), (0.4365, 0.2724114319086075), (0.4385, 0.2961907660961151), (0.434, 0.2682684198617935), (0.4365, 0.28200457978248594), (0.43875, 0.29770788991451264), (0.43875, 0.28414056634902957), (0.43675, 0.27290387892723084), (0.4375, 0.29974021410942076), (0.4335, 0.2764442993402481), (0.43225, 0.3102149637937546), (0.43175, 0.2692433236837387), (0.432, 0.2740262231826782), (0.43725, 0.29662336194515226), (0.43175, 0.29141005110740664), (0.4355, 0.28980784237384793), (0.43575, 0.24478482443094254), (0.433, 0.26053301882743835), (0.43575, 0.2913859000205994), (0.42875, 0.23680120301246643), (0.434, 0.23624954217672348), (0.43125, 0.24454879999160767), (0.42975, 0.24458771389722825), (0.43475, 0.23504469192028046)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           5       0.37      0.95      0.54      1000
           7       0.54      0.79      0.64      1000

    accuracy                           0.43      4000
   macro avg       0.23      0.43      0.29      4000
weighted avg       0.23      0.43      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [72]
name: alliance-2-dcs-72
score_metric: contrloss
aggregation: <function fed_avg at 0x7b1b20c41c10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=72
Partitioning data
[[7, 9, 1, 2], [6, 8, 1, 2], [4, 3, 1, 2], [0, 5, 1, 2]]
[(array([ 2102, 37010, 13182, 10158, 47275, 19636,  1365, 30216, 31980,
       36407, 45523, 26661, 26983, 44664,  5047,  5597, 35724, 26606,
       14517, 47983, 18828, 43060, 47323, 24898, 36346,  3578, 40182,
        3284, 13516, 13280,  6320, 20701, 22128, 10684, 23000, 17462,
          68, 29789, 31528, 24853,  5774, 34010, 34440, 10045, 24682,
       25310, 45343, 18922, 26291, 29644, 16477,  7598, 31321, 19599,
       15067, 47898, 19909, 28764, 44892, 35176, 40465,  7291, 29962,
         440,   589, 43892, 40340, 16350,  6999, 14202, 34511, 12474,
       30876, 48837, 15999, 42355, 35082, 12844,  3251,  7717, 23621,
       43124, 41850, 24698, 38512,  4809,  8984, 14489, 49610, 37131,
       21051, 10573,  6726, 45708, 16224, 17876, 47372,  5313, 29047,
        6822, 21787, 20581, 24995,  6653, 45054, 11484, 35108, 37019,
       18417,   152, 44440, 31364,  7055, 24013, 18528, 45480, 21394,
       15472,  2207, 32660, 14261, 23682,  7798, 12320, 27529, 10347,
       45747, 44567, 12941, 44135, 31871, 35812, 16828, 43449, 28977,
        5101, 27043, 15013, 35220, 31589, 38616, 43435, 24327, 44661,
       48858,  8521, 29910, 35336,  1742, 12272, 16996, 15712, 40234,
       32211, 40360, 25701, 15324, 21553, 23561, 23627,  8076, 36895,
       10886, 18405, 34015, 37482, 39949, 17537, 18017, 32646, 38826,
       44718, 31156, 10666, 13268, 19159, 39246, 15292, 27697, 21396,
         641, 19940, 12864, 11068,  2366, 26567,  3213,  6239, 28695,
        8887, 38155, 49071,  9279,  4729, 38955, 29272, 30330, 22846,
       26032, 20094, 14144, 49592,   994, 37222,  2801, 34107,  5200,
       19821, 24944, 44262,  1806, 32282, 41319, 15720, 20681,  5241,
       39259, 47092, 17942, 14242, 41129, 11621, 25283, 30163, 49363,
       30030,  1445, 42117, 12744, 49760, 16022, 21876,   789, 40061,
       28832,  7450, 13818, 32061, 40942, 42457, 41784, 32633, 46301,
        3157, 38042, 15271, 23292, 30845, 10041, 11553, 41280,  9606,
       16482, 31378, 47743,  9626, 40599, 48816, 23169, 11312, 22643,
       29228, 38056, 35813, 29306,  2199, 18344, 11876, 30080, 43946,
       27591,  7698, 37993, 12427, 35984, 16601, 23336, 10163, 10144,
       17466, 10231, 14892, 14250, 27551, 20552, 39581, 11258,  6149,
       32040, 17359, 47574, 12555, 16684, 32552, 11472, 23720,  2935,
        1232, 13969, 41765,  8089, 31722, 42407, 43960, 37249, 25303,
       36291, 20947, 14224, 18020, 28189,   102, 39174, 16809, 14913,
       36413, 48332, 33856, 25387, 40043, 45465, 42278, 33813, 23303,
       36434,  8821, 40015, 26229, 44678, 35184, 24712, 30850, 11292,
       15694,  8125,  5332, 45547, 48284,  1765, 44692, 30966, 33437,
       21408, 22156, 19193, 16744, 28846, 46646, 19122,  1638, 40413,
       36475, 12835, 13380, 40059, 21330, 49398, 42638,  2637, 41614,
         668, 45906, 27184, 32945, 38280, 26574, 35667, 32820, 18703,
       12385, 23426, 42106, 24152, 25407, 16054, 40387,  5591, 32112,
       28053, 15263, 16999, 35498, 16092, 30678, 12562, 40790, 19053,
       41495,  8841, 25091, 38267,  7461, 13370, 13881, 42385, 43346,
       16773,  5306, 49823, 39841, 46059, 41630, 47148, 14599, 16170,
       37600, 13869, 36577, 34773, 16725, 10448, 14424,  3175, 23908,
        3143, 20481, 37400, 42208, 36210, 30932, 11889, 44881,  9091,
       16262, 11173, 26647, 14197, 40052, 25752, 26262,  3702, 13529,
       36179, 33417, 16699, 34024, 22802, 23023, 22464, 34474,  9959,
       17945, 48210, 28684, 42851, 37730, 30420, 24938, 26468,  2058,
       10943, 47121,  1987, 16535, 27784, 36918, 24718, 34290,  6708,
        8749, 20176, 12288, 25988, 17097, 42973, 30351, 35379, 16688,
       34485, 24086, 20718, 17526, 12709, 22296,  5668, 37780, 46190,
        3648, 14825, 21696, 34078, 28046, 22240, 29148, 13475, 36384,
       36031, 36649, 33868, 18567, 19120, 37285, 41408, 26620, 28549,
        8146, 34427, 47175, 48577, 23877, 40152, 22533,  7992,  6371,
       11075, 27228,  6813,   743, 20619, 28526,  7513, 46904, 41948,
       16755, 16089, 11791, 34400, 14912, 41695, 37592, 26290,  2469,
       10882, 41392, 37929, 41847, 40076, 18321, 39902, 13323, 43636,
       12169, 49286, 41262, 49085, 39391, 19866, 46741, 48746, 25050,
       12629, 22938, 37844,  2431, 11804, 33034, 24292, 27174, 12064,
        8920, 49196,  4696, 33748, 43847, 10236, 30463, 36334,  9327,
       28347, 38596, 46207,  7358, 32994, 31395, 31208, 49993,  3830,
        7841,  8244,  4654,  3059, 10627, 21593, 40449, 32418, 10522,
       46259, 22747, 36672, 37611, 49426, 25279,  5816, 44886, 38919,
       28559, 49221,  6971, 43297, 49060, 20453,  2286, 10747, 21585,
       24010, 25201, 25488, 10790,  3467, 12373, 31407, 12814,  4145,
       11403, 13523, 44475, 38270, 11130, 47616,   427, 29019, 19667,
       38204, 32011, 21283, 13332, 46674, 18844, 39405, 45289, 15669,
       19446, 35335, 36891, 24008,  2727, 21215, 45599, 43240, 21871,
       33105,  1946, 24024, 16223, 18246, 34673, 45105, 28187, 28688,
       16275, 35666, 30050, 30696, 12917,  5146,  2769, 43068, 35757,
       22145,  9252,  8570, 29315,  5257, 10569, 31552, 28393, 22788,
       21850,  3273, 24194, 44448, 23263, 33292, 37616, 21829, 22636,
       48275, 22833, 47279, 22552, 35058,  1389, 39118, 49151, 20865,
       32681, 16956, 22553,  5769, 22659, 12981, 40145,   848,  8124,
       17091, 42130, 29264, 19454,  1565, 41338, 35207, 31599, 19164,
       49411,    96, 38892, 43932,  7649, 42734, 41663, 26794, 21423,
       44235, 49849, 31300, 20336, 13858, 14844, 15950, 31957, 14717,
        5282, 30120,   226, 47495, 27730, 14538,  8938, 38464, 11822,
       28552, 48234, 18926, 17586, 31297, 16613,  1541, 45687, 28532,
         834, 11574, 34349, 37460, 23607,   262, 39333, 10953, 34145,
       20368, 40220, 13468, 38493, 25910, 22399, 46514,  8191, 43600,
       41087, 29522, 24828, 14423, 29042, 21654, 40327, 48651, 11630,
       20125, 12992, 30767,  9903, 31467, 25161,  6390, 47675, 37581,
        7783, 34566, 19998, 44053,  5603, 20149, 38186,   463, 22803,
       48090, 25695, 32387,  1777, 24457, 48388, 47730, 33699, 32570,
       48594, 30795, 27328, 36778, 36583, 12180,   403, 20876, 45550,
       28705, 35230,  1415, 27430, 45109, 44216, 41764, 10480, 27816,
       49156, 18149, 32420, 26992, 26990, 34293, 19874,  4896, 48012,
        4700, 30531, 45428,  5634,  9580, 33851, 27325,  4168, 34187,
       14588, 43856,  5384,  4220, 43823, 31307,  6558, 47846, 41469,
       42428,  4511, 14814,  8077, 39436, 29612, 35764, 33503, 15813,
       31090, 42155, 38830,  4398, 40231, 49342, 13179, 36245, 42158,
        7868, 16739,  6146,  9001, 27091, 42815, 44842, 25263, 36657,
       22280, 45229, 22006, 25530, 25831, 29715, 39765,  1789, 16123,
       26321, 21650, 15477, 12748, 14063, 23670, 39648, 36465, 44758,
       26982,  1995,   689, 46877, 40860, 12860, 37647, 35880,  3416,
        3760, 43469,   933, 30005, 36355,  5781, 45238, 21500, 42020,
       10935,  8148, 37947,  6616, 37280, 19070, 17344, 21102,  6917,
       18184, 21018, 15820, 27973, 46082, 18287, 22770, 21392,  4656,
       18241, 34181,  1676, 40221,  3484, 31475, 10018, 34608, 45074,
       35319,  9827,  1077, 38502, 36094, 35482, 39390, 28788, 28664,
       48870, 30705, 32166,  6502,  1497, 14834, 43287, 10474, 22375,
       43593,   281, 19714, 23241, 49328, 42564, 18756, 45342,  4242,
       48348, 45855, 49449, 11923,    41, 42234,  7480,  6388, 34497,
       38457, 11606, 38480, 24780,  9455, 15731,  6799, 25130, 46089,
        8713, 20138, 17538, 22864, 44328, 39594, 21208, 12462, 12584,
       49101, 21643, 27186, 22279, 20696, 14620, 48694, 30505, 27746,
       24064, 35823, 23388, 43041, 44121, 37374, 27542, 49395, 35673,
       24254, 44910, 14739, 35226,  6448, 37941, 20737,  1852, 15124,
        1530]), [7, 9, 1, 2]), (array([36373, 22530, 35781, 29310, 42872, 10211,  7403,  1317, 37261,
       41534, 34020, 12031, 12369, 30864, 31558, 30212, 44814, 46252,
        7091, 43390, 12321, 43174,  3716, 42712, 43691,  9653,  3041,
       47580, 46974, 10176,  6938,  5263, 11999, 49512, 21132, 25793,
       18103, 14162,  6025, 18870, 46317, 23370, 17282, 12604, 42584,
       33608, 17925, 43734, 26054, 23152, 34056, 12597, 12445, 15294,
       10600,  5661, 45693, 19184, 19308, 29687, 33173,  3926, 19763,
       28011, 42975, 19489, 16696, 17523, 41533, 48983, 47953, 25169,
       29241, 23087, 43431,  1876, 27626, 17287, 18021, 14660, 43761,
       16960,  1047, 16499, 36463, 36913, 21475, 32248, 39630, 42077,
       31142, 23366, 40206,  2796, 12874, 11392,   836, 13776, 13965,
        7064, 44224, 42859, 35672, 36522, 18615, 30816, 12796, 18554,
        3192, 33988, 18330, 18665, 34612, 20146, 16242, 41552, 16862,
       35891, 23579, 11675, 40316, 24992, 39357, 30001,  7967,  9423,
       46009, 37933, 46334, 20775, 42723, 27678, 45420, 37084, 27051,
       14830, 46050, 35600, 16782, 33436, 40437, 33758, 30644,  9275,
        8436,  7968, 49655,  8474, 34552, 35091, 28370, 21447, 46587,
       11066,  8363, 10517, 46765, 25761, 45597, 42577, 44096, 23593,
        2607, 25729, 13162,  5409, 22295, 33405, 49129,  6566, 29931,
       28504, 49289,    22, 18420, 38771,  7321, 34148, 48203, 27183,
        7172, 17143,  2898,  6442,  9323, 33791, 20008, 10757, 34762,
        3290,  8803, 23956, 29971, 25747, 22282, 33948, 44417, 24852,
       27117, 45300,  8679, 41785, 16611, 44518, 23987, 47858, 44489,
       36111,  1596,  5497, 16778, 45253, 42250, 32397, 30906, 25851,
       44708, 42748, 45719, 23771, 29339,  2276,  2777, 32759, 21125,
       34139,  6145, 12048, 29194, 12151, 24307, 14444, 36998, 18025,
       32080,  4678, 27741, 23373, 41196, 45385,  9112,  3912, 15193,
        4951,   355, 29550,  4143,  7632,  7036, 27367, 41867, 28507,
       32356, 45898, 44057, 32370, 39419, 27386,   958, 41218,  4779,
       41447,   897,  7029, 48805, 36214, 14442, 37692, 40669,  9786,
        1162,  5365, 39069,  9882, 46462, 20719,  5480, 22409, 30844,
       42270, 37953, 33264, 22920, 44043, 23125, 14281, 35617, 16044,
       17034, 23214, 16775, 23625, 48120, 12443, 21679, 24185, 24984,
       23838, 49897, 46543, 15948, 12206, 43858,  6788, 41957,  8414,
       40671, 48864, 28180, 24138, 33613,  4995, 40008,  5091, 34135,
       44594, 13369, 49899, 27727, 33861, 31367, 13587, 22661, 46223,
        7007, 38850, 34910, 48393, 49479, 42506,  1429, 47780, 42840,
        6916, 46710, 41712, 19251, 41008, 47400, 12513, 10840, 43644,
        5910,  3904, 14693,  7083, 23669, 16430, 47052, 28006, 12557,
       27243, 33756, 13576, 48297,  5820, 35825, 45649, 16607, 35707,
       38862,   507, 28066, 23440, 10537, 39038, 11244, 38162, 20727,
        5255, 27067, 44054, 47380, 12792, 42917, 41485, 24765, 39971,
        8657, 37711, 41292, 13481, 27550, 44420, 39362, 40495, 40292,
       37420, 32714, 37643,   609, 32783, 49521, 11490, 13095, 46985,
       38136, 36131, 24455, 28842, 29477, 49681, 27715,  3516, 36547,
       31182,  9188,  5941, 35885, 31714, 34892, 25756, 24083, 25754,
       22465, 24456,  2344, 38754,   259, 21134, 36498, 12249, 27858,
       37633, 17290,  9063, 26583, 46111, 13197,  2635, 42847, 11832,
        3305,  8531, 34319, 49887, 28707, 35540, 47776,  2381,  5985,
       37550, 43713, 49625, 29055, 27561, 38276, 47713, 28012, 30398,
       35150, 47376, 29788,  1794, 16086,  3258,  9669, 23960, 14613,
       46479,  5948, 25924, 21635,  1239, 45325, 36526,  7156, 18112,
       24588, 38734,  4955, 14369, 34695, 17799,  6134,  6219,  8267,
       47021, 20822, 36882, 25573, 30303, 42720,   786, 29877, 44701,
        7310, 41250,   291, 31188, 45243, 24328, 42884, 38797, 25533,
       32271, 47770, 37959, 41256, 15043,  6366,  6453,  3085,  4100,
       25071, 40655, 35129, 10133, 32679, 36813,  4354, 49503, 12903,
       39097, 18671, 18123, 13973, 22564, 28175, 31969, 24607,  8932,
       35041, 38719, 11427, 27194,   396,  2862, 42413, 47745, 33181,
        1559, 17515,  7261, 18649, 39476, 37799,  7019, 47060, 18181,
        6074, 17030, 35705, 34287, 40582, 11698,  6241, 25611, 30782,
       17366, 38785, 49594, 30673, 47839, 41627, 11664, 22358, 13277,
       40505, 15710, 35688, 47618,  4500,  6639, 27773, 31830, 22982,
       29932,  3773, 33257, 16197, 12743, 19018,   482, 27321, 38637,
       30865, 25206, 31539, 46544, 29318,    79, 34138, 11767, 44210,
        7428,  2584, 24678, 43638,  6668, 23933, 27089,  7378,  8977,
       49748, 33207, 18625, 26008, 12451, 35799, 30810,  6670, 28655,
        1304, 32883, 44198, 40987, 49356, 28784, 10450, 29546,  6515,
       24161, 40118, 41051, 26576, 32580,  2570, 13558, 22388, 39769,
       19869, 39692, 26320, 23529, 44168, 43948, 19983, 17893, 17647,
       30951, 23116, 37551, 49345, 40554, 28086, 36237, 40713, 22013,
       40386, 24923, 35339, 44571, 35933,  4858, 20856, 33085, 20680,
       32042, 10022, 44620, 22450, 37588, 13615, 12923, 44423,  2582,
       40880, 47178,  4299, 33897, 36703, 43605, 46865, 16310,  4562,
       32492, 24224,  7587, 44109,  3924, 46953,  7659, 29473,  6933,
       38333, 17593, 19341, 42753,  7198, 21529, 29283,  5076, 41378,
       21080, 49066, 12470, 20114,  4444, 43797, 24191, 44433, 28127,
       13509, 40615,  4200, 46162, 44217, 11707, 45322, 49383, 29901,
       30489, 31703, 39204, 12123,  9602, 19002, 39622, 28027, 15748,
       35567, 36593, 28220, 26168,  5149, 13217, 42735, 42448, 46928,
       23280, 23423, 31193, 37468, 21538, 24986, 40046,  5195, 39674,
        5247, 17128, 39679, 11683, 15776, 41532, 41964, 16989, 34530,
       28844, 26712, 41579, 49362,  8812,  5552,  9442, 36950, 49676,
       45866, 23139, 14336, 25912, 31394, 34968, 38234,  6583, 12721,
       44774, 21189,  3610, 11241, 21711,   425, 49870, 47589,  7074,
       21591, 32480, 34609,  1929, 12742, 19439, 17134, 32071, 42792,
       24636, 40032, 13154, 26181, 27743, 16472,   171,  9089,  4367,
       11992, 22322, 31343, 23843,  8182, 28443, 11137, 17786,  3440,
       45893, 34200, 32153, 34088, 40452,  3669, 21115,  8032, 18882,
        4730, 18477, 13639, 13637, 19916, 14640, 11342, 14918,     6,
        1793, 23138,  1936, 45709,  7951,  5151,  5506, 28518, 18242,
       27580, 31469, 35841, 12519, 27204,  7542, 13340,  9408, 47816,
       33793, 34827, 32377,  4703, 13542, 15522, 36756, 33881, 25830,
       38509, 16406, 13955, 43692, 30969, 37503, 40515, 46541, 25398,
       45710, 24016, 16530, 17682,  8178,  9802, 43476, 23315, 22676,
       35031,  4604,  1492, 48712, 14195, 23919, 42147,  4755, 16306,
       47411,   808, 15929, 42696, 39233, 13970, 12346, 38676, 19695,
       27594, 35259, 18496,  9900,  7056, 19977, 46125, 42661,  9440,
       31410, 42480, 27744, 31979, 35104, 39646, 40371, 46686, 23696,
       23692, 12945,  8616, 43607, 42345, 17571, 21940, 34463, 28348,
       44831,  5818, 38096, 49450, 42185, 48417, 41401,  7661, 33021,
        4376, 20633, 45818, 33396, 36169,  5333, 35958,  3119, 12958,
       38060, 27738, 12697, 22988, 19108, 19504, 48885, 26398, 14899,
       26541,  4776, 48323, 33042,  6976, 35168, 20318, 40305, 38766,
       48364, 27966, 37278, 40087, 17382, 30067, 14017, 27823, 44736,
       19359,  2875, 48973,   544, 44660, 22373, 49678, 45451, 26070,
       39312,  6660, 32582, 20712, 18143, 22843, 49693, 32278,  4811,
       17949, 20073, 13362, 14926, 15560,  6593, 43179,  4633, 12471,
       19220, 31035, 40487, 16165, 20029, 46589,  5226, 38016, 49736,
       46475, 13226, 15589, 42988, 16352, 17092, 32526,  8676, 41157,
        3769, 32430,  5609, 30287, 14317,  5922, 25222,  3856, 20850,
       14619]), [6, 8, 1, 2]), (array([44851, 11250, 43147, 48478,  2710, 43494, 44071,  9885, 11859,
       22133, 30037, 41178, 31893, 39244, 31615, 30425, 36096, 14263,
       15919, 45315, 12333, 23879, 26854,  1832, 10166, 29052, 43386,
       41093, 33586, 20137,  5051, 33279, 13896,  6103, 31775, 35250,
       45555, 10224, 26423, 23654, 36690, 15036, 43228, 49402, 24737,
        7602, 27276, 10282, 43473, 32413, 28219, 21389,  4104, 13513,
        5631, 39615, 31256, 21659, 46011, 13708, 14041,   378, 39451,
        5880, 26047, 32756, 11649, 28654, 48143, 28074,   946,  4193,
       43033, 40840,  6180,  2654,  8260, 34331, 16181, 41582, 30561,
       34727, 14656, 35601, 44525,   153, 23200, 16151, 29125,  2695,
        9352,  1866, 35228,  3723, 29862, 22173, 45122, 23455,  9761,
        5486, 12132, 11565, 43049, 42303, 14810,  4129,  6560, 38701,
       14447, 46251,  5625, 27135, 33703, 13778, 46591, 35634, 37288,
       31075,  4384, 42811,  3089, 10728, 41665, 23252, 27166, 15790,
       31365,  2999, 37294, 19510, 42640, 10006, 31627, 42138, 43584,
       10877, 39293,  3948, 46372,  2856, 27258,  2245, 40862, 29291,
       18705, 28002, 23814,  9260,  3433, 33215, 19528, 41305, 19516,
       32201, 33027, 35526, 39853, 34752, 28881, 42629, 36428, 10134,
       49154, 38161, 10207, 19452,  1795, 28903,  5143, 25893, 40591,
       37627, 33370, 44729, 15918, 14852, 12672, 25992,  8899, 46506,
       27455, 20192,  1212, 33740,  4057, 28755,  3116, 22536,  2418,
       33669, 43203, 28583, 24171, 17800, 27644, 33378, 48425, 45132,
       17045,  5669,  7407, 19614,   904, 32523, 17594, 47311, 14996,
       32459,  2216, 14090,  2144,  5957, 48889,  2900, 29278, 36536,
        8981, 18115, 29613,  7634, 29070, 26975, 45397,  9593, 21041,
       22765, 35954, 33408, 45189, 14060, 27631,  7531, 38124, 30734,
       10604, 48502, 46490, 39284, 19178, 37469, 24846, 19112, 44586,
       13385, 19315, 14262, 42096, 48521, 14316,  9147, 12642,  9943,
       43922, 18414, 36926, 15210, 13330,  8722, 12033,  2564, 13254,
       29561,  9371,   691, 25384, 12016, 24332, 14543, 13991,  5190,
       17455, 32504, 19830, 42585, 26859,  7253, 45128, 31934,  9300,
       46712,  7235,  3177, 38485, 19449, 27114,  9670,  3109, 33132,
       40839,  7852, 42837,  7804, 43123,  3568, 11387, 40446, 18949,
       13925, 14171, 49083, 40370,  9841,  1265,  4674, 17449,    36,
       49280, 30424, 35794, 11623, 22842, 15523, 27115, 32358, 28162,
       23197, 24364, 11836, 21501,  3875, 17821, 40365, 44852, 30772,
        5586, 37304,  9152,  1030, 29096, 38891,  6162, 18222, 10468,
       34136, 33956, 37804, 22866, 48638, 27722, 44354, 47132, 16301,
       42482,  5848,  4310,  9702, 30459, 20367, 49466, 15766, 35016,
       20194,  1098, 21103, 22110, 18329, 49563, 45897, 16120, 19276,
       11103, 36601, 44381, 10209, 43955,  7539, 45789, 11970, 48629,
       18127, 19799, 10255, 11222, 27698, 43551, 11805,  6584, 20153,
       36425,  4994, 25559,  1655, 12342,  9429, 36930,  2432, 31141,
       10506,  4346, 27021,  3807, 12750, 49140,  8330, 39921, 37939,
        6229, 24749, 19853, 22568, 22891, 48452, 46374, 20058, 14018,
        1963, 10587, 28423, 47892, 49408,  6306,  7161, 33358, 36474,
        6905,  6169, 16808, 10929, 36080, 14330, 31868, 39980, 21544,
       32982, 17047, 30960, 29139, 42357, 38992, 35124, 17578, 38066,
       48007, 13793, 28563, 12528, 12425, 21147,  2383,  7517,  5797,
       22223, 23189,  9792, 16604,  6684,  5963,  7153, 36990, 42373,
        8467, 31094,  5007, 13907, 25899, 22660,  2562,  8028, 16796,
       15321, 39158, 33976, 46286,  3709, 49078,  3190, 24652, 45319,
       44543, 14800,  6567, 14014, 44957,  1120, 42650, 10653, 15444,
        8680, 13393, 11668, 47933, 19875, 12546, 27275, 25713, 31460,
       27665,  5183, 39438, 44390, 36828, 37769, 44859, 33978, 32732,
        4108, 46503,  7519,  6135, 37204,  4739, 26877,  7357, 49234,
        6226, 19609, 39058, 44311,  5102, 14498, 24520, 46893, 23463,
       14927, 18890, 12755, 39911, 38710, 43818, 47349, 35753,  2825,
       12335,  1548, 45187,   168, 14967, 42666, 39040, 27802, 18117,
        7370, 23151, 11308, 33426, 40060, 22627, 35406, 28626, 34675,
       19063, 30923, 27994, 49230, 25596, 14593, 34233, 48806, 33838,
        4517, 14668,  2587, 28045, 49186, 37743,  6064,  6967, 18480,
       48899,  4769, 42031, 25061, 47113, 11473,  7821,  3452, 34940,
       14546, 21369, 21137, 33050, 30439, 43098, 23439, 45195, 26424,
       25459, 43918,   714, 36490, 26074, 38037, 33112, 23533, 42347,
       22762,  7390,  8696, 13849, 42171, 26344, 43646, 49392, 37689,
       17219, 23032, 22151,  4983, 44841, 36298, 44601,  6121, 31534,
       25069,  7777,  9070, 20245, 49539, 34453, 19125, 47566, 19546,
        7728, 36844, 24228, 49232,    65, 43164, 18698, 39828, 23332,
        5685, 22043,   432,  7827, 21893, 47423, 20994, 20273, 42860,
       29209, 46796, 22529, 42498, 27402, 44575, 31694, 17409, 32348,
       33643, 31209, 29986,    44, 27792,  6464, 25105, 45202,  5747,
       42292, 41710, 22818, 13994, 37461, 20162,   753, 38902, 27745,
       26532, 24385, 36388, 15577, 40472,  9765, 47001, 28538, 23564,
       18238, 17255, 16766, 39234, 36400, 17126, 48185, 39964, 32886,
       41489, 48003, 32462, 49759,   761, 26470, 31523, 24528,  6544,
        1551, 14209, 38650, 36251, 19598, 24462, 24425, 28694, 41849,
       22217, 25614, 33114,  9863, 45877, 46026, 36379, 43195, 47280,
       15425, 27447, 28081, 48681, 10157, 48840, 20844, 23601, 20222,
       37698, 33907, 16992, 12551, 24932, 41138, 31607, 14425, 37981,
       35674, 20381, 49873,  6673, 17701, 27576, 28192, 21604, 13207,
       34845,  2600, 24979, 14966, 44436, 28643, 35037, 34364, 49959,
       40484,  3505, 49075, 33742,  9781, 13317, 41230, 47288,  8346,
        3851, 49121, 13845, 48074, 48733, 41174,  3488, 15065, 25528,
       20538, 42706,  8157,  2044, 25089, 23537, 13286, 41333, 42120,
       29087,  8234, 33564, 24927,  2343, 47172, 33170, 18646, 39412,
       41667, 28667, 19932,  5717, 42168, 41084,  3245,  4493, 39901,
       11171, 32310,  1787, 17767,  6073, 16349, 28802, 28426, 20959,
       19039, 34873, 20457,   709, 14435, 30834, 26287, 15926, 10391,
       26605, 18162, 47152,  6685,   963,  6856, 14840, 38365,  2133,
        1609, 29229, 41599, 35853, 33379, 23123, 35310, 14473,  3629,
        5707,  9463, 18503, 35148, 11382, 44730, 45252, 33350,  6757,
       16215, 38546, 40134, 28497,  6236, 35834, 16616,  8019, 15326,
       37934, 23064,   630, 43019, 33448, 43545, 35653, 14127,  6069,
        7332,  9368, 38001, 29907, 24134, 14021, 47248, 31884, 19611,
        5340, 30096, 16232, 49858, 48370,  7664,  7340, 24962,   779,
       48300, 31002, 41273,  3982, 40348, 15649, 26730, 11270, 21332,
       25195, 33519,  7662, 38058, 36112, 38592, 40084, 39580,  8819,
       39269, 36293, 43680, 42217, 46053, 22780,  5926, 29354, 45360,
       17883, 37713, 32067,  2846, 25587,  1219, 44238, 12287, 39960,
        1918, 24496, 22960, 47452, 22035, 22432, 41990, 13506, 13814,
       29925,  6469, 30229, 23716, 48608, 28410, 18027,   790, 18234,
       42394, 36726, 35860, 12067,  2354, 46410, 39540,  6488, 36138,
       48790, 40244,  4634, 37488, 21987, 28862, 14459, 29126, 37615,
        8112, 11629,  7919, 15563, 13757, 36883, 16319, 43569, 39112,
       28686,    63, 26629,  1475, 46284, 33277, 35645,  3692, 32257,
       14720,  5638, 35353, 11385, 25491, 26215,  9660, 30025, 37895,
       25174, 42889, 15285, 23217, 46373,  6041, 22667, 17769, 32030,
       11030, 16693, 35157, 49573,  6467, 27470, 41562, 38705,  9992,
       14478,  1189,  4356, 49725,  4091,  8990, 42172, 38750, 11454,
       44574, 40661,  8740, 37932, 23158, 28641,  7449, 48194,  9404,
        9989]), [4, 3, 1, 2]), (array([38010, 31880, 23847, 43277, 35763, 14567, 39230, 48930, 48136,
       30383, 33530, 30645,  7547, 45927,  5404, 11939,   765,  9860,
       48788, 11721,  6400, 42151, 31757, 46303, 33933, 37879, 27609,
       18299, 40851, 40548, 34125, 43289, 22200,  5743, 39396, 10777,
       49387,  7731, 12375, 23240, 41664, 12944, 27425, 42587, 31412,
       47805,  4854, 23419, 48852,  8355,  6428, 32058, 22608, 13650,
       49559,  8660, 10796,   405, 14456, 42754,  2277, 26144, 32508,
       42878, 39739, 24764, 30324, 37181, 26704, 26488, 48908, 17107,
        3362, 46614,  9119, 47754, 20236, 30537, 29280, 29364, 38639,
       34012, 35263, 29173, 49303,  2287, 21360, 16225,  2066, 29844,
        5103,  4599, 36470, 16191,  5249, 13860, 30428, 29940, 48556,
       12328,  3197, 44616, 44884, 33542,  3842, 26391, 23021, 27000,
       28302, 37504,  8258, 43924, 43996, 27440, 34032, 30304, 23767,
       49528,  3049, 24543, 46041, 40287, 35603, 39966, 37061, 17221,
       28469,  5163,  9616,  7509,  1878,  7225, 15571, 46380, 29634,
       15304, 23352, 13457, 26656, 41410, 11114, 30959, 32107, 19445,
       38943, 43442, 11633, 10713, 37189,  4165, 42332,  4477, 24491,
        7203, 28179, 16075, 29515, 43483,  9657, 43475, 12270,  1205,
       33328, 44940, 16634, 17233, 39584, 48324, 29257, 30052, 10729,
       43368,  1178, 45304, 24844, 38387,  6132, 46872, 10146, 14782,
       28584, 44176,  7464,  5174, 35391,  4774, 15900, 34787, 17453,
        3607, 28033, 39138, 14867, 22264, 10987, 43814, 41243, 10955,
       38753, 28014, 19188, 37558, 37034, 12284, 25420, 46019, 41546,
       12052, 33662, 21639, 46369, 48554, 44303, 37102, 47332, 18591,
       42517, 17465, 20393, 33380, 42103, 28636, 44721,  2171, 18106,
       15115,  8655,  8549, 39392, 18271, 29620, 49628, 28833, 37945,
       39196, 33128, 28675,  7930,  2692, 48257,  3292, 47468, 22769,
       29927, 47324,  4552,  2683, 21288, 37062,  9435, 47018, 27671,
       23503, 22895, 45018, 26785, 32999,  9205, 24452, 42820, 32386,
       23432, 34635, 48817, 22387,  5593, 42813, 41501, 31561, 24702,
       11947, 19896, 10104, 33835, 12371, 14185, 10760, 31555, 34941,
       27805, 27141, 19889, 15968, 40783, 21780, 41875,  8683, 29796,
       28299, 43653,  6978, 32944, 32530, 39714, 47440, 44119, 49541,
       12485, 39458,    83, 33702, 26236, 10366, 40503, 49333, 16236,
       42384, 35842,  3002,  4255, 13990, 40269, 28144, 20292, 42691,
       36332, 27422, 44441, 27066,  1991, 23121, 44229, 21810,  5367,
        6575, 39225,  8269, 40205,  5729, 49185,  4548, 24796, 26845,
       47672, 38424, 29268, 27956, 19524, 48454,  6232,  5201, 44665,
       10319, 21957, 12545, 47059, 42987, 30701, 33142, 28854,  7417,
       34007,  4495, 37531, 44046, 49467,  8601, 36350, 33965, 23668,
        8768, 22840, 37676, 13252,  8960,  1538, 37216, 21059,  8954,
        3604, 14759, 21804, 43404, 14533, 38784,  5970, 10194, 25589,
       25517,  1567, 44312, 28845,  7189, 27150, 40905, 38417, 28843,
       28221, 26171,  1943, 20562, 22834, 44106, 15898, 18732, 29996,
       28550, 15602, 49565, 19012, 39147, 39748,  6436, 33989, 47768,
       36246, 18971, 10175, 12069, 28877,  2108, 29398, 26827, 36854,
       42731, 40004, 28069, 26954, 14908,  6815, 47223, 39539, 44345,
       38513,  1204, 31264, 36439, 29818, 15320, 13249, 41997, 11155,
       24226, 14734, 46667, 36487, 44085, 47485, 31973, 35949, 15403,
       15709, 45422, 20076, 19222,  1873, 33889, 16361, 35386, 48181,
       48083, 28286, 10181, 34207,  5510, 14530, 34220,  6082,   784,
       18981, 46222, 46621, 32044, 32804,  4740, 16767, 29603, 17259,
       18894, 21586,  3426, 48797, 32516, 26998, 26676, 32798, 20059,
       47038,  8511, 22650,  7113, 11267,  5760, 24867, 10555, 12573,
       25170, 33723, 40831, 24819, 14748, 12198, 43189, 28595, 13121,
       35254,  3015, 24363, 21834, 42178, 13218, 13843, 23100, 25044,
        2851, 28362, 36715, 30582, 38484, 15114, 40024, 21027, 26636,
       27905, 18518,   991,  3822, 41289, 37136, 27129,  9565, 12311,
       31147,  9209, 14302, 16272, 49163, 15740, 27871, 40537, 14061,
       26787,  4553, 48655, 11163, 41189, 45077, 22048, 23635, 13118,
       40407, 31102, 17312, 31072, 40250, 13505, 45769, 44225,  7124,
       20674, 46406, 33252,   261,  6709,  1631, 25874, 47774, 43369,
       22379,  6161, 44529, 49080, 28344, 21971, 24003,  8529, 34830,
       16410,  8216, 17580, 26471,  6851, 30135,  8090, 37264, 11242,
        4250, 41550, 37491, 34206,   772, 25209, 36594, 21280, 48269,
        5632, 18754, 38818, 14849, 24119, 23662, 44097, 12389, 33314,
       16180, 41455,  5224, 45787, 11196, 49616, 13296, 16087, 38549,
       48898,  8174, 18879, 42684,  2828,  3238, 28617, 39965, 13543,
       40358, 32311, 45859, 25306,  5805, 12964, 33910, 42402, 34063,
       16929, 24746, 24450, 41063, 13999, 48709, 34862, 43954, 48631,
       21720, 21094, 48528,  2259, 22884, 37620, 23203, 22067,  3661,
        2180, 42119, 25558,  9918, 46588, 41666, 11799, 27273, 21795,
        3922, 23379,  4528, 43006,  2280, 27130,  2545, 25881, 21407,
       13327, 18776, 25314, 11692,  1494,  7707,  2731, 11082, 44385,
         466, 35827, 42531, 41502, 16029,  5157, 36181, 29133, 42124,
        6895, 25608, 34809, 26241,  7381, 35651, 20056, 16563, 43145,
        4443, 36445,  8556,  5951, 10476, 28183,  9007, 17861, 21150,
       46598, 14518, 27607, 30840, 35291,  4648,  9400, 32488,  5627,
       15042,  3935, 34548, 46330, 14519, 35605, 14311,  4661, 19930,
       41133,  6120, 39275,  6889, 12893, 27084, 44589, 35953, 26467,
       44643, 38725, 17460, 34358, 46794, 18744, 47040, 20811,   250,
       24206, 17700, 32472, 47701, 37687, 15814, 18066, 12299, 17492,
       10630, 31138, 21985, 36591, 25567, 29188, 17471, 13766, 38878,
       27510, 22270,  3808, 12703,  2202, 22953, 31684, 48282,  4189,
       41498, 10860, 19729, 47133, 35493, 20403,  2619, 11825, 35765,
       13698, 26279, 32711, 45489, 30239, 24699, 15584, 28648,  2436,
       37560, 10406, 44351, 48232,  2784, 46563, 36494,   701, 12549,
       22974, 41143, 45681, 19157, 46642, 44797, 44327, 43826, 39729,
       34482, 39222, 36116, 20990, 32687,  9217, 36616, 27076,  5417,
       49008,  2090,  1050,   796, 28158, 15751, 48354, 35466, 27546,
       22587, 10926, 23264, 40213, 15837, 37008,  3101, 41856, 29636,
        9519, 13643, 21224, 35824, 15789, 37613,  3608, 36658, 29243,
       39308, 40941, 32151,  6499, 27750, 37485, 38989,  4109, 29079,
       33677, 35862, 34789, 44203, 16757, 13626,   778, 32951, 15910,
       15369,  4122, 37127, 29812, 44732, 13418, 31550, 47958,  8340,
       49666,  2719, 36009, 48589, 26449, 22215,   724, 30470, 26757,
       36377, 10866, 13435, 30396, 41454, 16312, 29682, 14715,  1675,
       19296, 34254, 29658, 10898, 12819, 10885,  4588, 39360, 36410,
       42765, 46795, 30377, 13727, 47171, 48205, 34268, 18464, 49248,
       39110,  7151, 34925, 30572, 19453, 11713, 18612, 10160,  7336,
       31454,  9712, 12582, 31687, 17902, 34508,  8173, 35785,  9420,
       36130, 18733, 31057, 40917, 46389, 25989, 36651, 18248,  7688,
       29470, 28291, 47853, 48270, 29389, 26976, 11194, 25030, 48598,
       47492, 49224, 24978, 48365, 39693, 38565, 45335, 30680, 47481,
       14601, 48745, 19806, 21630, 15847, 14174, 43106, 46088,    55,
       28031, 19130, 31707, 29269, 19435, 11197, 10152, 14088, 17850,
       27121,  9876,  5012, 49093,  2993, 29005, 31492, 43285,  8482,
       37121, 38389, 14135, 49329, 46870, 13524, 15922, 20061, 18385,
       39055, 44652, 42328, 24977, 25296, 39881, 12044, 41422,  8726,
       30746, 27326, 33457,  5576,  3259,  9933, 41512,  9448, 38129,
       37797,   646, 21668, 34685, 45367,   885,  3845, 15297, 16906,
       17117]), [0, 5, 1, 2])]
Collaboration
DC 0, val_set_size=1000, COIs=[7, 9, 1, 2], M=tensor([7, 9, 1, 2], device='cuda:0'), Initial Performance: (0.25, 0.04502433729171753)
DC 1, val_set_size=1000, COIs=[6, 8, 1, 2], M=tensor([6, 8, 1, 2], device='cuda:0'), Initial Performance: (0.26, 0.044506120443344115)
DC 2, val_set_size=1000, COIs=[4, 3, 1, 2], M=tensor([4, 3, 1, 2], device='cuda:0'), Initial Performance: (0.249, 0.04467034757137298)
DC 3, val_set_size=1000, COIs=[0, 5, 1, 2], M=tensor([0, 5, 1, 2], device='cuda:0'), Initial Performance: (0.248, 0.04460585105419159)
D00: 1000 samples from classes {1, 2}
D01: 1000 samples from classes {1, 2}
D02: 1000 samples from classes {1, 2}
D03: 1000 samples from classes {1, 2}
D04: 1000 samples from classes {1, 2}
D05: 1000 samples from classes {1, 2}
D06: 1000 samples from classes {9, 7}
D07: 1000 samples from classes {9, 7}
D08: 1000 samples from classes {9, 7}
D09: 1000 samples from classes {9, 7}
D010: 1000 samples from classes {9, 7}
D011: 1000 samples from classes {9, 7}
D012: 1000 samples from classes {8, 6}
D013: 1000 samples from classes {8, 6}
D014: 1000 samples from classes {8, 6}
D015: 1000 samples from classes {8, 6}
D016: 1000 samples from classes {8, 6}
D017: 1000 samples from classes {8, 6}
D018: 1000 samples from classes {3, 4}
D019: 1000 samples from classes {3, 4}
D020: 1000 samples from classes {3, 4}
D021: 1000 samples from classes {3, 4}
D022: 1000 samples from classes {3, 4}
D023: 1000 samples from classes {3, 4}
D024: 1000 samples from classes {0, 5}
D025: 1000 samples from classes {0, 5}
D026: 1000 samples from classes {0, 5}
D027: 1000 samples from classes {0, 5}
D028: 1000 samples from classes {0, 5}
D029: 1000 samples from classes {0, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.295, 0.06421255218982697) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.06314386874437332) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.279, 0.0991387677192688) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.08198602849245071) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.07024952399730683) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.0713477740585804) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.1353405278623104) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.08995045691728593) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.43, 0.0827924791276455) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.07941373744606972) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.1567333093881607) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.112021663159132) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1102066800892353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.09507753160595894) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.384, 0.17660580858588218) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.1585041108801961) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.1454920856282115) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.1163012564405799) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.392, 0.18973852957785128) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.20079846078529953) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.14613931557536125) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.129363997079432) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.20482386314868928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.22117121162824332) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.1731434683352709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.13845351364091038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.19699945250153542) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2411895220540464) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.17792725160717965) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.16328691100515424) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2086139908656478) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.25990544859599324) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.19964710243418812) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.15808263261057437) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.22641488587111236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.2669675034582615) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.20615564992837607) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.16297005883976817) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.20922902297973633) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.30454038487095386) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[1, 2], M=tensor([1, 2, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.0, 0.38379942226409913)
DC Expert-0, val_set_size=500, COIs=[9, 7], M=tensor([7, 9, 1, 2], device='cuda:0'), Initial Performance: (0.954, 0.004131161715835333)
DC Expert-1, val_set_size=500, COIs=[8, 6], M=tensor([6, 8, 1, 2], device='cuda:0'), Initial Performance: (0.966, 0.0030926317796111107)
SUPER-DC 0, val_set_size=1000, COIs=[7, 9, 1, 2], M=tensor([7, 9, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[6, 8, 1, 2], M=tensor([6, 8, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7b1b10196250>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b1b100bdd30>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b1b00034dc0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b1b10372e80>, <fl_market.actors.data_consumer.DataConsumer object at 0x7b1b00034130>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO3', '(DO1']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0035221625976264476) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002354241312481463) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.18386021249741316) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.31833401720551774) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.005495381377637386) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.05138745877146721) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.611, 0.04711497538536787) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004041570998728275) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0025008824989199638) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.17904818304628134) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.3309083677215967) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.00834367676731199) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.030915080904960632) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.681, 0.03064527228474617) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004361590513959527) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0024912773398682473) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.17617730913311244) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.464, 0.3293173926549498) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.006965852086432278) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.0634345736913383) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.649, 0.04727604538202286) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0037544837035238744) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0023050397471524775) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.18372245874255896) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.32630942011985464) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.010531275779590942) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.548, 0.061159960687160494) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.64, 0.038703296676278115) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00406503688544035) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0028353199136909096) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.19084825753420592) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3295405399762676) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.008381078937090933) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.05960114695131779) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.0485256392583251) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO4', '(DO2']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004133877843618393) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0023589464761316777) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.20043354495614768) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3332724494190188) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.005084654035978019) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.545, 0.053410481132566925) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.634, 0.044868985913693905) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.003783723637461662) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.002880899906856939) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.2046310017965734) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.2975411652083858) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.937, 0.010570196706568823) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.504, 0.08840854343771934) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.552, 0.0751067014504224) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004600404906086624) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002855712175834924) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.18812722448259592) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.35907194894942224) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.010892057879362255) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.514, 0.09307936588674784) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.09479079065285623) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0035577586814761163) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0024817079552449284) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.18266505537182093) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.33825455740862526) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.008388279547914863) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.506, 0.08739521692693233) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.06594080097228289) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.00452441736496985) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0025534573614131657) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.180247542783618) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3168447942631319) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.009795925629441626) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.07835395070910454) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.0908039882350713) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO0', '(DO4']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0041838788231834765) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0035100969563936816) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.19556128771603107) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.3462472595400759) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.00711161315618665) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.503, 0.09019218799471855) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.585, 0.07141893716529012) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004384567371103912) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003296720242477022) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.19287516875565053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.33787162720091873) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.921, 0.010237305277958512) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.509, 0.09209848398715258) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.571, 0.07254933050274849) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004274816138669848) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0031698765095206907) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.20092415293306112) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.29332724752486683) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.944, 0.008504291361066863) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.10697155044227838) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.0836482191644609) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0051867583831772205) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0030884011521120558) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.17730192650854587) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.2995915189869702) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.007011504105874337) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.09270216604135931) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.606, 0.07051327857095749) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004022851251065731) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003805829043412814) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.18069262432307004) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.30738947535923217) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.006571149981842609) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.525, 0.08774808017164469) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.621, 0.052863385464996096) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO0', '(DO4']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0048152166670188306) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0037005656828405335) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.1869629035964608) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.30085092289187015) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.008903935134774657) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.08469153322465718) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.06685156486183405) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003989328656345606) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.00352043839963153) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.18020244571566582) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.3302752493198786) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.007831253650103462) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.08086594934388995) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.607, 0.05762522266060114) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004870001455303282) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.0029424887590575963) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.18418789630383253) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3278509486471448) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.009921526637084753) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.494, 0.1192357069030404) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.55, 0.10944057058641919) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004105246098712087) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0029428781586466357) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.17995811492949723) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.293106251881225) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.013149472659241837) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.492, 0.13136483296379448) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.531, 0.09531385103613138) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004253890760242939) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0027409988565486854) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.17192268759012222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.32628063821443354) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.007248848891351372) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.489, 0.10558764082379639) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.07221743055246771) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO0', '(DO3']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.00428725354000926) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002941045953892171) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.20608075834810735) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.33641849370638377) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.006153641883283854) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.506, 0.09847121477499604) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.548, 0.08381473668199033) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005182423121761531) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0028888752270140685) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.1725312401279807) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.31097608954660244) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.009702508732265415) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.1374344061492011) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.1038289817161858) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004070797581225634) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0029226292714593) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.1899268855508417) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.32906972370960286) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.009956440125770315) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.501, 0.10769976529665291) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.539, 0.09993451394233853) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004179065296426416) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0032598789796466007) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.17789871758595108) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.33869926084199686) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.005899787205751636) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.511, 0.09480030181258917) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.06869870704133063) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.004255195893347263) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0038332071884942705) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.1670384888648987) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.30529849406986614) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.011432616418042472) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.492, 0.12149926289962605) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.531, 0.08883572082221508) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO4', '(DO2']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0037987028742209076) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003457797548326198) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.18465007976815104) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.305204662292148) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.0050050808189553205) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.08531565684825182) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.582, 0.07947974296472966) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0037256068661808968) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0034778371330467053) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.1893707966655493) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.318592195186764) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.007692638761727722) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.526, 0.07693178515136243) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.597, 0.06044771133363247) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.003614762233570218) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0035289717577397825) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.1845285111963749) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.33643766116641927) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.009076316411468725) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.09213779946416617) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.07499386621732265) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004148833710700273) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0037284746484074274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.19957816214114427) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.31376058898004705) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.007677986195863923) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.0786966532226652) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.0647356337197125) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.003639570580329746) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0032130795264965857) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.19538650082983078) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3186998917285819) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.012776305509652957) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.502, 0.10744410343142226) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.59, 0.0675921737793833) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO5', '(DO4']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0038036999106407167) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0032407282760832457) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.22126757119596005) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3383158032404026) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.007412571769567876) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.08962069313507527) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.599, 0.0680255016349256) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0038903460004366935) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003509155035484582) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.22339246147871017) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.32091884703488904) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.006453720159002842) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.07608901207987219) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.06501387753151357) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004505973075982183) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004009791626158403) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.2126373286191374) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3430332345443021) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.006705102394471396) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.511, 0.09747843292262405) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.607, 0.06062903247028589) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004200360352406278) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003460024130617967) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.20393642991781236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.3533447125525272) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.00553861783704815) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.532, 0.08483748073875905) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.609, 0.05400278761982918) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004389744982356205) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003674470611520519) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.21699548076838254) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.33528383498292536) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.010117674861365231) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.08995370437996462) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.06166857491061092) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO3', '(DO4']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.004553033236414194) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003952157361374702) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.412, 0.23332873742468654) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.35759497376770016) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.0056315213893249165) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.499, 0.11867766288854181) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.553, 0.08180630638450384) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004025360015453771) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0037023653125797863) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.2269443319980055) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3450086441967869) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.00712512257911294) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.506, 0.1263811546661891) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.09692079655639828) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004781404203502461) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.004361297839419421) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.25097545693814755) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.3144464952341368) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.005363008955027908) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.08956181368976832) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.628, 0.0555214568823576) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0048112732176668945) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0044790397400538496) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.24442600490897895) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3055227238544612) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.007522558324508282) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.07848589916247875) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.647, 0.046248831432312724) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004409669436048716) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0037086950603552396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.22589104625582695) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.30655279696593063) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.010147301625919682) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.501, 0.10945426203869284) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.07214247000403702) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04502433729171753), (0.295, 0.06421255218982697), (0.447, 0.07024952399730683), (0.43, 0.0827924791276455), (0.458, 0.1102066800892353), (0.448, 0.1454920856282115), (0.472, 0.14613931557536125), (0.477, 0.1731434683352709), (0.47, 0.17792725160717965), (0.477, 0.19964710243418812), (0.477, 0.20615564992837607), (0.521, 0.05138745877146721), (0.632, 0.030915080904960632), (0.529, 0.0634345736913383), (0.548, 0.061159960687160494), (0.542, 0.05960114695131779), (0.545, 0.053410481132566925), (0.504, 0.08840854343771934), (0.514, 0.09307936588674784), (0.506, 0.08739521692693233), (0.52, 0.07835395070910454), (0.503, 0.09019218799471855), (0.509, 0.09209848398715258), (0.48, 0.10697155044227838), (0.543, 0.09270216604135931), (0.525, 0.08774808017164469), (0.529, 0.08469153322465718), (0.537, 0.08086594934388995), (0.494, 0.1192357069030404), (0.492, 0.13136483296379448), (0.489, 0.10558764082379639), (0.506, 0.09847121477499604), (0.488, 0.1374344061492011), (0.501, 0.10769976529665291), (0.511, 0.09480030181258917), (0.492, 0.12149926289962605), (0.552, 0.08531565684825182), (0.526, 0.07693178515136243), (0.527, 0.09213779946416617), (0.527, 0.0786966532226652), (0.502, 0.10744410343142226), (0.533, 0.08962069313507527), (0.555, 0.07608901207987219), (0.511, 0.09747843292262405), (0.532, 0.08483748073875905), (0.518, 0.08995370437996462), (0.499, 0.11867766288854181), (0.506, 0.1263811546661891), (0.524, 0.08956181368976832), (0.521, 0.07848589916247875), (0.501, 0.10945426203869284)]
TEST: 
[(0.25, 0.04384468254446983), (0.3, 0.06179358550906181), (0.45025, 0.0672670782506466), (0.438, 0.07863861978054047), (0.46075, 0.10416459986567497), (0.4575, 0.13624865448474885), (0.471, 0.13515578013658525), (0.473, 0.1617032497525215), (0.46975, 0.16537044155597685), (0.477, 0.18476930224895477), (0.475, 0.19292769503593446), (0.5085, 0.049594658568501475), (0.617, 0.031885359466075895), (0.5305, 0.0616301077157259), (0.52225, 0.06411781576275825), (0.5395, 0.05976144552230835), (0.5455, 0.05337210862338543), (0.5115, 0.08523830181360245), (0.496, 0.0901118965446949), (0.50675, 0.08427100804448127), (0.5195, 0.07485401317477226), (0.5045, 0.09078868296742439), (0.51075, 0.09220481806993484), (0.4895, 0.10711954751610755), (0.54125, 0.08964145678281785), (0.5325, 0.08499404102563858), (0.52275, 0.0825006255209446), (0.532, 0.07905989408493042), (0.4915, 0.11584325262904167), (0.49275, 0.12748400312662125), (0.498, 0.10268284916877747), (0.5095, 0.09252441784739494), (0.49075, 0.13581372475624084), (0.5075, 0.10281807041168213), (0.508, 0.09270265161991119), (0.48675, 0.1219315848350525), (0.541, 0.08238770735263824), (0.515, 0.07599413773417472), (0.52575, 0.09406894361972809), (0.52525, 0.07722375324368477), (0.50725, 0.10427484196424484), (0.53825, 0.0890689331293106), (0.53375, 0.07503791689872742), (0.51875, 0.09670595115423203), (0.5215, 0.0836260067820549), (0.5055, 0.08914461585879326), (0.49875, 0.11358042746782303), (0.5065, 0.12179488253593444), (0.5265, 0.08777408093214036), (0.5385, 0.07677607935667038), (0.5045, 0.1167546803355217)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.52      0.90      0.66      1000
           2       0.46      0.95      0.62      1000
           7       0.85      0.08      0.15      1000
           9       0.69      0.08      0.15      1000

    accuracy                           0.50      4000
   macro avg       0.63      0.50      0.40      4000
weighted avg       0.63      0.50      0.40      4000

Collaboration_DC_1
VAL: 
[(0.26, 0.044506120443344115), (0.378, 0.06314386874437332), (0.453, 0.0713477740585804), (0.447, 0.07941373744606972), (0.478, 0.09507753160595894), (0.479, 0.1163012564405799), (0.487, 0.129363997079432), (0.485, 0.13845351364091038), (0.486, 0.16328691100515424), (0.489, 0.15808263261057437), (0.483, 0.16297005883976817), (0.611, 0.04711497538536787), (0.681, 0.03064527228474617), (0.649, 0.04727604538202286), (0.64, 0.038703296676278115), (0.604, 0.0485256392583251), (0.634, 0.044868985913693905), (0.552, 0.0751067014504224), (0.516, 0.09479079065285623), (0.578, 0.06594080097228289), (0.556, 0.0908039882350713), (0.585, 0.07141893716529012), (0.571, 0.07254933050274849), (0.526, 0.0836482191644609), (0.606, 0.07051327857095749), (0.621, 0.052863385464996096), (0.598, 0.06685156486183405), (0.607, 0.05762522266060114), (0.55, 0.10944057058641919), (0.531, 0.09531385103613138), (0.589, 0.07221743055246771), (0.548, 0.08381473668199033), (0.528, 0.1038289817161858), (0.539, 0.09993451394233853), (0.584, 0.06869870704133063), (0.531, 0.08883572082221508), (0.582, 0.07947974296472966), (0.597, 0.06044771133363247), (0.556, 0.07499386621732265), (0.562, 0.0647356337197125), (0.59, 0.0675921737793833), (0.599, 0.0680255016349256), (0.584, 0.06501387753151357), (0.607, 0.06062903247028589), (0.609, 0.05400278761982918), (0.589, 0.06166857491061092), (0.553, 0.08180630638450384), (0.541, 0.09692079655639828), (0.628, 0.0555214568823576), (0.647, 0.046248831432312724), (0.584, 0.07214247000403702)]
TEST: 
[(0.2605, 0.04348890659213066), (0.38275, 0.06072431656718254), (0.45925, 0.06845111122727394), (0.451, 0.07635300573706627), (0.479, 0.09213218602538109), (0.482, 0.1134857217669487), (0.4865, 0.12708065676689148), (0.48525, 0.1371110547184944), (0.48575, 0.1597578677535057), (0.4875, 0.15451923394203185), (0.48675, 0.16020039916038514), (0.604, 0.04682619926333428), (0.6835, 0.028726908639073373), (0.6355, 0.0475630909204483), (0.64875, 0.03905782355368138), (0.60175, 0.04868839333951473), (0.6385, 0.043850510001182556), (0.542, 0.07302979752421379), (0.51575, 0.0920225732922554), (0.5905, 0.06374608072638512), (0.55025, 0.08885753193497657), (0.58875, 0.06805980046093464), (0.5605, 0.07626938691735267), (0.5545, 0.08018089410662652), (0.61775, 0.06652869147062301), (0.622, 0.051091536700725554), (0.5955, 0.066834149569273), (0.60325, 0.05687493994832039), (0.53475, 0.10495305600762367), (0.5345, 0.09402200922369958), (0.5605, 0.07420885565876961), (0.55675, 0.07941368746757507), (0.53475, 0.10072625496983528), (0.53775, 0.10109879344701767), (0.58075, 0.06691012784838676), (0.5245, 0.08895269647240639), (0.5745, 0.07548565791547299), (0.5925, 0.05860040234029293), (0.558, 0.07249265319108963), (0.56475, 0.06532153835892678), (0.57725, 0.06385978215932846), (0.6125, 0.06346276649832726), (0.58925, 0.06216464059054851), (0.61525, 0.05896672582626343), (0.612, 0.05251207199692726), (0.58375, 0.058428099289536475), (0.54275, 0.08186982691287994), (0.54725, 0.0953772374689579), (0.6225, 0.05241008907556534), (0.641, 0.04714115771651268), (0.5545, 0.07600985030829907)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.62      0.88      0.72      1000
           2       0.45      0.93      0.61      1000
           6       0.74      0.20      0.32      1000
           8       0.90      0.21      0.34      1000

    accuracy                           0.55      4000
   macro avg       0.68      0.55      0.50      4000
weighted avg       0.68      0.55      0.50      4000

Collaboration_DC_2
VAL: 
[(0.249, 0.04467034757137298), (0.279, 0.0991387677192688), (0.306, 0.1353405278623104), (0.354, 0.1567333093881607), (0.384, 0.17660580858588218), (0.392, 0.18973852957785128), (0.403, 0.20482386314868928), (0.403, 0.19699945250153542), (0.404, 0.2086139908656478), (0.422, 0.22641488587111236), (0.416, 0.20922902297973633), (0.408, 0.18386021249741316), (0.422, 0.17904818304628134), (0.417, 0.17617730913311244), (0.43, 0.18372245874255896), (0.415, 0.19084825753420592), (0.429, 0.20043354495614768), (0.427, 0.2046310017965734), (0.431, 0.18812722448259592), (0.428, 0.18266505537182093), (0.424, 0.180247542783618), (0.427, 0.19556128771603107), (0.43, 0.19287516875565053), (0.426, 0.20092415293306112), (0.422, 0.17730192650854587), (0.422, 0.18069262432307004), (0.424, 0.1869629035964608), (0.427, 0.18020244571566582), (0.427, 0.18418789630383253), (0.426, 0.17995811492949723), (0.428, 0.17192268759012222), (0.431, 0.20608075834810735), (0.425, 0.1725312401279807), (0.42, 0.1899268855508417), (0.424, 0.17789871758595108), (0.427, 0.1670384888648987), (0.424, 0.18465007976815104), (0.419, 0.1893707966655493), (0.426, 0.1845285111963749), (0.419, 0.19957816214114427), (0.423, 0.19538650082983078), (0.419, 0.22126757119596005), (0.416, 0.22339246147871017), (0.418, 0.2126373286191374), (0.422, 0.20393642991781236), (0.416, 0.21699548076838254), (0.412, 0.23332873742468654), (0.422, 0.2269443319980055), (0.416, 0.25097545693814755), (0.433, 0.24442600490897895), (0.417, 0.22589104625582695)]
TEST: 
[(0.2515, 0.043640657246112824), (0.267, 0.09514398372173309), (0.29675, 0.12978019261360169), (0.3455, 0.14937431919574737), (0.3785, 0.16722273707389831), (0.389, 0.17958619666099548), (0.39425, 0.19564056634902954), (0.3885, 0.18772674351930618), (0.39475, 0.1980825604200363), (0.409, 0.21704438626766204), (0.412, 0.19847762775421143), (0.40275, 0.17583742314577103), (0.41075, 0.16942558860778809), (0.40525, 0.16784597796201706), (0.41275, 0.17497918325662612), (0.404, 0.18199895739555358), (0.41, 0.1896017897129059), (0.41, 0.19568356204032897), (0.414, 0.17988643532991408), (0.41475, 0.1759766820669174), (0.41525, 0.17169854706525803), (0.41325, 0.18685327035188676), (0.41225, 0.18241507798433304), (0.41075, 0.19474096059799195), (0.41075, 0.17064579200744628), (0.40925, 0.17189207595586778), (0.4155, 0.17876838034391404), (0.416, 0.17524011182785035), (0.4175, 0.17597886401414872), (0.416, 0.17412792509794237), (0.4165, 0.16320803266763687), (0.413, 0.1994569067955017), (0.41475, 0.1649059609770775), (0.40925, 0.18274429339170456), (0.411, 0.17087822639942168), (0.4185, 0.15873599940538408), (0.41525, 0.17330454248189925), (0.4025, 0.17865080600976943), (0.41025, 0.17543030393123626), (0.40575, 0.19098983824253082), (0.40675, 0.18437568932771684), (0.41175, 0.20764293044805526), (0.4075, 0.20992211270332337), (0.40775, 0.20068238061666488), (0.40875, 0.19474701911211015), (0.41375, 0.2031334908604622), (0.41025, 0.22183357882499694), (0.4135, 0.21594828355312348), (0.41375, 0.23488667118549347), (0.4145, 0.2339666292667389), (0.41225, 0.212296755194664)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.00      0.00      0.00      1000
           3       0.54      0.72      0.62      1000
           4       0.35      0.93      0.51      1000

    accuracy                           0.41      4000
   macro avg       0.22      0.41      0.28      4000
weighted avg       0.22      0.41      0.28      4000

Collaboration_DC_3
VAL: 
[(0.248, 0.04460585105419159), (0.388, 0.08198602849245071), (0.468, 0.08995045691728593), (0.465, 0.112021663159132), (0.476, 0.1585041108801961), (0.46, 0.20079846078529953), (0.457, 0.22117121162824332), (0.463, 0.2411895220540464), (0.468, 0.25990544859599324), (0.472, 0.2669675034582615), (0.467, 0.30454038487095386), (0.477, 0.31833401720551774), (0.469, 0.3309083677215967), (0.464, 0.3293173926549498), (0.475, 0.32630942011985464), (0.472, 0.3295405399762676), (0.471, 0.3332724494190188), (0.471, 0.2975411652083858), (0.473, 0.35907194894942224), (0.474, 0.33825455740862526), (0.472, 0.3168447942631319), (0.47, 0.3462472595400759), (0.475, 0.33787162720091873), (0.473, 0.29332724752486683), (0.476, 0.2995915189869702), (0.469, 0.30738947535923217), (0.474, 0.30085092289187015), (0.474, 0.3302752493198786), (0.472, 0.3278509486471448), (0.476, 0.293106251881225), (0.477, 0.32628063821443354), (0.474, 0.33641849370638377), (0.471, 0.31097608954660244), (0.48, 0.32906972370960286), (0.477, 0.33869926084199686), (0.475, 0.30529849406986614), (0.466, 0.305204662292148), (0.474, 0.318592195186764), (0.478, 0.33643766116641927), (0.476, 0.31376058898004705), (0.473, 0.3186998917285819), (0.472, 0.3383158032404026), (0.476, 0.32091884703488904), (0.471, 0.3430332345443021), (0.478, 0.3533447125525272), (0.47, 0.33528383498292536), (0.469, 0.35759497376770016), (0.473, 0.3450086441967869), (0.472, 0.3144464952341368), (0.475, 0.3055227238544612), (0.475, 0.30655279696593063)]
TEST: 
[(0.23875, 0.043380969196558), (0.3875, 0.07882354038953782), (0.4575, 0.0859538711309433), (0.45425, 0.10616131201386451), (0.46675, 0.1488635730743408), (0.45475, 0.19176117146015167), (0.45075, 0.21221389746665956), (0.46, 0.22604575574398042), (0.46225, 0.24731591022014618), (0.46675, 0.25240520119667054), (0.4685, 0.2846794567108154), (0.46675, 0.30064474666118624), (0.46425, 0.31476924812793733), (0.45875, 0.31366590893268587), (0.47, 0.3090231018066406), (0.46425, 0.31122599899768827), (0.4685, 0.3161320595741272), (0.469, 0.2808144611120224), (0.47075, 0.3327020502090454), (0.46875, 0.3158009867668152), (0.467, 0.2979028247594833), (0.47025, 0.32436511290073394), (0.469, 0.31601544880867005), (0.471, 0.27676729738712313), (0.4725, 0.2837307184934616), (0.46775, 0.28832132828235624), (0.469, 0.28371425914764403), (0.47125, 0.30774762451648713), (0.46875, 0.3112962156534195), (0.4735, 0.27436411535739896), (0.47425, 0.3026272498369217), (0.474, 0.3144242397546768), (0.46775, 0.2960063804388046), (0.47175, 0.30645994448661806), (0.47075, 0.31505017292499543), (0.46625, 0.2913876268863678), (0.467, 0.2892656021118164), (0.4695, 0.2998173820972443), (0.47425, 0.3117482670545578), (0.474, 0.29367441165447233), (0.4715, 0.30041339695453645), (0.4735, 0.3183230729103088), (0.47275, 0.30200605487823484), (0.47125, 0.3210502940416336), (0.4715, 0.3379882661104202), (0.4685, 0.32073547065258023), (0.465, 0.33990876364707945), (0.47075, 0.3236062602996826), (0.4665, 0.3009311734437943), (0.4685, 0.2894306619167328), (0.46975, 0.29025882184505464)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.56      0.91      0.69      1000
           1       0.00      0.00      0.00      1000
           2       0.00      0.00      0.00      1000
           5       0.41      0.97      0.58      1000

    accuracy                           0.47      4000
   macro avg       0.24      0.47      0.32      4000
weighted avg       0.24      0.47      0.32      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [46]
name: alliance-2-dcs-46
score_metric: contrloss
aggregation: <function fed_avg at 0x760831b3dc10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=46
Partitioning data
[[0, 5, 1, 6], [8, 3, 1, 6], [4, 7, 1, 6], [2, 9, 1, 6]]
[(array([41745, 36201, 46585,  2513, 33328, 10243, 10637, 36506, 39196,
        5426, 33363, 17115, 40873,  6858, 31004, 43504, 13419, 38618,
       35855, 20383, 35417,  1214, 20045, 49799, 26565, 31391, 28380,
       36491,  9171, 37841, 22680, 42044, 48869, 35529, 15439,  3295,
        6015, 39619, 39286, 17717, 47865, 49525,  1913,  8072, 44866,
        2707, 32661, 21633, 33950,  5888, 44968, 43828, 29444,  3184,
       44074, 11762, 18451,  9124,  1147, 32290, 33438, 22264, 46287,
         695, 12355, 26049,  9905, 21044, 32369, 12659, 45288, 12553,
       19082, 19106,  2355, 31654, 20862, 41606, 30749, 27434, 33647,
       40761, 39471,  1470, 24383,  5557, 39995, 44430,  3644, 42204,
       44849, 41068,   600, 32839, 29634, 42566, 47645, 10578, 31339,
       30709, 42517, 39793, 32233, 27301, 47215,  1950, 40018, 47969,
        5984,  5765, 12056, 14520, 14025, 34567, 29219, 24695, 25191,
       29286, 41751, 11356, 21143, 24507, 31048,  7972, 39691,  9430,
        1988, 35934, 12325,  3197, 12419,  1329,  4176, 19859, 24101,
       30811, 19634,  7511, 17166,  4165, 35626, 44116, 22320, 34040,
       28865, 30233, 23449, 19332, 12328,   189, 46763,  8062, 39589,
       14666, 14567, 33516, 26018, 32931, 44618, 35414, 16253, 44666,
       15546, 23377,  7251,  2413, 21248, 24351, 41200, 24931,  5371,
       10686,  7971, 29620, 43012, 30240, 18768, 22034, 17107, 20452,
       18572, 17316,  2885, 10146,   457, 43542, 27415, 34783, 14213,
       41238,  6399, 32662, 41821, 47617, 26723, 39329, 49717, 21272,
       35422, 14023, 48378,  7095, 28487,  1338, 18782, 45423,  3136,
       38478,  7581, 36545,  1044, 49507, 33502, 40649, 11913, 22286,
       36052, 46706, 16559, 13457, 19409, 49263, 29104,  5414, 14069,
       25160, 12492,   199, 12704, 49868, 14210, 13894, 35570, 43639,
       48671, 29977, 29518, 33934,  6090,  4524, 20349,  8136, 44476,
       28092,   866, 47379, 17959, 34381, 43378, 18203, 15713, 40957,
        2013, 16199,   984, 38507, 45181, 19437, 33634, 27472, 16949,
       24831, 30356, 30162,  4467,  2884, 46441,  2182, 48890, 39463,
       22941, 11725, 14992, 35543, 18016, 42494, 48135, 36139, 42218,
        1519, 11143, 46112, 48110, 43994, 21977, 12371, 36692, 35534,
       33295, 28283, 15229, 25603, 11861, 43273, 47038, 20732, 23185,
       34074, 13257,  4456, 22879, 48395,    81, 30965, 20043, 26138,
        3021, 37375, 23090, 18212, 44832, 16185, 22681, 32864, 20809,
        6794,  1224, 29037, 42491, 41915, 19024, 14929, 46318, 35194,
       28484, 22434,  7425,  8039, 38529, 23238, 31551, 22445, 27219,
       33477, 13073, 41942, 19933, 32255, 18280,  9383, 12300, 35166,
       40456, 31243,  5645, 11735, 22003, 32926, 10835, 37531,  5208,
       43535, 26589, 30308, 31109, 36235, 22731, 16431, 34884, 43654,
        3069, 36057, 25840, 41886, 19786,  7115,  1157,  3810, 16979,
       14841, 26277, 45142, 36912, 31631,  7729, 44033, 38792, 33067,
       40887, 30188, 34634,  7562, 11077, 40889, 37894,  6058,  1199,
       46525, 30189, 30063, 12682, 25699, 17708, 28999, 36335,  5850,
       13461, 33965, 23437, 16496, 18096, 23556, 29358, 46201, 42323,
       13680, 15378,  5448, 34112, 11274, 30847, 16835, 18189, 46606,
       11004, 25925, 27359, 39456, 24741, 45915, 12832, 44089, 49715,
       11491,  5369, 29804, 27397, 44561, 44687, 24867,  3660, 44740,
       14001, 28505, 38784, 28425, 30126, 21839, 12089,  4319,  3017,
        3005,   999, 30662, 36769,  1280, 43115, 15207,  3420, 48910,
       22671, 35394, 10541, 45581, 42538,  5233, 22483, 33927,  1942,
        2916, 23249, 23544, 30523, 33504, 35604, 35822, 49431, 15177,
       42613, 37341,  3022, 13252, 35390, 27345, 11441, 21075, 35602,
       34048, 22895,  4355,  4968, 40871, 26680,  3060, 17165, 11043,
       31901,  4694,  5698, 13604,   535, 46083, 35748, 18894, 36120,
        8902, 47773,  1998, 47720, 21586, 34493, 23895, 46674, 17384,
       27731, 39792, 12594,  8556, 10571, 37491, 20057, 20366,  5311,
       37799, 11482, 23714, 48021, 16197,  5683,  7683,  6656, 16412,
       43372,  7649, 14096, 39824,  5236,  5301, 35041,  1305, 30486,
       38257, 30912, 49435,  1380, 48496, 29252, 48612, 16250, 47040,
       31776, 35174, 10790,  8149, 35399, 47310, 14448,   547,  9459,
       40505, 11336,   257,  4250,  4533, 45923, 11367, 33686,  7005,
       17606,  7912, 43976, 15386, 22859,  1651, 11515, 15195, 22090,
       42709, 47256,  9766, 48263,  4528,  7358, 41378, 37847, 44269,
       38911, 29792, 38078, 26344,  5663, 28906, 38744,  5149,  6161,
        5099, 18583, 37007, 35803, 47030, 46140, 31534,   855, 11130,
        9232, 26740, 30752, 35847, 23423, 45500, 16077, 45377, 43089,
       31300, 38795, 18388, 47288,  8307,  1064, 16810, 20856, 15662,
       10964, 34906, 18902,   893, 40172, 41519, 31737, 28352, 20616,
        3346, 25799, 49469, 34595, 36400, 41607, 13763, 14643, 21149,
        3661, 43618, 48275, 28027, 45714, 20304, 49973,  1694, 25750,
       19363, 18754, 23142, 11716, 14201, 32913, 26614, 16598, 41579,
       24220, 11683, 22957, 47237,  5392, 26450, 25196, 32937, 30182,
        4945, 40060,  6420,  3808, 21152,  5197, 24566, 15306, 49509,
       14546, 15710, 14040,  8449,    96, 48791, 42143, 22943,   311,
       20155,  2756, 17701, 25582, 45769, 12766,  6244,  7378, 20376,
       25323, 42710,  3444, 43251,  9878, 47900, 24463, 31151, 28848,
        8848, 35322, 45488, 39800, 43416, 31619,  3886, 34809, 36879,
       19181, 11971, 25135, 36838, 25652, 34080,  6967, 45084, 43609,
         282, 19616, 23198, 28580, 28780, 23338, 12875,  7324,   879,
       20074, 43186, 40427, 23407, 37092,  9080, 37779, 28790,  9480,
       35884, 11767, 25208, 12169, 27769, 18932, 42666, 28784, 25874,
       26433,  1631, 46135, 37235, 46448, 42121,  7659, 49777, 28441,
       35757, 41502, 30088,  2475, 44011, 21216,  9953, 49129, 33738,
       42509, 33178, 38769, 13168, 35830,  9727, 26492,  2439, 45944,
       43912, 49279, 37714, 20527, 26232, 36089,  3686, 26618, 34286,
       33733,  4208, 22550, 12302,  4875, 49952, 18604, 36720, 30812,
       48361,  3942, 45889,  3986, 10649, 43166, 24671,  3521, 14443,
       47472, 38475,  3861, 18974, 49550, 37045, 10554,  7393,  2729,
       22066, 24431, 15664, 49046,  7277, 12367, 19508,  3098, 26333,
       47649, 15726, 29973, 39939, 35370, 21917, 29381, 35950, 15771,
       10939, 42762, 16670, 10176,  9360, 22356, 17776, 15291, 11459,
       10635, 25373, 41160,  3403, 29391,  7553, 34662, 34056,  9877,
        5148, 25680, 24143, 22525,   770, 20666, 32359, 21644, 33123,
       31639, 42404, 17317, 17417, 21717, 41297,  7976, 43250,  6599,
        8069, 49928, 10364, 13029, 12796,  4126,  3095,    19,  8956,
        3437, 34684, 48947, 22295, 47465, 40038, 30727, 39203, 19588,
       22631, 14024, 22428, 38230,   807, 30536, 33623,  7675, 22786,
       24918, 40436, 20377,   249, 29217, 42742, 44194, 21497, 24888,
       49516,  2411, 36564,  5680,  8293, 36290, 35571,  2777,  6267,
       25382, 36360, 37048, 18978, 19763, 24155, 25622, 43027, 30212,
       37261, 42049,  2684,  7650, 35170, 35458, 23366, 43157, 32125,
       45622, 13127, 22236, 13479,  4810, 24531, 35745,   235, 35312,
       15193, 46991, 48650, 11101, 28421, 29360, 34139, 20022,  2924,
       40313, 14916, 43298, 23005, 30953, 45019,  4279, 13988, 47331,
        2690, 15823, 14845, 16639, 24354, 32856, 23539, 18682, 37134,
        7918, 29627, 31279, 10374, 18029, 28492,  3341, 15612, 17723,
        4550, 36304, 31118, 32812, 17697,  3739, 38290, 13866, 49804,
       18338,  3683, 41448, 38662, 38397, 12066, 10600, 39205, 27464,
       25234, 18270, 19268, 16218, 31128, 38845,  8256, 40498, 33627,
        3401, 38336, 13596, 27928, 40613,  6832, 38026,  6517, 43414,
       37694]), [0, 5, 1, 6]), (array([10313,  1060, 22492, 47609, 49025, 35505,  9527, 40702, 34894,
       48611, 49501,  2942, 22714, 24062, 26623,  1325,  3258,  4313,
       47376, 39971, 47430, 14149, 29933, 12249, 31714, 27643, 29362,
       25412,  3123, 23399, 31917, 31183, 39299, 17010, 17170,  4283,
       18395, 14166, 20617, 41335, 12907, 43623, 39605, 20259, 49388,
       24260, 18950, 40048,  3240, 11378, 47181, 10369, 12866, 31370,
        5546,  1239, 36295, 39433, 32671, 30510, 42917, 11248, 30433,
       34296, 47793, 21451, 45095, 35766, 34369, 32714, 35730, 22776,
       49712,  6717, 45461, 33954,   723, 27692, 34892, 23497, 37002,
         580, 13929, 40578, 42162, 37036, 41247, 41413, 15164, 20909,
       18081, 42898, 23960,  9427, 29075, 39243, 47212, 25513, 10096,
       32619,  7610, 41078, 44659, 25163, 29954, 40470, 37959, 24328,
       46223, 16942, 41727, 23757,  5280, 15022, 13386, 11063, 21050,
        3319, 38315, 45398,  9922, 30997, 10810,  4204, 29437, 35416,
       46028, 11965, 49379, 25691, 48168, 42713, 40785, 16978, 39335,
       48124, 37722, 29868, 28230, 24526, 13504, 40881, 10856, 29895,
       12207, 13859, 30443, 11674,  1141, 11352, 46543, 47561,  3777,
       36299, 36444, 39577, 27846, 36498,  7693, 29559,  1398, 17168,
       14281, 10923, 23723, 46975, 10172, 35911, 42467, 13886,  5658,
       12772, 23567, 25126, 29490, 17482, 35074, 30509, 29782,  6166,
       29676, 13338, 48432, 37072, 27808, 49654, 47342, 28436,  5378,
       17820, 18492, 45104, 46101, 20749, 28768,  7129, 46309, 37642,
       32532,  1751,  7083, 47978, 49127, 20984, 19186, 40402, 10702,
       12087,  2819, 16630, 37307, 12699,  4001, 23672, 22920,  8006,
       30691, 25237, 19097, 16792, 34274, 29479, 38689, 20726, 12543,
       24045, 29371, 25072, 46580, 14693, 23872, 39198, 12615, 13388,
        9122,  5348, 26487, 30770, 49851, 27068, 30664,   170, 39653,
        6529, 26720, 48261, 23595, 35491, 36438, 43381, 43490, 15872,
       35459, 25934, 40670, 13189, 11743,   785, 44451, 47100, 38193,
       36080, 43606, 38894,  7171,  2406,   395, 46230, 27441, 39871,
        8710, 14623, 38445, 37415,  7432,  3486, 26657, 42604, 26097,
       13259, 45928,  6783, 46950, 25705,  4672, 40712, 13201, 40249,
       45282, 48592,  9973,  3795, 45914, 26862, 20210, 18780, 40954,
       13873, 16983, 47067,  2051, 38472,  3222, 23389, 23763, 38360,
        3320,  6284, 22979, 45362, 36425, 30339, 30452, 30687,  4350,
       12786, 31479, 45433, 20721,  9051, 10753, 47062,   869, 10468,
        3447, 44103, 28368, 42646,  4628, 15467, 27608, 32134,  3108,
       19414, 40651, 43762, 48762, 45117,   740, 15470,   922, 17658,
       46682, 33600, 49478, 33236, 15873, 35472, 21655,  5553, 19061,
       44120, 38338,  4876, 18324, 36140,  6332, 26964, 11103, 39420,
       22913,  6199, 16583,  4697,  1895,  4392, 19255, 28697, 17214,
       33345,  3218,  9667,  4428, 38999,   241,  6714, 10452, 26707,
       41827,  9760, 12798, 15388,  4405, 33238,  2132, 42953, 41421,
       48540, 34199, 20425, 44944, 23286, 43123, 12973,  4558, 47897,
       41853, 25899, 44943,  1841,  7411, 21703,  2082, 17062, 39604,
       17565, 24991, 23702, 10186, 37194, 16335, 22042, 46604, 46977,
        2447, 39865, 33653, 34517, 22861, 25218, 47586, 14107, 21274,
        7667, 26482,  8846, 33033,  6151,  4519, 41271,  8635,  7107,
        5838, 47950, 22377, 26849, 22919, 35790, 22244, 30192, 42122,
       49742, 46058, 17096, 35307, 15545, 32230,  7615, 43487,  5502,
       37421, 35697,  3242, 15860, 23584, 21835, 25627, 45949, 23409,
       48758, 17655, 43598, 34160, 30318,  9846,   792, 25545, 28657,
       45933,  7122, 49726,  8686, 44197, 47603,  5539, 12904, 22484,
       21898, 14986, 46605, 21085, 16526, 45667, 27796,  5260, 36930,
       38764,  2359,  9008, 33709, 35380, 18462,  4110, 49032, 48368,
       48085, 25509, 23992, 30947, 34170, 13999, 12164, 40936, 21796,
        5125, 25046,  8058, 41710, 24268, 36848, 27419,  9584, 34364,
        6383, 31599, 44889,  5282, 27174, 10107, 43520,  6779, 46099,
       35688, 15669, 39964, 49022, 15577, 14662, 37727, 17772, 40964,
       40713, 30696, 37830, 44415, 11134, 30039, 17964, 43274, 10795,
       32737, 35863,  6915, 41072, 23263,  4132, 22500,  8476,  5759,
        2433, 49225, 27953, 24544, 19886, 43806, 38064, 44051, 14967,
       14376, 31890, 35344, 33181, 41153, 43480, 41051, 15114, 31720,
       27260,  2204, 28671,  6411, 34802, 14384, 16613, 38658, 29710,
       16543, 39322, 24292, 20674, 22227, 41075, 33085, 38710, 27132,
        7148, 25331, 30019, 46904, 12977, 41189,  9335, 37455,  9998,
       36067, 30545,  7390,  4993, 14238, 48465, 31665,  2771, 40220,
        5616, 46547, 17169, 39340,  6226, 32462, 22788, 47670,  1790,
       42437, 43655, 42438, 12516,  9085, 34400, 37203, 41760, 38482,
       15315, 14367, 41445, 32584, 34258, 34905,  9646, 16736, 35576,
        2362, 10168, 12306,  4958,   250, 44204,  4648, 35910, 29874,
       25600, 23768,  4200, 47989, 22060,  7371, 38420, 12340, 28268,
       45289,  3025,  8127,  9668, 27405,  9317, 16855,  7875, 18908,
       31645, 44266, 30183, 43954, 48970, 18178, 30083, 25165, 44273,
       47190, 26368, 41439, 45821, 47423, 30999,  4354,  7284, 12629,
       39892, 45308,  7887, 30650, 40449,  1736,   593, 43608, 12451,
       45068, 40046, 12292, 34278, 29429, 10826, 13044,  4244, 47226,
       19571,  4099, 23439,  8189, 45972,  3827, 22790, 38223,  7428,
        6819,  6121, 47264, 17460, 34794, 40076, 24075, 15183, 48269,
       13311, 35465, 26350, 10564, 18825,  7124, 21706, 45014, 41946,
       49847, 33658, 40472, 17031, 47033, 49977, 49769, 17205,  3142,
       18907,   761, 44601,  3131,    60, 39204,  1413,   176, 14676,
       12247, 42163, 13523, 16211, 49959, 16286,  6851, 45317, 11799,
       33832, 21416, 33118, 43482, 24673, 46043, 15028, 24872, 41260,
       33566, 31748, 19877, 28042, 35535, 31943,  7091,  6025, 29332,
        1063,  4682, 34322, 16493,  3994,  1876, 10828, 21636,  3865,
       11239,  7843, 47984,  8557, 17675,  2152,  9233,  9428, 31232,
       35221, 45355, 49464, 27295, 15657,  3168, 47752,  3307, 16768,
       26727, 18407, 25851, 42619, 18533,  7025, 16696, 24275, 23412,
       18866, 47976, 38834, 36666,  3091, 45836, 33990, 26192, 12641,
       24397, 19366, 18024, 25709,  4430, 38677, 39678,  6768, 30864,
       31598,  4966, 39323, 13213,  9974, 47878, 37593, 12635, 25139,
       15592, 47790, 39159, 20482, 29846, 27367, 25913, 48103, 40438,
       44128, 35090,  9543, 38069, 35332, 33787,  1017, 20628, 44696,
       40274,  2937, 14084,  2479, 22746, 17966, 37426, 40879, 25612,
       41328,  9530,  1031, 11911, 39464, 33491, 17938, 23888, 10117,
       16789, 30302, 46835, 23771, 11147, 41188, 31928, 45078, 20663,
       16915, 29136, 36274, 47451, 31570, 19107, 24973, 36285, 17562,
       29752, 42510, 29436, 41656, 33364,  2810, 30130, 30481,  3588,
       20885,   931, 13600, 42261, 13216, 34804, 10510, 48065, 12879,
        4086, 32228,  9847, 33192, 40836,   473, 37922, 18542, 31142,
       43269, 44612, 33948, 29357, 20106, 31396,  8873, 36842, 46340,
       36645,  9131, 14951,  9103, 19239, 46612,  5682,   488, 12609,
       20027,  6672, 36704,  1248, 10001, 19927, 41534, 10026, 27765,
       19562,  7272,  4899, 33758,  3775,  1989, 30202, 41293, 17796,
       20003, 33704, 13331,   819, 21572, 45607, 16404, 46473, 10718,
       32010, 18261, 46066, 49754,  1175,  3018, 38396, 33759,  7669,
       10379, 12293, 24321, 14864,  9515, 49636, 18342, 13076, 23036,
       17715, 31181, 33812, 26383, 20113, 44917, 27610, 13956,  9808,
       21756, 14621, 14694, 13066, 14065, 11429, 23628, 36847, 29103,
       25828, 23680, 29516,  8091, 46721, 24701, 33256, 34922, 12048,
       11751]), [8, 3, 1, 6]), (array([ 1937, 21260, 10485, 22369, 36168,  1657, 33408,  4753, 15238,
       19471,  5742, 28170, 40846,  6270, 40739, 23468,  5570, 19360,
       32799, 39182, 11765, 41618, 47630,  2573, 35228,  8423,  1169,
       11810, 16889, 42096, 13012, 42967,   665, 35324, 12406, 33401,
        8765,  6206, 27469, 15587,  9852, 15342, 27604, 21304, 16702,
       19340,  8462,  8379, 48087,    86,    20, 16173, 28658,  5880,
       42716,   816, 33749, 21966, 16878, 30037, 26838, 43509,  3908,
        8487, 38492, 18115,  4199,  3897, 44397, 37288, 48172,  1158,
       33209,  7984, 29065, 10477, 18293, 39947, 45605, 48484, 47857,
       19702, 29445,  1826,  3560, 32593, 37850, 23869, 21197,  2734,
       18493, 46790, 25871, 28555,  5631, 10629, 38123, 46251, 30561,
       42314,  9234,  6652, 33016, 11499,  3189,  1018, 18099, 21364,
        2792, 21016, 42104, 28049, 43151, 48945, 11435, 39265, 46164,
       33574, 30869, 22148,  7382,  4750, 29723, 20782, 16936,   934,
       15718, 42863, 22990,   844, 27649, 46036, 15781, 18821, 30384,
       31932, 26434, 28755,  1746,  3635, 14134, 30358, 41777, 15508,
       43241, 43637,  8046, 30527, 18690,  7100, 33245,  2155, 34777,
       49097, 19441, 49488, 14532, 18766,  3973,  8523, 20978, 43640,
        2486, 26301, 36552,  6432, 28892, 27390, 26233, 23611, 43055,
       23255, 21693, 24464, 47239, 15253, 18418, 36517, 37089, 12691,
        5653, 26823, 48699, 33947, 33272, 12843, 15656, 37313, 27479,
       27923, 18052, 43486, 11254, 11957, 42769, 30225, 40650,  6844,
       12282, 31505, 39125, 39036, 42442, 44629, 16659, 41246,  1264,
       13855,  3717, 38821, 30242, 42303, 31349,  7625, 42653, 19284,
        4394, 15663, 15538,  1182, 27614, 10426,   381, 24884, 25813,
       19790,  2943, 13896, 40665, 43044,  4290,  4781, 19833, 38281,
       48787, 14440, 27017,   563, 48801, 21254, 14397,  6452, 15760,
        1866, 43925, 24067, 23634, 32069,  8890, 45611, 44845, 37302,
       49763, 29158, 23880, 43531, 16260, 41624, 42423, 28759,   178,
        2275, 36495,   948, 39810, 40541, 42500, 32546, 15824, 49648,
       10305,  4216, 15002, 11935, 22136,  2807, 44986, 44798, 48857,
       23860, 45476,  8595, 37262, 38196, 36037, 16088, 11323, 21660,
       34733, 24050, 44338, 44858, 46747,   824,  5842,  3914,  2015,
       48423, 44105, 47082, 21317, 30032,  3135, 47263,  7385,  8498,
       24547, 18938, 40447, 43316,  6966, 49470, 49162, 45758, 35316,
       11656, 35922, 25724,   954, 39465,  3104, 48104, 19062, 19259,
       37893, 11428, 24129, 37472, 43498, 41418, 18667, 26165, 25310,
        5035, 22541, 46691, 42126, 28812, 39297, 46300, 44447, 33681,
       19772, 20900, 33019, 23304, 36680, 19642, 40461, 20846, 32668,
       19461, 30671, 23784, 35814, 23356, 18258, 35277, 19321, 17463,
       11761, 10528, 49231, 38131, 39246,  2601, 30195, 33220, 48727,
         163, 39961, 48759, 29733,  9462,  5809, 30036, 34788, 10852,
        7758,  3606,  3147, 44295, 44378,  6588, 41956, 40637, 34807,
       35407, 38178,   825, 41381, 37292, 39762,   636, 38358, 25889,
       26133, 30993, 27930, 32779, 21927,   641, 10573, 18443, 45561,
        7611, 29648, 13540, 32383,  8461, 16171,  6010,  5854, 31212,
        2783, 38220, 29592, 13482, 37882, 25090, 23196,  8367, 23298,
        6032, 19909, 43748, 41438, 14375, 37763, 17110, 17886, 33804,
       13373,  6235, 44297, 12364, 21787, 44249, 40010,  4554,  3487,
        6759, 41717, 48486,    11, 48837, 39494, 17503, 31262,  8351,
       27145, 22291,  5269,   131, 17872, 27672, 25778, 27400, 32660,
       10998, 20960,  7016, 46353, 29196, 12466, 48436, 27157,  9369,
       18997, 35164, 36654, 28936, 38688, 13185, 23829, 13083, 23993,
       29355, 20369, 12864, 49529,  3534, 25156, 31982,  3592,  4714,
       13493, 17589, 40694, 16824, 41185, 41877, 47828, 10602,  7101,
       26669, 22626, 28727, 41626, 28152, 38330, 24194,  9565, 19351,
       13852,  6668, 17312, 44725, 44436,  8856,  5827, 13323, 37257,
       39692,  1494,  3238, 29325, 29522, 43061, 12920,  2455, 38908,
        9854, 33897,  6371, 21752, 20312,  6295, 47529, 28183, 33126,
        1660, 16848, 29675, 34723, 11444,  7147, 10576, 16474, 12424,
       27073, 45600, 33230, 35914, 25718, 28938, 20313, 46238, 12100,
       46376, 40207, 33532, 15903, 32847, 14425,  2037,  9048, 34052,
        8965, 11910,  5257, 37505,  2615, 27601, 16925, 11163, 18160,
       32827, 16953, 46051, 14978, 47377, 32941, 42490, 21933, 42927,
       41291,  4101, 26298, 27284,  8625, 44591, 26937, 27195,  5287,
        9263, 41262, 49554,  7640, 37687,  4230, 41984, 35850, 32262,
       10359, 44767, 41654,  5917, 24975, 15051, 19667, 27802,  1037,
         978, 30489, 38822,   137, 38262,  6130, 43195, 41948, 22175,
       16089, 46731, 24680,  5722, 33845, 39250, 44841, 21331,    45,
        1079, 28655, 38725,  3907, 40935, 11571, 23959, 48528, 10035,
       23058, 26315, 11804, 15770, 29494, 45296,  8600, 10495, 17534,
        7832, 47568, 26899, 28005, 29485, 29285, 44541, 25209, 46717,
        3483, 14182, 48901, 32418, 35933, 19522, 34571, 30104, 32652,
       24398,  5552,  7374, 26026,  6639,  3992,  4334,  6589, 37801,
         493,  1571, 15885, 25611, 28896, 14502, 37551, 23648, 46649,
       44225,  7738, 40594, 17984, 21218, 40422, 35454,  4326,  9494,
       28265, 44272, 47606, 17626, 16400,  5422, 47470, 40081,  3020,
       36388, 49060,  9745,  3034,  9017, 19581, 22055, 13264, 44625,
       47088, 38579, 39654, 30458, 49629, 39679, 39771, 10569,  3285,
       38211, 46396,  7338,  6931, 17492, 16365, 29963, 45968, 39668,
        4636,  2545, 41563, 24527, 13006,  7312, 25279, 10022, 16937,
       44589, 22833,  8174, 21651, 13667, 32458, 21992, 21142, 15248,
       13608, 12335, 47162, 49121, 37920,  2762, 36376,   991, 44679,
       47370, 30227, 37630, 16003, 16069,  2888,  1159, 48400,  5833,
        8171, 40322, 28258,  7064, 44915, 43021, 14178, 30146,    25,
       27660, 14457, 36147, 21072, 10356, 40767, 17143, 12369, 49171,
       18224, 16389, 36620, 44985,  6747,  6941, 18816, 48866,  5398,
       32191,  5380, 39190, 35435, 13120,   863, 20717, 17908, 34786,
       44417, 30922, 47792,  6127,  4881, 24660,  9828, 46306, 17132,
       11038, 37997,    95, 45793, 41399, 42671,  5169,   103, 47396,
       35600, 34245,  7252, 42334, 13573, 23244, 26245, 34092, 30685,
       32637, 28432, 25657, 21560, 25747, 18025, 48664, 31500, 38936,
       10738, 36625, 32397, 49624, 48325, 27504, 29976, 14889, 10444,
       12396,  9762, 20362, 24418, 42027, 44163,  9732, 31820, 25406,
       21704, 39986, 26524, 11904, 31112, 36493, 37865, 40551, 18021,
        2019, 43933, 12014, 29214, 18325, 27577,  2915, 14547, 20878,
       13838, 48043, 32880, 43284, 10063,  6395, 40804, 47302, 11823,
        9126,  3589, 16822,  2965, 16762, 32105,  7339, 45878, 32762,
        4663, 45009, 34157, 26419,  7591,  6777, 12433, 38462, 46856,
        5708, 38298, 26798, 18334, 10783, 32739, 43431, 39759,  8215,
       22027, 36638, 29009, 12019, 28672, 37918,  2872,  3716, 33367,
       16097, 48977,  1023, 45225,  3603, 31616, 40047, 19731, 44626,
       12168,  9246, 30222, 23915, 48176, 10579,  8815, 27486,  3041,
       48819, 26054, 10936, 48710, 24519, 17597,  5558, 23881, 28501,
       17688, 45247, 22610,  7232, 29327, 14824,  2602,  7881, 14973,
       15343, 45443, 31891, 42327,  4751, 34542, 23470, 46791, 36586,
       39052, 19128, 36808,  7172,  5402,  8545, 19823, 11212, 31135,
        9356, 47953, 31784,  6664, 13806, 43353, 13143, 21722, 32212,
       15517,  2874, 43995,  3439, 43390, 39343, 15232, 31575, 23697,
       29426, 30033, 42358,  6035, 43439,  1176, 15231,  8827, 45663,
       46952,  8832, 18621, 18884,  7638, 14884,  3192, 22968, 40323,
       34564]), [4, 7, 1, 6]), (array([  742,  9067, 29220, 49888, 47450,   910,  6227, 41198, 43122,
       20779, 10433, 40281, 15176, 13786, 12711, 44392,  4588, 48893,
        3037, 12669, 21819, 44075, 41681, 45864, 28497, 41619, 32274,
       47632, 25041, 13433,  8543,  1492, 36125, 49397, 40701,  9552,
       38745, 12504,  7166, 38096, 26321,  7948,  5979, 14192, 29678,
       46114, 26541, 40398, 12742, 35311, 26186,  8881, 49802, 31495,
       34088, 38594,  7303, 28802, 14172, 26807, 43465,  9033, 21942,
       12583, 27825,  9001, 32951, 27498, 48589, 39062, 42267,  9989,
       32278, 29912,  1768,  1788, 26893, 13081, 43478,  3848, 37742,
       20547,  8186, 39019,  9526,  9455, 48545, 10160, 18652, 37745,
       24170, 30454, 22667, 48833,  3629, 47956, 44658, 48365, 41542,
       20580, 25449, 19964,  1530, 36025, 17844, 25491, 41933, 45074,
       28705,   108, 43010,  7853, 37560, 14229, 19950, 40177,  6408,
       22297, 42803, 40372, 15522, 33051, 34396,   820,  1444,  3462,
       11443, 31308, 18863, 13648, 20572, 48531, 42302,  1122, 43118,
       17955, 12046,   975, 31333, 28994, 28164, 19914, 43469,  1693,
       15696, 42648,  1040, 42468, 39991,  3476, 39051, 31813, 46819,
        6484,  5727, 28708, 19932, 49104,  7807, 40087, 33519, 45926,
       46522, 14829, 47491, 23911, 29728, 18027,  3119, 29448, 23530,
       19137, 12148, 22602, 33032, 10018, 37321,  2372, 16919, 35681,
       36450, 40583, 43766,  8871, 39436, 31562, 25859, 12602,  8843,
       18092, 32470, 25830, 23692, 20660, 47016, 31026, 31519, 24636,
       31759,  8800, 13564,  1533, 33449, 32816, 35482, 40403,  3101,
       22228,  9437,  3889, 41362, 16266, 18101, 31040, 14270,  1606,
       24694, 39687, 44739,  6228, 29181, 23453, 46530, 24496, 40225,
       44277, 15182, 36339, 12156, 27580, 37278, 27557, 41255, 38058,
       37816, 10767, 39928,  2033, 32321, 21167, 43134, 16279,  4426,
        6448, 20526, 19695, 38704,  8418, 49297, 17735, 29776,   369,
       22921, 42739, 21786, 10208, 38363, 28543,  6066,  7127, 45231,
       25319, 48017,   886, 17716,  3513, 27548,  3820,  6937, 29992,
       26069, 13881, 38379, 39884, 40699,  5437, 14099, 11230,  5611,
        1390, 39583, 40068, 47073,  2281, 41560, 13155, 27190, 30175,
       43039, 17468, 25009,  5671, 27420,  7793, 49926, 40303,  9213,
       17905, 22016, 27523,  3477, 24718, 10494, 49637, 49785, 46685,
        7457,    31, 31566,  1706, 25631, 36050, 24086, 26955,  2214,
       43511, 25159, 48371, 42346, 11312, 30853,   883, 34943, 32810,
       25837, 15750, 32242, 41976, 31287, 17218, 44052, 19842, 45906,
       10138, 21203,  3628,  5453, 35746, 42627, 37074,  1115, 37678,
       33200, 28391, 20587, 14809, 26061, 37709, 16641, 12835, 39806,
       27983, 39816, 26053, 40117,  2477, 24391, 33896, 43591, 11104,
       26496, 44132, 10178,  9025, 33865, 17228, 27784, 38380, 33025,
        6579, 43803, 36807,  6480, 36859, 46777,  8501, 15838,  6237,
       39984, 19302, 43346, 20713, 24542, 10390,  8505, 34796,  2061,
       27957,  5161, 23075, 27799, 41183, 46346, 36529, 38961, 22614,
       16720, 14224,  3850, 45063, 10467, 26057, 16347, 48192, 13795,
        6539, 32129,  1572, 41226, 14661, 39688, 19670,  3838, 29784,
        3409,  7010, 44058, 22651, 13827, 44716,  1231,   186, 20995,
       12104,  4364, 29193,  3652,  5465, 17090, 40450, 49442, 11755,
       45008, 38789, 35542, 39418, 31162,  5300, 40833,  8650, 35183,
       46781, 13096, 46269, 11777, 27336, 15142, 47512, 39453, 44339,
       42140, 23225, 11373, 34867, 47765, 36684, 30615,  4473, 45198,
       32115, 33417, 49147, 21584, 44469,  2908, 10312, 45498, 17798,
        9807, 13421, 11555, 28879, 32041, 15384, 19367,  3846, 24408,
       17068, 49003,  1972, 31403, 27380, 42973, 43619, 23137, 31490,
       34872, 14220, 14462, 41228, 27196, 12086, 46645,  3537, 26446,
       16591, 18878, 10781, 46861, 49385, 43646, 30814,  6353,  2143,
       43669, 43267, 48995, 46507,  9549, 29299, 46966, 13509, 19810,
       29473, 43290, 44487, 30821,  4856, 30006, 14593, 49663, 27404,
       45946, 16180, 49283, 14048, 47213,  2851, 13766, 40554, 24294,
       35866, 10868, 27130, 43710, 23313, 18670,  8427, 28963, 49426,
       29049,  5337, 46634, 28685, 43984,   834, 49585, 38157, 18527,
       14716, 31935,   325, 44185, 32811, 47706, 16471, 24336, 44235,
       35827, 27510, 30316, 30049,  4172, 19575, 36850, 18744, 31193,
       17319, 28383, 38958, 14964,  8729,  2428, 23060, 19745, 49326,
       13327,  7774, 11418, 38735, 41307, 40808, 47349, 19565, 34519,
       13191, 33615, 29959,  8893, 35207, 24151, 19678, 12981, 42237,
       47824,  4360, 47390, 49989, 41063, 11920, 32189,    97, 31502,
       45565, 10377, 44966,  2849,  5356, 35948,  5816, 31407,   134,
       34238, 41739, 23275, 10771,  6325, 19341, 10832, 21877, 22552,
       12158, 14455, 42753, 48917, 14074, 27538, 40290, 20047, 37309,
       42085, 49401, 43132, 41532, 43918,  2038, 27002, 28335,   364,
        9066,   936, 38452, 44474, 27582,  6429, 21947,  2844, 38542,
       36991, 26578,  1731, 11109, 37440,  4654, 38902, 19202, 21003,
       14249, 27730, 31583, 26818, 20680,  6272,  6831, 35508,  2101,
       15797,  4423,  9583, 16962, 35058, 16252, 29908, 32486, 34128,
        2318, 24338, 35531,  7297, 29999,   323, 36853,    79, 21150,
       27970, 14381, 28541, 29555, 34618, 17860, 29469,  3734, 21181,
        8124, 18495, 40431,  7019, 24923, 22499, 14101, 26158,  9113,
       39256, 37620,  6347,  8389, 26115, 43205, 45332, 43547,  7375,
       26331, 18238, 28122,  4171, 30009, 14855, 13997, 49148, 22906,
       30119, 27509, 38858, 19481, 46588, 23789, 33097, 42802, 40655,
       16777, 49933, 35756, 16981, 10693, 31655, 25300, 46889,  5971,
       28552, 26310, 22930, 29149, 48639, 38028, 34426, 41392,  2149,
       18425, 24807, 38818,  7137, 13890, 11654, 15390, 40298, 42291,
       24510, 31257, 34422, 47546, 15498, 13018,  6145, 18871, 36164,
       14392, 45806, 42660,  2989, 33701, 37907, 37926, 12359, 44431,
       20934, 11748, 24279, 44342,  5022,  3707, 31448, 47047, 12508,
       44018, 23025, 11066, 40640, 25704, 29749,  7848, 20332, 41379,
        2939, 17283, 48448, 41799, 37603, 29889, 11699, 21956, 40455,
       11090, 17238, 49076, 48789, 28194, 33069, 31812, 34926, 16584,
        4590, 31753,  1842, 45863, 40627, 46126, 19967,  5745, 17245,
       37119, 26335, 12077,  1837, 39252,  1678, 32922,  6754, 19701,
       40617, 29310, 49704, 27379, 12852, 15360, 11369, 28395, 11672,
       12305, 33543, 34550, 30193, 44684, 26190,  9477, 29506,  3833,
       30910, 15116, 45266, 11827,   326, 29223, 32385, 36880,  1955,
       40568, 33663, 15534, 15994, 22686,  9406, 46009,  5019,  7036,
       41537, 32859, 45958,  8820, 11875, 45585, 40153, 41909, 33594,
       38844, 35480,  6545, 31508, 13550, 10220, 34033, 35349,  3873,
       13484, 39337, 29457, 17484, 15804,  9367, 46429, 18380, 12434,
        6473, 38508, 42156, 10128, 29434, 48067, 17766,  7041, 23747,
       36576, 29802, 16337, 22266, 13188, 47454, 40982, 21084, 39046,
       24928, 21362, 35831, 11346, 13617, 45169, 12897, 22362, 35506,
        3926, 16177,  7267, 26206, 46092,  4789, 44712, 34942, 48337,
       12657, 39010,  8009, 15110,  4144, 23411, 22065,  7491, 34829,
       18742, 25519, 46913, 34031,  2267, 15179, 21177, 35351, 29408,
       35817,  7950, 21800, 35136, 11303, 18992,  4786, 25168, 22752,
       15991, 33726, 26872, 12273, 32381,  5025, 23563, 12933, 21935,
        4234, 43308, 25396, 27376, 40874, 40927,  5374,  4415, 33044,
       28869, 30469, 23618,  3241,  7820,  3070, 30569, 41749, 23328,
       22528, 28188, 31270,  8212, 22101, 48888, 28956, 38125, 33791,
       27937, 28454, 46948, 17680,   960, 10774, 48532, 31808,  4377,
       26066]), [2, 9, 1, 6])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 5, 1, 6], M=tensor([0, 5, 1, 6], device='cuda:0'), Initial Performance: (0.259, 0.0446454519033432)
DC 1, val_set_size=1000, COIs=[8, 3, 1, 6], M=tensor([8, 3, 1, 6], device='cuda:0'), Initial Performance: (0.249, 0.04491214168071747)
DC 2, val_set_size=1000, COIs=[4, 7, 1, 6], M=tensor([4, 7, 1, 6], device='cuda:0'), Initial Performance: (0.246, 0.04495431101322174)
DC 3, val_set_size=1000, COIs=[2, 9, 1, 6], M=tensor([2, 9, 1, 6], device='cuda:0'), Initial Performance: (0.251, 0.04425755798816681)
D00: 1000 samples from classes {1, 6}
D01: 1000 samples from classes {1, 6}
D02: 1000 samples from classes {1, 6}
D03: 1000 samples from classes {1, 6}
D04: 1000 samples from classes {1, 6}
D05: 1000 samples from classes {1, 6}
D06: 1000 samples from classes {0, 5}
D07: 1000 samples from classes {0, 5}
D08: 1000 samples from classes {0, 5}
D09: 1000 samples from classes {0, 5}
D010: 1000 samples from classes {0, 5}
D011: 1000 samples from classes {0, 5}
D012: 1000 samples from classes {8, 3}
D013: 1000 samples from classes {8, 3}
D014: 1000 samples from classes {8, 3}
D015: 1000 samples from classes {8, 3}
D016: 1000 samples from classes {8, 3}
D017: 1000 samples from classes {8, 3}
D018: 1000 samples from classes {4, 7}
D019: 1000 samples from classes {4, 7}
D020: 1000 samples from classes {4, 7}
D021: 1000 samples from classes {4, 7}
D022: 1000 samples from classes {4, 7}
D023: 1000 samples from classes {4, 7}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.418, 0.06142205399274826) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.411, 0.06509904915094375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.0901500660777092) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.08961409723758698) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.06931790328025818) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.07178343132138253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.285, 0.12618956410884857) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.273, 0.1021047906279564) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.0813574980199337) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.08620012563467026) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.322, 0.1603555630147457) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.12693029174208642) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.09731255877017975) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.11036875423789025) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.1944475924372673) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.15093016832321882) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.12286224584281445) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.15348628028482197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.21103904109448193) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.1675949716344476) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.13750439224019648) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.15476095771044493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.21013511917740108) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.21405988819152116) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.14960554386116565) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.17682396379858256) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2106591824889183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.2431645128428936) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.15374461515340954) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.18366238740086555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.24523430716246367) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.27802466233447193) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.16089225522428752) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.19264739524573088) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.2547052926644683) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2662605528570712) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.17724084822554143) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.1974615651369095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.2696862921901047) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.29413447110913693) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[1, 6], M=tensor([0, 1, 3, 5, 6, 8], device='cuda:0'), Initial Performance: (0.0, 0.3451866207122803)
DC Expert-0, val_set_size=500, COIs=[0, 5], M=tensor([0, 5, 1, 6], device='cuda:0'), Initial Performance: (0.944, 0.0053529364764690395)
DC Expert-1, val_set_size=500, COIs=[8, 3], M=tensor([8, 3, 1, 6], device='cuda:0'), Initial Performance: (0.936, 0.005041519492864609)
SUPER-DC 0, val_set_size=1000, COIs=[0, 5, 1, 6], M=tensor([0, 5, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[8, 3, 1, 6], M=tensor([8, 3, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7608240bc250>, <fl_market.actors.data_consumer.DataConsumer object at 0x760810095d30>, <fl_market.actors.data_consumer.DataConsumer object at 0x7607dc7e8490>, <fl_market.actors.data_consumer.DataConsumer object at 0x76082409d280>, <fl_market.actors.data_consumer.DataConsumer object at 0x760824602af0>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO4', '(DO1']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.00681096931360662) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005326986607164144) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.24795482557825743) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.30002766743209214) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.925, 0.006656469030305743) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.566, 0.043973002195358275) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.552, 0.061163750648498535) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.006631053438410163) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.00652714255079627) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.21287916172482074) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2728695821799338) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.005471877856180072) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.667, 0.03137161675095558) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.595, 0.041677166059613226) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.004923279954120517) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.0064142841752618555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.409, 0.20620561659522355) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.32131102004367856) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.00767708407095779) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.569, 0.07174770521745086) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.08379331595078111) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.0064794093377422545) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006291292537003755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.2141700718589127) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.31192026708088816) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.007422729161335156) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.567, 0.0606380693949759) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.06858647258579731) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006087482184404507) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006122962586581707) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.2010298516601324) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.32049203978851437) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.008719272228016052) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.07421028636582196) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.5, 0.09546068190038204) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO1', '(DO4']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.004812920149881393) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.0068457725122570996) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.19028245759755372) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.3312542861709371) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.007407631593756378) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.591, 0.059844103578478094) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.06895409336686134) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005932357878657058) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006337202657014132) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.198406883046031) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.3241231879414991) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.011787009762832895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.08111673868913204) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.1070020993873477) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.006402112097712234) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.0062388222366571424) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.17594213546812534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.33041530528105795) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.944, 0.010591172102591372) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.07064289556513541) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.07118851356208325) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.006273269979166799) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.006767840795218944) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.1784454032704234) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.38994095954392105) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.010288741401403968) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.07623586726281792) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.09977634957805276) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.0075555453243432566) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005882379829883575) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.1650060859248042) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.3705408712690696) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.011116042393507086) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.568, 0.06181972903292626) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.07149205236136913) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO0', '(DO4']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.008175370746757836) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.0063471380062401295) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.19123647987097503) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.38060229244735094) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.879, 0.014623087250394746) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.06864798128604889) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.10176159578561783) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.007729774570791051) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.00643419935926795) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.19773468235135078) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.3406844732235186) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.007831980969989672) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.53, 0.07460097946971656) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.521, 0.11997202338278294) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005326313138240948) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006977301970124245) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.17287417554110288) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.3549927529883571) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.011135476535319867) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.573, 0.06783806266263127) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.10131307419063523) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.00702842787723057) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005982719667255878) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.15998460517451168) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.34874051429191605) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.011989197289105505) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.58, 0.0691597609370947) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.08658508480899035) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.0075059656623052435) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.006643791154026985) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.1745913468338549) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.36089862601598727) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.01417472575372085) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.07677286081714556) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.12495338846743106) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO4', '(DO3']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005484560347860679) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006361711040139198) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.16771833604201675) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.3488742897985503) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.0046849226516205815) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.05089665159583092) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.534, 0.09082296818494796) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006176539804087952) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.005472456730902195) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.18102585861459375) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.3508404924441129) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.006415009678225033) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.052908858206123115) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.09754783378541469) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.008077107254532166) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005577542833983898) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.174574798528105) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.3521835003287997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.0057061520487768575) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.624, 0.04914791109412909) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.531, 0.08215946569293738) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.006766678176179994) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.0056459917426109315) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.16770780604705215) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.454, 0.343494473033119) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.007570518406406336) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.69, 0.03751009241491556) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.07933119892142713) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006999176798039116) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005685843385756016) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.17489788097515702) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.3736181068490259) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.009567470586713171) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.06025301178079098) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.519, 0.10047137652523816) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO5', '(DO3']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.008090914863845682) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006176568493247032) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.19048478404432534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.356747793949442) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.005922804781235754) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.658, 0.04696415659040212) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.0988232739008963) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.00791901966923615) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.0053539639040827754) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.18145827678591012) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.3183812234643847) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.006717495894692547) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.637, 0.056014500497374685) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.501, 0.09866446024365723) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007276712666207458) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.006021793410182) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.20600391821935773) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.3222155134831555) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.937, 0.008110026008449496) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.611, 0.05856756903231144) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.544, 0.09490463007986545) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.008149388553458266) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005839282292872667) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.16767128490284086) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.454, 0.28754090654850006) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.006794393506192137) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.676, 0.04362943880911917) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.09832850544899702) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007146856343140826) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006323306143283844) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.19244203187711537) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.2838235316844657) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.009283549557672813) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.653, 0.045500364010222255) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.518, 0.11571210746839643) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO4', '(DO0']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.00870707615185529) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.007104942172765732) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.1765168351456523) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.3114878219761886) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.00503671775944531) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.61, 0.054531948171555995) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.559, 0.08794065944105386) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006669376346690114) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005880982927978039) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.1899040335919708) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.2869299180400558) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.009684820950948051) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.66, 0.04735268833115697) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.10228049741685391) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.007127963495062431) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006054408475756645) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.20019521292299033) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.28756714005023243) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.009110473903296225) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.624, 0.056382902590557935) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.518, 0.0899442795701325) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.008474204753350933) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.0053175646662712095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.18017272815853358) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.30098611854761836) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.013333575255113828) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.564, 0.07146528118848801) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.13278724379092455) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.007468695165589452) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.005679843995720148) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.21548863060213624) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.2944354251273908) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.013314257340064955) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.59, 0.07850644644885324) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.11851650137966499) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO5', '(DO3']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.007805046063382178) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.0061647370215505365) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.20840612544119358) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.28552463369444014) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.007540546191856265) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.638, 0.050990639999508855) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.537, 0.09706314366310835) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.005667447291198186) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005629834841936827) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.22715511994808912) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.2966108890278265) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.005736303337209392) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.664, 0.045313769817352294) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.10912230292707682) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006910664840950631) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005340956345200538) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.1865906358063221) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.3334855201849714) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.007014065997907892) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.056315727256238464) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.513, 0.10222429409250618) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.007161606399458833) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.004992500826716423) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.20852795139886438) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.34055804085056296) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.010713629946578294) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.06471969848871231) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.503, 0.12459013491123914) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.0072846385899174495) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.006030151613056659) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.19415132585912942) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.3311109632293228) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.955, 0.005904941371991299) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.06316769711673259) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.538, 0.10779218719899654) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO4', '(DO3']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.00764358635944518) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005382684729993343) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.20779915237240493) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.2841403236463666) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.961, 0.007894232047758123) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.0485425786152482) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.12678111987374724) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.00823486565676285) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.0059420901834964755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.2156131567172706) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.2983513999353163) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.01629619833707693) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.07877125768340193) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.1277572870226577) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006759923295758199) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006105449624359608) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.2053078173138201) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.2840713277391624) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.010501747967835514) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.608, 0.07143624145491048) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.11244883792847395) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006970462302764645) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005878515519201755) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.21190348448231816) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.2958631407618523) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.007693084794824245) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.579, 0.057263339921832086) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.1394242549277842) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.008239366204696126) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.007161052294075489) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.2048171814084053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.27718833289202305) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.006960947980172932) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.0491945219784975) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.521, 0.11798150296509266) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.259, 0.0446454519033432), (0.418, 0.06142205399274826), (0.46, 0.06931790328025818), (0.467, 0.0813574980199337), (0.461, 0.09731255877017975), (0.458, 0.12286224584281445), (0.462, 0.13750439224019648), (0.452, 0.14960554386116565), (0.453, 0.15374461515340954), (0.469, 0.16089225522428752), (0.472, 0.17724084822554143), (0.566, 0.043973002195358275), (0.667, 0.03137161675095558), (0.569, 0.07174770521745086), (0.567, 0.0606380693949759), (0.529, 0.07421028636582196), (0.591, 0.059844103578478094), (0.531, 0.08111673868913204), (0.554, 0.07064289556513541), (0.536, 0.07623586726281792), (0.568, 0.06181972903292626), (0.52, 0.06864798128604889), (0.53, 0.07460097946971656), (0.573, 0.06783806266263127), (0.58, 0.0691597609370947), (0.554, 0.07677286081714556), (0.609, 0.05089665159583092), (0.612, 0.052908858206123115), (0.624, 0.04914791109412909), (0.69, 0.03751009241491556), (0.634, 0.06025301178079098), (0.658, 0.04696415659040212), (0.637, 0.056014500497374685), (0.611, 0.05856756903231144), (0.676, 0.04362943880911917), (0.653, 0.045500364010222255), (0.61, 0.054531948171555995), (0.66, 0.04735268833115697), (0.624, 0.056382902590557935), (0.564, 0.07146528118848801), (0.59, 0.07850644644885324), (0.638, 0.050990639999508855), (0.664, 0.045313769817352294), (0.606, 0.056315727256238464), (0.583, 0.06471969848871231), (0.606, 0.06316769711673259), (0.627, 0.0485425786152482), (0.563, 0.07877125768340193), (0.608, 0.07143624145491048), (0.579, 0.057263339921832086), (0.62, 0.0491945219784975)]
TEST: 
[(0.259, 0.043567125290632246), (0.40925, 0.05918567603826523), (0.46025, 0.06647206515073777), (0.46125, 0.07821208986639977), (0.4535, 0.09296726769208909), (0.4575, 0.11661085388064385), (0.45925, 0.12995803904533387), (0.44975, 0.14063709473609926), (0.44975, 0.1442626576423645), (0.4685, 0.15252220165729521), (0.4685, 0.16789883202314376), (0.58575, 0.041183685168623925), (0.64275, 0.03246996833384037), (0.57325, 0.07304445150494575), (0.574, 0.06311572708189488), (0.54575, 0.07307581064105034), (0.59425, 0.060321390211582185), (0.54675, 0.08223793062567711), (0.5625, 0.069680967181921), (0.539, 0.0755006497502327), (0.5865, 0.06050761198997497), (0.538, 0.0708705081641674), (0.54375, 0.07707978364825249), (0.5785, 0.06751599323749542), (0.59525, 0.06654368685185909), (0.5715, 0.08011194464564324), (0.6405, 0.046994447216391566), (0.644, 0.05131246431171894), (0.65125, 0.04981437335908413), (0.7045, 0.03739053233712911), (0.643, 0.05927690196037293), (0.66725, 0.0482732537984848), (0.64575, 0.05535628812015057), (0.61125, 0.05857585629820824), (0.68025, 0.04331234648823738), (0.655, 0.04693685747683048), (0.62075, 0.05549358719587326), (0.65575, 0.05015272672474384), (0.632, 0.054917435869574545), (0.566, 0.07701232859492302), (0.59325, 0.07703853943943978), (0.6395, 0.05281706505268812), (0.66475, 0.04511865195631981), (0.63425, 0.05092341969907284), (0.599, 0.061405393242836), (0.624, 0.06211519733071327), (0.6295, 0.04948695532977581), (0.58025, 0.08015568488836289), (0.631, 0.06811120094358922), (0.602, 0.05672149346768856), (0.63875, 0.04730492885410786)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.91      0.43      0.58      1000
           1       0.62      0.86      0.72      1000
           5       0.56      0.78      0.65      1000
           6       0.65      0.49      0.56      1000

    accuracy                           0.64      4000
   macro avg       0.69      0.64      0.63      4000
weighted avg       0.69      0.64      0.63      4000

Collaboration_DC_1
VAL: 
[(0.249, 0.04491214168071747), (0.411, 0.06509904915094375), (0.459, 0.07178343132138253), (0.462, 0.08620012563467026), (0.462, 0.11036875423789025), (0.467, 0.15348628028482197), (0.47, 0.15476095771044493), (0.467, 0.17682396379858256), (0.467, 0.18366238740086555), (0.47, 0.19264739524573088), (0.468, 0.1974615651369095), (0.552, 0.061163750648498535), (0.595, 0.041677166059613226), (0.505, 0.08379331595078111), (0.515, 0.06858647258579731), (0.5, 0.09546068190038204), (0.522, 0.06895409336686134), (0.511, 0.1070020993873477), (0.528, 0.07118851356208325), (0.491, 0.09977634957805276), (0.525, 0.07149205236136913), (0.468, 0.10176159578561783), (0.521, 0.11997202338278294), (0.488, 0.10131307419063523), (0.524, 0.08658508480899035), (0.511, 0.12495338846743106), (0.534, 0.09082296818494796), (0.522, 0.09754783378541469), (0.531, 0.08215946569293738), (0.511, 0.07933119892142713), (0.519, 0.10047137652523816), (0.515, 0.0988232739008963), (0.501, 0.09866446024365723), (0.544, 0.09490463007986545), (0.525, 0.09832850544899702), (0.518, 0.11571210746839643), (0.559, 0.08794065944105386), (0.528, 0.10228049741685391), (0.518, 0.0899442795701325), (0.491, 0.13278724379092455), (0.535, 0.11851650137966499), (0.537, 0.09706314366310835), (0.532, 0.10912230292707682), (0.513, 0.10222429409250618), (0.503, 0.12459013491123914), (0.538, 0.10779218719899654), (0.523, 0.12678111987374724), (0.489, 0.1277572870226577), (0.491, 0.11244883792847395), (0.491, 0.1394242549277842), (0.521, 0.11798150296509266)]
TEST: 
[(0.25425, 0.04382993084192276), (0.417, 0.06269155278801918), (0.465, 0.06890970841050148), (0.4675, 0.0824232129752636), (0.466, 0.1052005781531334), (0.47175, 0.14798526340723037), (0.474, 0.14918128669261932), (0.47475, 0.1711675282716751), (0.474, 0.17799552154541015), (0.4715, 0.1876283633708954), (0.47575, 0.19261900579929353), (0.544, 0.059509091898798945), (0.60525, 0.03855216972529888), (0.51075, 0.08029916059970856), (0.51425, 0.06712053470313549), (0.489, 0.09361049622297286), (0.5165, 0.0657485128045082), (0.5085, 0.10259015575051307), (0.54025, 0.07075631088018418), (0.501, 0.09657372289896012), (0.5315, 0.07156151694059372), (0.485, 0.1003359497487545), (0.51, 0.11748852634429932), (0.49575, 0.09851418682932854), (0.53125, 0.08388979086279869), (0.51325, 0.12092034819722175), (0.53475, 0.08805081349611282), (0.5345, 0.09429142928123474), (0.544, 0.077739053606987), (0.53475, 0.07547198539972305), (0.52325, 0.10170565366744995), (0.53175, 0.09658154934644698), (0.52025, 0.09893653890490532), (0.53075, 0.09671170708537102), (0.529, 0.09700810858607292), (0.5165, 0.11366516700387001), (0.56175, 0.08187085261940956), (0.528, 0.09960174709558486), (0.53175, 0.0901152290403843), (0.50325, 0.13062539848685265), (0.5315, 0.11919039076566697), (0.53475, 0.09522650840878487), (0.53075, 0.10572892221808433), (0.503, 0.10134429281949997), (0.51425, 0.11815588471293449), (0.526, 0.10621752497553826), (0.50525, 0.12416842910647392), (0.48825, 0.12445374444127083), (0.48925, 0.11145166382193565), (0.49075, 0.13602007633447646), (0.52775, 0.11289180800318718)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.48      0.92      0.63      1000
           3       0.61      0.14      0.23      1000
           6       0.53      0.86      0.65      1000
           8       0.83      0.19      0.31      1000

    accuracy                           0.53      4000
   macro avg       0.61      0.53      0.46      4000
weighted avg       0.61      0.53      0.46      4000

Collaboration_DC_2
VAL: 
[(0.246, 0.04495431101322174), (0.25, 0.0901500660777092), (0.285, 0.12618956410884857), (0.322, 0.1603555630147457), (0.379, 0.1944475924372673), (0.396, 0.21103904109448193), (0.418, 0.21013511917740108), (0.404, 0.2106591824889183), (0.419, 0.24523430716246367), (0.417, 0.2547052926644683), (0.416, 0.2696862921901047), (0.408, 0.24795482557825743), (0.416, 0.21287916172482074), (0.409, 0.20620561659522355), (0.413, 0.2141700718589127), (0.417, 0.2010298516601324), (0.416, 0.19028245759755372), (0.417, 0.198406883046031), (0.425, 0.17594213546812534), (0.419, 0.1784454032704234), (0.421, 0.1650060859248042), (0.417, 0.19123647987097503), (0.424, 0.19773468235135078), (0.425, 0.17287417554110288), (0.432, 0.15998460517451168), (0.428, 0.1745913468338549), (0.417, 0.16771833604201675), (0.425, 0.18102585861459375), (0.422, 0.174574798528105), (0.417, 0.16770780604705215), (0.426, 0.17489788097515702), (0.427, 0.19048478404432534), (0.427, 0.18145827678591012), (0.429, 0.20600391821935773), (0.421, 0.16767128490284086), (0.43, 0.19244203187711537), (0.425, 0.1765168351456523), (0.427, 0.1899040335919708), (0.422, 0.20019521292299033), (0.416, 0.18017272815853358), (0.416, 0.21548863060213624), (0.422, 0.20840612544119358), (0.428, 0.22715511994808912), (0.425, 0.1865906358063221), (0.431, 0.20852795139886438), (0.427, 0.19415132585912942), (0.428, 0.20779915237240493), (0.43, 0.2156131567172706), (0.428, 0.2053078173138201), (0.427, 0.21190348448231816), (0.433, 0.2048171814084053)]
TEST: 
[(0.2375, 0.04377405685186386), (0.25, 0.08629248949885368), (0.2875, 0.1204801225066185), (0.33075, 0.15272677546739577), (0.383, 0.18404866141080856), (0.396, 0.1998128536939621), (0.4165, 0.19864237356185913), (0.41075, 0.20010809701681137), (0.4185, 0.232751971244812), (0.4215, 0.2435290548801422), (0.42125, 0.25688072097301484), (0.403, 0.23545075440406799), (0.41975, 0.2019817939400673), (0.41775, 0.19775947564840315), (0.41125, 0.20490743494033814), (0.41875, 0.19217830300331115), (0.41675, 0.18168154060840608), (0.41975, 0.18943533319234848), (0.428, 0.16760538870096206), (0.42375, 0.16905593997240068), (0.4205, 0.15736935621500014), (0.4185, 0.18217562663555145), (0.431, 0.18629773050546647), (0.434, 0.16516957604885102), (0.4335, 0.15153857529163361), (0.42275, 0.16629375857114792), (0.42725, 0.15967606592178346), (0.4295, 0.1718483698964119), (0.431, 0.16602205783128737), (0.42075, 0.1579225931763649), (0.4315, 0.16458003556728362), (0.422, 0.17984903562068938), (0.4265, 0.1712195789217949), (0.43275, 0.19567332029342652), (0.4245, 0.16076010072231292), (0.43175, 0.1836474786400795), (0.4345, 0.16843656104803084), (0.42825, 0.18069443023204804), (0.426, 0.19009565901756287), (0.431, 0.17062706470489503), (0.42825, 0.20408461552858354), (0.43075, 0.1963118736743927), (0.42975, 0.2179517166018486), (0.43175, 0.17871909713745118), (0.4295, 0.19703590792417527), (0.4325, 0.1877291578054428), (0.43075, 0.1978641185760498), (0.43875, 0.20330806839466095), (0.434, 0.19445263916254044), (0.43375, 0.200991632938385), (0.435, 0.19433642143011093)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.35      0.92      0.50      1000
           6       0.00      0.00      0.00      1000
           7       0.61      0.82      0.70      1000

    accuracy                           0.43      4000
   macro avg       0.24      0.43      0.30      4000
weighted avg       0.24      0.43      0.30      4000

Collaboration_DC_3
VAL: 
[(0.251, 0.04425755798816681), (0.25, 0.08961409723758698), (0.273, 0.1021047906279564), (0.392, 0.12693029174208642), (0.402, 0.15093016832321882), (0.411, 0.1675949716344476), (0.459, 0.21405988819152116), (0.462, 0.2431645128428936), (0.46, 0.27802466233447193), (0.463, 0.2662605528570712), (0.458, 0.29413447110913693), (0.455, 0.30002766743209214), (0.429, 0.2728695821799338), (0.458, 0.32131102004367856), (0.455, 0.31192026708088816), (0.453, 0.32049203978851437), (0.462, 0.3312542861709371), (0.462, 0.3241231879414991), (0.451, 0.33041530528105795), (0.46, 0.38994095954392105), (0.453, 0.3705408712690696), (0.459, 0.38060229244735094), (0.451, 0.3406844732235186), (0.455, 0.3549927529883571), (0.458, 0.34874051429191605), (0.455, 0.36089862601598727), (0.457, 0.3488742897985503), (0.452, 0.3508404924441129), (0.442, 0.3521835003287997), (0.454, 0.343494473033119), (0.444, 0.3736181068490259), (0.448, 0.356747793949442), (0.436, 0.3183812234643847), (0.449, 0.3222155134831555), (0.454, 0.28754090654850006), (0.459, 0.2838235316844657), (0.463, 0.3114878219761886), (0.46, 0.2869299180400558), (0.456, 0.28756714005023243), (0.451, 0.30098611854761836), (0.457, 0.2944354251273908), (0.459, 0.28552463369444014), (0.441, 0.2966108890278265), (0.46, 0.3334855201849714), (0.458, 0.34055804085056296), (0.46, 0.3311109632293228), (0.453, 0.2841403236463666), (0.46, 0.2983513999353163), (0.444, 0.2840713277391624), (0.456, 0.2958631407618523), (0.46, 0.27718833289202305)]
TEST: 
[(0.2505, 0.04323132234811783), (0.25, 0.08614569437503815), (0.2725, 0.09757168874144555), (0.40125, 0.1206749073266983), (0.39925, 0.14345088809728623), (0.415, 0.15829271918535232), (0.46625, 0.20534850442409516), (0.4725, 0.2301350491642952), (0.46875, 0.26313387537002564), (0.4705, 0.2506633816361427), (0.4695, 0.2793130035400391), (0.46325, 0.2828454539775848), (0.43925, 0.2567571042776108), (0.47075, 0.30496298825740814), (0.46525, 0.2943211176395416), (0.46875, 0.30296196818351745), (0.46975, 0.3132696305513382), (0.4705, 0.306045547246933), (0.466, 0.3100515432357788), (0.472, 0.3680103735923767), (0.46175, 0.34747909355163575), (0.47075, 0.36312540078163147), (0.45825, 0.32426022720336917), (0.46525, 0.3381721967458725), (0.46975, 0.32751664757728577), (0.465, 0.3418446743488312), (0.46525, 0.3318506735563278), (0.46575, 0.32815715730190276), (0.45225, 0.3357056618928909), (0.466, 0.3239580273628235), (0.459, 0.3558789908885956), (0.46175, 0.33701856327056884), (0.448, 0.3058668202161789), (0.462, 0.30707684278488157), (0.462, 0.27461817955970763), (0.46475, 0.26802481085062024), (0.47025, 0.29225748145580294), (0.46775, 0.27138515627384185), (0.46475, 0.27448495733737943), (0.46275, 0.287072709441185), (0.46875, 0.2785010520219803), (0.4675, 0.2725639827251434), (0.4525, 0.2826107814311981), (0.47175, 0.318208034992218), (0.4695, 0.32747227609157564), (0.47275, 0.3143275272846222), (0.46925, 0.2724280376434326), (0.47225, 0.2840541373491287), (0.45775, 0.2675218676328659), (0.4705, 0.28308551716804503), (0.47175, 0.26509765720367434)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.45      0.98      0.62      1000
           6       0.00      0.00      0.00      1000
           9       0.50      0.91      0.64      1000

    accuracy                           0.47      4000
   macro avg       0.24      0.47      0.31      4000
weighted avg       0.24      0.47      0.31      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [42]
name: alliance-2-dcs-42
score_metric: contrloss
aggregation: <function fed_avg at 0x7fe4d7794c10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=42
Partitioning data
[[6, 3, 1, 0], [4, 9, 1, 0], [2, 7, 1, 0], [5, 8, 1, 0]]
[(array([44073, 21183, 23596, 47728, 22362, 44695, 31128, 11101, 38769,
        9468,  7705, 30033, 16760, 33704,   313,  4389,  8937, 40664,
       11672, 12356, 23451,   326,  9481, 31835,  9625, 38853, 40544,
       13754, 33212, 15185,   347, 14513, 38677,  1596, 13137, 27554,
       36188, 47513, 39889, 26364, 46971, 40041, 15983, 12491, 13390,
       18884,  5526, 42015, 31564, 19941,  8679, 39252,  7516,  1529,
          22, 22337, 32010, 20734,  8171, 26966, 24289, 36360, 15498,
       17372, 18655, 10058, 47115, 15052, 26940, 36841, 46052,   525,
       17596, 18621, 22795, 25168, 27678, 40685, 41088,  7964, 47396,
       31706, 33824, 13479, 35439, 48841,  8597, 37887, 15612, 15492,
        7392, 25819, 46429, 22384, 18257, 10532,   124, 35928, 12804,
       33731, 14231,  7823, 42460, 39637, 37574, 20717, 42186, 49537,
       22058, 31948,  9367, 34028, 37332,  3833, 25967, 48565, 29798,
        6810,  4143,  2888, 48753,  1360, 43697, 19959, 42690, 11363,
       34805, 20106, 25904, 43581,  3290, 38662, 46188, 29538,  2623,
       23876, 12238, 17317, 34912,  2674,  6749,  7215,  2411,  3048,
        4644, 19955,  9320, 37974,  7867, 45247, 18656, 45900, 34706,
       12550, 40128, 17680,  7265,  2503, 24261,  5223, 32339,  7092,
        2997, 37682, 29241, 11048, 14042, 49200,   154, 32198, 27385,
       34563,  8983, 30143, 13331, 35672, 25067, 35830,   327,  3567,
       40836,  1733, 30193, 29680, 11644, 28322, 45305, 10938, 22065,
       26240, 36788, 32082, 11466, 41641, 42778,  5921, 49082, 14257,
        3861, 30323,  9126, 23771, 48658, 49094, 26054, 39986,  7003,
        7968, 14894, 48607, 29123,  1228, 35732,  6125,  3227,  3780,
       43661, 49059, 38166, 17759,  3849,  6859, 10029, 39678, 33488,
         409, 15192, 48047, 36285, 37033, 49829, 10828, 24852, 35904,
       11875, 40462, 30074, 23787, 14749, 17151, 48446, 40797, 17603,
        3266,  2115, 36486, 12996,   125, 14889, 36282, 11192, 42141,
        3178, 42131,   315, 42949, 20520, 39924, 26609, 38931, 33925,
       32215, 37175, 37162, 30708, 48686, 20868, 39609,  3763, 45152,
       19465, 40314,  2525, 34407, 28717, 43813, 10869, 31626,  3320,
       39166, 22922, 39130,  7049, 41918, 11211, 26357, 31535, 39500,
       33422, 36212,  9304, 43123, 28472, 21070,  4745, 38317, 10967,
       32451,  9976, 10843, 25882,  8656, 15912,  4657,  7909, 37770,
        6557,  6167, 36664, 43283, 33371, 37187, 30838, 35009,   159,
       42647, 36781, 10254, 23995,  5389, 30683, 15175,  1030, 32392,
       21574,  7412, 25492, 41952, 46568, 29692, 21909, 37215, 35163,
       10468,  1573, 18452, 32145,  6071, 27270, 13552, 35124,  5610,
       44047, 13471, 35071, 11028, 29235,   895,  9694, 38702,  7230,
         367, 34682, 23322, 15844, 10611, 14125, 22701, 38879, 44859,
        5578, 49468,  9052, 48315, 26154, 14574,  4439, 11449, 39822,
       47819, 18889, 15083, 46402,  7906,  8781, 39871, 49855, 31141,
        9710, 42789, 12821,  1059, 46594, 39150, 20601, 21415, 20631,
       10746, 36930,  4449, 42206, 29177, 46880,  4428, 47217, 12601,
         258,  2723, 14578, 21524,  1265, 22030, 47413, 35804, 46109,
       31261, 14155, 26711, 16827, 29950, 47523, 42223, 39420, 29004,
       30479, 43744, 19187, 49582, 20496, 15954, 31479, 44103, 32043,
        5859, 30586, 45379, 47851, 27619, 40584, 12013, 42865, 13678,
       17449, 46232,  7460,  9609, 24082, 16705,  5870, 39300, 19352,
        6304,  3447, 12776,  3591, 35976,  8255, 37109, 15383, 10948,
        6200, 26126, 17841, 39220, 26472, 17018, 36745, 43554, 23977,
       41369, 26575,  3289, 21015, 29074, 36051, 19847, 17124, 39575,
        8330, 17879, 43034, 26859, 34756, 17814,  8507, 21156, 30737,
       12174,  4564, 25933, 21147, 15397, 25535, 46838,  9738,    74,
        8056, 42633, 19820, 14122,   334,  5065, 48601, 37863, 17565,
       22008, 41057, 28731,  7128, 12454, 36929, 21685, 43908, 46396,
       12773, 48122, 38084, 42606,  6751, 18908,  8812, 30135, 19522,
       30182,  9189,  2180, 15583, 45214,   354, 11750, 30154, 24725,
       24463, 47109, 15770, 48358,  7255, 24400,  8232, 10190, 23921,
       24620, 47266, 28671, 28615, 36641, 38577, 21971,  7379, 41922,
        8667, 29188, 37801,  4047, 11407,  2582, 35861, 48595, 38596,
       39953,  6064,  9690,  2143, 35576,  9646, 47616, 20994, 35910,
       13277, 38514, 22399,  2204, 21528, 38725, 38682, 48740, 10292,
       10755,  4648, 25150, 22072, 15867,  9658, 19481, 37588, 12982,
       22111,  1559, 49409, 47470,  4983, 42805,  5197, 37030, 47863,
       42918,  1304, 46674, 30227, 13908,  9138, 16726, 10644, 28910,
       14859, 44448,  3064, 24284, 31737, 25323,  1660, 31512, 41439,
       19533, 30457, 49356, 21094, 12127, 45470, 34794, 35850, 12276,
       26304, 10363, 36791, 12920, 15299, 34347, 17752, 35649,  4100,
       22212, 12451, 13895, 24288, 17309,  9318, 20079,  9595, 28937,
       32087,  9509, 36028, 30747, 19886, 33059, 13608, 47772,  3934,
        8107, 36879,  6779, 40410,  9878, 30570, 11824, 29347, 16766,
       20746,  1907, 20753, 46840, 24774,  5137, 47987, 26141, 13849,
       44146, 23151,  2286,   568, 19354, 49447, 47980, 24833, 31274,
        7147, 31268,  9946, 48820,  9889, 40250, 40713,  4334,  1731,
       36040, 35106, 26467, 48264, 47151, 42413, 37618, 13755, 37179,
       46237, 22564, 33126, 46544, 43655, 24527, 38347, 12628, 43935,
       41701, 38174, 10824, 41471,  7130, 23578, 23864,  4993, 40958,
       18311, 13940, 49438, 16810, 24224, 33198,   536, 25459, 16012,
       21631, 16853, 24520, 36774, 13462, 35366, 24999, 48845, 45228,
       49998, 30560, 32684, 27488, 14549, 34336,   917, 42658, 16723,
        5420, 26794,  5202, 36736, 23232, 30047, 14048, 32665, 16986,
       42437, 10910,   815, 40204, 40595, 39735,  6831,  2775, 39387,
       34809, 30344, 37468,  5969, 40275, 43245, 18009, 23386, 29944,
       29364, 47106, 44599, 48378, 12469, 29596, 26162,  2718, 44906,
         332, 27961, 22026, 28972, 31164, 47678, 12030, 40747, 46576,
       17485, 23478, 36858, 27026, 38344,   349,   628, 47320, 31837,
       45242, 34856, 47691, 16238, 10553, 30977, 47379, 20265,  8382,
        7727, 27104, 26430, 20759, 11989, 18857, 31521, 44294,  6165,
       21881, 23194, 15473, 13326,  5873, 10394, 25337,   752, 26842,
       38551, 40945, 36750, 40287, 32247,  4295,  9014,  9140, 26546,
       18218, 33596, 47991, 17196, 30743,  4957,  4859, 25763,  4759,
       37558, 39716, 28978, 40644, 28208,  4645, 24387, 48470,  4504,
       47802, 31654, 15594,  2490, 23964, 10215,  6085,  2707, 15886,
       29480, 47579, 25350, 10334, 22962, 38855,  9813, 21874,   165,
       13357, 19294, 30378,  3318, 36149,  6242, 44715, 16840,  7660,
       42476, 19945, 36055, 34106, 36163,  8857, 18291, 41238, 39739,
       21825, 48671, 22847,  4861, 41300, 46898, 10043,  6396, 20924,
       23786, 21144, 49420, 23825, 29819, 36836, 42399, 10686, 13051,
       38977, 13835, 31459,  6276, 47550, 14949,  4765, 21195,  2578,
       43292, 45096, 17490, 46200, 19860, 31083, 36623, 12836, 38224,
        3549,  8383, 12767, 32592, 15863, 44167,   940, 42755,  1988,
       14534, 11897, 37628, 19237, 49556, 41258,  3362, 14213, 38656,
       49856, 38187,  8623, 11416, 21469, 23684, 12010, 47711, 36254,
        3618, 40411, 15541, 17054, 11572, 46614,  9493, 34017,  2513,
       47560, 48516, 14194, 25748, 42678, 32194, 42205, 14441, 40022,
        7347, 41731, 37646, 36786, 12429, 39670, 18953,   695, 28456,
       16100, 34919, 18047,  6904, 31488,  2435, 14594, 37729, 29246,
       11783, 24521,   694, 35758, 18677, 37806, 33750, 28636, 40479,
       16230, 49018, 29540,  5533, 18955, 43375,   748, 31119,  8199,
       35343, 39179, 10732,  5699, 35209, 21119, 26269, 27749, 32267,
        1039]), [6, 3, 1, 0]), (array([ 4666, 12143, 39947, 36083, 20608,  5946, 45609, 19936, 21684,
       29799, 48902,  8772, 28174, 30790, 33066, 20342, 43758, 26854,
       43923,  7100,  7062, 42938,  8117, 23393,  7383, 41270, 34261,
       48008, 19957,  5143, 26417, 49207, 14303,  4773,  6475, 27914,
       20485, 30341, 24377, 49024, 48879, 24933, 14664, 13787,  2922,
       35630, 41927, 30616, 16936, 48429, 31799, 18014,  3566, 45311,
       16085, 10385,  2710, 41635, 29131, 32026, 22727, 44473, 14082,
       13292, 44790, 12213, 16155, 22139, 30881, 37234, 43974,  7154,
       27469, 27213, 36896, 24236, 46940, 10629, 25949, 38802, 31433,
       24827, 22619, 29613,  6311,  5364, 26897,  1557, 12797,   982,
       25346, 27663, 28414, 30384,  2976, 32803, 15909, 34784, 27787,
       38353,  3802, 28885, 35925,  9452,  1639,  3393, 41788, 29368,
       22480,  3985, 48533, 44254, 29922, 14385, 14923, 10426, 17179,
       17819,  8523,  5594, 14810,  1535, 47305, 41820, 48478, 49114,
        8378, 31993, 28260, 31514,  2444, 31848,  6594, 33222, 40844,
        8208, 16294,  3970,  8379, 26653,  1148, 26351,  3556, 46339,
        3957, 24241,  7099, 49758, 39061, 27817, 41669, 42969, 22519,
       20956, 23823, 47441, 41097, 48945,  9848,  5721, 48163,  4943,
        6844, 22133, 18912, 34384, 26638, 42418, 12898, 26540, 20192,
       14041,   420, 18821, 36843,  6437, 49352, 34808, 44846, 37871,
       11092, 29756, 34082, 25846, 48884, 13282,  3688, 39734, 15018,
       28649,  3383,  5625, 15105, 26475,  4497, 47928, 23474, 18669,
       38755, 39202,  3141,  1407, 27214, 36533, 34215, 27429,  3326,
       27596, 36914, 45170, 18060, 41547,  6421, 33094,  9771, 30719,
       15674, 46255, 42087, 22246,  1349, 36777, 48521, 18320, 12106,
       36247, 27417, 29083, 21771, 17845,   398, 10865, 36600,   945,
        6532, 36199, 15487, 35066, 31549, 22944, 35809, 18300, 41453,
       49645, 27647, 36976,  1366, 35522,  6144, 27180,  4865, 36734,
       44242, 28459, 22989, 48431, 39556, 23908, 47690, 27717, 29073,
        8323, 29438, 19525, 31909, 36807, 26530, 19486, 48672, 36223,
       11678, 12708, 10071, 25499,  3876, 13588, 43596,  1117, 24609,
       40931, 32273, 16143, 20231, 44013, 26439, 24893, 40315, 34995,
       22009, 32952,  4584, 10092, 23645,  5595, 37600,  9673, 10030,
         749, 35178, 14471,  5300, 42058, 46685, 31458,  8506, 14723,
       20517, 26150, 36887, 32778, 27569, 35203, 39721, 19076, 14954,
       18235, 26050, 41904, 25510, 45055, 18668, 45197, 29238,   443,
       47343, 11258,  1827,  4643, 26627, 13609, 16803, 40693, 23302,
       43325, 18982,  8402,  8754, 28215, 18681, 49833, 40324,  9277,
       41899, 11388,  9959,  4142,  1767,  2935, 44315,  2251, 21887,
       30285, 27581, 10717, 35251, 21067, 36878, 46335,  4665, 29197,
       19432, 16642,  8126, 15221, 23886,  2219, 41539, 11620, 21801,
       23624, 22285, 16152, 21478, 42704, 49181, 31638, 45059, 33547,
       36834,  2129,  5133, 21714,  9027, 30175, 23979, 26121, 16718,
       49427, 23325, 14868,  1437, 40885, 27369, 33746, 28901, 34370,
       26848, 47755, 18968, 42624, 44380, 17747, 37347, 32439, 16773,
       48568, 15978, 25285, 31517, 23961, 41505, 26681, 34774, 42911,
       15384, 19891,  5801, 32084, 45735, 25945, 43104,  8567, 33676,
        9513, 48868, 32041, 27040, 23798, 35676, 44669, 10762,  7629,
       48165, 26763, 43760, 43560, 31768, 36557, 45174,  9936, 14198,
       39930, 24453, 23840, 34402,  3819, 21093,  5394, 46438, 28235,
       15350, 20296, 25936, 11430, 31790, 34423, 48919, 43616, 12986,
         683, 37604,  5382, 10467, 28379,  7583, 44056, 48449, 13208,
       19826, 17600,  3510, 15501, 48921, 45302, 38687, 26466,  6934,
       38035, 36794, 44469, 46351,  5485, 23471, 31012, 49904, 31618,
       13962, 43652, 46852, 35852, 41156, 41436, 47419, 37115, 10781,
       29384, 18488, 31771,  4028, 13780,  7587, 38858,   238, 33505,
       20327, 23431, 41963, 32311, 46519, 28876, 26008, 25389,  9620,
        4354, 10145, 22656, 34595, 10747, 12311, 21418, 48269, 29829,
       44412, 28530, 27802,  2771, 18670, 30912,  2339, 30086, 12360,
       40297,  8191, 38658, 11196, 41105,  7738, 14167, 21283,  7019,
       30103, 22490, 32121, 49674, 42239, 29763, 40431, 33845, 26298,
       40185, 14721, 19575, 13323, 17726, 38549, 37145,  8149, 28008,
       25689, 49652, 36046, 33314, 15261, 35666, 25463, 14900,  1287,
       37301, 32474, 31969,  8189, 37260,  8559, 12123, 27538, 15306,
       46953, 16286, 44876, 18331, 49705, 35757,  3735, 33646, 43520,
       35406, 25544, 45856, 15934, 16774, 40936, 43797, 33292, 39902,
       22217, 49269, 29794, 37995, 25105, 34128, 49769, 44466, 45099,
        4803, 28054, 39912, 18238, 26093, 21215,  6969, 47370, 16309,
       25750,  9478,  6639, 13240, 33561, 38078, 49286, 32003, 18562,
       43988, 42130, 31890, 36086,  9251,  4927, 22586,  2630, 17275,
       31300,  4099, 33951, 31147, 23100, 32235,  5827,    94, 44824,
       24936, 17205, 24146, 18456, 33643, 46794, 49121, 37183, 25069,
       34618, 23524, 26742,  5195, 40964, 19095, 10642, 11515, 23311,
       39990, 27330,  6813, 18844, 48639, 16412, 37327, 34862, 41946,
       28268, 10249, 16956, 14358, 25619, 45988, 39208,  2039,  5635,
       45910, 16958, 17017,  8757, 30236, 24297, 26550, 10458, 19574,
        5895, 21152, 44183, 37512, 25682, 20463, 41862,  4949, 20475,
       21429, 28617,  1586, 38637, 26368,  1790, 43984, 43585, 15855,
       25048, 27260, 49381, 31552, 18649,   874, 48263, 20595, 37461,
       22545, 38463, 49777, 27700,  8628, 42335, 13056,  7349, 46786,
       17106,  2615, 23357, 38231, 13191,  4132,   561, 24075, 19760,
       42976, 43290, 39437, 18160, 15783,  6269,  9584,  2727, 11580,
        2227, 42709, 25007, 34913, 13922, 38157, 42912, 44936, 29283,
       20336,  8085,   947, 27460, 45188, 44319, 40590, 40194, 28133,
       45348, 24735, 36889, 14689, 12192, 29666, 40131,  7470, 25362,
       10855, 29380, 21674, 43002, 47483, 18767, 14488, 22326, 40861,
        8453, 17200, 29854, 36370, 29367, 28653, 32076,  7168,  6374,
       40928, 47805, 44307, 32149, 41355, 47912,  8376, 10323,  4387,
       19632, 19386, 46910, 23767,  9860,  7826,  2006, 24939, 40607,
        1119, 17692, 25198, 32484, 42110, 45123,  2895,   341, 17520,
       17237, 20682,  7547, 11280, 21231, 29378, 24445, 28681,  3940,
        3536, 31728,  5529, 39959, 22800, 30539, 14334,  6015,  3903,
       16838, 15554, 38233, 37049, 39396, 37504, 28311,  7093,  4524,
       21760, 12181, 25457, 19329, 20257,  2613, 18525,   317, 26124,
       41179, 35028, 39431, 34173, 32874, 49510,  4516, 22521, 29444,
        9945, 49656, 37519,  7360,  5642, 40635, 41019, 35927, 45615,
       31450, 10487, 29641, 20233, 31927, 28068, 46839, 24750,  3180,
       48729,  6663,  2478,  1376, 43564, 45106, 30487, 48851,  4281,
        3564, 19277, 44645, 34780,  6328, 23024, 26178, 13817, 26296,
       16559,  4176, 23126, 15236, 21522, 24661, 25422,  5346, 12944,
       42241, 32683, 20473, 30913, 28865, 44552, 21774, 33003,  8478,
       26599, 19662, 39927, 16346, 10031, 43160,  1329, 35599, 18999,
       28665,  8549, 35377, 21227, 37032, 44785, 26391, 39504, 37905,
       13656, 35883,  4574, 35509, 23522,   598, 35763, 43667,   129,
       38307, 27080, 45384,  5094, 24795, 34189, 40343, 36363,  3128,
       48618, 21439, 27703, 13613, 21782, 48071, 46046, 35626, 49423,
       40556, 36415, 34646, 32631, 33509,  5924, 19445, 12764, 49174,
       15044, 12916, 36862, 38478, 43838,  7647,  4348, 47274, 33542,
       19730, 29189, 37250,  2169, 41792, 44619,  6735, 30408, 27315,
       31556,  8569, 29492, 41414, 18211,  3184, 24592,  3042, 17093,
       46367, 30321, 23491,  1382, 48958, 16253,  7196, 36632, 15414,
       41456]), [4, 9, 1, 0]), (array([37315, 14106, 29220, 39412, 37615,  8641, 37821, 20651, 44177,
       33293, 46673, 25455, 25038, 31269, 17779, 48074,  8219, 47587,
       19204, 10417, 43190, 48493, 40189, 33550, 25478, 40256, 15303,
       26990, 13290, 16221, 39931, 34529,  9319, 15751, 40385, 48146,
       31926, 13671,  2557,  8121,  1107, 31941, 18837, 42588, 28403,
       41312,  5922,    54,  3639, 28834, 40148, 15477, 13837, 22618,
       38476,  2247, 15563, 48518,  5016, 32499, 44302, 20880,  7844,
         830, 34495, 15522,  9424, 28224,  4902, 22045, 32141, 39508,
        8341, 43116, 28935, 37403, 14926,  7410, 46491, 14404, 12448,
        8234, 43312, 17826, 36585, 44670,  1334, 18086, 12782, 30102,
       40213, 22287, 42144, 12141, 30874, 26551, 14299, 19343, 43873,
       37613,  4023,  8776,  4719,  7807, 37352, 47335,   218, 30542,
        7919, 34630, 12346, 35460, 27560, 28948, 33183, 30466, 17817,
         411, 31026, 11921, 30023, 33564,  7948, 31040, 13556, 49224,
       18964, 29150, 13727, 40371, 31882,  1110, 19163,  7178, 49397,
       19789,  9227, 18863, 38007,  1789, 48244,  6946, 24902, 35537,
       36726,  3963, 39550, 25901, 15638,  9577, 21836, 41538, 31041,
       30478, 24739, 29581, 39976, 14146,  4491, 40115, 38022, 24706,
       19287, 24904, 17980,  8543, 33081, 19231, 21521, 24978,  9660,
       12721, 47449,  9240, 27580, 12536, 47481, 35734,  9740, 21442,
       25379, 42821, 13408,  5131, 26696, 32103, 14768, 25200,  7452,
       39514, 17206, 13786, 17380, 10474, 19655, 30654, 37549, 31420,
        8032, 23692, 17070, 47492, 14120, 43162, 23237, 43513, 31361,
       17262,  7194, 47533, 48287, 17834, 17844,  6392, 46810, 45341,
       45021, 29637,  7633, 20957, 44218, 23493, 14317,  3254, 32047,
       30669, 35230,  9954, 25743, 35294, 45238, 11528,  6502, 37766,
       31058, 35022, 13448, 43631, 24429, 11991,  7576, 34373, 35862,
       46179,  5527, 49991, 11813, 12656, 19964,  4588, 33450,  9224,
       24908, 39518,  3135, 39755, 30761, 14400, 28945, 26702,   623,
       28446,  4157, 32296,  8681, 18258,  7182, 38611,  8533, 31990,
       47092, 48767, 27748, 39494, 43919, 32673, 11881, 27634, 10259,
       44578, 14503, 22782, 10496,  1898, 47665, 36133, 19849,  2969,
       12658, 45851, 40930, 47153,  6739, 37211, 43671, 18941, 21482,
       18411, 43491, 10077, 48649, 14680, 41700,  1650, 34487, 39378,
       35814, 13124,  4736, 14745,  8814, 29588, 33921, 33151,  3487,
       23621, 14144,  2392, 23506, 20186, 46454, 38252, 30987, 21025,
       30319,  2813, 34899, 13268,  4933, 46348, 16025, 14977,  1276,
       14319, 25213, 32660, 21001, 23860, 45027, 37200, 35108, 40300,
       46988, 23834, 43083, 40912, 19962, 33981, 43580, 25968, 41052,
        7894, 37601, 38955, 49407, 32335,  2000,  4790, 44222, 43515,
         440, 13376, 14333, 21108, 13272, 18491, 41624, 40723, 29137,
       44356, 32696,  5058, 38110, 35975, 47170, 43163, 22031,  1054,
       29916,  6218,  1365,  4238, 40604, 19250, 33397, 47241, 49942,
       27781, 26796, 15296, 33496, 41001, 13423, 45058, 38474, 20016,
       43544, 48826, 19956,  6326, 33349, 14095,  3348, 34584,  6748,
       27916, 24012, 40439, 49264, 31520, 25365, 26032,  1488, 47748,
       19794, 36513, 10386, 10562, 43629,  2554,  6726, 43791, 14983,
       23744,  5737, 10348, 24970, 23053, 34228, 12381, 16914, 39345,
       23292, 30216,  5560, 47455, 48853, 25788, 27915, 17956, 37908,
       30030, 34372, 27086, 43293, 28150, 12683,  3835,  5771, 13058,
       41000, 22267, 14556, 37884, 24547, 15081, 17023,  1490,  7204,
       18601,  8751, 41959, 29062, 23880, 24698,   178, 33855, 22566,
       19461, 45722,   181, 28639, 19326,  5890, 24982,  9778,  8817,
       21822, 35623, 13818, 33684, 32507, 47185,  4076, 34583, 25410,
        5317, 41843, 30935,  3651, 12763, 27930, 29669, 26259, 26343,
       41805, 26017,  8461, 44826, 24954, 15662, 39696, 45859, 41519,
       29044, 46463, 28761,   565, 33532, 10630, 13973, 27024,  7875,
       19226, 44282,  9066, 33625,  3404, 40472, 29485, 28526, 31522,
       37412, 44198,  9822,  6703, 36751, 30446, 33539, 38327,  9365,
       25596, 23514, 30676, 34358, 45613, 24398, 39769, 32319,  9127,
       15609, 38491, 26937, 22822, 46730, 15100, 23379, 47256,  5311,
       23317, 27009,  8801, 13701, 40822, 41133, 25363, 39974, 32083,
       23139, 48515, 29616,  6429,  7102, 45475, 13740, 11007,  1631,
       17746, 24008, 43089, 23164, 34364, 41336, 14311, 11264, 31765,
        1689, 31711, 13509, 16882, 28896, 27545,  7495, 24807, 42113,
       20057, 46679, 24220, 28476,  8475, 39674, 22421, 45977,  4533,
       35635,  3230, 30183, 20811, 39352, 12260, 19898,  8825, 44960,
       40075, 42171, 28187,  9809,  5831, 42669, 11767,  5112, 22316,
       46956,  6683, 13244, 10377, 17209, 32402, 35508, 32486, 48749,
       37937, 20088, 49494, 42925, 14129, 22145, 28297,  3025, 11640,
       26471, 34802,  5515, 19564, 26474, 16278, 34069, 16938, 35264,
       18438, 18907,  8938, 25196, 10655, 25799, 39622, 10090, 36015,
       43547,  8181, 25857, 49993, 22261,  4101,  3922, 20661,  5811,
       21398, 20764, 30083, 45332, 42143,  8977, 22372, 41051,  2568,
       40808, 22037,  1820,  6130,  6668, 28394, 24157, 45317, 44835,
       28163, 27194,  8573, 12849, 26331, 26456,   617, 43618, 22818,
       48128, 13534,  1020, 49973,  5146, 29120, 21280, 21720, 19526,
       45403,  8809, 49977, 15512,  4171, 47839, 27607, 28858, 35357,
       45329,  1320, 25611, 17002, 39490, 40615, 35070, 25697, 27512,
       44751, 21003,  6494, 30120, 29875, 40090, 42150, 43480, 43061,
       29066, 32847, 30166,  4609, 20353, 24228,  8307, 31158, 25135,
       24151,   962,   991, 41248, 37687,  3483, 31925,   206, 18415,
       36593,  5627, 45427, 39275, 22055, 14040, 16953, 31319, 23275,
        5552, 41663,  6959,  5010, 49507, 19858, 10399,  8522, 36386,
       45604, 47468,  9859,  1335,  4991, 25439, 14520, 42309,  5477,
       24543, 42622, 10617, 19749, 36311,  1960, 26045, 35470, 16625,
         179, 46554,   284, 10516, 13649, 44637, 49633,  8292, 44932,
       44350, 12131, 11176, 25489,  9619, 40752, 22993,  2391, 10265,
       47271,  6570,  8889, 14754,  9616, 39628,  2863, 13260,   407,
       11474, 28305, 27646, 47456, 18847, 40190, 36925, 42754,  6428,
       46287,  4096, 39904, 29295, 36491, 44037, 15581, 13436, 22109,
       24212, 28512, 40525, 30426, 17203, 10115, 29099, 25546, 37432,
       21217, 17107, 30895,  1338, 12785, 42934, 17076, 21721, 18630,
        9124, 16564,  9059, 11107,  1514, 34989, 17328, 44609,  2946,
       45358, 32885, 21308, 49439, 15003, 21892,  9181, 44770, 41628,
       47163,  7299, 38618, 42952, 43700,  7925, 11304, 48327, 41164,
       38943, 11122,  9966, 22692, 33229, 25037, 12363, 11255, 49671,
       46876, 12078, 48119,  6373, 44116, 29732, 32403, 32460,   557,
        6920, 22129, 33249, 47969, 47916, 48197, 37493,  9328, 12328,
       32033, 32629,  3053, 38479,  5955, 41740, 44027, 44662, 42784,
       17183, 10811, 38613, 18381, 24258, 14350, 12133, 36252, 35042,
       10847, 40241,  7788, 24360,  7183, 28063,  3859, 28400, 27751,
       15030,  7060,  2967, 46553, 48289,  3382, 10024, 45716, 48359,
       44082, 12660,  3842, 24021, 43811,  3918, 43951, 39104, 25531,
       31339, 34270, 31214, 24802, 28267,  6640, 42867, 15396, 26427,
        3665, 22716, 12435, 25106, 32659, 14023, 17100, 13864, 39926,
       13850, 39899, 36886, 17011, 34568, 45509, 39113, 42563, 35286,
       16892, 32669,  4726,  1473, 10690, 42456, 20537, 20831, 45232,
       36068, 45262, 44145, 30172,  5640, 11224, 48691,  8118, 13876,
        5414, 22458, 45982, 48852, 11276, 12270,  4906, 38617, 25190,
         843,  6954, 14528, 43219, 18603,  9696, 48986, 10326, 16214,
       24367]), [2, 7, 1, 0]), (array([17346, 10744, 37508, 24120, 21089,  9397, 47629, 28022,  8204,
       25333, 15019, 41531, 10119, 39649, 15499,  2963, 31064, 29603,
        8400, 25248, 41098, 46780, 23268, 17186, 37880, 21379,  5201,
       42776, 37589, 47524, 48403,  1411, 49638, 27445,  1545,  5305,
       36378, 22430, 46767, 16698, 11496, 11174, 11636,   784, 48625,
       31132, 26889, 33319, 43991, 45653, 32079, 26543, 12573, 26173,
        5057,  9035, 36308, 13352, 20122, 20615, 37395,  4067, 44154,
       30757,  4424,  3152, 17375, 13111, 10541, 15485, 35748, 39677,
       16643, 24953, 20647, 20020, 49715, 26010, 48645, 13252, 31631,
       44024, 47905, 18939, 39558, 33295, 28754, 22390, 17773, 21148,
       17425, 21961, 14681, 28099, 35390, 28340, 44632, 11753, 18262,
        8502, 20322, 23639, 48161, 34216, 16212, 48135, 25202, 10586,
         450, 24656, 42494, 48208, 26917, 46471, 42152, 25057, 49052,
       30140, 27968, 34757, 47334, 44211,  1331, 20672,  5288, 36127,
       32636, 33795, 45278,  2167, 19105,  7864,  1786, 10294,  4893,
       20939, 32889, 24128, 15915, 14696, 17670, 10740, 49644, 27093,
       11189, 47672,  1810, 24037, 45222, 49333,  1298, 28967, 44112,
       15133, 33768,  6807,   607, 10759, 11396, 18001, 20185, 28306,
       12791,  8276, 22424, 21534, 38851, 31543,  5547, 21075, 27671,
       24006, 45831, 47012,  3748, 33552, 46165, 16496, 28902, 32782,
       20776, 34859, 47781, 28756, 43467, 12326, 21689, 17619, 40681,
        8457, 14663,  4874, 29774, 19252, 18003, 21266, 37706, 20092,
       44033,  1717, 30310, 48477, 17354, 48048,    40, 46010, 19537,
        9987, 20484, 48139, 44229, 43455, 44832, 15544, 20495, 24130,
       48341, 38788, 40310, 39176, 31248, 42645, 45184, 43338, 29660,
       26738,  1033, 21436, 47621, 31240,  8768, 17882, 34403,  8671,
       34887, 27890,  8287, 11259, 38629, 44306, 32452, 36030, 26073,
       12581, 24452, 13190, 42384,  4817, 29816, 40528, 41403,  7668,
       25618,   240, 26980, 25104,  8668, 21036,  4594, 42275, 20962,
       25472, 44720,  9648, 29695,  7496, 15676, 40881, 40137, 26481,
       26437, 30148, 16435, 22255, 23572, 31663, 22920, 12258, 34814,
        3740, 15241,  6756, 10702, 15223,  4652, 10810,  9324, 41314,
       21905,  1771, 44384, 14402, 14883, 16676, 18312, 12172,  2586,
       41264, 42048,  7306,  2166, 18382, 17854, 20150, 45017, 34983,
       28807,  2754, 20492, 42207, 34096, 23962, 34712, 37823, 29493,
        5523, 37994,   777, 11141, 47793, 16738, 32322, 49393, 16381,
        1357, 19195, 34963, 23538, 33231, 49830,  8278,  8445, 16483,
       38458, 43713,  1391, 23839, 12353, 33554, 49184, 22635, 31370,
        5732, 34276, 21456, 27490,  4272, 13004, 12891, 38525, 12201,
       34521, 24093,  9262,  7384, 13388, 13883,  9226, 10096, 41288,
        1994, 37002, 49960, 35150, 40500,  1616, 10098, 32698, 11842,
       17899,  5054, 47212, 29903,  5911, 38524, 43348, 30788,  8045,
       28037, 36584, 26220, 38002, 24787,  4974, 34152, 35864, 14587,
       29661, 45777, 13227, 46001, 40093, 48678, 26923, 34523, 27621,
       49744, 23080,   741,  3065,  2434, 27034, 38436, 40789,  7156,
       22056,  3541, 14245, 16159, 42032, 16898, 20198, 35374, 43899,
       23560, 20209, 35736, 45966, 14711, 41078, 37483,  3881,   456,
       42693, 42325, 19557, 44659, 36553, 27692, 38408, 45626, 25450,
        1720,  1250, 15743,  7790,  8337,  1686,  9826, 25521, 38120,
       15276, 14176,  5360,  7934, 20219, 31996, 36939, 21759, 47992,
       13693, 10088, 30514, 23219, 45648, 32819,  1914,  5427,  8489,
       41225,  1506, 36765, 15927, 16846, 20594, 39507, 30762, 45650,
        9883, 26951, 24271, 40124,  6912, 47445, 22862, 42557, 32543,
       39766, 24281, 22758, 21823,  8968, 23625, 30164, 36893, 39079,
       48299, 12954, 30745, 35274, 18182,  2284, 32800, 14934, 13958,
       43913, 47960, 21579, 38857, 48164, 30949, 12917, 32168, 48496,
       27262, 11571, 40358, 42507,  4050, 21137, 14855, 29791, 14481,
       12977, 45677,  7448, 48915, 17112, 21407, 30902, 49151, 14776,
       25201,  8090, 41489, 29007,  3120, 25488, 34442,  8944, 18594,
       16925,   942, 17255, 12080, 46259, 23901, 41733, 37027,  6851,
       42753, 41445,  6161,  8174, 36237, 31445, 13218, 42292, 39749,
       37203, 25209, 13217, 30712, 41602, 12648,   140, 16543, 17995,
        7913, 10826, 19865, 26288, 45655, 38950, 44726, 25306, 48075,
       28027,  4858, 14145,  7597, 18495, 42652, 46547, 25210, 38919,
       15172, 22710, 30374, 21661,  4565,  2928, 10248, 28810, 31534,
       49435, 17223, 40178, 38482, 10251, 32305, 42833, 27794,  3212,
       48030, 19446, 15740, 33970,  2844, 48294, 14381,  1570, 23662,
       11966,  7288,   997, 32366,  1476, 39039,  8300, 34153, 12283,
       43186, 39940, 35465, 31749, 13924, 21150, 45488, 17750, 43167,
        5076, 26519,  1547, 45923,  4040, 37938, 32252, 12981, 48574,
       26905, 31241, 15655,  1037,   105, 21498, 12158,  6922, 11558,
       37799, 40762,  2795, 19581, 22395, 36537, 29410, 12841, 36868,
       24255, 31288, 20914, 18276, 24561, 11464, 18698, 26424,  8875,
        8600, 22522, 16128, 41463, 30281, 27868, 34778,  3399, 12927,
       45866, 16576,  7640, 36672, 33034, 23564,  6786, 19484, 43602,
        3257, 38929, 18321,   743, 22984, 34162, 38630, 39532,  3954,
       44411, 21627,  4379, 22396, 34923, 21328, 36039, 28818, 13332,
       23635,  2600, 19628,  1611, 15710,  4953, 11791,  3072,  6020,
        4360, 28098, 10711,  1464, 30439,  5301,   840, 20197, 18527,
       46859, 16089, 47226, 35587, 15195, 41110, 45466,  6453, 24263,
       38522, 41844, 16594, 33624, 37491, 41525, 46966, 24979, 16829,
       27871, 19539, 20244,  6122, 47968, 19869, 12183, 46078, 16385,
       49045, 20546, 26881, 46147,  2422, 33206, 33907, 30104, 14964,
        6731,  9549, 15248, 21564, 47981, 23854, 41274, 11963,  3689,
       21488,  2421,  5266, 45373, 19210, 26081, 28428, 47771, 15502,
        7530,  4523,  8833, 23866, 32251, 14309,  4468,  2258, 14407,
       41751, 24534, 13660,  1950, 27155, 34828,   637,  7757, 18765,
       13880, 49169, 46420,  2598, 31880, 12915, 37324,  5516,  5037,
       46548, 42674, 33242, 14567,  8160, 40654, 44001, 40807,  2824,
       44296,  7700, 17998, 32378,  7663, 13561, 34746,   371, 12461,
        2355, 23449, 19682, 15931, 16529, 43442, 11440, 22308, 22907,
       31726, 34381, 11238,  5702, 18605, 26007, 27638, 49530, 38683,
       22665, 21815, 44468, 27873, 37165, 39213, 46113, 30749, 35524,
       11305, 38586, 36180, 39723, 31246, 47401, 34547, 46434, 44866,
       47763, 17963, 27522, 34676, 18315, 38898, 20606, 34711, 15884,
       40516,   189, 38254, 49245, 20622, 23096, 31600, 27071, 33700,
       12922, 46782, 16002, 27426,  3361, 36286, 30078, 20694, 20916,
       33510,  2451, 40649, 17717, 31888, 20980,  1243, 38937, 21605,
        4030,  3713, 33336,  5194, 44470, 38518, 41848, 17947, 44742,
       45085, 28273, 35285, 36134,  2171, 39080, 42844,  1926, 43867,
        7537, 19527, 36573,  7972,  3979, 30904, 16553, 13043, 13225,
         415,  9384, 38052,  8102, 16225, 40802, 37881, 36004, 26764,
       19103, 44884,  2855, 40876, 19347,  9495, 20985, 18200, 49555,
        2932,  2413, 31648,  2652,  1999, 16420,   687, 20708, 40843,
       46294, 28408, 11721, 27179, 22080, 20611, 24589, 40612, 35059,
       18659, 42231, 28401,  2010, 14413, 28126,  4559, 39477, 20035,
        5719, 41467, 34998, 44143, 13594, 21647, 47799,  5216,  1216,
       25288,  8842, 14727, 19639, 33076, 36358, 47089,  9759, 15240,
       39372,  3900, 24721, 11762, 22628, 17763, 28467, 12993, 41234,
        9287, 26722,  6923, 25083, 31947, 17838, 16920,  2060, 34722,
       27147, 16939,  8098, 43944, 21533, 20105, 16371, 31004, 18680,
       14124]), [5, 8, 1, 0])]
Collaboration
DC 0, val_set_size=1000, COIs=[6, 3, 1, 0], M=tensor([6, 3, 1, 0], device='cuda:0'), Initial Performance: (0.242, 0.04443864643573761)
DC 1, val_set_size=1000, COIs=[4, 9, 1, 0], M=tensor([4, 9, 1, 0], device='cuda:0'), Initial Performance: (0.218, 0.044824156880378725)
DC 2, val_set_size=1000, COIs=[2, 7, 1, 0], M=tensor([2, 7, 1, 0], device='cuda:0'), Initial Performance: (0.243, 0.04442102932929993)
DC 3, val_set_size=1000, COIs=[5, 8, 1, 0], M=tensor([5, 8, 1, 0], device='cuda:0'), Initial Performance: (0.244, 0.044828352451324466)
D00: 1000 samples from classes {0, 1}
D01: 1000 samples from classes {0, 1}
D02: 1000 samples from classes {0, 1}
D03: 1000 samples from classes {0, 1}
D04: 1000 samples from classes {0, 1}
D05: 1000 samples from classes {0, 1}
D06: 1000 samples from classes {3, 6}
D07: 1000 samples from classes {3, 6}
D08: 1000 samples from classes {3, 6}
D09: 1000 samples from classes {3, 6}
D010: 1000 samples from classes {3, 6}
D011: 1000 samples from classes {3, 6}
D012: 1000 samples from classes {9, 4}
D013: 1000 samples from classes {9, 4}
D014: 1000 samples from classes {9, 4}
D015: 1000 samples from classes {9, 4}
D016: 1000 samples from classes {9, 4}
D017: 1000 samples from classes {9, 4}
D018: 1000 samples from classes {2, 7}
D019: 1000 samples from classes {2, 7}
D020: 1000 samples from classes {2, 7}
D021: 1000 samples from classes {2, 7}
D022: 1000 samples from classes {2, 7}
D023: 1000 samples from classes {2, 7}
D024: 1000 samples from classes {8, 5}
D025: 1000 samples from classes {8, 5}
D026: 1000 samples from classes {8, 5}
D027: 1000 samples from classes {8, 5}
D028: 1000 samples from classes {8, 5}
D029: 1000 samples from classes {8, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.07661269408464431) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07654814468324185) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.09640297073125839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.07325242793560029) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.07945466989278793) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.08400962515175342) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.278, 0.12523775127530098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.09349438312649727) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.377, 0.09021134239435195) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.09025510634481906) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.15705580693483354) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.14240596529841423) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.11596032911539078) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.412, 0.11271193671226501) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.19727019335329532) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.16480454625189303) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.13193382388353347) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.12182052563875914) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.20265718460083007) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.21888766702078283) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO5']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.12846777415275573) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.140342478916049) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.21342508929222823) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.24219384985789658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.12581581400334835) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.14166802729293704) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.2270521096549928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.27305388302356004) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.11846597637236118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.15023153780028223) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.24203452307730913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.26939495775196703) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.388, 0.12462650395929814) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.17387545125558973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.2579819469116628) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29477349990280344) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.39, 0.14789085798710586) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.19303536528348922) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.267256860435009) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29640268966881556) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[0, 1], M=tensor([0, 1, 3, 4, 6, 9], device='cuda:0'), Initial Performance: (0.0, 0.39164984703063965)
DC Expert-0, val_set_size=500, COIs=[3, 6], M=tensor([6, 3, 1, 0], device='cuda:0'), Initial Performance: (0.78, 0.015118347033858299)
DC Expert-1, val_set_size=500, COIs=[9, 4], M=tensor([4, 9, 1, 0], device='cuda:0'), Initial Performance: (0.932, 0.004831772923469544)
SUPER-DC 0, val_set_size=1000, COIs=[6, 3, 1, 0], M=tensor([6, 3, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[4, 9, 1, 0], M=tensor([4, 9, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7fe4bc52e190>, <fl_market.actors.data_consumer.DataConsumer object at 0x7fe4bc255ca0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7fe4c93cc1c0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7fe4bc2cfbe0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7fe4c8048d00>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO2', '(DO1']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.816, 0.013623159885406494) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.003951671320945024) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.22960870119184257) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3371907848324627) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.898, 0.012114560725167393) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.03327320522069931) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.579, 0.04259853515028954) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.808, 0.015813011512160302) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.0039044502060860396) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.2366888452824205) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.295455738930963) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.921, 0.008944567733444273) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.63, 0.03518916761875152) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.684, 0.02760743622481823) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.014454739689826966) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.004532153335399925) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.424, 0.2237983683794737) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3324814289479982) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.915, 0.0124593272828497) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.548, 0.06674703687429429) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.567, 0.055440423756837845) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.015327820420265198) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.0038544284589588644) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.21164736951515079) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.32773079082055484) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.009790850923804101) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.05617418409883976) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.04554247554577887) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.802, 0.018346358798444273) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.004695789663121105) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.21581539477035402) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.34862902016210134) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.903, 0.015827704433275358) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.06532805587351322) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.588, 0.06395933274552226) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO5', '(DO1']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.822, 0.016118780374526978) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.0037459822595119477) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.19614045030996202) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3430978617221699) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.009131747890962287) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.06009200153499842) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.624, 0.050148758105933666) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.012946626670658589) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004176362656056881) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.19363816899061204) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.33815361770795427) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.014680169455707073) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.579, 0.06746088790893555) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.573, 0.06771461363136769) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.015483400791883468) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.00406934268027544) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.2129457389935851) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.32490580514399336) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.914, 0.01039409415051341) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.07127337756752967) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.594, 0.057834875792264935) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.798, 0.01694578178226948) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.0041325056925415994) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1735377303622663) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.3265296587602934) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.01564799189241603) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.10931113491207362) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.566, 0.07177280266582965) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.806, 0.017331746511161327) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004286423107609153) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.16479179935157298) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.34018476697948064) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.921, 0.014168451374396682) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.0942899264767766) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.06351472976803779) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO4', '(DO3']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.014950806081295013) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.004206812805496157) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.19898282311484217) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3253972700007434) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.923, 0.009890132184140385) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.508, 0.09169202837906777) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.544, 0.0653500208593905) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.015479213878512383) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.00390908207371831) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.17866512263193726) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.31501067836116997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.927, 0.0075005013588815925) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.544, 0.06696937327831984) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.615, 0.05633704297989607) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.014370638489723206) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.004809955766424537) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.18542744836956263) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3357834734985663) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.014363764680572785) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.486, 0.11281656597927213) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.504, 0.09121366058290005) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.01469383167475462) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004347382958978414) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.1639763366803527) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3033173082342837) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.010726510212465654) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.509, 0.10425110609363765) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.07179991602431983) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.016114526964724065) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.004528122584335506) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.19515224239975215) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.38166177578693894) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.015031673278361269) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.489, 0.12258040457870811) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.519, 0.09457737705856561) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO2', '(DO1']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.013991448044776917) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005544900985434652) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.17674107662402094) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.31922820964769927) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.933, 0.008433990311808885) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.499, 0.09764718233793974) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.616, 0.05469020216166973) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.017105687581002713) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006529035585233942) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1855120692625642) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.35193505106111117) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.01182148765452439) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.51, 0.10836697126179934) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.585, 0.06113107100874186) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.814, 0.01773606948554516) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004280060423072427) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.19329150774143636) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.32070267097544275) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.010611240316882686) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.09444492107816041) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.07613846186269074) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.81, 0.014891550987958907) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.004757876061368733) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.22479695722460746) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.3590226922050933) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.010805731417494826) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.10254619359970092) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.626, 0.05411215067654848) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.816, 0.018151832230389117) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.005009501743130386) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.19618702696822585) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3810500155718182) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.013032892712548346) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.504, 0.10024467813968659) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.566, 0.07191571754217148) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO3', '(DO1']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.01874016012996435) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.924, 0.0070593022983521225) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.19638450679183006) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3238292219626019) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.011520547933876515) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.10774801188893617) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.509, 0.09973231137543917) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.822, 0.01676194030791521) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.0046499876221641895) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.19277456772327423) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.3138647742249304) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.925, 0.00859819111879915) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.523, 0.09518448279052973) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.06914431074634195) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.012742064774036407) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.005303497397573665) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.20278345477953552) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.32073185880761595) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.011043551849317736) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.481, 0.11405303306411951) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.585, 0.07016514492034912) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.0166374926045537) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.004740134323015809) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.2173713155379519) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3416651785986323) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.011428974017268046) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.12457468011975288) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.571, 0.06469953174889087) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.82, 0.016605754107236862) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005694103764835745) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.20471367272362112) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.30371276412950826) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.00901176364400817) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.09644338813424111) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.575, 0.06543749448284507) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO2', '(DO1']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.798, 0.01721136674284935) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005852812174707651) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.22929514249227942) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.33550133746155186) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.008500854037236423) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.497, 0.09979334761761129) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.654, 0.05445963767915964) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.82, 0.016908722534775735) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.006132614824571647) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.21076039696112275) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.28658451335655993) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.926, 0.012238563454739051) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.486, 0.10753923844452948) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.07383510035276412) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.016440776184201242) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006074986442690715) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.21606218293122947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.3138341967755114) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.014510847128811292) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.49, 0.12263370362296701) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.56, 0.08109193274378776) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.016393396079540254) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.004149726214818656) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.23307044417224823) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.3330171062709997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.915, 0.015022177036851644) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.514, 0.09582011517137289) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.579, 0.06595518735051155) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.014430922821164132) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.00487723944382742) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.24558457152172924) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3310681827080261) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.933, 0.016175015019252895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.09613354619592428) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.617, 0.06371975170820951) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO1', '(DO0']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.824, 0.015559712022542953) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.0075330015944782645) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.23911567895673216) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3231262169240508) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.013109816345615399) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.523, 0.09093587348610163) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.614, 0.05736757950577885) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.856, 0.014761104583740234) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006709193786256946) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.22029287193715572) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.2982577663846314) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.927, 0.013306160549633163) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.14330688516050577) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.548, 0.08660179260896984) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.013736829295754432) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.0058887842601398005) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.22082224096357822) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.30822097557462985) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.018709653509547935) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.16585690816678106) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.10505970133232767) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.01596590634435415) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.00488809445919469) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.23306926120817661) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.2889434558946523) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.926, 0.015033735715622356) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.13940430286899208) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.097505460010143) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.82, 0.01611949035525322) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.007312335447815712) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.24900254847854375) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.3146056656868895) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.010638789636613183) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.499, 0.1262849223650992) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.534, 0.0913280004542321) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO1', '(DO3']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.014739735811948776) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.0042882930829655375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.21819162118062377) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.30706117058062227) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.926, 0.018200553926670183) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.14268303490243853) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.08281493036262691) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.012769521683454514) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.006291485956578981) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.2664951599948108) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.3306866613429447) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.924, 0.020417166524215644) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.17522678338736294) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.549, 0.09463651506183669) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.82, 0.015384157150983811) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.006059555049054324) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.2468448590449989) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.343511315951313) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.021119580115240694) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.17000220785290002) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.10997899811808019) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.017439360111951827) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.0062415362010360695) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.2558841128498316) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.3011584130289266) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.01769515291296557) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.14519484841823577) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.10113935227692127) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.015489837348461152) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005341324667446315) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.2525498651973903) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.2923350859351922) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.915, 0.019896026156560765) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.16041306728124619) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.1279882591702044) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.242, 0.04443864643573761), (0.354, 0.07661269408464431), (0.275, 0.07945466989278793), (0.377, 0.09021134239435195), (0.41, 0.11596032911539078), (0.408, 0.13193382388353347), (0.398, 0.12846777415275573), (0.394, 0.12581581400334835), (0.378, 0.11846597637236118), (0.388, 0.12462650395929814), (0.39, 0.14789085798710586), (0.629, 0.03327320522069931), (0.63, 0.03518916761875152), (0.548, 0.06674703687429429), (0.562, 0.05617418409883976), (0.558, 0.06532805587351322), (0.562, 0.06009200153499842), (0.579, 0.06746088790893555), (0.54, 0.07127337756752967), (0.48, 0.10931113491207362), (0.516, 0.0942899264767766), (0.508, 0.09169202837906777), (0.544, 0.06696937327831984), (0.486, 0.11281656597927213), (0.509, 0.10425110609363765), (0.489, 0.12258040457870811), (0.499, 0.09764718233793974), (0.51, 0.10836697126179934), (0.516, 0.09444492107816041), (0.521, 0.10254619359970092), (0.504, 0.10024467813968659), (0.516, 0.10774801188893617), (0.523, 0.09518448279052973), (0.481, 0.11405303306411951), (0.488, 0.12457468011975288), (0.524, 0.09644338813424111), (0.497, 0.09979334761761129), (0.486, 0.10753923844452948), (0.49, 0.12263370362296701), (0.514, 0.09582011517137289), (0.536, 0.09613354619592428), (0.523, 0.09093587348610163), (0.469, 0.14330688516050577), (0.462, 0.16585690816678106), (0.474, 0.13940430286899208), (0.499, 0.1262849223650992), (0.463, 0.14268303490243853), (0.46, 0.17522678338736294), (0.472, 0.17000220785290002), (0.474, 0.14519484841823577), (0.469, 0.16041306728124619)]
TEST: 
[(0.2285, 0.04340292808413505), (0.35375, 0.07347347635030746), (0.27775, 0.07620753061771393), (0.3925, 0.0865502817928791), (0.4165, 0.11079292529821395), (0.403, 0.12526093649864198), (0.401, 0.12224474203586579), (0.3945, 0.11966084939241409), (0.38425, 0.1124050145149231), (0.38725, 0.11857743835449219), (0.3925, 0.13983969384431838), (0.64875, 0.03031096564233303), (0.638, 0.034241673797369004), (0.53475, 0.06377956767380237), (0.565, 0.05545113421976566), (0.55725, 0.06578577791154384), (0.5845, 0.05783853189647198), (0.578, 0.06558981977403164), (0.555, 0.06790088638663291), (0.49, 0.107906076669693), (0.52, 0.09338752511143684), (0.51225, 0.09107886719703674), (0.56475, 0.06660360115766525), (0.485, 0.1117519396841526), (0.511, 0.10407393404841422), (0.49725, 0.12227860188484192), (0.5005, 0.09611313065886498), (0.5075, 0.1082544722855091), (0.52225, 0.09370655715465545), (0.5205, 0.10133339244127273), (0.505, 0.09805602192878723), (0.499, 0.10797604098916054), (0.52875, 0.09638275581598282), (0.4825, 0.11416374462842942), (0.4915, 0.12589909866452217), (0.51575, 0.0951968216896057), (0.50575, 0.09554273471236228), (0.49325, 0.10754653277993202), (0.50075, 0.12113677483797074), (0.5225, 0.09270264732837677), (0.5355, 0.09191489192843437), (0.534, 0.08690248313546181), (0.473, 0.14026709628105163), (0.46275, 0.1624335040450096), (0.47925, 0.1387524910569191), (0.49375, 0.12560338300466536), (0.46225, 0.13917860507965088), (0.45925, 0.17489789187908172), (0.469, 0.16792176067829132), (0.47575, 0.14401750081777573), (0.46725, 0.15310351103544234)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.45      0.90      0.60      1000
           1       0.48      0.93      0.63      1000
           3       0.86      0.01      0.02      1000
           6       0.96      0.02      0.04      1000

    accuracy                           0.47      4000
   macro avg       0.69      0.47      0.32      4000
weighted avg       0.69      0.47      0.32      4000

Collaboration_DC_1
VAL: 
[(0.218, 0.044824156880378725), (0.25, 0.07654814468324185), (0.252, 0.08400962515175342), (0.381, 0.09025510634481906), (0.412, 0.11271193671226501), (0.434, 0.12182052563875914), (0.467, 0.140342478916049), (0.461, 0.14166802729293704), (0.474, 0.15023153780028223), (0.469, 0.17387545125558973), (0.466, 0.19303536528348922), (0.579, 0.04259853515028954), (0.684, 0.02760743622481823), (0.567, 0.055440423756837845), (0.604, 0.04554247554577887), (0.588, 0.06395933274552226), (0.624, 0.050148758105933666), (0.573, 0.06771461363136769), (0.594, 0.057834875792264935), (0.566, 0.07177280266582965), (0.565, 0.06351472976803779), (0.544, 0.0653500208593905), (0.615, 0.05633704297989607), (0.504, 0.09121366058290005), (0.561, 0.07179991602431983), (0.519, 0.09457737705856561), (0.616, 0.05469020216166973), (0.585, 0.06113107100874186), (0.561, 0.07613846186269074), (0.626, 0.05411215067654848), (0.566, 0.07191571754217148), (0.509, 0.09973231137543917), (0.589, 0.06914431074634195), (0.585, 0.07016514492034912), (0.571, 0.06469953174889087), (0.575, 0.06543749448284507), (0.654, 0.05445963767915964), (0.556, 0.07383510035276412), (0.56, 0.08109193274378776), (0.579, 0.06595518735051155), (0.617, 0.06371975170820951), (0.614, 0.05736757950577885), (0.548, 0.08660179260896984), (0.527, 0.10505970133232767), (0.525, 0.097505460010143), (0.534, 0.0913280004542321), (0.557, 0.08281493036262691), (0.549, 0.09463651506183669), (0.536, 0.10997899811808019), (0.524, 0.10113935227692127), (0.515, 0.1279882591702044)]
TEST: 
[(0.22, 0.043847873479127886), (0.25, 0.07381470426917076), (0.2505, 0.08048938992619514), (0.386, 0.08601839923858642), (0.4165, 0.10769178631901741), (0.44125, 0.11695740818977356), (0.4685, 0.13551646131277084), (0.4675, 0.13753553491830825), (0.4745, 0.14645577955245973), (0.47025, 0.1688867705464363), (0.47025, 0.18691782051324846), (0.5665, 0.04241857168078422), (0.6605, 0.02911391546577215), (0.5465, 0.057815764620900156), (0.591, 0.04545810204744339), (0.56675, 0.06682359725236893), (0.6125, 0.051442310124635696), (0.5585, 0.07185533875226975), (0.572, 0.05969066181778908), (0.54275, 0.07595896643400192), (0.56675, 0.06710518617928028), (0.5325, 0.06854208347201347), (0.5905, 0.060298381969332696), (0.5015, 0.09139396992325782), (0.54825, 0.07415666517615319), (0.50625, 0.09494493088126182), (0.5935, 0.0564899435043335), (0.56925, 0.06292915135622025), (0.5575, 0.07766016972064972), (0.618, 0.056871152356266975), (0.54875, 0.07333857220411301), (0.501, 0.0998904751241207), (0.576, 0.06941240325570107), (0.56175, 0.07284756276011467), (0.56925, 0.06648701903223991), (0.57675, 0.06722711381316185), (0.6245, 0.05739014463126659), (0.55275, 0.0743001247793436), (0.551, 0.08067949371039868), (0.559, 0.06779898568987847), (0.59925, 0.06483204647898674), (0.60575, 0.05952722728252411), (0.537, 0.08868079727888108), (0.52875, 0.10526618227362633), (0.52425, 0.09939516258239746), (0.53125, 0.092851716786623), (0.546, 0.08425242277979851), (0.53425, 0.10258776700496673), (0.50825, 0.11573158276081086), (0.50825, 0.10561345091462135), (0.5135, 0.13503946736454964)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.52      0.88      0.65      1000
           1       0.46      0.92      0.61      1000
           4       0.91      0.17      0.28      1000
           9       0.71      0.08      0.15      1000

    accuracy                           0.51      4000
   macro avg       0.65      0.51      0.43      4000
weighted avg       0.65      0.51      0.43      4000

Collaboration_DC_2
VAL: 
[(0.243, 0.04442102932929993), (0.25, 0.09640297073125839), (0.278, 0.12523775127530098), (0.396, 0.15705580693483354), (0.42, 0.19727019335329532), (0.426, 0.20265718460083007), (0.421, 0.21342508929222823), (0.43, 0.2270521096549928), (0.427, 0.24203452307730913), (0.435, 0.2579819469116628), (0.429, 0.267256860435009), (0.419, 0.22960870119184257), (0.419, 0.2366888452824205), (0.424, 0.2237983683794737), (0.436, 0.21164736951515079), (0.433, 0.21581539477035402), (0.44, 0.19614045030996202), (0.438, 0.19363816899061204), (0.438, 0.2129457389935851), (0.441, 0.1735377303622663), (0.435, 0.16479179935157298), (0.435, 0.19898282311484217), (0.44, 0.17866512263193726), (0.435, 0.18542744836956263), (0.445, 0.1639763366803527), (0.447, 0.19515224239975215), (0.445, 0.17674107662402094), (0.441, 0.1855120692625642), (0.442, 0.19329150774143636), (0.44, 0.22479695722460746), (0.435, 0.19618702696822585), (0.443, 0.19638450679183006), (0.445, 0.19277456772327423), (0.443, 0.20278345477953552), (0.438, 0.2173713155379519), (0.438, 0.20471367272362112), (0.444, 0.22929514249227942), (0.438, 0.21076039696112275), (0.436, 0.21606218293122947), (0.432, 0.23307044417224823), (0.436, 0.24558457152172924), (0.44, 0.23911567895673216), (0.444, 0.22029287193715572), (0.442, 0.22082224096357822), (0.443, 0.23306926120817661), (0.446, 0.24900254847854375), (0.443, 0.21819162118062377), (0.443, 0.2664951599948108), (0.442, 0.2468448590449989), (0.442, 0.2558841128498316), (0.442, 0.2525498651973903)]
TEST: 
[(0.246, 0.04339280295372009), (0.25, 0.09244508910179138), (0.2815, 0.12015084260702133), (0.38875, 0.15123850136995315), (0.42325, 0.18991599303483964), (0.4235, 0.19383455294370652), (0.42825, 0.20367060768604278), (0.42975, 0.21929274678230284), (0.4355, 0.23072633934020997), (0.44425, 0.245837830722332), (0.442, 0.25503220057487486), (0.43325, 0.2195211583971977), (0.42175, 0.22802734446525574), (0.437, 0.2130603255033493), (0.43625, 0.2020497642159462), (0.43925, 0.20408692049980165), (0.44375, 0.18647344428300858), (0.447, 0.18563854426145554), (0.448, 0.20358427768945694), (0.44275, 0.16694441848993302), (0.446, 0.1575377367734909), (0.44225, 0.19255419713258742), (0.448, 0.1706940644979477), (0.44175, 0.1756548961997032), (0.4435, 0.1571242322921753), (0.4455, 0.18511888635158538), (0.447, 0.16845464271306992), (0.44575, 0.17734523057937623), (0.4445, 0.18399815142154693), (0.4395, 0.21425488007068633), (0.43575, 0.18539628660678864), (0.4455, 0.18318774700164794), (0.44725, 0.18172043234109878), (0.4425, 0.19029256027936936), (0.44275, 0.2056498323082924), (0.44225, 0.19499299275875093), (0.446, 0.219846724152565), (0.44425, 0.2018614250421524), (0.445, 0.20499279034137727), (0.4405, 0.22108235162496567), (0.44725, 0.23447264736890794), (0.4445, 0.22678864061832427), (0.44825, 0.20752809327840804), (0.44525, 0.20930486565828324), (0.444, 0.22130849307775496), (0.4465, 0.23847599864006042), (0.4425, 0.2082044351696968), (0.4445, 0.25573424363136293), (0.44775, 0.23209385579824449), (0.4475, 0.24285209524631501), (0.448, 0.24255261510610582)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.00      0.00      0.00      1000
           2       0.33      0.95      0.49      1000
           7       0.74      0.84      0.79      1000

    accuracy                           0.45      4000
   macro avg       0.27      0.45      0.32      4000
weighted avg       0.27      0.45      0.32      4000

Collaboration_DC_3
VAL: 
[(0.244, 0.044828352451324466), (0.392, 0.07325242793560029), (0.46, 0.09349438312649727), (0.474, 0.14240596529841423), (0.469, 0.16480454625189303), (0.469, 0.21888766702078283), (0.47, 0.24219384985789658), (0.477, 0.27305388302356004), (0.471, 0.26939495775196703), (0.477, 0.29477349990280344), (0.477, 0.29640268966881556), (0.479, 0.3371907848324627), (0.478, 0.295455738930963), (0.477, 0.3324814289479982), (0.476, 0.32773079082055484), (0.478, 0.34862902016210134), (0.479, 0.3430978617221699), (0.473, 0.33815361770795427), (0.472, 0.32490580514399336), (0.476, 0.3265296587602934), (0.475, 0.34018476697948064), (0.477, 0.3253972700007434), (0.477, 0.31501067836116997), (0.479, 0.3357834734985663), (0.479, 0.3033173082342837), (0.474, 0.38166177578693894), (0.477, 0.31922820964769927), (0.477, 0.35193505106111117), (0.481, 0.32070267097544275), (0.478, 0.3590226922050933), (0.479, 0.3810500155718182), (0.477, 0.3238292219626019), (0.478, 0.3138647742249304), (0.473, 0.32073185880761595), (0.471, 0.3416651785986323), (0.475, 0.30371276412950826), (0.473, 0.33550133746155186), (0.478, 0.28658451335655993), (0.478, 0.3138341967755114), (0.479, 0.3330171062709997), (0.477, 0.3310681827080261), (0.475, 0.3231262169240508), (0.473, 0.2982577663846314), (0.474, 0.30822097557462985), (0.476, 0.2889434558946523), (0.473, 0.3146056656868895), (0.476, 0.30706117058062227), (0.475, 0.3306866613429447), (0.475, 0.343511315951313), (0.477, 0.3011584130289266), (0.478, 0.2923350859351922)]
TEST: 
[(0.2495, 0.044014592796564105), (0.40075, 0.07023401990532875), (0.46475, 0.08952063956856728), (0.47275, 0.135820614695549), (0.476, 0.15784210336208343), (0.47525, 0.20845930409431457), (0.47475, 0.2282492750287056), (0.48075, 0.2596552656888962), (0.47775, 0.25685111624002455), (0.48, 0.28122049593925474), (0.47975, 0.28348508703708647), (0.47875, 0.3236257050037384), (0.4795, 0.28294439375400543), (0.4785, 0.31829737401008606), (0.4805, 0.31246652019023896), (0.4805, 0.33063957691192625), (0.47675, 0.3246897542476654), (0.477, 0.3202412536144257), (0.47875, 0.3078643081188202), (0.47825, 0.31005058872699737), (0.48025, 0.3249789937734604), (0.479, 0.3108242050409317), (0.47825, 0.29954965579509735), (0.48025, 0.32010967314243316), (0.4805, 0.2876306929588318), (0.479, 0.3633961395025253), (0.482, 0.30225689339637757), (0.48, 0.336012527346611), (0.48075, 0.30668666696548463), (0.4805, 0.33847610676288603), (0.4805, 0.3625691252946854), (0.48175, 0.30745348846912385), (0.481, 0.2946911664009094), (0.48075, 0.3026592917442322), (0.47675, 0.32469818711280823), (0.4775, 0.29187430250644686), (0.47825, 0.31862812435626986), (0.47725, 0.27312228387594223), (0.4795, 0.2978971192836761), (0.4805, 0.3150517404079437), (0.47975, 0.31728574681282046), (0.47725, 0.30785964703559876), (0.47675, 0.2830634207725525), (0.4795, 0.29549392986297607), (0.47775, 0.2770689208507538), (0.4795, 0.30114511597156524), (0.47925, 0.2931687468290329), (0.4785, 0.31765022706985474), (0.47775, 0.3298540630340576), (0.47875, 0.2911114526987076), (0.4815, 0.28246919310092927)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.00      0.00      0.00      1000
           5       0.72      0.95      0.82      1000
           8       0.36      0.97      0.53      1000

    accuracy                           0.48      4000
   macro avg       0.27      0.48      0.34      4000
weighted avg       0.27      0.48      0.34      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [31]
name: alliance-2-dcs-31
score_metric: contrloss
aggregation: <function fed_avg at 0x760c4d7a9c10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=31
Partitioning data
[[2, 8, 0, 7], [5, 3, 0, 7], [1, 9, 0, 7], [4, 6, 0, 7]]
[(array([32045, 13820, 18987, 26795, 19845,  5574,  2772, 11006, 30325,
       24699, 30286, 16712, 47074, 45079,  7576, 28872, 34555, 22733,
        8186,  4375,  9827, 25022, 17379, 46652,  2007, 28176, 28479,
       39508, 20226, 26282, 39765, 49710, 20651,  2387, 25296,  8627,
       32706, 31375, 19963, 21987, 15380,   300, 48344, 44660,  3019,
       29517,  2133, 48242, 46589,  4183,  1122, 31707, 36186, 48270,
       13655, 43715, 20563,  4806,  7868, 46214, 29351, 48835, 20038,
        1523, 19163, 49756, 36242, 31293, 40832, 24734, 48314,  1129,
       48472, 46244, 15888, 34203, 17817, 19231,  6805, 46633,  5531,
       47632, 43795,  6614, 43469, 48739,    42, 42854, 34747,  4209,
       44351, 11050, 27399,  7962, 35686, 37855, 33183, 43541, 33137,
       27169,  5836, 44186, 12720, 46810, 14085, 44938, 46759, 17329,
       30751, 16530, 22732, 35958, 15754, 41604, 43106, 19364,  9498,
       33134,  9255, 20850, 40720, 34580, 11500, 23775, 32730, 43959,
       27980, 13698, 40281, 17981, 39466, 35740, 40791, 23650, 32981,
       27592, 15054,   271, 21034, 31501,  1789, 27877, 37754, 43930,
       20323, 26648, 27091, 22275, 48549, 27368, 25130, 46870, 43287,
        7662, 39145, 21162, 48300, 49972, 42856,  2938,  9579, 28568,
       25998, 40112, 45359, 34363, 47411,  3796, 18733, 48989, 14423,
       16165, 23653, 22570,  9089,  7761, 12752, 36245, 40917, 21057,
       24555, 15944,  6976,  9553,  1372, 32332, 49274, 19296, 20212,
       31614, 42988, 28164, 18489, 17481, 20308, 22376, 47593, 23074,
        6445, 23038, 43658, 35901, 35787, 23705,  4189, 34050, 33550,
       30003, 15820, 29076, 12070,  5554, 29682, 34959, 30596, 13413,
       22877, 29678, 34222, 13177, 28291, 27744, 18855, 49395,  9403,
       14887, 16786,  1576,   924, 29925,  2224, 34663, 18248, 34266,
        3769, 45006, 26730, 48095,  3446,  8395, 12694, 46972,  2833,
       36469, 23237, 15477,  1438,  7807,   303,  1838, 12258, 12614,
       16697, 33195, 24083, 34342,  7716,  2735, 10613, 13641, 39658,
        5687, 29782, 40255, 28424, 32034, 10763,  4863,   716,  7973,
       43755, 23583, 15011, 47993, 26563, 31950, 28154, 48911, 32138,
       30076, 23839, 10663, 48168,  1060, 39011,  9844, 19910,  5272,
       48698,  2344,  9226, 34655, 35266, 34718,  8961, 13674, 20612,
       45045, 44577, 48604, 22426, 15743, 30788, 36670, 19788,  1957,
       45446, 36571, 41413, 33322,  8852, 14969, 46138, 44594, 47713,
       45017, 20500, 18274, 29056, 36484, 21919,  2797, 45372, 45161,
       35078, 14971, 43916, 15648, 24142, 49379, 20898, 24140, 26981,
       27919, 10785, 26703, 18177, 12206, 15721, 25472, 15768, 31258,
       35617,  7668, 49042, 15617, 26337, 42506, 15927, 32618, 48959,
       42236, 18055, 17063,  7109, 11095,    69,  7248, 19622,  9623,
       21880, 25088, 46210,   259, 16479, 48457, 25118, 39106, 34636,
       49696, 42799, 37858,  4287, 28269, 48541, 37681, 44059, 22504,
        3268, 29747, 11368, 40070, 14183, 14402, 23793,  1720, 31787,
       46582, 24260,  3800,  2687, 13116, 37023, 16476, 27896,  9682,
       41002, 47652, 46926, 35489, 19149,  1229,  6646,  8495, 36939,
       14208, 18236, 21608, 48107,  3269,  6346, 11265,  2333, 12605,
       26055, 21840, 33109, 24748, 31182, 18036, 29868, 42316, 39579,
       25513, 48774, 27057, 37420, 30504, 40093, 18565, 40122, 20596,
        9514, 22551,  3670, 37411,  6435, 48670, 36933,  1397, 13266,
       13425,  6823, 40344,  3881, 37398, 45025, 18718,  3359,  2740,
       34319, 28450, 17729, 35236,  2368, 28396, 32663, 17345,  1439,
       17416, 42822,   723, 12935,   901,  4979, 18675, 27864, 17588,
       42247, 12731,  8787, 38332, 27337, 35177, 10346, 32562, 30872,
       29530, 43334, 38322, 14728, 26038, 30147, 41714, 27839, 33815,
       43381, 49654, 34208, 48752, 42245, 28940, 16156,  4303, 44981,
       48481, 19415, 11832, 26672,   566, 40241, 36771, 14708, 44961,
        1878, 20267, 46486, 32905, 23010,  8941, 34250, 49869, 13707,
       18925,  3184,  8102, 46586, 39959, 19974, 24764, 22769, 14456,
        1454, 24792, 35059,  7647, 46224, 47242, 40159, 49535, 39288,
       12009, 43944,  7464, 35354, 39709, 41222, 32483, 10493, 13016,
       45827,  8759,  1243, 46041, 25352, 15628, 18402, 41741,  3128,
       23182,  1188, 31087,  4721, 41842,  4141,  9800, 45364,  5371,
         757, 10548, 38586, 34856, 20831, 39793, 33767, 16004, 16920,
       34259, 42044,  6015, 28653, 34601, 24971, 29412, 46749, 13578,
       46277, 39553, 36337, 13549, 25144, 39780, 34998, 33923, 25816,
       14698, 29944, 22622,  1205, 16313, 31358, 10741, 17103,  9860,
       36501, 46116, 19395, 35281, 24285, 47486, 29140, 36289, 10872,
       17387, 41040, 18538, 36981, 49891, 35731, 48850, 29666, 23371,
       37322, 38479,  2446, 35738, 32346,  4571, 34780, 43292, 15580,
       17080, 49513, 33243, 41030, 42364, 27426,   694, 11240, 44875,
       24589,  4079, 43211, 48246, 47444, 35858, 40022, 10812, 33363,
       38342, 38635, 30205, 23507, 43831,  9390, 22249, 32365, 27855,
       24859, 10365, 19004, 47771, 27431, 33700, 42151,  9292, 42808,
       42797, 17438, 46689, 33229, 23175,  2169,  3073, 10690, 16497,
       22293,  4577, 12010, 24661, 48968,  4097, 14407, 33087, 34454,
       16169, 31249, 37664, 43992, 25930, 47799, 30280, 47274, 30429,
       24383, 46200, 24600, 42621, 42272, 17026, 15044, 35700, 24404,
       33943, 10401, 26412, 30408, 43054, 26913, 36148, 36634, 10713,
       30415, 41792,  1168,   752,  8623, 35079, 42883,  5828, 40906,
       35411, 46839,  9096, 26286,  5969, 29293, 28400, 34607,   348,
        8242, 11849, 36789, 47215, 38098, 41266, 42945, 17458, 11691,
       43301,  4621, 31976, 19639, 26178,  4778, 26007,  8200, 27104,
       19982, 13011, 25212, 38716, 18080, 10516,  6947, 25023,  8072,
       34026, 16840, 12130, 18973, 35316, 15120, 38286, 45034, 39231,
       48041, 31851, 41462, 21410, 25917, 39767, 37082, 19250, 28566,
        3606, 31965, 45365,  5200, 12535, 43717, 12980, 39164, 22599,
       27505, 32700, 13333, 27327, 43272,  5058, 33902, 41959, 22541,
        3844, 13146, 32586, 38803,  8115,  2360, 20062, 41704, 19300,
       39944, 37724, 42330, 44374, 37286, 32282, 29461, 38104, 25411,
       19074, 17913, 40055, 33299, 38938,  9279, 23699, 16224, 13039,
       18227, 43227, 24794, 44406, 36619, 36773, 22554, 45409,  3920,
       18723, 31980, 22725, 22324, 48596,  8966, 41660,  2611, 45643,
        3284,  2834, 34487, 15079, 12838,  7945, 32704, 11611, 28117,
       48720, 27998,  8498,  4747, 42299,  1931, 19636,  9456, 37980,
       41449, 49451, 23794,  7291, 28249, 12658, 19010,  8640, 31871,
        9079, 28296,  6839, 24612, 46711, 37763, 42052, 37802,  8694,
       33884,  5996, 35164, 24406,  2612, 32995, 49772, 12220,  1679,
       40374,  6998,  4238, 42126, 32032, 30498, 24499,  8901,  2502,
       34113, 34239, 11621, 14671, 22592, 42381, 32948, 29588, 28911,
       38110, 33343, 33382, 21317,  7624,   589,  4900, 22723, 12529,
       13284,  8595,  8909, 43445, 34380, 13336, 17587, 27039,   739,
       26389, 10672, 44958, 16565, 26037, 20309,  5044,  9422, 19037,
       45352,  5989, 42137, 38611,  3611,  7611,  1650,  7185, 29245,
       23643, 16091,  4820, 29789, 35767, 33843, 16000, 19121, 11741,
       12842, 49341, 14630, 15167, 27574, 22204,  1561, 28914, 19691,
       11323, 32646, 18481,  7939,   842,  8461,  4499, 42555, 38249,
        2458, 39314, 35982, 30401,  8604, 12566, 29823,  7641,  9958,
       28710, 12415, 15332, 22359, 34708,  4386, 44413, 26661, 14955,
       13818, 44262,  2397, 33648, 35120, 21843, 20356, 17775, 39835,
       36017, 31873,  2201, 33074,  8924, 26820,  6372, 23585, 43844,
       33527, 35988, 46098,  2776, 28987, 18326, 17489, 11377, 13525,
       48420]), [2, 8, 0, 7]), (array([16110, 17711, 49675, 41400,  8457, 27977,  8989,  1634, 18317,
       36308, 37436,  8497, 19007, 22355, 20644, 42846, 23335, 17305,
        8824, 46381,   515, 48083, 28438, 45583, 28475,  8204, 31877,
        1346, 31129,  3060, 15133,  9677, 34859, 29683, 39845, 44583,
       45326, 41703, 38199, 27903, 45134, 21436, 38313, 30072, 30359,
       23689,  9337, 25390, 48220, 22978,  8795, 42038, 27397, 30482,
       31273, 36150, 49867, 18732, 25141, 46853, 19042, 38854, 48630,
       27292, 35452, 45184,  9301, 47625, 24160, 38471,  6222,  3040,
       42731, 12687, 20031, 48882, 47192,  3746, 40800, 27956, 31376,
       12931, 41221, 47662, 38302, 33304, 35821, 41942, 30519, 33629,
       23733, 16079, 38632, 10315,  8274,  5840, 40421,  5981, 40717,
       45973, 46911,  1661, 16351, 16534, 36335, 44929,  3767,  1847,
       18966, 35495, 19825, 11463, 49531,  5690,  1580, 23233, 33720,
       15006, 14931, 48495, 19293, 43345, 29304, 11551,  7435, 26914,
       45882,  9274, 16444,  1976, 27590, 17152, 17065, 47480,  1072,
       31064, 40351, 14530, 27771,  8768, 25205, 22099, 45501,  8805,
       45153, 26581, 49988, 32255,  3206, 22079, 47223, 10319, 17102,
       13829, 20361, 40272, 30042, 11819, 18516,  9264, 19319, 27711,
       35802, 37484, 18945, 32924, 17721, 29836, 33800, 43878, 19362,
       30395,  6455, 36617,  5305, 48139, 28659, 21678, 46997, 46918,
       17882,  1983, 41559, 33161, 28999, 25886,  2906, 45271, 40997,
       26751, 14627, 42194, 28047,  4728, 37151, 25894,  4990,  2384,
       14734,  3604, 38343, 34391, 24714,  1134, 28756, 30502, 33295,
        9622, 46811, 27345, 48750, 21379, 31347, 28982, 18708, 38564,
        5205, 18459, 24006, 29896, 25157, 19487, 23285, 37708, 36962,
       36217, 45629, 35604, 14465, 36769, 23039, 19663,  7364, 17795,
       28070, 40871, 19177, 34117, 13610,  6017, 15775, 22679, 19271,
       47629, 16166, 46112, 47768, 47913, 13111,  5905, 17785, 13463,
       35026, 40712, 38931, 42501, 31260, 49430,  4128, 20345, 33113,
       20965, 23168, 40249, 13941, 37297, 27235, 47722, 48524,  9421,
       48933, 40314, 22463, 43462, 38679, 45542, 43456, 47933, 40129,
       37408, 39614, 42206, 35154, 37756, 16301,   992, 20806, 18393,
        4697, 21921, 37809, 26711, 36140, 33267, 32509,  6151, 37223,
       48376, 11476, 35581, 41144,  1546, 22042, 11294, 33355, 49609,
        8386, 30318, 25745, 10960, 25447, 37038,  1499, 16120,  5147,
       23700, 19234, 38066, 39438, 39294,  8790, 41497, 41182, 14380,
       31150,  3749, 25273,  4329, 23884,  8420, 49227, 23270, 46542,
        6905,  9185, 49835,   922, 20153,  7122, 44635, 49545, 33944,
       37215, 39175, 25997, 45931, 44354, 15778, 35935,   377, 26472,
       43971, 10418, 29038,  1281,  3646, 20685, 41154, 44543, 47100,
       12952, 10427, 33101, 32219, 38982, 29692, 25103, 13064, 16449,
       21510, 15345, 25378,  1345, 40922, 37135, 45251, 32649, 17919,
       10993, 40365, 23761, 38432,  5088, 29923, 49713, 43779, 20625,
       37429, 24961, 45319, 37300, 13928,  8319, 34742,  2785,  6422,
       40668, 25706, 26323, 40007, 21373, 18089, 44550, 12601,  6167,
       35770, 46402, 46349, 31861, 47866, 33463,  6637, 46799,  9343,
       47840,   494, 39681, 28839, 18412, 45236,  9904, 25062, 27413,
        5354,  3758, 33607, 33211, 15181, 29213,  8381, 49970,    80,
        8164, 39979, 15996, 11544, 19449, 17058, 30479,  8864, 36970,
       37417, 20917, 33346, 42647, 10341, 48019, 32938,  3617,  3813,
       14422, 17036,  4982, 34658, 33393,  8876, 47204, 33238,  8916,
       46337, 37733,  2414, 20968, 43827, 15321, 19327, 33779, 49054,
       36772, 19093, 43318, 22759, 47475,  8151, 28592, 46812, 29397,
       11291, 25639,  6071, 13082,  5800, 42417, 26064, 24538, 37904,
       47823, 24839,  4657, 49325, 43313, 15279, 30777, 10501, 41012,
       34160, 46727, 34901, 28880, 14703, 23061, 11169, 11879, 31556,
       32016, 19094,  1306, 31481, 13594, 21874, 26376, 41971, 43451,
        9385, 16559, 32318,   448, 39027, 47969,  4787, 49671, 15542,
       34938, 43989, 18215, 28742, 23924, 38828, 22924, 26137, 18047,
       43252, 45102, 17232, 16002, 22408, 11238, 12757, 31015, 48327,
        8392, 21469, 28300, 23999, 19730, 13473,  8292,  5314, 11975,
       10425, 18657,  3721,  2855, 14079, 15102, 13084, 47525, 13522,
        1626, 22557, 10553, 18765, 45156, 44618, 28178,  7179, 47685,
       26493,  4202, 30517, 39196, 45919,   401, 32217, 36195,  2613,
       32033, 35594, 25905, 25004, 44484,  6682, 35608, 13289,  1340,
       40194, 16015, 31635,  5266, 26592, 29468, 44275, 37869,  7592,
       32662, 36763, 17328, 38900, 17140, 24931, 12345, 33130, 30272,
       34831, 32839, 21243, 39301, 11424, 23746, 37619, 33347, 42148,
       30160, 45096, 24887, 21272,  7328, 11002, 45064, 32062, 33558,
       41960,  7970, 25763, 24447, 35434,  9121, 11682, 25814, 12420,
        2804, 27686,  1477, 34948,  1980, 13225, 24690,  6008, 16793,
        8889, 32316, 27931,  7655, 37083,   284, 10010, 23462, 19860,
       11014,  1144, 33451, 45564,  2885,  9687, 26040, 32206, 43245,
       21105, 48355, 20817, 24212, 16253, 45845, 25160,  8190, 18381,
       31970, 34018, 32076, 25422, 28080, 27301, 21467, 36322, 40211,
        1270,  9819, 24676, 27522,  4523, 23113, 25316, 20709,  9064,
        1473, 25167, 12469, 44122, 47048, 10689, 37432, 31904, 49164,
       35403, 28223, 35952, 35546, 22326,  2773,  3466, 40334, 23667,
       42205, 20924, 44968, 25546, 11400, 17056, 11263, 30400, 28794,
       33732,  8778, 13668, 35761, 48317, 22991,  7196,  3180, 22609,
        5614, 17969, 15008, 37165, 37103, 42166, 28428, 47332, 15309,
       30383, 24695, 16553, 11919,  4338, 44785, 11237, 13629, 33011,
       14712, 19804,   783, 23863, 32267,  8963, 14460, 45387,  2617,
        8147, 40401,  3618, 37556, 44815, 10278, 43748, 21589, 35008,
        7598, 19832, 46117, 42964, 42184, 34528, 37810, 34717, 31073,
       19349,  3573, 10879, 47189, 21378, 40977, 26358, 32000, 30419,
        5563, 20179, 27423, 31982, 32152, 30617, 43246, 42951, 12770,
       24129, 44789, 11414, 44921, 23842, 37919, 47372,  9294,  2783,
       46034, 40563, 47856, 27320, 12848, 40306, 31468, 26259, 28152,
        3598, 20757, 35987,  3209, 40141, 27672, 17627, 26104, 33763,
       24091, 12099, 15644,  3627, 36248, 29208,  9044, 32888, 12502,
       24439, 23909, 23993,  9186, 39180,  4088, 39518, 36932, 44115,
       11527, 47110, 22846, 38425,    84, 36680, 25741, 25080,  6441,
       17463, 16881,  9823, 18712, 45054, 36535, 39717, 29671, 17472,
       24768,  5887, 45172, 13900, 41985,  7833, 16764,   926, 14233,
       26910, 44249, 38062, 20417, 41898, 23381, 37660,  1445, 18504,
       11870, 22198, 38798, 35890, 26826, 26733, 10932,  7686, 30993,
       20369, 31529, 40238, 41795, 36637, 35962, 15099, 30803, 37458,
       27182, 37430, 48927, 49456, 47724, 47996, 33981, 32633, 47308,
       33450, 38667,  6239,  2746, 32546, 17575, 31513, 24613, 17467,
         492, 49203,  4119, 38435, 43247, 44762, 38505, 23676,  3543,
        2396, 32287, 36776,  7304, 29619, 47748, 26673, 49794, 43320,
       14057, 37655, 32097,  2135, 29618,  6992, 43536, 46946, 36346,
       16114, 47270,   133, 10358, 36936, 41791, 18443, 17521, 10858,
        2801, 22128, 23402, 16144, 48829, 43128, 19802, 47696,  8142,
       13638,  1835,  1054, 37273, 30556,  6552, 45724, 10816, 24052,
       37010, 38713,   256, 12722, 48466, 13494, 23034, 49814,  5121,
       21773,  2151, 44559, 15514, 32329,  5854, 31364, 41039, 44025,
       21120,  4357,   652, 29533, 24930, 13632, 47522,  9581, 47286,
        6748, 24508, 17037, 28398, 21430, 28392, 40465, 29359, 36730,
       12439, 14745, 42887,  5809, 49139, 40360, 16031, 29998, 33618,
       11334]), [5, 3, 0, 7]), (array([31193, 38816, 44841, 42424, 19783, 23139, 26637, 45118, 38211,
        3620, 46233,  9322,  8305, 11130,  9372, 16012, 15528, 20742,
         978, 22270,  8973, 35106, 14129, 34832, 33970,  9849, 47642,
       38522, 35004, 41404, 12164, 43711, 45157, 44575,  5822, 19351,
       35454, 14764, 39192, 26550, 49875, 40076, 47365, 15704, 33198,
       43098, 34300,  6899, 29283,  7401, 37937, 47113, 37689,  5149,
        4050, 37494, 11804, 46259, 38047, 12551, 25874, 35587, 11336,
       36379, 48917, 18345,  8107, 11645, 11850,  7358, 46127,  6779,
        2389,  5247,  5102, 47264,  8576, 49572, 31345, 26115, 49733,
       12100, 27037,  2101,   160, 16989, 28054, 17334, 17666, 31665,
       16598,  2844, 31100, 40204, 28640, 30553, 21088, 27794, 16682,
        6420, 35605, 47618, 15646,  9863, 15583,  7600, 49919, 16958,
       16256, 10495,   176, 19574, 19865, 25910,  3231, 33599, 36237,
       47377,  2289, 30198, 43207, 15069,  7841,  7381, 41807, 46865,
       45296,  4261, 47616, 43684, 23275, 29117, 49847,  2023, 38177,
       41837, 30457, 20680, 37792, 23648, 46928, 30570, 41637, 22063,
       10479, 25731, 42163,    45, 41061, 35048, 32860, 31158, 49959,
       35068, 42352, 10544,  9392, 37401, 11619, 32135, 39638, 10292,
       48639, 21073,  4172, 17673, 42912, 45014, 18354, 24484, 21020,
       22298, 49021, 41896, 24110, 20391, 42188, 34385, 44671, 41733,
       38804, 11109, 43413, 44751, 33941, 16577, 31300,  6871,  1446,
       30006, 42916,  8932, 44415,  3922, 23607, 41844, 22303, 19399,
        3808, 26498,  8389, 36838, 42060, 33897, 22212, 22952, 24520,
       32319, 40207,  5722, 48375, 11037,  9610, 16172, 15783, 21871,
       28089, 22975, 33050, 49489, 33611, 23357, 34206, 38637, 45070,
       17586, 38078, 39566, 25405, 49759, 24590, 36208, 24561, 31352,
       42921, 11692, 44960, 38919,  8812, 45602,  1455,  3505, 35810,
        3483, 28981, 43668, 18551, 31072, 11543, 34890,   202, 22685,
       30566, 37295,  2845,  3228, 29438, 46236,  4865, 45547,  2935,
        5562, 39548, 45841, 15480, 11834, 13370, 41436, 46335, 18343,
       23019, 36717,  8532, 14436,  1034, 46755,  9955, 11688, 42407,
        8281, 22166, 20176, 21355, 49833, 42208, 37538, 24133,  5002,
       33746,  4407, 26447, 16143, 29142, 41173, 13148, 14387,  8103,
        4246, 12662,  3930, 47343, 49073, 33856, 27365, 34175, 11773,
       18805, 48034, 19114, 47154, 11227, 46278, 45943,  1241, 33766,
       45426, 32072, 35384,  3392, 23001, 25361, 47076, 38752,   511,
        9919,  8975, 44564, 32552, 23679, 14467, 32550, 43614, 13208,
       28359, 37443, 27570, 30360, 16378,  7556, 47896, 35618, 41427,
       38324, 20481, 18800, 14401, 37634, 43900, 16488,  4071, 30343,
       37678,   166,  8413, 23336, 46355, 31662, 15758,  7453,  5692,
        1114, 45860,   438, 34474, 40079,  3613,  6546, 36117, 21489,
       31387,  2991, 42106, 31986,  8428, 29519, 42362,  9701, 25839,
       18513, 43169,  7854, 19199, 36508, 38056, 36859, 23243, 37968,
       26888, 13577,  7167,  3983, 26229, 46298,  5203, 13381, 27811,
       35498,  6002, 49260, 24078, 19048, 30364, 36397,  5382, 49785,
       24627,  6937, 42733, 37132, 19534, 48996,  8840, 36413, 23287,
       17171, 19367, 16454,  1472, 45612, 29956,  5602, 45302, 23284,
        5974, 11918, 24477, 41605, 37070, 42727,  7479, 23975, 20720,
       40233,  2655, 47767, 32156, 40494, 12986,  2002, 47086, 23819,
       10178, 49278, 43176, 41560, 47641, 32612, 35233, 14197, 36878,
       15821, 36507, 19190, 15005, 35019, 46685, 49864, 26021, 17367,
        4041, 17228, 13343, 41813, 37031, 29179, 29167, 32952, 28351,
       27889,  7657, 33088, 39816, 31298, 32209, 33991, 31210, 15096,
       36461,  8673, 44157, 40157, 27622, 18692, 21470,  6036, 14475,
       29551,  8575, 44809, 24731, 15056, 29805, 16517, 29700, 17990,
        5668, 26061, 31186, 21999,  5300, 44178, 23822, 32107, 34989,
       46992,  5426, 36886, 19009, 35283, 32721, 21044, 18803, 43628,
       16541, 16918, 26656, 36491, 34270,  9314, 27278, 46367, 36993,
       25578, 32081, 24104, 48560, 32713,  2660,  9890, 20035, 49856,
       46876, 37949,  6663, 19273, 19216,  8882, 17865,  2429, 37519,
       39927, 19196,  5702, 27155, 34040, 47381,  5331, 40418, 14138,
       16103, 39121, 34123, 31023, 45556, 12817, 48476, 12095, 25624,
        1664, 42456, 11280, 21023, 46756, 42630, 34799, 17303, 38368,
        9281, 26000, 42784, 40485, 32903, 49556, 27315, 25339, 36632,
       40343, 48389, 17093,  7256,   605, 28548, 37747, 15483, 45166,
       31144,  8365,  7377, 15622, 36482, 47243,  5873, 15636, 46576,
       45666, 12740, 45304, 32232, 28828, 49263, 33179, 28917, 24319,
       20233, 15707, 29879, 44476, 15949, 24834,  2675, 29099, 21440,
        6954, 19173, 32369, 37328, 41220, 42399, 44067, 36909, 22350,
       10911, 48105,  8444, 43138, 42628, 14325, 19664, 37358, 40448,
       20824, 39480, 37334, 43368, 38003, 46046, 48174,  4726, 29596,
       44749, 40420,  5216, 21903, 29634, 29444, 19812, 12764, 47456,
       21217, 34676, 20242, 38796, 33905, 44970,  2322,  4935,  7015,
       36506,  4941, 37738, 20124, 19370, 24814, 12730, 13660,    77,
       10267,  9619, 11643, 23224, 41457, 35490, 35548, 48096,  4337,
       47865, 19682, 34715, 23254, 46922, 15502, 18630, 46934,  5323,
       32444, 19828, 26027, 13223,  8524, 35524, 36415, 15473,  6919,
       36183, 24851, 40607, 17948, 49359, 37566, 47227, 21288, 31213,
         373, 22716, 25532, 10782, 19210, 25862, 27286, 33000, 44721,
       43220,  9697, 47754,  4311, 45457, 16554, 32761,  7528, 27358,
       42332, 18905, 16450, 22684,  6396, 33780, 17233, 34009, 38633,
       43667,  1142, 26744, 13650, 13055, 13649, 27533,  4229,  5589,
       48979, 48363,  4869,  6711, 40479, 43993, 30093,  6440, 35690,
       24051, 46987, 27434,  3958,   847, 38780, 23344, 27915, 42238,
       48697, 43150, 17398, 38595, 25539, 42996, 48204, 18017,  3364,
       27421, 40684, 39168, 22380, 38220, 27187, 46783, 34046, 30805,
       49183, 13058,  5784,  7621, 23970, 30385, 23456,  6876,  7076,
       16839, 15296, 38067, 43435, 40061,  1742, 29281, 31034, 43848,
       11974, 14983,  7325, 31316, 45103,  4399, 24898,  8817,  3434,
       17782, 13026, 15433, 44135, 46167, 39181, 46875, 19973, 33254,
       16589, 35093,  6776,   952,  1698, 22179, 27230, 12097, 32640,
       26494, 19064, 25122,  7836,  7756, 42360, 13703,  5117,  7030,
       34067, 16109, 15712, 48265, 32239, 27086, 25809, 40502, 21508,
       12690, 23778, 39307, 30195, 44874, 47556, 22046,  2049,  7849,
       18577, 28719, 25711,  8580, 12683, 25948, 26285, 41239, 40570,
       23265, 20550, 33191, 24974, 41626, 22782, 32298, 42685, 16227,
       34446, 48759, 47046, 24758, 32898, 30296, 22219, 10891,  8312,
        4903, 26249, 42659, 17314, 17181, 35523, 11059, 34329, 11469,
        8996,  1215, 17581, 39494, 24941, 38767, 16025,  8360, 37293,
       38440, 11520, 21852, 47184, 45027, 17469,  9741,  6398, 11393,
       12163, 41416, 22222, 17979, 49728, 27078, 11502, 44922, 26697,
       36839,   972,  6696, 47673, 18339,  5934, 26628,  6864, 18411,
       35253, 47039,  3413, 30972, 18823, 28150,  3208, 10884, 16171,
       40106, 41781,  8202, 21891, 29621, 42390, 24786,  5991, 11950,
       14692, 24995, 20893, 44457, 18108, 37930, 38176, 39077, 46754,
       48695, 28240, 16504, 33082, 22669,  8744, 19346, 43130,  2319,
       36065,  3989,  7986, 25637, 29370,   478, 39276, 37837, 13950,
        8646,  2311,  3191, 29016, 36897, 30004, 34277, 23547, 42959,
        8369, 40198, 33945, 31847, 39425, 19777, 19467,  5401, 46301,
       36265, 36409, 48453,  3553, 41496, 34226, 31263, 25329,   181,
       30984, 38394, 27936, 30781, 39547, 30053, 43255, 35133, 23486,
       32568]), [1, 9, 0, 7]), (array([15790, 19345,  2235, 18272, 28885,   247,  2385, 49024,  9395,
        9845,  9528,  7634, 37691, 31393, 49598, 47922, 32368,  7536,
       35429, 33666, 41294,  1923, 15533, 10861, 35526, 40375, 12108,
       41659, 16085, 21171, 22820, 30499, 32525, 41202,  4224, 44629,
       36630, 31124,  1728, 16968, 16426, 14957, 12094, 45255, 36105,
       44525, 18566, 25692, 28313, 42553, 28271, 45315, 28620, 25474,
       39740, 17498,  6652,  3981, 35978,  7110, 37954, 24883,   420,
       42310, 22749, 27199, 35395, 42795, 14429, 10697, 25371,  5042,
       19784, 49187,  4695, 21346,  3322,  2442, 23604, 17323,  4905,
       19168, 30958, 27205, 23071, 41836, 21693, 43032, 37903, 27801,
       47293, 49138, 17145,   632, 17552, 27757,  3556, 31585, 43620,
        3612, 40550, 18065, 43575, 31998, 37053,  5075, 30291, 19406,
       15773,  4158, 27276, 14284, 35450,  1644, 27923, 32224, 42656,
       29835, 41216, 40071,  4032, 37214, 38375, 27812, 30885, 13038,
       45571, 28577, 28649, 36896, 41737, 39265,  5756, 30289, 32643,
       11872, 34770, 27725,  3776, 35877, 40949, 40301, 46998, 16461,
        2858, 44994, 20654, 23638, 33369, 42229,  4129, 27391, 31143,
       45132,  1925, 42219, 25781, 21779,  9592, 37741, 46823, 23231,
       45515,  3176, 46276, 34352, 25401, 49743, 46446,  4640, 30707,
       23739, 33586,  1832, 30268, 20282, 36764, 45170, 36884, 31699,
        1406, 43364, 47928, 41127, 43180,  8263, 22318, 42398, 12896,
       18977, 23574,  9937,  6569, 49114, 31588, 10131, 47907, 49515,
       18606,  7218, 30413,  5625, 38248, 18628,  1001, 12208, 35113,
        2830, 27534, 42138, 13652,  5944,  1407, 41610, 39891, 15112,
       44844, 19721, 25024, 29202, 36276, 39958,   661, 18043, 22371,
       35641, 39853, 30911, 41955, 43857, 21949, 43746, 46648, 32494,
       14105, 38386, 20120, 28904, 35143,  4231, 13708, 43810, 42277,
        3283,  1953, 31515,  2155, 20498, 31683, 42040,  4234,   200,
       36365, 11911, 12635, 28395, 40031,  4606, 34394,  8215, 37759,
       14553, 14719, 18179, 45582, 43426,  5443,  9877,  7650, 12359,
        6664, 41049, 33178, 46123, 29436, 31784, 15060, 29097, 41641,
       24231, 28722, 13890, 15338, 37481, 22393, 10676,  3139, 24155,
       15351, 39511, 16387, 23082,  8877, 44342, 38396, 37261, 15349,
       49061, 17105, 23087,  1371, 34674, 20297, 24321, 23987, 46550,
       36069,  2100, 25234, 34786,  2729, 10025,  5159,  8431, 25396,
       26921, 37188, 48710, 31112, 44251, 16451, 21846,  4926, 28588,
        6096, 48854, 24882, 38671, 21024, 26139,  4389, 13802, 27708,
        1678, 47650, 19823, 26243, 36696, 27567,  5857, 19707, 32768,
       47513, 32739, 23108, 23616, 32381,   451,  3500, 23084, 15211,
       41707, 11233, 24722, 19731, 49636, 20977, 49200, 28252, 19798,
       26782,  1403, 27357, 47302, 31598, 20021,  6267, 47580,  2299,
       11647,  8363,  7911, 29426, 31629, 40921, 18309, 25681, 37564,
       32240, 17715, 46677, 12447, 49464, 30193, 24673, 24204, 45518,
       40549, 13965,  9241, 40302, 48070, 13306, 27240, 12004,  9084,
       43390, 23152, 23324, 27648,  1228, 46092, 32027, 31421,  7316,
       43764, 42641, 22746, 28859, 21292,  6686, 11729, 16883, 42046,
       12512, 40501, 40685, 36995, 39982, 39969,  3241, 22795, 49039,
       18615, 31175, 39161, 40437,  9994,  6273, 13209, 28998, 25430,
       12831,  8128,  5419, 28533, 44597, 14608, 48897, 44244, 13939,
        6750,  9208,  7064, 31140, 13455, 12168, 42616,  5689,  3070,
        1978,  2874, 39068, 42460, 30176, 38141,  6960,  1038, 49209,
        4659, 15185,  2267, 13878,  3686, 31118,  8697, 21588,  9303,
       20003, 39007, 26228, 49948, 17697, 28980, 26619, 35830, 23747,
        6294, 33630,  3341,  7473, 16442, 38786, 44999, 12293, 41466,
        9233,  9788, 35001, 25298, 39986,   862,  2941,  4421, 33244,
        7575, 38707, 49798, 18085, 13988, 29244, 28750,  8116, 33530,
       46631, 29343, 24445,  9925, 27080, 11451,  6904, 45348, 32479,
       17917,  5441, 44149, 41300, 21573, 10334, 43998, 12052, 42083,
       17453, 30257, 39149, 29732, 26723,  6609,  2034, 44709,  7674,
       13902, 11633,  1960,  7925, 46420, 26269, 33516, 22847, 38335,
       21758, 38217, 42984, 19386, 33003, 27706, 26824,   457, 25070,
       35603, 13110, 28126, 38921, 28360, 20127, 45190, 41435, 29336,
       24144, 21248, 38044, 40890, 13086, 32140, 41557,  3903,   940,
       12916,  8044, 34235, 25082, 24641, 15225, 41242, 26840, 13436,
       35179, 35269, 39914, 40648, 44443,  1594, 30968, 45393, 37955,
       47375, 43849, 14520,  5194,  5883,  7174, 27645, 22540, 41848,
       41055,  7251, 38036, 44770, 48489, 27562,  1234, 15466, 28408,
       13915, 16831, 40542,  8953, 15620, 19065, 49006, 24291, 29844,
       16116, 45621,  4940, 37693, 38307, 43657, 27004,  7168, 27850,
        3725, 28401, 19527, 38154, 20014, 20910, 46366, 17088,  8553,
       17107,  7212, 34917, 42103, 25270, 28003, 36149, 27758, 29641,
        6276,  6923, 42226, 26505, 38977, 19961, 39628, 44454, 35544,
       34448, 42199, 20015, 34421, 29772, 29888, 17156, 27873, 18436,
       40768,  4543, 49507, 36573, 25177, 28305, 25016, 22286, 41124,
       16903,   223,  5642, 11583, 34280,  9069, 37240, 26361, 23004,
       35381, 29578, 39083, 17226, 15735, 33368, 16191, 42241, 38341,
       49308, 10487,   905, 35798, 41019, 37199, 39435,  7537, 15509,
        7971, 10729, 18578, 11671,  5010, 36464, 49130, 44175, 48851,
       46362, 38433, 38222, 47678, 12634,  3438, 12785, 40287, 11178,
       33186, 43771, 48842, 19493, 27944,  7423, 30295,  1871,  7527,
       28515, 24685, 15255, 35859, 19371, 32538, 12509,  2345, 39606,
        3644, 21064, 45516, 32631, 33325, 34283, 35162, 22628, 26003,
       38320, 29665, 42998, 16346, 27724,  8136, 35201,   293, 28212,
       37718, 35683, 32432, 48464, 11446, 28256, 14403, 46869, 43229,
        5188,  5336, 45087,  9957, 42962, 14636,  6072, 34807, 36562,
       10547, 33524, 13022, 23298, 17069, 36495, 25617, 49141, 40119,
       26067,  5608,  7223, 49961,  4795,  5804,  2074, 43309, 20699,
       10529, 27248, 46097, 18828,  5313,  8859,  4933, 29526, 26895,
        7874, 43193, 44923,  7416, 15932, 17377, 21904, 42322, 38057,
       40567, 26166, 16654, 32286, 17804, 27583,  4325, 29441, 15824,
       17910, 35214, 30253, 15850,  7863, 25156,  2897,  4216, 34257,
       46172, 44753, 11951, 43060, 12394, 21967, 40439, 25410, 33850,
       18519,  4285, 14582,  5710, 33349, 37884,  1828,  4833, 25271,
       28201,  1577, 25197, 39328, 27952, 22678,  7609,  2063, 26515,
       25575, 24327, 18447, 23835, 39219, 25102,  8984, 37472, 24311,
       21301, 34240, 47125, 15081, 25228,  5536, 19408, 17129, 31528,
       45171, 45459, 27782, 38539, 22854, 42788, 34592,   956, 25916,
       22076,  3933, 20773,  1806, 33388, 37474, 30348, 10121, 24468,
        4871,  1808, 23753, 44845,  8907, 28264, 36833, 28206, 42831,
       19200, 47249, 44839, 37683, 34126, 37897, 47638, 37205, 49226,
       44514, 24235, 42338, 14095, 16868, 26107, 26562, 14144,  3261,
       25778, 20158, 49695, 33496,  1113, 16260, 28868,  5317,  8499,
         329, 16666,  2977, 31326, 17997, 12023,  5126, 41974,  6301,
       39782, 11278, 21876, 23435, 20476, 37635, 34569,  7798, 33203,
       36098, 42117,  5802, 49476, 28243,  9133, 14837,  6793, 25454,
       41195, 10988, 18041,  7186,  2480,  4872, 45833, 29800,  7845,
       28920, 31472, 12751, 14050, 44567, 14862, 33475, 16633,  8481,
       11214, 45999, 47285, 20218, 33553, 32148, 10552,  4809, 36034,
       40709, 40539,  7331, 28977, 29155, 16095,  2481, 24159,   382,
       11350,  9981, 39406,  7450, 10386,  9560, 46524, 42751,  4078,
       26669, 33860, 43531,  8391, 32992, 45407,  7725,  5560, 36829,
        6986]), [4, 6, 0, 7])]
Collaboration
DC 0, val_set_size=1000, COIs=[2, 8, 0, 7], M=tensor([2, 8, 0, 7], device='cuda:0'), Initial Performance: (0.222, 0.04485496115684509)
DC 1, val_set_size=1000, COIs=[5, 3, 0, 7], M=tensor([5, 3, 0, 7], device='cuda:0'), Initial Performance: (0.25, 0.044860809803009036)
DC 2, val_set_size=1000, COIs=[1, 9, 0, 7], M=tensor([1, 9, 0, 7], device='cuda:0'), Initial Performance: (0.221, 0.04458320701122284)
DC 3, val_set_size=1000, COIs=[4, 6, 0, 7], M=tensor([4, 6, 0, 7], device='cuda:0'), Initial Performance: (0.25, 0.04539239990711212)
D00: 1000 samples from classes {0, 7}
D01: 1000 samples from classes {0, 7}
D02: 1000 samples from classes {0, 7}
D03: 1000 samples from classes {0, 7}
D04: 1000 samples from classes {0, 7}
D05: 1000 samples from classes {0, 7}
D06: 1000 samples from classes {8, 2}
D07: 1000 samples from classes {8, 2}
D08: 1000 samples from classes {8, 2}
D09: 1000 samples from classes {8, 2}
D010: 1000 samples from classes {8, 2}
D011: 1000 samples from classes {8, 2}
D012: 1000 samples from classes {3, 5}
D013: 1000 samples from classes {3, 5}
D014: 1000 samples from classes {3, 5}
D015: 1000 samples from classes {3, 5}
D016: 1000 samples from classes {3, 5}
D017: 1000 samples from classes {3, 5}
D018: 1000 samples from classes {1, 9}
D019: 1000 samples from classes {1, 9}
D020: 1000 samples from classes {1, 9}
D021: 1000 samples from classes {1, 9}
D022: 1000 samples from classes {1, 9}
D023: 1000 samples from classes {1, 9}
D024: 1000 samples from classes {4, 6}
D025: 1000 samples from classes {4, 6}
D026: 1000 samples from classes {4, 6}
D027: 1000 samples from classes {4, 6}
D028: 1000 samples from classes {4, 6}
D029: 1000 samples from classes {4, 6}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.06002717798948288) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.0816035498380661) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.304, 0.09341755312681198) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.0939168337881565) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.06308483767509461) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.08910550057888031) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.352, 0.13188596749305725) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.11786670678853989) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.07532635158300399) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.289, 0.10358087551593781) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.15207326704263688) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.15404279613494873) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.105178024366498) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.11446061944961548) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.17986074367165567) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.19359765738248824) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.12571522931009532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.351, 0.12731943529844283) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.17620865178108217) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.21025887111574412) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.14941069076955318) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.357, 0.1479798818230629) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.17783011223375797) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.2532032940760255) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.15933214899897574) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.16829006430506707) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.21066069585829975) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.24311571361124515) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.19129687175899743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.365, 0.16548691460490228) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.20810078812390567) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.27205386623740196) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.1845419636927545) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.15964543256163596) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.23719379422068596) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2844371333681047) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.2080296144457534) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.367, 0.12997722536325454) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.219299509100616) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.3079838444199413) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[0, 7], M=tensor([0, 2, 3, 5, 7, 8], device='cuda:0'), Initial Performance: (0.0, 0.33866382503509523)
DC Expert-0, val_set_size=500, COIs=[8, 2], M=tensor([2, 8, 0, 7], device='cuda:0'), Initial Performance: (0.912, 0.007428182298317551)
DC Expert-1, val_set_size=500, COIs=[3, 5], M=tensor([5, 3, 0, 7], device='cuda:0'), Initial Performance: (0.734, 0.018887594819068907)
SUPER-DC 0, val_set_size=1000, COIs=[2, 8, 0, 7], M=tensor([2, 8, 0, 7], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 3, 0, 7], M=tensor([5, 3, 0, 7], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x760c300411f0>, <fl_market.actors.data_consumer.DataConsumer object at 0x760c0424cd00>, <fl_market.actors.data_consumer.DataConsumer object at 0x760c0419f4f0>, <fl_market.actors.data_consumer.DataConsumer object at 0x760c04272040>, <fl_market.actors.data_consumer.DataConsumer object at 0x760c40429970>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO5', '(DO0']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.00441553357616067) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.732, 0.0211842959523201) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.2036310649588704) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.32747741363570093) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.917, 0.00967766376864165) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.04906645208597183) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.03577966687083244) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.004724188733845949) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.738, 0.022142806053161622) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.19229688239842654) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.33649664038419724) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.010982068341225386) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.643, 0.03214793187379837) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.06138224221765995) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.004584783827885986) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.74, 0.022561999320983888) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.20021727247536183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.38600017932476477) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.009993843083269895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.601, 0.04801330307126045) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.0801974680647254) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0046767621338367465) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.744, 0.023536759376525877) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.20310662303119897) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.3749631053362973) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.913, 0.01033382852561772) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.048506146535277364) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.10430976708978415) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005044621486216784) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.738, 0.021660637736320494) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.19070784934610127) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.3731861675195396) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.009628200585022569) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.561, 0.06107257369160652) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.481, 0.09994173619896174) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO2', '(DO4']
DC 3 --> ['(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.004918262589722872) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.744, 0.024190089583396912) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.1689554807394743) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.3686417160015553) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.007198441304266453) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.564, 0.06455847738683224) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.09723518718034029) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004559732677415013) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.766, 0.02101418173313141) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.1792403512299061) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.364385165354237) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.011448697978630662) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.07153521419689059) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.13465036561340094) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005156973468139768) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.746, 0.019540634453296662) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.1814088097885251) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.372601313367486) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.914, 0.011482607734506019) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.08643370085954666) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.13158346676826477) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0054411700908094645) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.75, 0.024382736682891846) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.20557643680274487) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.37473319175839426) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.903, 0.01644074076916877) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.08139923707395792) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.13786697933729739) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005946563266217709) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.742, 0.024530553340911865) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.19915205325186253) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.3742286728657782) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.011273877628147601) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.514, 0.08316984529793263) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.12433975057303906) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO4', '(DO0']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005378667199984193) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.744, 0.027541272103786468) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.20623954266309738) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.36860995979607103) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.943, 0.005163439184427261) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.571, 0.06523805662989617) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.10044974098354578) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005259381735697389) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.724, 0.023025532603263853) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1800753628909588) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.36482275036070494) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.937, 0.009394818209402729) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.07873428281396627) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.1316934523805976) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006490989617072046) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.738, 0.026288924634456635) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.18916064555943013) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.3926366530880332) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.009774532020790502) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.06813502561300994) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.11829829369857907) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.006192061538342387) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.73, 0.026470993399620057) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.1806728651598096) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.3802986639924347) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.010751318679307587) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.07788430847227573) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.12931941991858184) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0056977336104027925) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.744, 0.024523152410984038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.17896264816075563) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.3724618175178766) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.947, 0.009756018650718033) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.07508405846357345) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.12967559649050237) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO4', '(DO0']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005904041146859527) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.754, 0.028151296496391295) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.17122400576621294) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.3816517299353145) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.918, 0.016311098073725588) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.541, 0.08777906958758831) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.13688021048530935) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.006353272574953735) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.732, 0.025669027268886566) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.1813142495304346) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.37188495222479107) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.01074423722497886) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.07885100957006216) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.12765613913815468) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.007565307176439092) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.75, 0.027263707339763642) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.18485890039801597) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.40716812532395125) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.925, 0.015568710416788235) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.574, 0.06708239785581827) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.465, 0.18589208103716373) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006456035735085607) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.744, 0.029272633552551268) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.18641311668977142) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.3854063894848805) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.921, 0.01008086174260825) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.582, 0.058742246180772784) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.0880264972448349) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.00614185515185818) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.74, 0.028648243069648742) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1935600913837552) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.424038912541233) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.013185384002514183) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.0970235292762518) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.11769291480630636) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO4', '(DO3']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0058404436502605675) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.728, 0.02870996731519699) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.21398460396751762) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.4062331854384392) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.927, 0.0068646698547527195) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.0693990423232317) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.5, 0.09267461040616036) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004418918853625655) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.748, 0.027047998905181884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.21804999727010727) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.36484908319637177) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.00872702476195991) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.579, 0.067644519880414) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.12267595111299306) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006011068697087466) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.752, 0.02401289451122284) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.21781821943819524) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.40486001267516986) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.009788815844920465) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.07344745621085166) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.11152436571195722) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004449683265760541) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.73, 0.032493491649627684) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.20416157456487416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.4140723744016141) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.009044239399023353) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.574, 0.06856524293124676) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.10352532221842557) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005482096739346161) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.736, 0.02788230150938034) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.20453039857000113) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.4064948058063164) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.009638832580763847) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.622, 0.049754623845219614) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.1406771965753287) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO2', '(DO5']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.00613285843282938) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.732, 0.029679792642593385) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.23212354827299714) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.4043378993528895) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.937, 0.010035023098520468) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.579, 0.05914492528140545) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.12815904891118407) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.006094790142029524) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.742, 0.028603414952754973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.24259926507249474) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.43956838847347535) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.935, 0.009491386892739683) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.602, 0.053875890105962755) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.11925559291709215) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005565867303870618) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.72, 0.028364150166511535) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.2420233118571341) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.37071821190603077) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.925, 0.013472032258985564) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.589, 0.058864815823733804) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.11558125951047987) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.00563838327024132) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.748, 0.027053027629852296) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.25116850252449513) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.4233957543056458) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.924, 0.01544486689241603) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.07375920674204826) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.495, 0.10979203881323338) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005816703439224511) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.742, 0.025393360137939452) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.227486635312438) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.39602041759761053) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.010670148208373576) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.599, 0.056977585673332214) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.13900087654776871) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO0', '(DO2']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.006947937439894304) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.726, 0.026272192239761353) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.2671052640341222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.4160943165048957) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.005924860857892782) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.623, 0.048042711198329924) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.494, 0.12341342902928591) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006530223835725337) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.728, 0.025638095676898957) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.2658625461868942) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.4443858838379383) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.016764433506818024) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.07733103032410145) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.15354919087281452) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006078086584806442) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.728, 0.02493933367729187) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.2684289029575884) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.4374007253274322) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.927, 0.01085051651415415) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.06205658283829689) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.15095457112044097) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004762966708745807) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.738, 0.022957479774951935) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.24841840276122093) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.4222600713013671) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.00874156273837434) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.602, 0.05546376818045974) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.14113419826887547) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.006646828824363183) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.728, 0.02936410367488861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.29759143583476544) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.43474383678659795) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.905, 0.013497359958477319) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.561, 0.06494410949945449) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.11621478743851185) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO4', '(DO0']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005335976764326915) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.742, 0.025394456684589384) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.28297291678190234) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.43969245535880325) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.919, 0.011467760069528595) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.544, 0.0840404379889369) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.13861779838986696) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005116593582089991) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.738, 0.02300615221261978) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.28626675678789615) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.46548123020958154) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.012747019773581997) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.07982059617340564) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.1187683248128742) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.003926728887483477) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.758, 0.02522514349222183) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.2985933459624648) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.4200397818312049) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.014620859417249448) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.08500651010870934) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.12738726800493896) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004580614291597158) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.744, 0.021626798450946807) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.25883852066099644) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.44182221650332215) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.012483707251725719) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.572, 0.06781332489661872) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.1703528838083148) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005155177865410224) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.736, 0.021461014628410338) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.29234863063693045) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.4105236178440973) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.014306553728660219) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.505, 0.09477403430640698) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.16237050973577424) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.222, 0.04485496115684509), (0.362, 0.06002717798948288), (0.451, 0.06308483767509461), (0.463, 0.07532635158300399), (0.463, 0.105178024366498), (0.465, 0.12571522931009532), (0.47, 0.14941069076955318), (0.464, 0.15933214899897574), (0.469, 0.19129687175899743), (0.468, 0.1845419636927545), (0.456, 0.2080296144457534), (0.528, 0.04906645208597183), (0.643, 0.03214793187379837), (0.601, 0.04801330307126045), (0.586, 0.048506146535277364), (0.561, 0.06107257369160652), (0.564, 0.06455847738683224), (0.54, 0.07153521419689059), (0.524, 0.08643370085954666), (0.517, 0.08139923707395792), (0.514, 0.08316984529793263), (0.571, 0.06523805662989617), (0.562, 0.07873428281396627), (0.552, 0.06813502561300994), (0.537, 0.07788430847227573), (0.552, 0.07508405846357345), (0.541, 0.08777906958758831), (0.543, 0.07885100957006216), (0.574, 0.06708239785581827), (0.582, 0.058742246180772784), (0.533, 0.0970235292762518), (0.586, 0.0693990423232317), (0.579, 0.067644519880414), (0.558, 0.07344745621085166), (0.574, 0.06856524293124676), (0.622, 0.049754623845219614), (0.579, 0.05914492528140545), (0.602, 0.053875890105962755), (0.589, 0.058864815823733804), (0.552, 0.07375920674204826), (0.599, 0.056977585673332214), (0.623, 0.048042711198329924), (0.54, 0.07733103032410145), (0.577, 0.06205658283829689), (0.602, 0.05546376818045974), (0.561, 0.06494410949945449), (0.544, 0.0840404379889369), (0.539, 0.07982059617340564), (0.542, 0.08500651010870934), (0.572, 0.06781332489661872), (0.505, 0.09477403430640698)]
TEST: 
[(0.21525, 0.0437830693423748), (0.36075, 0.05810387420654297), (0.442, 0.06061009407043457), (0.4575, 0.07175614351034164), (0.465, 0.09945581325888633), (0.45975, 0.11858130222558975), (0.4675, 0.140495234310627), (0.4625, 0.14899606961011885), (0.46325, 0.1790197706222534), (0.465, 0.17290244770050048), (0.4485, 0.19369042420387267), (0.54175, 0.04561127617955208), (0.61525, 0.033570218451321125), (0.58325, 0.048833264276385305), (0.5725, 0.05028250971436501), (0.5575, 0.06346995025873184), (0.56875, 0.06539389689266682), (0.53375, 0.07407131955027581), (0.51675, 0.09091908913850784), (0.513, 0.08476926985383033), (0.52125, 0.08493730968236923), (0.56325, 0.06787916171550751), (0.5525, 0.0765104937851429), (0.54625, 0.0707725949883461), (0.53475, 0.08007347925007344), (0.5405, 0.07717900387942792), (0.5335, 0.08406937471032143), (0.54525, 0.08308302330970764), (0.57875, 0.0695430829077959), (0.58225, 0.06056686221063137), (0.52775, 0.10225986152887344), (0.5655, 0.07350354239344598), (0.57325, 0.06723758745193481), (0.559, 0.07566562189161777), (0.585, 0.06673641723394394), (0.60375, 0.049734153881669046), (0.568, 0.06070458298921585), (0.599, 0.0556554686576128), (0.5665, 0.06104181954264641), (0.53225, 0.07829677814245224), (0.58125, 0.058331891104578974), (0.6185, 0.049367204874753955), (0.5425, 0.07692113953828812), (0.5695, 0.06405630362033844), (0.57775, 0.057533562421798704), (0.55275, 0.06975903490185738), (0.53925, 0.08844392862915992), (0.5435, 0.08252788224816322), (0.54425, 0.09011661031842232), (0.57025, 0.06797467635571956), (0.518, 0.09311937394738197)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.45      0.88      0.59      1000
           2       0.68      0.20      0.31      1000
           7       0.56      0.92      0.69      1000
           8       0.85      0.07      0.13      1000

    accuracy                           0.52      4000
   macro avg       0.63      0.52      0.43      4000
weighted avg       0.63      0.52      0.43      4000

Collaboration_DC_1
VAL: 
[(0.25, 0.044860809803009036), (0.25, 0.0816035498380661), (0.25, 0.08910550057888031), (0.289, 0.10358087551593781), (0.366, 0.11446061944961548), (0.351, 0.12731943529844283), (0.357, 0.1479798818230629), (0.366, 0.16829006430506707), (0.365, 0.16548691460490228), (0.373, 0.15964543256163596), (0.367, 0.12997722536325454), (0.556, 0.03577966687083244), (0.498, 0.06138224221765995), (0.487, 0.0801974680647254), (0.464, 0.10430976708978415), (0.481, 0.09994173619896174), (0.476, 0.09723518718034029), (0.467, 0.13465036561340094), (0.467, 0.13158346676826477), (0.461, 0.13786697933729739), (0.473, 0.12433975057303906), (0.49, 0.10044974098354578), (0.485, 0.1316934523805976), (0.478, 0.11829829369857907), (0.475, 0.12931941991858184), (0.475, 0.12967559649050237), (0.471, 0.13688021048530935), (0.473, 0.12765613913815468), (0.465, 0.18589208103716373), (0.498, 0.0880264972448349), (0.484, 0.11769291480630636), (0.5, 0.09267461040616036), (0.491, 0.12267595111299306), (0.485, 0.11152436571195722), (0.487, 0.10352532221842557), (0.48, 0.1406771965753287), (0.487, 0.12815904891118407), (0.498, 0.11925559291709215), (0.484, 0.11558125951047987), (0.495, 0.10979203881323338), (0.483, 0.13900087654776871), (0.494, 0.12341342902928591), (0.47, 0.15354919087281452), (0.471, 0.15095457112044097), (0.478, 0.14113419826887547), (0.487, 0.11621478743851185), (0.487, 0.13861779838986696), (0.472, 0.1187683248128742), (0.478, 0.12738726800493896), (0.466, 0.1703528838083148), (0.475, 0.16237050973577424)]
TEST: 
[(0.25, 0.04378252673149109), (0.25, 0.07828381612896919), (0.25, 0.08545986431837081), (0.2835, 0.09934729871153831), (0.34575, 0.10978565776348113), (0.357, 0.12227776199579239), (0.35475, 0.1418950108885765), (0.36225, 0.16125718301534653), (0.361, 0.15870756310224532), (0.36225, 0.15440980315208436), (0.36025, 0.12609325283765793), (0.534, 0.036387127384543416), (0.49375, 0.06073338815569878), (0.494, 0.07861720272898674), (0.4795, 0.10324328199028969), (0.4915, 0.09896762102842331), (0.4955, 0.09665969973802567), (0.46575, 0.132520246475935), (0.4665, 0.13420670819282532), (0.45875, 0.13478391480445862), (0.47625, 0.12397754207253456), (0.48925, 0.10078570383787155), (0.47775, 0.13552043503522873), (0.473, 0.12006545180082322), (0.4725, 0.13281734037399293), (0.4755, 0.12924580654501916), (0.458, 0.14210415375232696), (0.47875, 0.12970085695385933), (0.46125, 0.1875926966071129), (0.505, 0.0890518693625927), (0.478, 0.12078838866949082), (0.50275, 0.09298296412825584), (0.485, 0.12478072893619538), (0.47875, 0.11156368806958199), (0.4965, 0.10484470239281654), (0.48075, 0.1377414911687374), (0.485, 0.13011071828007698), (0.481, 0.1289030394256115), (0.484, 0.1205246279835701), (0.48375, 0.113883838057518), (0.47175, 0.14431480926275253), (0.49275, 0.11930290171504021), (0.4775, 0.1497337721288204), (0.476, 0.14514680951833725), (0.479, 0.13638815546035768), (0.496, 0.1117496035695076), (0.48425, 0.1375197970867157), (0.48325, 0.11953937268257141), (0.47975, 0.12728595358133316), (0.4685, 0.16691366457939147), (0.47575, 0.16607711893320085)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.73      0.92      0.82      1000
           3       0.62      0.04      0.07      1000
           5       0.62      0.01      0.01      1000
           7       0.35      0.94      0.51      1000

    accuracy                           0.48      4000
   macro avg       0.58      0.48      0.35      4000
weighted avg       0.58      0.48      0.35      4000

Collaboration_DC_2
VAL: 
[(0.221, 0.04458320701122284), (0.304, 0.09341755312681198), (0.352, 0.13188596749305725), (0.397, 0.15207326704263688), (0.413, 0.17986074367165567), (0.423, 0.17620865178108217), (0.408, 0.17783011223375797), (0.403, 0.21066069585829975), (0.425, 0.20810078812390567), (0.425, 0.23719379422068596), (0.431, 0.219299509100616), (0.415, 0.2036310649588704), (0.435, 0.19229688239842654), (0.437, 0.20021727247536183), (0.439, 0.20310662303119897), (0.436, 0.19070784934610127), (0.439, 0.1689554807394743), (0.442, 0.1792403512299061), (0.435, 0.1814088097885251), (0.456, 0.20557643680274487), (0.441, 0.19915205325186253), (0.44, 0.20623954266309738), (0.441, 0.1800753628909588), (0.442, 0.18916064555943013), (0.442, 0.1806728651598096), (0.444, 0.17896264816075563), (0.44, 0.17122400576621294), (0.446, 0.1813142495304346), (0.442, 0.18485890039801597), (0.441, 0.18641311668977142), (0.441, 0.1935600913837552), (0.444, 0.21398460396751762), (0.447, 0.21804999727010727), (0.448, 0.21781821943819524), (0.45, 0.20416157456487416), (0.441, 0.20453039857000113), (0.441, 0.23212354827299714), (0.44, 0.24259926507249474), (0.442, 0.2420233118571341), (0.439, 0.25116850252449513), (0.438, 0.227486635312438), (0.445, 0.2671052640341222), (0.443, 0.2658625461868942), (0.446, 0.2684289029575884), (0.445, 0.24841840276122093), (0.443, 0.29759143583476544), (0.45, 0.28297291678190234), (0.448, 0.28626675678789615), (0.444, 0.2985933459624648), (0.446, 0.25883852066099644), (0.442, 0.29234863063693045)]
TEST: 
[(0.231, 0.043520193308591845), (0.31675, 0.08954067873954773), (0.36975, 0.12627470338344574), (0.4045, 0.14565940433740615), (0.3995, 0.17283708488941193), (0.41275, 0.16908486545085907), (0.3955, 0.17159293580055238), (0.3935, 0.20377891224622727), (0.42225, 0.20144247567653656), (0.41775, 0.2316529153585434), (0.41525, 0.21458645927906037), (0.40075, 0.19640768760442734), (0.42325, 0.19064207303524017), (0.4275, 0.1936377297639847), (0.432, 0.19824053007364273), (0.42775, 0.18501348811388016), (0.43425, 0.16680630415678024), (0.43275, 0.17633352190256119), (0.43225, 0.18037730330228804), (0.43325, 0.20172637689113618), (0.4355, 0.19394711768627168), (0.435, 0.20030117940902709), (0.43375, 0.17951179027557373), (0.42375, 0.18331172078847885), (0.4355, 0.17471659308671952), (0.4315, 0.1756042503118515), (0.43275, 0.16520550245046617), (0.43625, 0.17635428857803345), (0.43625, 0.1774183059334755), (0.429, 0.18084617388248445), (0.43125, 0.18746844512224198), (0.43375, 0.20950802344083785), (0.432, 0.21100668603181838), (0.43475, 0.21019422549009323), (0.4355, 0.19703019326925278), (0.4365, 0.20035075515508652), (0.4305, 0.2245148269534111), (0.4275, 0.23508934450149535), (0.4365, 0.23472921270132066), (0.4335, 0.24153532737493516), (0.4345, 0.21683062213659288), (0.4325, 0.2577077451944351), (0.43275, 0.2543322962522507), (0.438, 0.2564748913049698), (0.43, 0.2421973911523819), (0.43825, 0.2853791214227676), (0.43225, 0.2727738609313965), (0.4335, 0.2768554049730301), (0.4335, 0.2884051077365875), (0.43225, 0.25265153872966767), (0.42925, 0.27959512317180635)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.65      0.80      0.72      1000
           7       0.00      0.00      0.00      1000
           9       0.33      0.92      0.49      1000

    accuracy                           0.43      4000
   macro avg       0.25      0.43      0.30      4000
weighted avg       0.25      0.43      0.30      4000

Collaboration_DC_3
VAL: 
[(0.25, 0.04539239990711212), (0.25, 0.0939168337881565), (0.396, 0.11786670678853989), (0.425, 0.15404279613494873), (0.428, 0.19359765738248824), (0.41, 0.21025887111574412), (0.425, 0.2532032940760255), (0.445, 0.24311571361124515), (0.433, 0.27205386623740196), (0.429, 0.2844371333681047), (0.428, 0.3079838444199413), (0.436, 0.32747741363570093), (0.44, 0.33649664038419724), (0.413, 0.38600017932476477), (0.437, 0.3749631053362973), (0.447, 0.3731861675195396), (0.447, 0.3686417160015553), (0.445, 0.364385165354237), (0.449, 0.372601313367486), (0.442, 0.37473319175839426), (0.432, 0.3742286728657782), (0.443, 0.36860995979607103), (0.441, 0.36482275036070494), (0.449, 0.3926366530880332), (0.449, 0.3802986639924347), (0.443, 0.3724618175178766), (0.441, 0.3816517299353145), (0.448, 0.37188495222479107), (0.443, 0.40716812532395125), (0.442, 0.3854063894848805), (0.443, 0.424038912541233), (0.446, 0.4062331854384392), (0.446, 0.36484908319637177), (0.442, 0.40486001267516986), (0.444, 0.4140723744016141), (0.446, 0.4064948058063164), (0.428, 0.4043378993528895), (0.44, 0.43956838847347535), (0.434, 0.37071821190603077), (0.439, 0.4233957543056458), (0.44, 0.39602041759761053), (0.446, 0.4160943165048957), (0.447, 0.4443858838379383), (0.445, 0.4374007253274322), (0.448, 0.4222600713013671), (0.448, 0.43474383678659795), (0.448, 0.43969245535880325), (0.448, 0.46548123020958154), (0.442, 0.4200397818312049), (0.446, 0.44182221650332215), (0.447, 0.4105236178440973)]
TEST: 
[(0.25, 0.044226822316646576), (0.25025, 0.09019724974036217), (0.40925, 0.11303245615959168), (0.4305, 0.14766822332143784), (0.43575, 0.18539260530471802), (0.41875, 0.20131079435348512), (0.4305, 0.24464485538005828), (0.4475, 0.23358500742912292), (0.4465, 0.2583946565389633), (0.439, 0.27387344324588775), (0.4385, 0.2942857506275177), (0.4445, 0.3157788754701614), (0.451, 0.3241825635433197), (0.41875, 0.36426732993125915), (0.44775, 0.3573821120262146), (0.448, 0.35756357192993166), (0.44875, 0.35521082866191867), (0.45325, 0.3525261458158493), (0.45375, 0.35913816809654237), (0.45375, 0.3560465216636658), (0.442, 0.35146829605102536), (0.44975, 0.3554567221403122), (0.4495, 0.3525204541683197), (0.453, 0.37531868982315064), (0.451, 0.36151085793972015), (0.452, 0.36027654027938844), (0.446, 0.3666204501390457), (0.452, 0.3596519502401352), (0.453, 0.3913941189050674), (0.4455, 0.3733704595565796), (0.45225, 0.40794327259063723), (0.45075, 0.38842312479019164), (0.4545, 0.35323964726924895), (0.446, 0.3846286350488663), (0.4485, 0.4006347942352295), (0.45425, 0.3896746680736542), (0.4355, 0.38056536483764647), (0.4525, 0.421202872633934), (0.44875, 0.3559823443889618), (0.45375, 0.4063817309141159), (0.44775, 0.37759697103500367), (0.455, 0.39862968170642854), (0.458, 0.42757619738578795), (0.45575, 0.4154670978784561), (0.457, 0.40245846486091613), (0.458, 0.4173643553256989), (0.45675, 0.4173838626146317), (0.45275, 0.44531553936004636), (0.454, 0.4053697193861008), (0.4535, 0.4261568899154663), (0.45225, 0.3931095609664917)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           4       0.33      0.94      0.49      1000
           6       0.78      0.87      0.82      1000
           7       0.00      0.00      0.00      1000

    accuracy                           0.45      4000
   macro avg       0.28      0.45      0.33      4000
weighted avg       0.28      0.45      0.33      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [23]
name: alliance-2-dcs-23
score_metric: contrloss
aggregation: <function fed_avg at 0x72ba4d757c10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=23
Partitioning data
[[0, 6, 4, 1], [5, 7, 4, 1], [9, 8, 4, 1], [3, 2, 4, 1]]
[(array([32659,  7396,   871,  8190, 33489, 44742,  8515,  5082,   965,
       11208,   700, 39329, 40740, 22304, 44430,   974, 28487, 44470,
       46964, 24750, 26632, 28921, 43184,  6663, 28273, 30029, 46367,
       39397, 38683, 15767, 48108, 33229, 13937, 41040,   276,  9831,
       45344,   927, 35963,  8842, 30591, 38617, 45559, 43659, 27768,
        2617, 28750,  1234,  5549, 34259, 20709, 33584, 29856, 34421,
       39553, 49734, 33380, 49130, 11897,  7445,  5003, 11958, 18763,
       26018,   600, 46936, 26667, 12477, 25023, 32870, 43077, 35570,
        9077, 16939, 46873, 32990, 37041, 39271, 47483,    93,  9121,
       33662, 34724, 33478, 49672, 12807, 45297,  6328, 20985, 35633,
       46625, 12674, 31391, 16554,  3347,    77, 48059, 30820,  9945,
       18102, 22924, 12297, 48777, 17222,  5557, 21815, 36415, 42231,
       12944, 47726, 47068, 30888,  7256, 33268, 22951, 42165, 39628,
       29380,  3564, 21143, 37035,  2427, 28000,  2010,   757,  7203,
       22118, 21803, 41745,  5349, 28272, 47329,  7925, 47339,  1715,
       21458, 16196, 38711, 12492, 17316,  5414, 48001, 21964,  1831,
       28305, 11218, 24113,  5642,  7263, 47994, 25199, 43548, 40956,
       42456,  4567,  4726,  9614, 28716, 32233,  2622, 13656, 27202,
       11372, 38791, 22458, 26656, 43427, 17620,  3731, 45232,  5093,
       45594, 20452, 16553, 11122, 14666, 44552, 24983, 19094,  6952,
       13850, 14434, 32107, 43475, 26764, 38307,  1376, 22194,  3721,
        1887, 43253, 34989, 10141, 10334,  4239, 17200, 33573,   348,
       14732, 31450, 46419, 39196, 41880, 14325, 11002, 38748, 45652,
       29881, 16647, 13031, 11615,  3987, 32056, 23776, 21469, 40556,
       20574, 49349, 13348,  1757, 29593, 44770, 13229, 10265,  2459,
       31083, 41242, 46749, 19004, 42929, 11682, 16235, 33258,  7772,
       48317, 37646, 17237,  5640,   436, 37078, 42829, 16632, 36386,
       38217, 44806, 26918, 39774, 45166, 43045, 28014, 37774, 45607,
       29833,  8942, 35340, 16404, 26339,  2252, 39512,  6096,   745,
       36348, 13137, 46342,  7848,  6581,  5785, 23691,  5176, 40551,
       24184,  6941,  5708, 42510, 18816, 20104, 13825, 39091, 17072,
       41444, 14293, 31042, 24955, 39889, 17957,  3307, 34684, 27177,
       15726, 14678, 49462, 33216, 46404, 23050, 45862,  8950, 19228,
       40879, 35455, 35585,  6093, 47497, 26275, 30222, 23451, 17105,
       31894, 42415, 40675, 18924, 42671, 25551, 47821, 10327, 13188,
       34804, 20310,  4094, 15552, 41672, 11070, 23539, 44258, 20141,
       18334, 31820, 12930, 21831, 15220,  5497, 13838, 23683,  5263,
        3655, 36913, 36192, 39148,  6521, 20734,  5419,  9125, 42046,
       16611,  7824,  9862,  9237,  5055, 42712, 31808, 40568, 34986,
        4785, 21155, 23760, 42850,   914, 34056, 19230, 35435, 21192,
       22909, 28106, 35600, 49747, 14024, 40929, 42778, 23390, 42460,
       11303,  8466, 35891,  6179, 33278,  7825, 15664, 12411, 48526,
       44422, 25680, 29381,  7632, 15517, 31806,  7335, 10431, 29241,
        3403, 22157, 30800,  3823,  4959, 44314, 10176, 31780, 38747,
       44612, 45637, 38864, 22986,  3435, 38893, 47513,  3841,   819,
        7715,  7705, 35444, 45361, 48176,  1154, 36704, 48460, 39756,
        6750, 30212,  3976, 47352, 23412, 19949, 39464, 35977, 47962,
        1529, 14274, 46322, 47276, 35900, 27240, 20771,  6968,  4926,
       34394, 29391, 28860, 24394, 49088, 21572, 41311, 36110,  2179,
       32060, 39326, 23517,  7025, 15536,  9021, 35480, 14940, 33690,
       42679, 33298, 32340, 38853, 26085,  2898, 11204, 47858, 26355,
       14163, 25793, 25430, 12641, 26415, 28501, 26478, 19239, 36396,
       45140, 30588,   863, 21941, 48027, 22295,  1035, 21958, 44573,
       35131, 38771, 34447, 39010,  7137, 44579, 35577,  3528, 48664,
        2810, 39537, 10665, 45895, 42836, 34829, 22332, 24673, 34139,
       12407, 24577,  3554, 42311,  1053, 31775, 32608, 35185, 30911,
        8254, 44215, 31281, 36711, 43486, 42665, 47605, 45665,  1269,
       36457, 34752, 45749, 38906,  1310,  9937, 34450, 43714,  3112,
        3225, 43032, 48447, 40088, 15777,  8375, 45767, 30600,  5677,
       43172, 47570, 42700, 23603,  6022, 39626, 22438,  4666, 47348,
       37656, 40827,  7822,  7362, 43758, 13778, 30435, 40529, 39501,
       25879, 11565, 42979,  1315, 41148, 16505,  9013, 36182, 29891,
       21382, 22139,  5173, 48334,  6844,  3442, 46333, 30950, 19585,
       22902,   158, 35839, 33872, 21161, 21659, 18363, 31902, 26710,
       15963, 36631, 18507, 38640,  6603, 48367, 39932, 22259, 49605,
       48881,  6569, 34425, 32302, 47015,  8804, 28649,  1826,   520,
       29771, 11586, 43914, 15667, 44465, 43683, 31932,  5742, 30501,
       35413,  5726, 48889, 38708, 48159, 39803, 29083, 27787, 20511,
       33998,  3360, 32388, 12416, 23525, 21473, 33038, 21281, 25153,
       21702, 27644, 32026,  4497,  5786, 22183,  2415, 27946, 47571,
       16893, 25781,    82,   429, 46660,  1804,  3077, 38593, 39169,
       34819,  3999, 41897, 24529,  4741,  9873, 20439, 18699, 24874,
        7163,  1923, 17608, 13959, 14258, 10925, 37172, 38794, 18293,
        2573, 20828, 41058, 44717,  2629, 44760,  3577,  2832, 16383,
       14857, 10076, 35318, 25024, 13464, 25444, 45812, 32784, 45644,
       17420, 15009, 49055, 31431, 45609,  2486, 34881, 35324,  6770,
       37004, 17513, 42915, 39833, 44572, 46521, 47318,  6990, 43975,
       22949,  3322, 17161, 10477,  4857, 10766, 31239,  8792, 42675,
        1569, 47530,    89, 43055, 31514, 23958, 10890, 14075,  2078,
       34816, 48521, 15415, 35915, 26926, 37089, 44850, 41249, 32786,
       39036, 23591,  8405, 22246, 33066, 12136, 28823, 32797, 28074,
       43228, 13310,  5607, 27428, 44516, 35187, 46036, 26844, 24239,
       31751, 48997,  6879, 18875,   343, 17266, 39356, 14490, 13903,
        4561, 19358, 46234, 46649, 24428, 18698, 44549, 26293, 24979,
       45470, 41404, 29318, 34327, 23317, 39769, 49314, 18562, 22270,
       22335,  2800,   547, 45258, 14035, 22067, 44824, 43359,  4326,
        1242, 10236, 29473, 14005, 43600, 11558, 29876, 24625, 37652,
       48113, 40520, 31288, 27089,  5301, 27994, 44266, 45455, 14904,
        6889, 36251, 36107, 32502, 36067, 12743, 37849, 14117, 36929,
       11955, 35291, 15756, 15299,  7992,  8374, 37260,  4909,   212,
       49234, 22816, 35613, 22422,  8244,  6908, 16848, 17986,   606,
       25710, 21615,  4171, 16882,  5282, 42007, 36400, 32462, 44488,
        7312, 23685, 31072, 20595,  5616,  4648, 30999,  3678, 31951,
        6122, 11037, 43932,    45, 15506, 49787, 49075, 47836,  1660,
       42802, 12914, 34097, 10248, 36611, 20079, 25463, 24470,  2306,
       40758, 48610, 11134, 19522,  3085, 33832, 16673, 19866, 31100,
       34802, 41607, 47824, 48294,  1565, 10122, 37098,  5885,    64,
        9610, 11744, 17539, 17950,  5287, 32235,  9524, 15797, 13240,
       49768,  9717,  5144, 11403,  7408, 14336, 46078,  4244, 29616,
       11308, 19564, 30218, 30712,  8339, 20536, 16755,   119, 40751,
       26937, 31147,  8093, 17014,  3034, 30445, 21850, 40311, 45201,
       44311, 42658,  9781,  4500, 38735, 19382, 26065, 26652, 36614,
       40431, 19341, 11163, 30103, 43290, 12080,  7587, 37428, 24541,
       19481, 21181, 48215, 47362,  1421, 32295, 10450, 28798, 27576,
       22419, 41922, 19125, 41768, 11109, 31720, 15447, 35357, 17772,
       19581,  4533, 13884, 12275, 40352, 41908,   561, 10678, 30154,
       30553,  4040, 29205, 17460, 17128, 37743, 19454, 29143, 18276,
       42663, 15814, 24873, 34118, 42976, 17017, 26294, 11954, 23020,
       31297, 18994, 38031, 12348, 14512, 34721, 35134, 11972, 48749,
       47001,  7019, 36040, 25388, 38192, 25135, 40250, 22499, 32988,
       29044, 16077, 41807, 15114, 43893,  9088, 12946, 12169, 29911,
       27371]), [0, 6, 4, 1]), (array([33576,  1922,   839, 20020, 33494,  4878,  8287,  3388,  4639,
       26548, 44538, 47095,  9288, 24553, 17293, 25774, 18928, 25740,
       29970, 33454, 31240, 18574,   339, 49070,  7518, 32122,  8282,
       17670, 25356,  9284,  8261, 46436, 47209, 14026, 47757,  8020,
       48181, 32199,  9391, 47884, 47211, 45043,  7846, 48843, 49552,
       22099, 42348,  5755, 15186, 37632, 23641, 47463, 41115,  4319,
       30140, 31439, 42645,  1849,  9537, 25722, 26836, 11560,  4579,
        4990, 15980,  1198, 22059, 26969, 43455, 39929, 47386, 44532,
       19012,  1280, 35534,  6338, 38600, 15301, 27691, 28659, 45731,
       21186, 29256, 49380, 49327, 15371, 34755, 18083, 14437,  3810,
        4920, 25583, 45108, 35034, 16852, 48189, 23234, 31782, 30196,
       11201, 47388, 24344, 10803, 14932, 28730, 20669,  5439, 29608,
       48555,   173, 36227, 43399, 35762, 26747, 23874, 19756, 40620,
       13872, 12912, 22890, 45215, 38366, 43979,  6958, 42088, 34763,
       22779, 25057,  8252,  4312, 29961, 48011, 23515, 46916,    27,
       41062, 29713, 41789, 13121, 17740, 37749, 25515, 28660, 11770,
       45448,  9665,  6697, 49188, 36954, 27377, 13073, 44706, 31355,
       18131, 22932,   784, 42776, 48207, 19182,  1618, 18569, 26418,
       46708, 16470,  2686, 29857, 41358, 10768, 19334, 24744,  7981,
       32598,  8240, 30931, 35892, 26587, 23689, 22941, 22473, 48161,
       46264, 19757,  8198, 25903, 27094, 45636,  1860,  1993, 43654,
       37921,  5729, 30370, 14875,  5561,  8894, 25870, 40934, 37451,
       47661,  3229, 16510, 22590, 21233,  7140, 49867, 22021, 26143,
       11347, 14033, 31781,  4862, 39757,  5305, 32957,  2916, 32889,
        6140, 39662, 41406, 41759, 30042, 37434, 35640,  7082, 25517,
        5690, 28306, 43599, 30235, 33578, 49661, 29302, 32364, 27633,
       33534,  5411,   670, 12051, 28321, 30523, 33142,  4641, 19652,
       17343,  1928, 10597, 41442, 39742, 26501, 46030, 47046, 46816,
       32347,  1898, 15073, 19466, 21995, 22115, 10584, 27374, 33469,
       49231,  2319, 34592, 25090, 20094, 20618, 12994, 30434, 39408,
       37683,  5258, 23773, 42598, 41795,   968, 42338, 26249, 38142,
       17627, 32374, 23486,  1783,  5710, 30883, 37695, 12985, 45147,
        8238, 49630, 23000, 12163, 38678, 47777, 48773, 29359, 39064,
       36495, 42284, 26733,  4088,  8343, 28262, 26217, 13442,   329,
       14319,  8907, 49109, 10277, 13039, 45127, 21394, 21576,  8498,
        7969, 46709,  7922, 45523, 47056, 14219, 21820, 26166, 35980,
       16582, 22980, 16187, 18802, 25241, 25132, 40010, 21409, 17129,
       29036, 32995,    11, 37100,  3213, 23828, 21857, 15647,  3364,
        5989, 19931, 33775, 36205, 33712, 49310, 32660, 17113, 31350,
       25370, 33921, 44986, 44374,  5812, 29937, 49116, 33585, 31341,
       12690, 45561, 27529, 28152, 23794,  3138, 34900, 44378,  4782,
       19372,  8744, 20576, 24948, 35407,  1927, 28257,  8360, 22625,
       30030,  4513, 10772, 46368,  6164, 13237, 31876, 40570, 18590,
        2119, 39136, 16091, 11414,  3473, 33527, 13547, 43229, 27916,
       27039, 31276, 22457, 24729, 44091, 17913,  3146,  1544,  4554,
        2211, 49648, 30797,   114, 23903,  1900, 28639, 36174, 15850,
       29557, 27098, 27910, 45747, 49103, 41473, 28392, 18938, 34990,
       22901, 27574, 25594, 42651, 24311,   825, 38196, 17616,  3627,
        6235, 33299,  5116, 31924,  9347, 49925, 37980, 47321, 44228,
       43724,  1113, 26422,  2620, 28977, 48579, 34260,  5792, 18339,
       29872, 46228, 29369, 41180, 38194, 10927, 40998, 18431,   388,
        6734, 23627, 14947, 38667,  6398, 21096, 20170,  9906,  9472,
       22031, 16162, 13284, 10687, 38286, 38225, 22669, 11260, 20175,
       40478, 39246, 10259, 29859, 15072, 47275, 48960, 12432,  3054,
       20423, 12364, 23585, 38094, 29486,  9719, 11990, 16025, 42887,
       42238,  3151, 43322, 13950, 29499, 30917,   669, 30794, 29582,
       13815,  8630,  2293, 40156,  6249, 24884, 46319, 30228,  3494,
        4103, 34852, 44963, 42662, 16426, 33667,  7563, 31075, 14090,
       14474, 31227, 24080, 46132, 10593, 41294, 13636,  7536, 37577,
       22673, 40523, 13164, 12108,  3952, 45221, 12406, 16194,  1557,
       26745, 13764, 10415, 16135, 11649, 32186, 39047, 40377, 43926,
       36945,  6212, 32969,  9963, 14744, 13282,  3936, 44705, 15503,
       10404, 47244, 46276, 37775, 49154, 21016, 33458, 18669, 31003,
        4973, 41216, 11862,  5067,  9962, 46415,  4286, 32224, 15715,
       31701, 38995, 40597, 37345, 16689, 14789, 25785, 40798,  2673,
       33666,  9695,  7595, 18014,  5919, 14957, 33370, 41101,  9254,
       42849, 41665,  5051, 27479, 48801, 48488, 29237, 38263, 13231,
       41093, 15131, 37469,  9780, 32229, 34144, 47735, 42312,  7100,
       27461, 40240, 19178, 46476,  8639, 13363, 39473, 18314, 33482,
       49204, 38824,  7365, 46789, 21222, 21081, 47868, 49902, 48018,
        6234, 14113, 31755,  4058, 22946,  2131, 15663, 39505, 20303,
       25692, 17437, 20060, 28407, 49597, 17845, 21708, 25834, 18473,
        7383, 47622,  7984, 44761, 44002, 23231, 21694, 30799,  9776,
       41191, 13102, 31429,  3695, 29635, 16659, 17123, 20373,  9153,
       28387, 23179, 41202,  9544, 28076, 37639, 25941, 48191, 27801,
       16020,  9342, 34119, 44747, 14469, 18690, 48309, 22580, 29254,
       46968, 30156,  6505, 46940, 36372, 18434, 16254, 43567, 15036,
       41100, 23637, 45645, 29404,  1952, 16073, 20054, 15668, 36671,
       45205, 34671,  3507,  4156, 30593,  1238,   674, 32590,   435,
       33971, 42313, 38092, 23022, 49951, 18912, 23421, 35598, 32280,
       26047,  2787,  5178, 31033, 41931, 49355, 28785, 32284, 15196,
       19721, 25383, 13663, 44891, 40116, 39853, 21642, 12333, 28614,
        3560,  9915, 30915, 33862, 23200, 41246,  5940, 20904,   831,
       40757, 17634, 49421, 35605, 12412, 25117,  6851, 15275, 43050,
        9582, 27238,  5076,   375,  3512, 34982, 27272, 49509,  8450,
        4261,  7738,   250, 23504, 40509, 11776, 31079,  5224, 22013,
       24486, 33640, 18885, 25479, 37611, 16711,  1724,  4206, 31395,
       43583, 26452, 26878, 44309, 44499, 31345,   493, 49326, 17409,
       36686, 34830, 20057, 49483, 38596, 27402, 29146, 48279,  9938,
       19226, 44575, 34533, 40518,  5084,   565, 12800, 40024, 18594,
        9407,  7147, 17626,  5947, 20731,  5971, 25459,  9232, 28640,
       16813, 31634, 44876, 37248, 49491, 24075,  8432, 30943, 38610,
       14501, 34902, 27716, 23489, 13217, 15456, 46135, 23107, 14524,
        8371,  9946, 29894, 41869, 34778,  5515,  5841, 12920, 11799,
       43251, 45672,  3212,  5629, 40386, 25278, 45602, 21245, 26450,
        9584, 47980, 18057, 44647, 14851, 19202, 49221, 41602, 21661,
        5666, 21199, 36086, 34813,  8124, 24043, 27404, 44589,  7401,
       26292, 11532, 32679, 42669,   427, 27330, 34400, 21627,  7659,
        7605, 12516, 14182,  3020,  6074, 31348,  5559, 18345, 39820,
        5791,  1973, 34064, 28393, 33194, 43398, 47540, 13843, 16272,
       21020, 25162,  3523, 40713, 15610, 46408, 39132,  6928, 34385,
       48182, 45834, 24580, 34940, 13877, 13849,  8995, 26267, 28626,
       21154,  6589,   599, 10298,   761, 47423, 17726, 16937, 34701,
       25852, 31351, 36058, 18134,  8944, 39349,  2259, 35033,   874,
       10479, 37844, 16576, 38650,  9646, 46240,  5567, 39692, 45988,
       24923, 33261, 27607, 14409, 17995, 13615, 30792, 48799, 26115,
       49409, 32168,  8625, 33561, 38911, 35783,  5834, 19265, 30716,
       27792, 26405, 33198,  6040, 20353, 17334, 22358, 36941, 42335,
       41259,  6736, 13701, 27970, 43984, 29875,  9889, 40422,  4021,
        9442, 49588, 43608, 20616, 21933, 29999, 41948, 44217, 12964,
       17309, 46219, 43609, 45866, 29974, 42785,  8305, 17205, 21808,
       40076]), [5, 7, 4, 1]), (array([ 2818, 19498, 43780, 29418, 43121, 36659, 32407, 28871, 21289,
       37575, 23657, 40875,  4041, 10219,  5140, 42253, 41920, 27900,
       23111, 40015, 28713,  4675, 47967, 42420, 39043, 24627, 21008,
       41905, 22482, 19724, 15215, 13711, 41089, 26016, 31291, 44860,
        2637,  2985, 26222, 47315,  7571, 37413,  2853,  1612,  9434,
       10806, 16396, 31881, 37678, 43903, 39581, 41899, 16046, 17404,
       18796, 17142, 30823,  5776,   428, 15082, 21960, 31956, 22285,
       17176, 41840, 17158, 38406, 26828,  6671,  3977, 47262,   269,
       33568, 22240, 22538, 28010, 29059, 15936,  7708,  1638, 22464,
       13429,   915, 41128, 18878, 27025, 38793, 22127,   408, 32766,
       38017, 36713, 19805, 22182,  4489, 33784, 49360, 47073, 40638,
       31869, 20518, 26561,  1767, 16803, 39930,  7583, 18779, 22623,
        4121, 42473,  7302, 38378,   270, 33483,  2313, 49583, 17289,
       27311, 46280, 22199, 47512, 35218,   867,  5668, 37074, 41415,
       39371, 40976, 24076,   881, 18963, 28747, 41771,  2609, 15901,
       36549, 13407, 20380, 46346, 30220,   672, 49831, 29073, 37040,
       15652, 21614, 34767, 39264, 21887, 44404, 10698, 14970, 35197,
       25094,  4584, 41161, 43882,  8992,  1000, 33813, 26769, 18943,
       21745, 36250,  6903, 41363, 20172,  3846,  2308, 48816, 26101,
        2890, 42994, 27975, 47697,  1078, 40043, 34223, 21170, 26511,
       23908, 19086, 42058, 29000,  2281,  9229, 26622, 48606, 19876,
       37070, 46654, 38481, 37762, 27755,  2829, 28783, 22543, 39449,
       33538, 34390,  1043, 12568,  9880, 17201, 15252, 11139, 15495,
       49057, 10762, 40819, 15027,  4289, 26012, 49208,  9511, 25736,
        9028, 25433, 28390, 40166,  6258, 44605, 23840, 36031, 35019,
       42139, 27815, 14112,  6364,  1940,  2671,  5203, 47435, 36797,
       19824, 28514, 39567, 24027, 49783, 21401, 16078, 31233, 34402,
       15249,  9432, 15031, 13705,  6984, 26506, 10309,  4003, 13157,
       42617,  6238,   190, 44059, 39613, 26623, 47713, 40070, 31462,
       46453,  1479, 14054, 17167, 27352, 35896,  6667, 30087, 18644,
        4272,  1512, 28016, 30639, 44416, 39336, 20762, 43408, 22340,
        9068, 26440,  3305,  9960, 14728, 43705, 12544, 39702, 30601,
       16700, 20942, 10876, 10139, 13233,  7455, 49253, 26105, 17130,
        4222, 35671, 23669, 27410, 29600, 47378, 21028, 36803, 23883,
       36744, 39075, 43175, 39139, 17617, 28768, 45617, 34615, 21541,
       14710,  3482, 33124,  4828,  1914, 48374,  9648, 20873, 36892,
       25783, 15322, 28134,  5600, 15630, 27153,  9567, 22518, 46068,
       48959, 27618, 21998, 42371, 21710, 48805,  3430, 25632, 31037,
       33225, 48081, 37710, 34468, 26538, 10688, 42270, 23960,  5687,
       15951, 37335, 25047, 28707, 32465,  7287,  8229,  5855,  1357,
       18593, 40270,  2721, 35726, 36108, 41240, 23872, 48641,  1261,
       39802, 23148, 43699, 15699, 27027, 18295,  6459,  4916,  1721,
       13355, 10016, 21568, 36255,   987, 35778, 31938, 29362, 39916,
       33119, 47791, 37064, 45635, 22862, 36876,  1289,  3681, 22440,
       14049, 13684, 12128, 18307, 47391, 45608, 28985, 32207, 44362,
       41281, 22147,    62, 33888, 17854,  4287, 30483, 45115, 44570,
       35909, 12319, 33651,  4474, 22600, 38808,  1702, 48457,  3313,
       30488,  3770, 13836, 13883, 46392, 32974, 21716, 23707, 40881,
        4794,  8678,   901, 19748, 23845, 20962, 45784,  7443, 10004,
         259, 38339, 14866,  3765, 34825, 48874, 46386, 35828, 41647,
       34248,  7535, 20740, 37641, 29734, 21830, 23399, 19768,  9269,
       41403, 30210, 15914, 24270,  2663, 49567, 41941, 10763, 19336,
        8831, 39350,   430, 26108, 48943, 34527, 13720, 11993, 23011,
       28450, 17252, 33570, 38322,  5904,  8384, 32281, 35628, 23818,
       12286, 28295, 41187,  8968, 30660,   971, 37483, 10245, 22218,
       27436, 21164,  2640,  4936, 46617,  7218,  7751, 23279, 42434,
        3296, 18627, 44270, 43549, 22050, 41265,   336, 19180, 35151,
        7110, 35443, 16749, 23133, 49352, 26512, 16636, 29471, 15625,
       15155, 31053, 33408, 32300, 24067, 11045, 11254, 30967, 49981,
       31549, 15773, 30815, 47611, 14649, 41554, 28200,  2488, 31939,
       40707, 41878, 33480, 47144, 12921,  1624,  2842, 49685,  2691,
       36523, 39977, 28711, 17145, 24659, 43239,    28,  7634, 15958,
       46077, 44450, 14921, 22820, 47591, 43586,   979, 15643,  2703,
       37466, 43238, 38869, 20810, 48235, 37214, 37987, 37977, 11997,
       42967, 31547, 48505, 18512, 22749, 24664, 12997, 18550, 23817,
       37234, 28219, 13005, 41788, 15409, 46049,  5844, 26351, 15864,
       43460, 10489, 15737, 45255, 13772, 37154, 18841, 47126, 40285,
       22677,  5998, 47374, 48969, 32017, 37960, 40111, 22750, 30125,
       20253, 43529, 40981, 28084, 43682, 46884, 15140, 34255, 19426,
       36965, 37241, 22203, 26360,   526, 20173,  8279,  1595,  9550,
       47855, 20769, 39752, 17402,  8361,  8504, 46185, 35564,  3957,
       34193,  8541,  6415, 47344, 49292, 42629, 26953, 20343,  1813,
       13466, 27535,  7382, 45729, 29670, 17150, 30425, 28228, 29329,
        8551,  8233,  2155, 36160, 16897,  5957,  5187, 26114, 39067,
       23654, 40486, 39596, 23671,  4924, 47384, 27417, 11810, 45712,
        5790, 28260, 47664, 36428,  2442, 12124, 32519, 44942, 37875,
       44162,  8939, 10237, 28769, 29376, 20485,  2461, 13855, 39005,
       43746, 39467,   372, 31476,  2327, 14787, 10447, 34784, 25017,
       19307,  4527, 30869, 37805,  7119, 19990, 25147, 24658, 45764,
       29361, 42643, 48860, 28892, 22985,  3383, 37288, 44563, 27860,
       34744,  8263,  4989, 24241, 28744, 29278, 36976, 11957,  1526,
       28903,  5501, 39565, 12261,  5931, 30681, 42769, 32929, 19746,
       45226, 25429, 41822, 16058, 19452, 15256, 42938, 18502, 24799,
       49390,  1974, 33209, 29121, 46507, 25817, 47519,  2339, 37512,
       48000,   617, 14659, 34832, 27506, 42113, 34862, 36594, 15779,
       21585, 21333, 17515,   261, 14871, 28784, 31899,  7706, 14859,
       23338, 33063, 16286, 32161, 29330,  7077, 24566, 44412, 15512,
       35105, 10100,   227,  1052, 49381, 45977, 31890, 29633,  4609,
       39256, 11890, 22450, 47968,  3485, 36938, 42292, 28958,  8221,
        1631, 42065,  4101, 15740, 32736,   168,  8475, 36388, 40046,
       23598, 11580, 22882, 16250, 35145, 29314, 25608, 16850, 49616,
       20777, 14713, 25700, 26059, 15891, 26737, 19616, 38278, 41289,
       11664,  2769, 38420,  6295, 48617, 40537,  8175,  4230, 19710,
       33114,   699,   396, 42281, 16275, 19399, 36672, 11683,  1571,
       36751, 36033,  2390,  8812, 19867, 20197, 48995, 15205,  6429,
        6191, 48636, 37269, 24284, 29311, 42918, 30014, 32603, 20214,
       43447, 34145, 40576, 28347,   311, 22165,  1410, 46731, 18926,
       38974, 20510, 26578, 39109, 44541, 45704, 31512, 38725, 41279,
       19628, 49894, 16855, 48970, 21437, 14900, 41794, 17856, 29325,
       25652, 39204, 49705, 20783,  5959, 10491, 11791, 18063, 43217,
       38892,  6272, 20988, 37995,  6120, 10393, 48740, 20914, 11427,
       17755,  8294,  3386, 24398,  2039, 28906, 43641, 29188, 49494,
       13526, 34068, 38571, 26637, 45580,  2268,  1694, 45859, 42211,
       26794, 15832,  2280,  9549,  7827, 23662, 21153,  4423, 12923,
       25150, 30245,  5759, 41322, 38549, 29285, 32410, 42734, 34426,
       43847,  6371, 14722, 22529,  6701,  2173, 22586, 18495, 10748,
       39240,  3840, 31107,   947, 11579,  6899, 37203, 14067, 46259,
       46359, 28539, 43132, 30486, 22163, 19500, 35454,  8875, 15496,
       21073, 12774, 33327, 23311, 41356, 19609, 11698, 39974, 12622,
       20874, 48845, 10953, 23578, 49121, 36085, 45011, 49436, 18206,
        2511, 15704, 43605,  9765,  8571, 12373,  3414,  2227, 36872,
        3059]), [9, 8, 4, 1]), (array([ 6304, 37804, 27012, 40216, 43318, 23884,  9846,  6309, 25738,
        3864, 20721, 17440,  3813, 15453, 26707,  4197, 47851,  2127,
       29685, 20447, 10869, 41892, 32250,   846,  8210,  6607, 40165,
       41826, 44173, 15421, 19820, 16619,  9293, 11998, 11477,  6014,
       40246, 37973, 43462, 37662, 11387, 16366, 46825,  7753,  7086,
        1608, 46058,  6405, 34099, 41285,   998, 13700, 16301, 15603,
        9034, 11564, 22420, 38304, 12428, 28953, 25490, 15383,  4558,
       27233, 35124,  1510,  8319, 30868, 33133, 29810, 42752, 16547,
       22030, 49970, 24726,  1958, 43922,  4674, 18814, 37770,  6619,
       12111,  8356, 13001, 30817, 38992,  1057,  7432, 35092, 22620,
        9024, 18127, 29012, 14368, 31178, 18998, 28864, 21686, 43843,
       46121, 21053, 22913, 14108, 26849, 10193, 24749, 47100, 10746,
       41412, 35128, 25882, 32451,  1539, 40584, 12174, 42930, 39108,
       27407, 15974, 10575, 47901, 23376, 11231, 13349, 20252, 13013,
       11120, 22406, 41016,  8781,  6913, 30266,  4402, 49079,  1556,
       34984, 44263, 27909, 20329,  5321, 47554, 38679, 21085, 19126,
       32297, 46542, 31179, 32846, 22919, 14118, 28253, 25281, 14070,
       32541, 33092,  2447,  2011, 20820,  1895, 23498, 22793, 49741,
       46182, 41229, 10958, 29215, 43411,  7906, 39294, 15545,  5455,
        2353, 45502, 11047, 39885, 11714, 27275, 48907, 29705, 20770,
       46215,  3571, 26357,  5823, 36211, 29520, 33267,  3905, 22294,
       36422, 32936,  8645,  9830, 11105, 43328,  7671,  6649, 46157,
       14089, 29839, 48040, 14171, 14703, 40232, 26044, 24060, 38364,
       17615,  1961, 17243, 45646, 16070,  3194, 24081, 48999, 11724,
       37906, 15053, 16295, 23145, 41917, 10942, 17047, 33463,  9720,
        4025,  5520, 44550, 22662, 27114,  5218, 22411,  1573, 38781,
       35323, 15247, 38932, 46417, 46605,  6540, 47722, 17018, 26559,
       45382, 38634,  9183, 23594, 18570, 21510, 42744, 14472, 27738,
        7151, 37255,  2784, 32899, 11137, 20558, 15977, 37686, 18143,
       25200,  9940, 44644, 32377, 33474, 40372, 33083, 30632, 11286,
       16833, 48733, 14595, 32807,  6626, 16739, 26425, 21099, 14811,
        7466, 12583, 17981, 45951, 39499, 20651, 31621, 36423, 38349,
       31040, 38038, 37856, 20673, 43826,  8055,  6616,  2957, 11454,
       16215, 20038, 30940, 10926, 40924, 28664,  2789, 14619, 10440,
        4616, 16462,  1258, 16581,   802, 11270, 34307, 43990, 34488,
       25101, 38592, 15314, 33519, 14785,  4038,  1107, 14157, 33054,
       35653, 30572, 24773, 38801, 44535, 20073,  3963, 14383, 38216,
        3569, 29625, 19280, 14285, 10671,   912, 14601,  4168, 15398,
       15789,   990, 28667, 26386, 47171, 18163,  3484, 37373, 41024,
       13934, 33183,  7807, 47936,  1475, 13487,   885,  2514, 17693,
       27239, 42227,  2004, 30437, 11003, 24927, 29779, 33175, 33350,
        6028, 30786, 26999, 39368,  7521, 20009, 30631, 27186, 17302,
       43244, 31884, 23673, 40398, 27546, 25440, 40403, 31454, 41884,
       11843, 42894, 34203, 13290,  7330, 27948, 42428, 47710,  4511,
       48779, 30665, 17834, 36502, 26976, 24064, 39847, 15443, 14106,
       40348, 33498, 40217,  9803, 46564, 43572, 21262, 29180, 34222,
       48169, 22511, 26588, 46819, 11884, 16446, 18204, 14229, 27498,
        6334, 26176, 43823, 33531, 32409,    90, 25982, 49914, 13814,
       11779, 10412,   522, 23062, 15369, 32763, 32480, 15589, 44634,
         804, 38459, 11089, 13273, 34873,  8706, 11587, 32057, 34885,
       21500, 13081,  2191, 27865, 47368, 27685, 33842, 36382,  5647,
       44879, 48414, 25030, 39062,  1798, 28834, 46575,   283,  3558,
       38182, 48092, 48608, 11443, 46214, 27594, 25398, 23453,  4237,
       26308,  6177, 43287, 10378,  5219, 31361,  4656, 17955,  8219,
        4634, 17273, 24623, 45533, 15929, 16609,  5384, 43692, 21063,
       26630, 19615, 33776, 47584, 12060,  2450, 32094, 22619,  5704,
        6943, 18099, 23634, 28174, 47908, 32478, 22743, 16908, 45133,
        8688,  7903, 14134, 40530, 40794, 28885, 33383, 45598,  5706,
       45786, 41625, 38354, 47795,  5298, 24221, 48195, 40098, 27017,
        5676,   621, 46153, 26497, 29070, 36142, 26329, 19059, 15126,
        9018,  1284, 27372, 14786, 30522, 34969, 20280,  8913,  8631,
        1704,  9750, 21052, 10604, 45022,  7043, 45110, 26943, 49687,
        6652, 12717,   477,  1348, 14853, 35378,  4137, 49586, 39945,
       36552,  1657, 30725, 21693, 33099,  9592,  4642, 41770, 43420,
       16105, 16880, 23943, 21794, 17450, 10299, 32523, 48179, 31306,
        8701, 11845, 17707, 23076, 48360, 30614, 48605, 41608, 39209,
       40301,  1984, 43897, 19704, 26882, 39878, 29852,  9364, 47576,
       46145, 27531, 41077, 39293,  9500,  1396, 45396, 21582, 45410,
        2190,  5225, 33221, 45756, 21188, 44994,  9507, 35395,  3556,
       14726,   959, 25779, 41103, 31954, 17613, 26886, 33215,  1937,
       45293,  9041, 27259, 18409,  9285, 14798, 36345, 24265, 43494,
        1339, 29128, 15734,  5829, 15454, 20341,  3283,  4846,  8987,
       40036, 22274,  7670, 49475, 37528,  9273, 19528, 17260, 43222,
       42695, 34776, 41097, 34727, 20629, 24925, 32494,  8137, 22385,
       20967, 22369,  3046, 38728, 16672,  5227, 37677, 47232, 48796,
       21649, 33080, 10500, 36071, 36731,  8114, 30077,  9779,  4885,
       11794, 47147, 44750, 47120, 32201, 20435, 18197, 44061,  2528,
       32253, 16263, 25798, 24572, 26975, 42907,  6344,  7394, 25142,
        3509, 40219, 17039,  5880, 48429, 46591, 47796,  2105,  7957,
       35457,  8066, 37741, 31898, 32029, 12975, 39946, 15238, 14635,
        9250,  4199, 15909, 16101,  9749, 18185, 15730, 27303, 44283,
       25643,  2297, 18264, 26233, 23739, 39541,  1406,  1640,   381,
       33688, 27762,  3913, 40077, 30243, 16525, 35050,  6049, 44525,
       17552, 48945, 31588, 37588,  5769, 38929, 27584, 25300, 35694,
       33390,   524,    99,  8085, 28089, 36331, 19575, 26424,  3836,
        3103, 45856, 18458,   301, 31102, 13108, 31445,  1240, 48669,
       23533, 28559,  7300,  8556, 34530, 31251, 30039, 15662, 39277,
       12741, 49769,  8001, 20840,  5897, 14593,  6375, 10403, 16023,
       36236, 33625, 36703, 43145, 19063, 33650, 49820, 17030,  4764,
       44818, 42707, 20605, 14519, 11007, 20442, 14598, 33126, 33624,
       44415, 16834, 42606, 12894,  2958,  4299,  2656, 48375, 41984,
       17621, 41888, 25596, 33050, 16964, 34263, 38078, 26741, 16588,
        8690, 30471, 17732, 47178, 29029, 14643,  9085, 22697, 16962,
       31523, 26746,  6779, 24268,  4942, 31329, 45001, 31830, 13002,
       17860, 22745,  6241,  8893, 47060, 17268, 22303, 41579, 31245,
       49230,  7540,  7375, 24620, 44725, 27636,  1494, 29394, 31587,
       43195, 33399, 31711,  3009,   226,  4578, 42031, 49447,   126,
       22545, 28394, 11817, 13037, 42121,  4464, 25377, 46323, 11242,
       14662, 45105, 11829,  4661, 23546, 13470, 42341,  6988, 12893,
       30182, 18927, 33615, 19301, 45946,  7214, 25910,  1251, 27194,
       37239,  5635, 28245, 46679, 32348, 16745, 32737, 19983, 24068,
        7503, 25961, 44971,  3233, 14964, 26471,  6495,  8779, 45715,
       42991, 20273, 23142, 40749, 32311, 38902, 30062, 13444, 45968,
       21888, 46730, 45097, 13311, 33598, 19880, 12560, 23088, 28957,
        8090, 26230,  1548, 30244, 41563, 22689, 41736, 25827, 29230,
        9012, 37755, 37320, 33237,  6456, 47453, 21893, 24561, 38682,
        5975, 49875, 35878, 46928, 22145,  4607, 41862, 26074, 21530,
       34791, 37253,  2067, 27228, 34618, 19484, 33569,  2597, 32042,
       13899, 24157,  8201,  2320,  4352, 30165,  9318, 33646,  9459,
       18223, 19242, 21230, 17274, 13647, 49392, 41739, 41026, 38838,
       17457, 28344, 27773, 41701, 47208, 30086, 33196, 39824, 41378,
       40762]), [3, 2, 4, 1])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.255, 0.04439314603805542)
DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.245, 0.04447525298595428)
DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.244, 0.045177888870239255)
DC 3, val_set_size=1000, COIs=[3, 2, 4, 1], M=tensor([3, 2, 4, 1], device='cuda:0'), Initial Performance: (0.209, 0.04494925320148468)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {0, 6}
D07: 1000 samples from classes {0, 6}
D08: 1000 samples from classes {0, 6}
D09: 1000 samples from classes {0, 6}
D010: 1000 samples from classes {0, 6}
D011: 1000 samples from classes {0, 6}
D012: 1000 samples from classes {5, 7}
D013: 1000 samples from classes {5, 7}
D014: 1000 samples from classes {5, 7}
D015: 1000 samples from classes {5, 7}
D016: 1000 samples from classes {5, 7}
D017: 1000 samples from classes {5, 7}
D018: 1000 samples from classes {8, 9}
D019: 1000 samples from classes {8, 9}
D020: 1000 samples from classes {8, 9}
D021: 1000 samples from classes {8, 9}
D022: 1000 samples from classes {8, 9}
D023: 1000 samples from classes {8, 9}
D024: 1000 samples from classes {2, 3}
D025: 1000 samples from classes {2, 3}
D026: 1000 samples from classes {2, 3}
D027: 1000 samples from classes {2, 3}
D028: 1000 samples from classes {2, 3}
D029: 1000 samples from classes {2, 3}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.06242937207221985) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06935057532787323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.10054781334102154) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09110854172706603) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.44, 0.07027108192443848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.07565721029043197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.10466695122420788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.323, 0.11809220957756042) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.08399075710773468) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.405, 0.08414981603622436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.10775397792458534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.355, 0.1430300070643425) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.09190552899241447) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.10456353434920311) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.15356273053586483) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.356, 0.16536943846940994) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.11256972487270832) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.11904050388932227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.17955271257087588) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.354, 0.1911297039538622) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.12555305766686797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.392, 0.12016960312426091) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.20252362125553192) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.376, 0.1672702697366476) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.13640131926350296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.12200692869722843) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1993113804385066) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.384, 0.1897293770611286) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.1368831439372152) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.1383049213439226) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.22230591307021677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.387, 0.205445771753788) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.15904230438452213) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.13590771255642176) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.251293695660308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.379, 0.1797317993193865) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.15739924981538206) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.422, 0.15744038719683887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.24946794794872404) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.2138650175333023) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[1, 4], M=tensor([0, 1, 4, 5, 6, 7], device='cuda:0'), Initial Performance: (0.0, 0.3024236650466919)
DC Expert-0, val_set_size=500, COIs=[0, 6], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.938, 0.005567504147067666)
DC Expert-1, val_set_size=500, COIs=[5, 7], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.844, 0.013536606922745704)
SUPER-DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x72ba305011f0>, <fl_market.actors.data_consumer.DataConsumer object at 0x72ba30228d00>, <fl_market.actors.data_consumer.DataConsumer object at 0x72ba3017cfa0>, <fl_market.actors.data_consumer.DataConsumer object at 0x72ba04233940>, <fl_market.actors.data_consumer.DataConsumer object at 0x72ba4a0faee0>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO2', '(DO1']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.004127507352735847) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.842, 0.014867444228380919) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.21396257594414056) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.21732402542233467) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.917, 0.007959464845247568) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.03893713650852442) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.56, 0.03947352324426174) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.003754283563233912) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.011353756673634053) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.2019252243321389) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.389, 0.20565791372954845) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.006377910486422479) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.721, 0.02872029834985733) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.591, 0.041179577484726906) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.003755414746934548) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.011819861702620983) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.20880340154562146) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.2291084222048521) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.012764433719217778) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.047137636795639995) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.517, 0.06900667379796505) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.004473389737773686) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.012388505913317204) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.20281571475602686) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.22353157429397105) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.944, 0.006419177708681673) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.607, 0.05029303043335676) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.08229575799405575) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004207797614624724) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.012727144047617912) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.1900108699519187) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.22569028033316135) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.00877004381781444) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.687, 0.03513099190592766) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.579, 0.04869123400747776) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO5', '(DO3']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.004229386326740496) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.86, 0.013904402077198029) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.18591010761074722) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.28172453477978704) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.944, 0.007232302551856264) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.656, 0.04907999721169472) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.55, 0.06044437692314386) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004418988283025101) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.012638215839862823) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.17312875663675367) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.2860072252750397) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.005513405551202596) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.7, 0.029518605902791022) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.632, 0.04321546119451523) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0036861084152478726) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.015237985994666815) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.16964567117765547) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.27693781098723413) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.009658857614296721) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.0634143395870924) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.08081659153848887) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004216449702711543) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.015473596382886172) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.17540301353670656) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.26421140231192114) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.010421143836947522) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.06078840420022607) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.08438127960637212) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004476468621753156) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.013988821275532245) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.1745607022047043) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.29483157986402514) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.007782833977136761) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.705, 0.037435181349515914) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.617, 0.046825285650789736) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO1']
DC 2 --> ['(DO0', '(DO3']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004705489826446865) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.014980192262679338) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.16164412384387106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.391, 0.3265850615501404) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.933, 0.007206337568117306) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.683, 0.03775641690194607) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.06404453778639435) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.004555014982586727) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.01394092222303152) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.449, 0.17430713838431985) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.3229255262315273) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.004832015942898579) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.717, 0.03574278699606657) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.636, 0.044881325662136075) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0035187767061870545) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.01420122016966343) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.16166685731336475) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.3122727960646153) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.007586066197662149) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.643, 0.05003344184905291) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.513, 0.08570785227045417) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004227683992299717) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.013387944988906383) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.16720248041581362) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.3478995704650879) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.005873983337398385) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.617, 0.06359561578929425) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.509, 0.09606576376408339) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0035677349295001478) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.013476253665983677) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.16138897153921425) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.409, 0.3178991679251194) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.009241860074969736) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.06233097901009023) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.512, 0.09573117703199387) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO0', '(DO1']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.004850818621605867) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.013131689630448818) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.17397897907020524) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.3649251551032066) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.959, 0.004907162410090677) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.659, 0.05400856038928032) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.49, 0.11249394039437174) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0043962418905284725) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.013823337271809578) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.1734808777635917) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.33083755159378053) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.006171521411422873) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.573, 0.08838519757986069) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.1073730627335608) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004327275464384002) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.0147400903403759) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.17658057682961226) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.3335383904874325) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.004746130337938666) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.691, 0.04656796824932098) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.55, 0.07200913746654987) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004553636466240277) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.014777560468763114) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.18727117575984448) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.28380913931131363) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.0053008524338147255) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.673, 0.0501801345795393) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.09268485552072525) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004290300063730683) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.858, 0.015583296611905098) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.17187949767755345) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.3245014344751835) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.0072750191423092475) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.595, 0.08772367969015613) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.13688933105021714) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO2', '(DO1']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004556745659836451) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.017703266888856886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.1740441382266581) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.3120652152448893) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.961, 0.0053413564196089286) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.651, 0.06502061221748591) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.08345817120373249) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004436868886143202) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.013403407547622918) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.19023985480098055) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.34334443168342116) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.00695458524744754) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.05886896278895438) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.09196123117208481) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0046960962257580835) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.014687983594834805) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.21808019507490098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.3324373926669359) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.006922847410787653) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.0630022142007947) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.503, 0.09938487233966589) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004465142059198115) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.014284164562821389) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.2043118249529507) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.33507137748599053) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.015204386561548745) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.59, 0.08619408253766596) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.12401573947444558) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004420109082944691) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.014033919431269169) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.21280315943667666) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.3078152932822704) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.007638202011818066) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.652, 0.05377110855281353) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.519, 0.1088713323622942) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO5', '(DO2']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004107662930415245) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.854, 0.016280566453933717) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.20645634380728006) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.30642455820739267) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.007761082914556027) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.653, 0.04915592380613089) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.09792219178378582) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004977530915261014) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.013308265462517739) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.2056139448378235) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.328941770285368) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.008587958949312451) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.05352987864427269) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.502, 0.10312158824503422) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004246622654449311) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.015431493744254112) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.445, 0.19973192601371556) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.3107540704011917) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.009875285481186439) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.614, 0.08640639236941934) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.15401092506945133) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004649901660333853) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.015886043809354305) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.21458589586312882) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.2919933432340622) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.010022587424900848) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.603, 0.0621677357852459) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.09812879296019673) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.005509056553361006) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.86, 0.0167637572363019) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.21044911045650952) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.33286566951870916) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.963, 0.01402577118796762) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.08732453486323356) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.5, 0.1049650136064738) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO3', '(DO0']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004285401160741458) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.013224848475307227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.20706523453723638) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.2992829284667969) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.011204537100100425) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.06918052014522255) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.11617146759480237) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005741756942175926) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.012435030698776245) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.24119389910518657) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.3451630851626396) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.011259816161184573) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.06305459253210574) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.503, 0.12257391491532325) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.00446957312367158) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.01283188484236598) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.22064276103000158) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.3195404217094183) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.010380118506201142) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.569, 0.07877261379454285) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.12588347789645196) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0037980235779978104) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.856, 0.015279770515859128) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.23637546660087538) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.3093393190205097) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.014569506875106008) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.566, 0.0854614333640784) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.506, 0.09793947895616292) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0043373425488971405) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.012424828216433524) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.44, 0.19885402902076021) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.3069999773651361) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.014810961156701939) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.613, 0.07375918015278876) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.10744064565002918) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO2', '(DO1']
DC 3 --> ['(DO4']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005751396227951773) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.014376179229468107) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.253402545016841) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.29863434843719006) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.959, 0.005233323467662558) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.75, 0.030365810289978982) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.602, 0.05192104405909777) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0049978223673879255) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.01262326042354107) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.2059705190442037) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.2915055101364851) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.955, 0.009394041824548368) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.08346937674586662) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.497, 0.10986935928277672) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.005149908761456572) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.012651749920099974) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.446, 0.2581504957349971) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.29884742538630965) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.007329397465829971) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.07173250887915492) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.518, 0.11809611555933952) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004811412131857651) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.014556468717753887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.22882126950798556) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.30206027437746524) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.961, 0.008727294625454988) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.57, 0.0886360498063732) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.51, 0.11900640178099274) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.005249119706320016) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.013818211622536183) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.22469826759956776) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.2779772319197655) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.012070908340762231) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.624, 0.07321977760409937) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.08764013799186796) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.255, 0.04439314603805542), (0.276, 0.06242937207221985), (0.44, 0.07027108192443848), (0.455, 0.08399075710773468), (0.465, 0.09190552899241447), (0.459, 0.11256972487270832), (0.464, 0.12555305766686797), (0.461, 0.13640131926350296), (0.475, 0.1368831439372152), (0.461, 0.15904230438452213), (0.469, 0.15739924981538206), (0.639, 0.03893713650852442), (0.721, 0.02872029834985733), (0.632, 0.047137636795639995), (0.607, 0.05029303043335676), (0.687, 0.03513099190592766), (0.656, 0.04907999721169472), (0.7, 0.029518605902791022), (0.609, 0.0634143395870924), (0.604, 0.06078840420022607), (0.705, 0.037435181349515914), (0.683, 0.03775641690194607), (0.717, 0.03574278699606657), (0.643, 0.05003344184905291), (0.617, 0.06359561578929425), (0.629, 0.06233097901009023), (0.659, 0.05400856038928032), (0.573, 0.08838519757986069), (0.691, 0.04656796824932098), (0.673, 0.0501801345795393), (0.595, 0.08772367969015613), (0.651, 0.06502061221748591), (0.626, 0.05886896278895438), (0.626, 0.0630022142007947), (0.59, 0.08619408253766596), (0.652, 0.05377110855281353), (0.653, 0.04915592380613089), (0.639, 0.05352987864427269), (0.614, 0.08640639236941934), (0.603, 0.0621677357852459), (0.554, 0.08732453486323356), (0.618, 0.06918052014522255), (0.618, 0.06305459253210574), (0.569, 0.07877261379454285), (0.566, 0.0854614333640784), (0.613, 0.07375918015278876), (0.75, 0.030365810289978982), (0.563, 0.08346937674586662), (0.628, 0.07173250887915492), (0.57, 0.0886360498063732), (0.624, 0.07321977760409937)]
TEST: 
[(0.2515, 0.043328432649374006), (0.2885, 0.05995646780729294), (0.4465, 0.06739776784181595), (0.4545, 0.0804382722377777), (0.46875, 0.08793868029117584), (0.46075, 0.10819000673294067), (0.46475, 0.12106116461753845), (0.46375, 0.1310386021733284), (0.475, 0.13236147272586823), (0.4615, 0.15334183567762374), (0.46875, 0.1515594100356102), (0.6535, 0.0362905383259058), (0.71875, 0.027457794599235058), (0.62525, 0.04770275554060936), (0.59825, 0.05138922916352749), (0.6955, 0.033239762991666796), (0.6465, 0.04789462123811245), (0.71675, 0.027838457830250265), (0.588, 0.06343251557648182), (0.601, 0.05873287153244019), (0.71175, 0.034869913078844544), (0.704, 0.03526301770657301), (0.72, 0.029680034399032593), (0.65075, 0.046557154044508935), (0.625, 0.05832916605472565), (0.6195, 0.060609287425875666), (0.668, 0.05054136341810227), (0.5705, 0.08697933545708657), (0.6935, 0.04244335998594761), (0.67375, 0.0473743856549263), (0.58675, 0.08381384864449501), (0.6475, 0.06158714255690575), (0.61525, 0.0595499005317688), (0.606, 0.06142983634769916), (0.56875, 0.0877712190747261), (0.64925, 0.055015583291649815), (0.63675, 0.050568396493792536), (0.63175, 0.05266031338274479), (0.617, 0.08696257996559142), (0.611, 0.059781081527471545), (0.54775, 0.08422490751743317), (0.60825, 0.06920341065526009), (0.599, 0.06369360379874706), (0.573, 0.07918088388442993), (0.58025, 0.08446345619857311), (0.6095, 0.07434843876957893), (0.74075, 0.028985210672020912), (0.55875, 0.08291037890315056), (0.61425, 0.0731484138071537), (0.5715, 0.08958792763948441), (0.62725, 0.06954992389678955)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.90      0.37      0.52      1000
           1       0.66      0.96      0.78      1000
           4       0.50      0.85      0.63      1000
           6       0.78      0.33      0.46      1000

    accuracy                           0.63      4000
   macro avg       0.71      0.63      0.60      4000
weighted avg       0.71      0.63      0.60      4000

Collaboration_DC_1
VAL: 
[(0.245, 0.04447525298595428), (0.25, 0.06935057532787323), (0.352, 0.07565721029043197), (0.405, 0.08414981603622436), (0.389, 0.10456353434920311), (0.376, 0.11904050388932227), (0.392, 0.12016960312426091), (0.382, 0.12200692869722843), (0.384, 0.1383049213439226), (0.416, 0.13590771255642176), (0.422, 0.15744038719683887), (0.56, 0.03947352324426174), (0.591, 0.041179577484726906), (0.517, 0.06900667379796505), (0.499, 0.08229575799405575), (0.579, 0.04869123400747776), (0.55, 0.06044437692314386), (0.632, 0.04321546119451523), (0.526, 0.08081659153848887), (0.511, 0.08438127960637212), (0.617, 0.046825285650789736), (0.561, 0.06404453778639435), (0.636, 0.044881325662136075), (0.513, 0.08570785227045417), (0.509, 0.09606576376408339), (0.512, 0.09573117703199387), (0.49, 0.11249394039437174), (0.524, 0.1073730627335608), (0.55, 0.07200913746654987), (0.528, 0.09268485552072525), (0.486, 0.13688933105021714), (0.522, 0.08345817120373249), (0.528, 0.09196123117208481), (0.503, 0.09938487233966589), (0.493, 0.12401573947444558), (0.519, 0.1088713323622942), (0.523, 0.09792219178378582), (0.502, 0.10312158824503422), (0.498, 0.15401092506945133), (0.526, 0.09812879296019673), (0.5, 0.1049650136064738), (0.516, 0.11617146759480237), (0.503, 0.12257391491532325), (0.493, 0.12588347789645196), (0.506, 0.09793947895616292), (0.516, 0.10744064565002918), (0.602, 0.05192104405909777), (0.497, 0.10986935928277672), (0.518, 0.11809611555933952), (0.51, 0.11900640178099274), (0.536, 0.08764013799186796)]
TEST: 
[(0.23225, 0.043421665340662004), (0.25, 0.06652063864469528), (0.345, 0.07260621449351311), (0.4005, 0.08074362868070603), (0.388, 0.10054817044734955), (0.381, 0.11419055730104447), (0.39725, 0.11531978017091751), (0.3775, 0.1179304256439209), (0.38325, 0.13432818323373794), (0.41475, 0.1329578117132187), (0.41625, 0.15371146613359452), (0.56075, 0.03800373265147209), (0.61025, 0.03754348982870579), (0.5415, 0.06316724961996079), (0.50825, 0.07768943178653717), (0.59325, 0.04648787985742092), (0.57025, 0.058996072590351105), (0.64425, 0.04209390890598297), (0.528, 0.07887469494342804), (0.529, 0.08117965674400329), (0.61825, 0.04705694565176964), (0.55825, 0.06140273952484131), (0.62825, 0.04550787183642387), (0.524, 0.08435971614718438), (0.53, 0.08969017335772514), (0.51175, 0.09493792113661766), (0.49725, 0.10681572195887566), (0.536, 0.10339610886573791), (0.56075, 0.06647349330782891), (0.53125, 0.08770353358983994), (0.48975, 0.1316111549139023), (0.53375, 0.07916747391223908), (0.546, 0.0879463433623314), (0.512, 0.09270404773950577), (0.49975, 0.11776129877567292), (0.5235, 0.10309436282515526), (0.5275, 0.0925578923523426), (0.5205, 0.09571743336319924), (0.499, 0.1463209485411644), (0.52925, 0.0916202973127365), (0.509, 0.10099947428703308), (0.525, 0.10839029651880264), (0.518, 0.11446916303038597), (0.5065, 0.12145023971796036), (0.51275, 0.09276575133204461), (0.548, 0.09675192201137543), (0.613, 0.04822501145303249), (0.5125, 0.10459778985381127), (0.523, 0.10914684161543846), (0.5235, 0.11136574235558509), (0.5515, 0.08161836943030357)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.72      0.96      0.83      1000
           4       0.40      0.89      0.55      1000
           5       0.75      0.20      0.31      1000
           7       0.89      0.16      0.27      1000

    accuracy                           0.55      4000
   macro avg       0.69      0.55      0.49      4000
weighted avg       0.69      0.55      0.49      4000

Collaboration_DC_2
VAL: 
[(0.244, 0.045177888870239255), (0.25, 0.10054781334102154), (0.252, 0.10466695122420788), (0.38, 0.10775397792458534), (0.413, 0.15356273053586483), (0.395, 0.17955271257087588), (0.436, 0.20252362125553192), (0.441, 0.1993113804385066), (0.441, 0.22230591307021677), (0.464, 0.251293695660308), (0.456, 0.24946794794872404), (0.443, 0.21396257594414056), (0.442, 0.2019252243321389), (0.459, 0.20880340154562146), (0.454, 0.20281571475602686), (0.454, 0.1900108699519187), (0.444, 0.18591010761074722), (0.44, 0.17312875663675367), (0.439, 0.16964567117765547), (0.439, 0.17540301353670656), (0.458, 0.1745607022047043), (0.447, 0.16164412384387106), (0.449, 0.17430713838431985), (0.444, 0.16166685731336475), (0.433, 0.16720248041581362), (0.442, 0.16138897153921425), (0.444, 0.17397897907020524), (0.452, 0.1734808777635917), (0.441, 0.17658057682961226), (0.442, 0.18727117575984448), (0.441, 0.17187949767755345), (0.439, 0.1740441382266581), (0.445, 0.19023985480098055), (0.427, 0.21808019507490098), (0.45, 0.2043118249529507), (0.448, 0.21280315943667666), (0.431, 0.20645634380728006), (0.431, 0.2056139448378235), (0.445, 0.19973192601371556), (0.438, 0.21458589586312882), (0.443, 0.21044911045650952), (0.448, 0.20706523453723638), (0.448, 0.24119389910518657), (0.426, 0.22064276103000158), (0.448, 0.23637546660087538), (0.44, 0.19885402902076021), (0.448, 0.253402545016841), (0.441, 0.2059705190442037), (0.446, 0.2581504957349971), (0.441, 0.22882126950798556), (0.433, 0.22469826759956776)]
TEST: 
[(0.22875, 0.04402987053990364), (0.25, 0.09631045669317245), (0.25525, 0.0999217810332775), (0.39225, 0.10327717873454094), (0.41875, 0.14785195577144622), (0.405, 0.17185581016540527), (0.43225, 0.1947092747092247), (0.43925, 0.19275807696580888), (0.451, 0.21640128707885742), (0.4615, 0.24251160019636153), (0.452, 0.24052831733226776), (0.446, 0.20726180547475814), (0.4465, 0.1955526915192604), (0.45925, 0.20083175045251847), (0.44925, 0.19421878933906556), (0.451, 0.18251998656988144), (0.452, 0.17801003175973892), (0.4495, 0.16749511247873305), (0.4505, 0.1645185649394989), (0.44875, 0.16983251261711121), (0.45725, 0.1668777294754982), (0.45075, 0.15709669607877733), (0.453, 0.16894363242387772), (0.45025, 0.15625398671627044), (0.44375, 0.16142740309238435), (0.448, 0.15678464949131013), (0.44725, 0.1691381283402443), (0.453, 0.16731515389680862), (0.449, 0.17096377342939376), (0.45025, 0.18136699879169463), (0.4425, 0.16686505150794984), (0.4515, 0.16423037707805632), (0.44425, 0.18454006439447404), (0.43975, 0.21046450340747833), (0.45325, 0.19774382305145263), (0.4485, 0.20487813472747804), (0.447, 0.20113639348745346), (0.4445, 0.19910237354040144), (0.453, 0.1959738621711731), (0.44125, 0.21016900020837784), (0.44275, 0.2052472048997879), (0.44875, 0.2011288633942604), (0.45475, 0.23439677232503892), (0.44075, 0.2100129949450493), (0.44625, 0.22826712876558303), (0.44925, 0.1953158112168312), (0.4545, 0.24329597902297972), (0.44425, 0.20127942901849746), (0.447, 0.24728498888015746), (0.4485, 0.21980126494169236), (0.4445, 0.2183629720211029)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           8       0.38      0.99      0.55      1000
           9       0.57      0.79      0.66      1000

    accuracy                           0.44      4000
   macro avg       0.24      0.44      0.30      4000
weighted avg       0.24      0.44      0.30      4000

Collaboration_DC_3
VAL: 
[(0.209, 0.04494925320148468), (0.257, 0.09110854172706603), (0.323, 0.11809220957756042), (0.355, 0.1430300070643425), (0.356, 0.16536943846940994), (0.354, 0.1911297039538622), (0.376, 0.1672702697366476), (0.384, 0.1897293770611286), (0.387, 0.205445771753788), (0.379, 0.1797317993193865), (0.393, 0.2138650175333023), (0.397, 0.21732402542233467), (0.389, 0.20565791372954845), (0.399, 0.2291084222048521), (0.401, 0.22353157429397105), (0.399, 0.22569028033316135), (0.397, 0.28172453477978704), (0.392, 0.2860072252750397), (0.404, 0.27693781098723413), (0.396, 0.26421140231192114), (0.398, 0.29483157986402514), (0.391, 0.3265850615501404), (0.402, 0.3229255262315273), (0.398, 0.3122727960646153), (0.4, 0.3478995704650879), (0.409, 0.3178991679251194), (0.401, 0.3649251551032066), (0.402, 0.33083755159378053), (0.399, 0.3335383904874325), (0.402, 0.28380913931131363), (0.399, 0.3245014344751835), (0.405, 0.3120652152448893), (0.403, 0.34334443168342116), (0.402, 0.3324373926669359), (0.396, 0.33507137748599053), (0.41, 0.3078152932822704), (0.401, 0.30642455820739267), (0.396, 0.328941770285368), (0.402, 0.3107540704011917), (0.401, 0.2919933432340622), (0.406, 0.33286566951870916), (0.407, 0.2992829284667969), (0.405, 0.3451630851626396), (0.402, 0.3195404217094183), (0.406, 0.3093393190205097), (0.408, 0.3069999773651361), (0.404, 0.29863434843719006), (0.404, 0.2915055101364851), (0.405, 0.29884742538630965), (0.406, 0.30206027437746524), (0.397, 0.2779772319197655)]
TEST: 
[(0.20325, 0.043842192202806475), (0.2555, 0.08742595201730728), (0.3265, 0.11303198945522308), (0.359, 0.13696498495340348), (0.36825, 0.15801430696249008), (0.3605, 0.18254355472326278), (0.382, 0.1589944772720337), (0.3905, 0.17838659977912902), (0.39525, 0.1921792615056038), (0.3875, 0.17070769482851028), (0.395, 0.20351121479272843), (0.39925, 0.2043839541077614), (0.3985, 0.19592777556180954), (0.40175, 0.2170550719499588), (0.40225, 0.20914197015762329), (0.4055, 0.21304831695556642), (0.3965, 0.2665013109445572), (0.4025, 0.26919121730327605), (0.40125, 0.25681304788589476), (0.40375, 0.2496322569847107), (0.4065, 0.27163473999500276), (0.407, 0.29958353102207186), (0.4095, 0.29506540501117706), (0.40225, 0.2904158356189728), (0.4035, 0.3211871122121811), (0.41, 0.2999620999097824), (0.40475, 0.3400289602279663), (0.4075, 0.30722737026214597), (0.404, 0.3123062644004822), (0.40775, 0.26314532601833346), (0.39975, 0.29868734955787657), (0.406, 0.28530207681655884), (0.404, 0.31548061811923983), (0.4035, 0.3061513682603836), (0.40525, 0.3033917883634567), (0.40725, 0.2873365993499756), (0.4085, 0.2872566764354706), (0.4105, 0.3009328328371048), (0.408, 0.28620529878139495), (0.40825, 0.2669469268321991), (0.41075, 0.3061243406534195), (0.408, 0.2802017365694046), (0.4055, 0.31942218673229217), (0.40425, 0.3021452896595001), (0.4065, 0.28457657968997957), (0.4075, 0.28889883625507357), (0.4075, 0.2784287511110306), (0.40425, 0.27236425268650055), (0.40175, 0.28122211158275606), (0.40775, 0.28109111166000367), (0.4055, 0.25924285292625426)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.37      0.86      0.52      1000
           3       0.45      0.77      0.57      1000
           4       0.00      0.00      0.00      1000

    accuracy                           0.41      4000
   macro avg       0.21      0.41      0.27      4000
weighted avg       0.21      0.41      0.27      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [12]
name: alliance-2-dcs-12
score_metric: contrloss
aggregation: <function fed_avg at 0x762c36da9c10>
alliance_size: 2
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=12
Partitioning data
[[6, 1, 7, 4], [5, 0, 7, 4], [8, 3, 7, 4], [9, 2, 7, 4]]
[(array([40565, 32339, 39457, 16008, 30469, 15600, 36274, 46340, 27651,
       15424,  4824,  4421,  6548, 25892,  7823, 36318, 14392, 32170,
       25039,  1579, 38310, 46334,  6486,  2424, 38993, 15349, 25294,
       15048, 20755,  9406, 18256, 28382, 47734, 42975, 20535,  3401,
       25877,  3412, 11070, 32198, 33824, 45836, 20800,  7137, 49129,
       49110, 36522,  4201, 49584,   234, 25526, 49754,  7481, 43933,
       46846,  8734, 10007, 32904, 32752, 25704, 28177,  4118, 45759,
       14701, 26235,  6101,  2978, 18816, 45863, 35591, 20025, 42881,
       21719, 19834, 35577,   248, 37835, 22889, 35615, 39258, 36625,
       15657,  3122, 34245, 30390, 33367, 28504, 42209, 16064, 36613,
       23105, 48229, 16960, 22004, 30031,  6581, 36348, 46550,   298,
       30759, 12254, 45140,  1159,  8344, 17554,  1222,  2596,  3018,
        3676, 10783, 22567,  8053, 18025, 37045, 25448, 25569, 46134,
       14959, 41142,  3223, 12635, 49966,  8820, 33990, 36062, 17173,
       17858, 20324, 22058, 24231,  3873,  1585, 22363, 27843, 38337,
       27708,  9130,  4105, 25612, 25155,  2583, 13390, 42380,  5206,
       48206, 29214,  7247, 20789, 35780, 26542, 23760, 41491, 13176,
       34602,  4731, 15517, 37748, 27454,  9303, 20278,  1917, 33812,
       27602, 16508, 10176, 44573,  8171, 47961,   620, 25641,  8015,
        2777, 34564,  2439,  5497, 21730, 37796, 34264, 45637,  3926,
       31099, 43624,  5679, 39469, 42960, 31651, 12221, 21673, 37715,
        2179, 49808, 39337, 18396, 47407,  8867, 29089, 37404,  2043,
       32637, 33178,   738, 38786, 25551, 14291, 42584, 30089,  8827,
       30249, 46418, 29357, 21965, 32497, 40824, 49094, 40929, 34552,
        7044, 35060,   132,  5964, 23927, 39133, 41275, 41045,  6046,
       38959, 10757,  3554, 18571, 30100, 26231, 40206, 31826, 22295,
       11917, 26502, 15338, 42015,  4805, 11233, 27087, 25035, 30292,
       39808,  8170,  5169, 11310,  8128, 20717, 16927, 27868, 35550,
       13763, 14074, 34287, 19144, 44273, 37301, 39990, 33228, 23514,
       32603, 41576, 33112, 41445, 39936, 21088, 28347, 30923, 11580,
         466, 32418,  8124, 28847, 19310,  5211, 30676, 46219,  7722,
       44541, 28193, 34258, 30269, 30166, 12384,  7326, 16925, 43600,
       32458, 10363,   873,  9946, 34830, 49993,  9138, 45672, 42655,
       12741, 23275, 29966, 34376, 39097, 41087, 14101, 44097, 45569,
       16233, 25363, 36046, 38812, 13755, 22532, 24450, 40178, 36614,
        3231, 37153,  5171,   841, 39338, 13839,  5685, 18117,  2584,
       44109, 12865, 16083, 23933,  9118, 42734,  7735, 36736, 38756,
        4360,  2582, 13899, 12064, 23923, 13488, 20312, 23648,   262,
       37235, 26994, 33452,  7475, 26452, 13056, 39654, 45704, 10112,
       48612, 31512, 13897, 44483, 44971, 21933, 34637, 20475, 24873,
       39848, 26578, 40987, 21706, 36595,  5810, 11421, 21073, 42709,
        1304,   536, 31113,  9754, 19574,  4358, 37620, 18333,   690,
       19810, 21796, 14005,  6895, 28137, 27002, 24297, 32959,  7841,
       41525, 15295, 16320, 31072, 28882, 45488, 10271,  2910, 34700,
       37461, 32979,  4132, 44217, 40972, 31747, 10236, 19930, 44185,
       48294, 34064, 49186,  7622, 24284, 12676, 31195, 44686,  6318,
       26080, 24219, 36107, 49416, 46519, 40505, 34238, 16673, 27538,
       18066,  1694, 45444, 22762, 49937, 23100, 21651, 24417,   364,
       44134, 39805, 43251, 47540, 44726, 46177, 29791, 15655, 44993,
       45988, 19500,  8573, 37666, 24121, 21547, 11782, 27132,    75,
       33114, 15883, 28122, 32684, 19301, 15557, 34690, 29314, 29894,
       16368, 16275, 26637, 46396,  1781, 12516,  3620, 10614,  6068,
       48465, 38858, 12923, 30049,   815, 40751, 31539, 48312, 24466,
       44524, 11646, 15756, 20376, 38047, 36813, 23714, 12054, 49225,
       18832,  7130, 19546,  7832, 45736, 27871, 38950, 33859, 44487,
       37979, 44311, 16800, 14507, 29309, 38286, 33712,  1445,  9587,
       38703, 14641, 19461, 24917, 35908, 23785, 31777, 36114, 10281,
       10573, 40637, 24850, 17181, 15079, 29272, 17737, 10873, 26991,
       18165,  6573,  8897,  4037, 48567, 48055, 48041, 16026, 16750,
       37150, 26046, 34707,  8859, 27006,  7813, 44922,  7223, 33825,
       38195, 20678, 37930, 39259, 14680, 35682, 49903, 16537, 49745,
       12995, 27288,   181, 32374, 26107, 38279, 49058, 24197, 17436,
        3135, 10278,  8129, 26420, 38243,   972,  9133, 48837,  3637,
        5326, 11088,  5774, 39378, 18447, 42485, 41758,  2135, 48104,
       15070, 15511, 14449, 12658, 42895, 17521, 39408, 21660, 48697,
        9110, 41526, 12889, 42514, 49226, 26166, 43625, 13569, 28328,
        3955,  3157, 10858, 28987, 46754, 43360, 27985, 45000, 39307,
       10602, 21107, 36608, 23186, 15245, 42268,   797, 40300,  1930,
       13703, 28366, 17463, 48204, 44353, 25079, 37842, 28498, 20780,
       13039, 26017, 48256, 29196, 46620, 32208, 40820, 35696,  7969,
       31944,  3050, 39257, 14775,  8106, 21482, 42593, 19844, 45999,
       22204,  7331,  1742,   289, 41805, 44493,  6329, 39711,  1218,
       36278,  8595, 11996, 17636, 29535, 20109, 38693,  1071, 12787,
        1042, 31357, 40912, 20565, 17910,  6677, 46412,  8038, 16504,
       49925, 18227, 18468, 49183, 11426,  9805, 29644, 46672, 36589,
       35241, 28197, 35290,  5363, 40692,  9374, 21335, 34876, 27775,
        5842, 48848, 31117, 26733, 36369,  5269, 14493, 14290, 26947,
       37262, 36936, 42463, 37896, 10818, 34584, 43060, 40356, 22569,
        7621,  1309, 12900, 26056, 19200,  2801, 27187, 49627,  2458,
       35483, 18579, 12961, 16171, 33485, 40893, 16914,  6449, 23571,
         771, 44610, 46988, 44798, 35164, 28639, 10235,  1908, 34044,
       15235, 30422, 21535, 38667, 44424,  2103, 18517,  1308, 23910,
       24901, 31520,   973, 39633, 19638, 12244, 20016, 16654, 35869,
       21269,  2301, 38308, 45767, 15196, 44147, 43595, 31464, 14670,
        1169, 48505, 10250,  6838, 44336, 40146,  1387, 38135,  7436,
       23545, 28250, 24883, 28167, 19528, 33171,  9724, 15168, 45170,
       35066, 23779, 10861,  6144,  9529, 36671, 27494, 14557, 43549,
       49687, 10865, 39005, 46883, 49390, 44450, 11092, 46753, 19742,
       27461, 24448, 28837, 20515, 13453, 32818, 30464, 25579, 45515,
        2123, 26819, 28576, 19643, 45315, 10633, 10392, 10539, 35774,
       44400, 15024, 17801, 40228, 47716, 28464, 13846, 45723, 23460,
         856, 25779, 26638,  3577, 42750, 30616,  2106,  1022,   665,
       44125,  1882, 40962, 15482, 37214, 19264, 37854,  3945, 19846,
        1784, 47144, 37089, 23634, 28561, 22944, 13987, 10519, 28666,
       23329, 36294, 22243, 18409,  3897,  3936, 37431, 32893, 48634,
       20888,  1813,  9349, 23337,  7365, 41237, 31689, 47287,  5229,
        4057, 31033, 15703, 41825, 12444, 45408, 13469, 40674, 18404,
       29291, 49504, 20412, 18340, 41518,  1863, 23817, 23468, 33666,
       18052, 37274,  6654, 26584, 25434, 18875, 25620, 46940, 27384,
       26118, 14911, 30300, 40259, 39470, 16073, 30230, 30715,  6344,
        1575,  6844, 12136, 33342, 16802, 22536,  5998, 42725,  9848,
       30077,   153, 44705, 26772,  8523, 23211, 48429,  1516, 24046,
        1069,  2839, 32928, 26911, 19689, 18490, 32415,  6234, 46789,
       41263, 48211, 40210, 47345,  2842, 45323,  2835, 43553,  5405,
        6270, 41523, 26854, 17623,  1814, 10925, 21753, 40531, 45820,
       16323, 33408, 22461,  2647, 36976, 18304, 20782, 49402, 49954,
       10536, 41101, 38350, 24308, 47798,   572, 15515,   660, 19475,
       14262, 33140, 49902, 34670, 27817,  9061, 27558, 35714, 48997,
        1832, 43926,  1238, 14726,   868, 46528,  5514, 12522, 14019,
         490, 17211, 46446, 36523, 36317,  7656, 11033, 38395, 20543,
        8541, 13292, 40033, 30020, 32413, 12505, 41852, 31256,   934,
        8473]), [6, 1, 7, 4]), (array([ 9035, 26874, 11513, 48114, 30657, 23488, 40417, 41232, 30871,
       32769, 18463,  3173, 13676,  2864,  8494, 43759, 36121, 42293,
       34833, 19274, 41046, 28982, 42866, 41094, 31646, 29268, 27653,
       44838,  4125, 15861,  4627, 33957, 14495, 46608, 37114, 32849,
       32530, 16949, 40069, 20801, 27991, 42194, 20348, 45992, 46428,
       49267, 44284, 46165, 37508, 15653, 31409,  6580, 13251,  6436,
       16626,  1091, 20207,  6982,  9871, 41139, 27947,  5367, 26194,
       23273,  3451,  3080, 21677, 11995, 40632,  9337,  4890, 45553,
       38990, 28779, 30155, 26079,  7090, 18966, 14004, 45215, 39817,
       44358, 25333, 22890, 37899, 48935, 48832, 46310,  6561, 47218,
       12177, 16580, 17009, 22059, 30723, 37880, 40434, 38632,  5993,
       46853, 31059,  2089, 40051, 43038, 13579, 47103, 49988, 14627,
        2148, 18399, 27348, 40904,  8252, 40763,  6220, 13283, 43651,
        6807, 25751, 18347, 32044,  1199, 14259, 41340, 24877,  1545,
       29450,  2182, 19169, 24656, 47602,  9772, 47919, 11740, 32559,
       45587, 39124, 32658, 28425,  4120, 15450, 21233, 46130, 14992,
       27265, 39783, 47920, 41499, 26072, 46022,  6689, 42613, 36202,
       49547,  2374,  7398, 38944, 41886, 47659,  6058, 29630, 15544,
       34112, 34728, 49675, 27223, 47647, 31120, 49316, 11980,  1355,
       28090,  7140,  9445, 29422,  4990, 27941, 42549, 32124,   424,
       38739,  1061, 21598, 11396, 40688, 20493, 48754, 38403, 15019,
       21046, 37708, 48083, 30231, 47230,  8274, 20809, 29804, 28889,
       22483, 29836,  2194, 20075,  7446, 30952,  6037, 38709, 30054,
       10258, 23392, 24648, 18852,  8511, 27397, 20421, 19652, 36123,
       18772, 13057, 27666, 34299, 30162, 24909, 25552, 20171, 28438,
       19933, 20277, 36235, 33079, 18986, 49810,  9929, 33039, 41817,
       47555,  6707, 22908, 15336, 11600, 30393, 24953, 40105, 39846,
       24650, 46483, 46690, 43599, 27305,  4632, 40765,  2895, 28767,
       40432, 48556, 31023, 38312, 43659, 47560, 21930,  8753, 34070,
       16375, 38716, 15959, 43253,  3609, 28138, 38307,  1329, 28919,
       16513, 19409, 46137, 23010, 35528, 42183, 39396,  7808,  5684,
       41164, 21573,  8102, 45390, 20295, 43731, 46192, 44731, 10578,
       28003, 22123, 31481, 13964, 49130, 15153, 22271, 19273, 44967,
       12469,  2490, 20902,  4295, 18694,  8437, 44592, 25489, 10804,
       43138,  8660,  2156, 47805, 11218, 27278, 25668, 34711, 29732,
       16316, 16065, 15115, 34748, 29189,  2863, 37068, 29039, 26546,
       46934, 17221,  6904, 39819,  4721, 25477,   782, 28063,  6688,
       32363, 13714, 12936, 19945, 19974, 44430, 37788, 10031, 39138,
        9171, 38321, 18614, 43077, 31600, 24111,  3909, 45357, 13060,
        7588, 10486,  4619,  7643, 35873, 36530, 43245, 17082, 38320,
       11431, 37664, 16752, 48148, 34341, 11056, 21322, 35546, 11276,
        3987, 19493, 37146, 23629, 47557,   650, 27211, 36323, 47994,
       30539,  4952, 37165, 11709, 35952, 27901, 32140,  7599, 11178,
       38586, 11114,  9435, 20396,   284, 35608, 29293, 49513,  9236,
        5738, 48069, 19106, 22026, 16146, 21472, 32683, 12028,  4746,
       36497, 45186, 48239, 49799, 41167, 14468, 41212, 29518, 27042,
       46041,  4861, 31015, 46303, 22993,  4524,  2617, 48096, 29544,
       18864, 33530, 28650, 14216,  6518,  1708,  6187,  3582, 49340,
       31955, 42190, 29620, 45438,  4523, 11304, 32056, 25905, 47931,
       47645, 23746, 12542, 33386, 26081, 42844, 28068, 20743, 34568,
       28467, 38190, 49375,  7985, 39311, 40012, 11721, 17994,  3585,
       17154, 41030,  4096, 33711,  6090, 35028, 14958, 33062, 38580,
         332, 30888, 18763, 47271, 36007, 31246, 48059, 49992,  5441,
       45182, 37712,  2959, 48378, 30467, 38334, 30909, 48677,  1187,
       23984, 10326, 41879,  9058, 27080, 33831, 24713, 49346,  5331,
       34705, 28515, 26342, 48554,  8242, 37467, 34900,  2716, 37787,
       14143, 24026, 38142, 18997,   470, 36806, 20884, 49277,   948,
       12751, 36829, 40942, 42909, 11323, 27374, 28184, 44101, 16074,
       24267, 39717, 38225, 43066,  8966, 17913, 49809, 16719, 42184,
        5114, 14818, 21109,  6850,  9778, 17095,  6283, 47092, 27565,
       25742, 41347, 33096, 19250, 32920, 40120, 23290, 11046,  9071,
         318,  1165, 35284, 26494, 11393, 29718, 10927, 36897, 20550,
        9795, 43617, 20436, 32568, 46671, 46719, 27242, 17951,  6821,
       43435, 40684, 47923, 10226, 17494,  2612, 19418, 24782, 10685,
        9560, 22730, 12980, 43229, 24025, 35986,  5307, 13147, 23577,
       22207, 37190, 26397, 49610, 21100, 24970, 42651, 37131, 47880,
       31800, 26821, 17499,  4396, 48343,  8664, 39262, 35815, 30036,
       49229, 34273, 47835, 23356,  6460, 34191, 24371, 39261, 47438,
       35972, 26749, 27015, 36597, 31262, 23034, 18874, 29481, 41780,
       15853, 22291, 14603,  2814, 41370,  9239, 12838, 44911,  7482,
       19883, 32514, 49454, 45929,  6695, 43906, 38512, 43528, 41584,
       10884, 13272, 14191,  8745, 39799, 35421, 32888, 33949, 38155,
       26811,  6072, 49037, 30278, 47856,   688,  4555, 11284, 44081,
       37637, 25197, 15472,   575, 44115, 45376,   329, 26067, 46691,
       13540, 39155, 35962, 47169, 32354, 33884, 27078, 39017, 39720,
        7895, 19997, 20369, 10598, 23490, 11502, 13136, 34113, 43830,
       33349, 33636, 46996,  4903, 33155, 10106, 24954, 30972, 44286,
         746, 36034, 11102, 21329, 15551, 32688, 23880,   362, 43305,
        8764,  7717, 43671, 18037, 48029, 43782,  8643, 24088,  9768,
       23835, 23842,  1857, 17872, 12844,  5798, 33045, 18881, 12848,
       47590, 43242, 34826,  6653, 46834,  1395,   294, 23331, 33908,
       46982, 11260, 20099, 25304,  2000, 36037, 30410, 23778, 42251,
       49010, 12402,  4513, 22723, 40585, 26702, 33556, 34528, 33139,
        1046, 37497,  6696, 21899, 31939, 20120, 30915, 19936, 24889,
       13412,  4231, 19033, 26717,  1855, 43756, 33221, 28795, 37124,
       37778, 41500, 42403, 38455, 20253,  6465, 49669, 13834, 48529,
       46398, 47300,  9907, 30794, 25371, 45598, 24513, 37465, 41219,
       23247,  5259,  6785,  3668, 34410, 27897, 16383, 49421, 25114,
       10641,   130, 43714, 11573, 26062, 29070, 47482, 19822,  3252,
       38794, 34840, 35432, 39734, 42072, 24548,  1388, 47796, 14244,
       47415,  4297, 23279, 13023, 46339, 32088, 10640, 15909, 22949,
       13882, 30917, 36466, 33352, 35868, 26830, 21788, 22727, 14858,
       11967, 10706, 39169, 41453, 23748, 39573,  5940, 27074, 40707,
       14943, 18553, 14036, 17083, 42809,  9749, 17199,  1639,  9506,
       30942, 10979, 30435, 30911, 20306, 28100,  7368, 43306, 32533,
       43203, 29098, 43499, 15625, 24171, 11994, 16294, 42040, 44563,
       16241,  3314, 32989, 22288, 23512, 27247,  3288,  7052,  9615,
       48179,  1018, 45136, 21980, 22740,  4695, 28913, 21676,  1968,
       16433, 14111, 31305,  3522, 15596, 17958, 40770, 17412, 46241,
       36646, 20108, 16865, 37438, 28489, 30094, 32069, 17671, 23394,
       46164, 11209, 38787, 43327, 31767, 20325, 20407,  5285, 47139,
       44851, 24040, 27489, 45801, 36420, 29849, 44572, 34331, 10851,
       21666, 39945, 39344, 48738, 34589, 14256,  3112,  3360, 21708,
       42640, 30970, 37442, 18043, 31476, 34450, 46891, 47855, 21176,
       15582, 41246, 19657, 34082, 29420, 35135, 32518, 40301, 38593,
       15377, 42695,  5173, 41337, 22886, 27276, 22241, 40665, 48616,
       27180, 44040, 17420, 31627, 14181, 27017,  4429,  3761,  5495,
        8631, 35563, 15339, 14827, 16659, 13269, 28793,  9888, 22509,
       13764,  6378, 26586, 25147, 20060, 40071, 34051, 40793,  3176,
       49243, 11757, 38070, 38953,  6562, 19426, 47975, 48716, 16891,
       15256, 18418,  5788, 12022, 33129, 17719, 29024, 46255, 21629,
       14051]), [5, 0, 7, 4]), (array([25450, 26156, 12543, 48247,  1236,  2881, 16730, 20375, 23858,
        2417, 35995, 12400, 19402, 18221, 37437, 34655, 14767, 25987,
       27370,  4245,  9117,  2979, 29747, 19696, 40389, 47021, 15161,
        1770, 17416, 31714, 35315, 18691, 45697, 30087, 48469,  6632,
       40784, 16042, 14427, 10098, 48133, 43807, 19123, 27281, 16381,
       33626, 31879, 46392, 21477, 43662, 15260, 19603, 38867, 32896,
       40368, 10606, 46736,  5648, 45138, 32447,  5985, 18623, 32463,
       20668, 21383, 39981, 42467, 30770, 16607, 30835, 45168,  9960,
       43423,  4670, 32728, 17799,  2929, 22696, 28450, 42497, 40691,
         418, 27398, 33372, 12418,  9076, 16512, 24456, 46926, 43899,
        7112,  7387, 24787, 47114, 46175,  6337, 43001, 43883,  1479,
       41070, 48445, 28006, 23818, 12725, 40671, 22440, 49372, 41585,
       37318, 40881, 48698,  2489, 48794, 22374, 46938,  3130, 28284,
       25096, 20812, 17617, 19374, 23294, 26437, 28205, 13266,  5278,
       12650,  2820, 20405, 31692,  8489, 39838, 43344, 25756, 21257,
       48552, 32327, 42300, 42203, 12258, 21249, 45025, 40789,  8270,
        9572, 43540,  9669,  4502, 16692, 45354, 26195, 17519, 19167,
       42840,  8961, 26328, 43119,  7561, 21312, 17348, 45309, 34171,
       35173, 19720, 41674, 14211, 16483, 36229, 32207, 41008, 44937,
       37557, 29056,  4602,  9972, 42826,   562, 35864,  8211, 29968,
       37858, 43755,  5941,  6139, 27640, 49166,  9599, 13694, 32246,
       39895, 17007, 13139, 39802,  9651, 42965,  4713, 40495, 25951,
       29476, 34212, 16727,  2483, 38689,  8022,   222, 22649, 13886,
        8831, 32465, 32138, 48475, 35588, 16861, 16084, 49253, 24629,
       26694, 23172, 36742, 34893, 23085, 29929, 26254, 46735, 43895,
       33287, 49625, 25902,  4723, 27056, 29877, 19314, 21565, 48125,
       22682, 36803, 39621, 49916, 49553, 38469, 45853, 38809, 42763,
       25629, 16049,   244, 41298, 10191, 31317, 28800, 24652, 20638,
       33525, 16289,  1104, 34160,  4542, 35769, 49408, 33917,  3679,
       19830, 30995, 33007, 24733, 19101,  3429, 15313, 19578, 31400,
       44621, 18414,  4954,  7906, 48296, 13541, 14047, 25633,  5190,
       24696, 24751, 31463,  1461, 43490, 32732,  9794, 36351,  9164,
       11409, 42504, 22891,  1070,  7651,  8710,  6482,  8101, 26688,
       30586,  1010, 22011, 29523,  9298, 26224, 12039, 38068, 10506,
       34199, 11564, 14466, 28330, 22239, 13530, 30839, 19327, 49975,
       25307, 15122, 39021, 14993, 37187, 14161, 32405,  2141,  4151,
       27608, 20301, 22850, 40249,  7006, 19922, 21299, 36230,  6849,
        8330,  7050, 32702,   266,  4402, 44987,  2165, 46094,  9006,
        8028, 33280, 36440, 41713, 33964, 41269, 14574,  9700, 11357,
       22905, 24099, 34661, 35937,  7680, 44173, 20639, 35715, 23094,
       32139, 11386, 19221, 16731, 35772, 22411, 38364,  9008,  3997,
       24174, 44943, 42738, 35229, 42736, 34889,  2412, 29442, 34710,
       46425, 24434, 16851, 33600, 20625, 14187, 40973, 11902, 43827,
       31414, 43784,  4221,  6883, 30774, 49319, 15640, 16495, 36772,
       30346, 44381,   639, 16561, 20326, 32085, 42953, 20577,   922,
       17920,  5592, 29654,  3355, 25769, 44576, 11268,  5945, 49140,
       29464,  8408, 29960, 10017,  3571, 38647, 46347, 43737,  1265,
        2127, 47062, 44226, 34999, 44768, 31542, 42837, 17828, 46023,
       45670, 15080,  8246, 18520, 41271, 10931, 20882, 48346, 14253,
        7200, 25471, 13862, 41910, 20330, 36212,   857, 10414, 16646,
       43440, 27274, 37965, 20685,  9343, 40232,  9897, 43744, 17074,
       45402, 34185, 16271, 13824, 26900, 47838,  8790, 24148, 23372,
       29050, 39778, 10634, 10869, 27906, 15224, 42496,  2892, 14239,
       10066,  8041, 48772,  6724, 49918, 16341, 37370, 39158, 12517,
       38215, 47654, 26907,  5744, 48376, 12147, 17155, 38006,   207,
       13235, 27475, 47717, 28880,  8105, 30265,  1135, 26931, 10384,
       46045,  5620, 42063,  5995, 49843, 38176, 31336,  7817, 39107,
       32068, 10988, 45103, 28520, 32487,  7023, 26810, 34260, 43436,
       22601, 40646,  7756, 34511, 27600, 10295, 16557, 23344, 33681,
       39395,  6326, 43791, 33773, 43144, 27573, 44935, 40694, 23889,
       38595, 12722, 43091, 27824,  3611, 46211, 14614, 13132, 39400,
       27423, 45353, 25741,  7940, 12229, 46383, 29824, 10333, 20879,
       39304, 32440, 38121, 29195, 44076, 31818, 31671, 13732, 36407,
       39345, 17040, 36805, 48548, 19542, 24311,  2139, 15579, 47082,
       44784, 18481, 37636,  2807, 37302, 26767, 27400,  9486,  7682,
        4820, 29719, 29441, 29588,  3434, 36239, 17464, 41185, 22501,
       16399,  6966, 36959, 27043, 45343, 40709, 49470,  9891,  2244,
        1226, 28553, 27189, 42421, 44292,  1758, 27568, 30199,  6481,
       36256, 27226, 44363, 24505,  2113, 14222,  2750, 31432,  2278,
       21601, 21367, 33191, 40497, 43579,  4886, 10823, 20900, 13430,
       10216, 22901, 42052,  5579, 26916, 34308, 13974, 43309, 25131,
       18519, 25343, 26895, 24439, 39797, 12466, 13224, 40916, 47003,
       40295,  5200, 32211, 15979, 35982, 39111, 47828, 43940,   739,
       37055, 34955, 41700,  3279, 38539, 35698, 18405,  3914, 27748,
       33722, 10121, 32700, 15317, 31389,  3996,  8646, 46154, 48714,
       16184, 12901, 31331, 48307,  7016,  3623, 12163,  2776, 46517,
       20614,  9741, 10916, 47572, 28951, 40266, 44356, 15881, 23794,
       20158, 39700, 39276, 10621, 35003, 43215, 38732, 30066,  8946,
       39767,  5368, 47890, 25876, 41205, 47575, 14837, 16348, 12565,
        5863, 26633, 36136, 44222, 10712, 25116, 44135,  3150, 20960,
       22312, 35511, 29320,   722, 40837,  3699, 37670, 18828, 19361,
       13790, 40897, 42353, 17001,  2119, 26983,  3213,  9726, 33176,
       26104,  3933, 11329, 36417,  9672, 35153, 27036, 35002, 23330,
       37405, 37001, 22403, 36740, 41382, 41958, 17077,  2995,  4707,
       49488,  7933,  6314, 26838, 27790, 10888, 14186, 30559, 39501,
       22617,  1903, 37928, 31902, 32384, 31143,   831, 35522, 47155,
       28881,  2515, 10055, 37731, 14618, 42982, 34604, 31860, 44974,
       39413, 49352,  2210, 40277, 12213, 20778, 44656, 14310,  6168,
       23826, 18977, 44465, 47147, 26048, 35915,  5909,  7283, 24866,
        1762, 10015, 10179, 17513, 44900, 12417, 16240,  9638, 40943,
       31799,  3182, 19585, 18142, 31227,  5225, 14560, 20409, 18523,
        5815, 43758, 45586, 48046, 42795, 40195, 25014,  1569, 14082,
       27135, 19569, 20192, 43983, 41878, 49718,  3556, 17350, 43139,
       32100, 45623, 37530, 13310, 30306, 35186, 33674, 45397, 28696,
        7100, 10059, 38242, 22318, 42544, 30206, 45279, 41623,  4106,
        3397, 48235, 32654, 37123, 37960, 28796, 19006, 40067,  6720,
       46076, 27519,  6266, 26434, 42285, 48876, 32169, 13636, 45812,
       16155,  8331, 32643, 25703, 47348, 31306, 36755,  3322, 30841,
        2488, 31446, 46036, 30674, 45121, 30228,  3952, 22820, 23393,
        8913, 15525, 22210,  3189, 45749, 33027, 27613, 27199,  9153,
       27029, 25576, 47908, 13984, 46778, 39036, 27853, 35151,  4725,
       46011, 45688, 30242, 42603, 27100, 30425, 39576,  1303,  8447,
       46947, 40219, 29377, 43950, 39459, 43196,  9791,  5488, 31290,
        7636, 26380,  2175, 10196, 14635, 33401, 40088, 18829, 22443,
       26307, 27624, 39146, 49252,  2293, 30061, 26844,  2088, 29830,
       27227, 13803, 31053, 43765, 15744, 45712, 20978, 14914, 28658,
       47605,  1937, 47408,  5264, 28191, 40085,  2703, 33057, 33947,
        7984, 10765,  6894, 16136, 33657, 19170, 32373, 36694, 19548,
       44360, 37075, 10570, 34298, 38660,  1889, 28603, 20303, 35743,
       29582, 25863, 15917,  8361, 19627, 33979, 36105, 13427, 22513,
       30131, 21033,  7207, 14415, 33334, 36142,  5193, 32018, 35443,
       48223]), [8, 3, 7, 4]), (array([46298, 15758,  3526, 26668,  4622, 30615,   147, 28019,  5228,
        1504, 30038, 26888, 45235, 30578, 14599, 22252,  9683,  6643,
       46611,  4675,  1384, 28353, 21159, 32563,  3895, 27837,  9229,
       30973, 28358, 24107,  3237, 10405, 29356, 27815, 41419, 48140,
       18642, 20944, 41244,   278,  6293,   768, 17163, 25308, 28337,
       38426, 36321,  2767, 47641, 21300, 49249, 26394, 34515, 18260,
       36280,   657,  1231, 23410, 49287, 24638, 29529, 17139, 40875,
       29697, 32163,  3199, 39699, 30764, 39806, 17600, 18963, 33671,
       13782, 32838, 41765, 47765, 36521, 43619,  6158, 39319,  8841,
       43406, 23305, 14071, 47340, 45405, 39917, 31870, 30223, 29258,
       24550, 14205,  1332, 29193, 33248, 26945,  2158, 42414, 39117,
       18344, 37017,  2303, 26136, 32040, 17896, 20765, 26021, 47187,
        2585, 18539, 28656, 30503, 32072, 43141,  8402, 32156, 34409,
       26229, 30934, 47660, 31077, 44052,  8303,  7652, 13780, 43673,
       32552, 34077, 16267, 19670, 43614,  6934,  1390, 20129,  7752,
       35545, 12902, 27957, 30903,  3086, 38357, 19620, 11096, 27668,
       43539, 19369, 26518,     1, 32192, 11552, 34502,  5394, 29753,
        9501, 18430, 27791, 36508, 24133, 26506, 45984,  4618, 42102,
       24391,  8323, 36549, 29221, 48839, 29585, 33620, 12262, 21907,
        2838, 36288, 47896, 33676, 42362, 45871, 12011, 36887, 45673,
        3755, 33483,   360, 26600,   541, 21696, 24880,  7943,  9434,
       47875, 45684, 35218, 23075, 27965, 36504, 48751,  9513, 33224,
       41976, 13662, 36668,  2129, 19055, 44716, 33672, 14813, 49035,
       25483, 49266, 14626, 15616,  5133, 18666, 12186, 32245, 13380,
       24407, 23359, 26530, 10144, 44969, 42556,  1045, 30180, 17903,
        3977, 31176,  3298, 17716, 40387, 35423, 23856, 35184, 10622,
       38402, 11814, 28879, 22652, 40412, 15725, 42420, 37538, 48308,
       43591, 40693, 46723,  5138, 41580,  5300, 44280, 21189,  7410,
       19806, 34959, 41556, 42542, 18163, 14180,  7633, 35575, 45953,
       39687, 15118, 19969,  6315,  3193, 24210, 42489, 47563,  3454,
       36710, 16439, 13081,  9093,  8881, 40189, 34600, 13794, 10457,
       34433, 16357, 17114, 32361, 12720,  3520, 31074, 38170,    47,
       37515, 24500, 48504,  4525, 15977, 27685, 25268, 32470, 14500,
       29625, 40487, 34555, 21316, 33282, 27204,  1921, 22012, 38565,
        5634,  8789,  7340, 46633, 42155, 33564,  3161, 25379, 40444,
        1207,  3847, 48870, 27738, 29650, 34609, 37990, 20570,  1812,
       44658,  9989, 34866, 24994, 34398, 11500, 28689, 26146, 39339,
       49800,   646,  5087, 28426,  6477, 17344, 16742, 26956, 20226,
       31307, 31486, 37353,  1884, 21031,  2948, 42536, 19343, 25222,
       46967, 45277, 18457,  2533, 47940, 20660, 25628, 18964, 29902,
       16372, 49159, 15697, 47172, 39928, 13757, 36972, 13985, 15088,
        8037,  6073, 38129,  3944, 12043, 26651, 44070, 15430, 19899,
       26213, 30513,  2372, 22279, 42120, 42990, 23843, 18573, 16593,
       31475, 46405, 47632, 31333, 28452, 26990, 18269, 48498, 31636,
       19964,  5836, 46800,  5576,  9144, 16683, 46014, 46179, 44879,
       14588, 41407, 46258,  9952,  7074, 24610,  4811, 37166, 28227,
       44008, 27046,  2833, 48092, 27542, 31308,  8092, 48146, 26893,
        9408, 37811, 42995, 39935,  8121, 33531, 14172, 43441, 34088,
       35486, 48973, 30333,  9558, 30746, 48480, 16569, 47500, 44739,
       45803, 32582, 48733, 49397, 35420, 30371, 47360, 24719, 21210,
       31285, 10512, 16462, 42889, 33503,  1024,  2678,  5127, 35723,
       25095,   630, 16757, 41401, 10725, 45218, 18720, 36152, 29126,
       42588, 26760, 38638, 17339, 21063, 32706, 45021,  2354, 33834,
       23222, 21039,   957, 35947, 33403, 45335, 43367, 10018,  7258,
        3164, 35063, 21090,  5189, 20051,  5618, 35834, 18888, 42427,
       10594, 17188, 19918, 14583, 44181, 21301, 26276, 11867, 40955,
       15567, 18712, 24596, 17648, 19058,    43, 45708, 36513, 10552,
       26451,  2541, 46301,  6752, 43536,  8238, 49737, 34228, 42117,
       39030, 47406, 22405,  6939, 45610, 30484, 26165, 46792, 28048,
       23435, 24812, 16618, 49350, 40563,  1927, 45747, 48453, 12925,
       30150, 28502, 31789, 36691, 42386,  8052, 34412, 26826, 49396,
       28759, 32414,  1276,  6839, 46117, 15916, 35922, 43768,  7672,
       11469,  2616,  7565, 17720,  7463, 43902,  8465, 34329, 36562,
        7613, 20423,  5846, 25917, 13566, 48727, 42959,  8927, 24370,
       10957, 33150, 23364, 10348, 43532, 28195, 49796, 18292, 19600,
       24568, 48099, 30996, 40730, 27255, 25724, 13622, 32296, 33299,
        4582, 20707, 22854, 28771,  5403, 38500, 40025,  3711,  5116,
       17177,  7400, 36987,  8924, 17840, 13547, 25617, 44958, 42598,
       25778, 16025, 11621, 45281, 49774, 21317, 15550,  6726,  7141,
       11742, 17549, 48586, 10779, 48578, 24013, 40541, 23547,  3151,
       29018, 24995,  6827,  8984, 34788, 41473,  6335, 30890, 42812,
        8751, 23784, 38109, 26208, 29592, 25182, 14202, 35008, 28453,
       18124,  4078, 42951, 17989, 37935, 28429, 47039, 47665, 32053,
        6291, 15413, 43150,  7564, 33450, 47246, 25592,  4401, 49592,
       44762,  7101, 19064, 43322, 23486, 23909, 23609,  5761, 20314,
        4241, 33660, 13597, 37319,  8604, 37902, 45945, 33775, 27969,
       10034,  6641, 45150, 41390, 25836, 24233, 30556, 31529, 19744,
       18339, 45336, 44560,  7836,  2370, 19599,  9369, 11581, 29975,
       11414, 46857,  6513, 28965, 31224, 34469, 47422, 38325, 32329,
       22624,  8580, 22806, 21324,  9279, 31217, 22309, 17678, 41846,
        8109, 12627, 10041,  6680,  3733, 26752, 38965, 46167,   647,
       18139,  6979,  6864, 17159,  5996, 14431, 24090, 25226,   994,
       18914, 26411, 49850, 36144, 38284, 43168, 22687, 28911, 12888,
       27915, 25269, 24715, 30822, 45689, 12580, 18912, 29756, 48399,
       47205,  1435, 22205, 22149, 21251, 23853, 20276, 34793, 46521,
       16908,  9777, 23532, 49237, 37719, 45441, 25898, 27193, 35276,
       41766, 35624, 12510, 43335, 15656,  1347, 37875, 41737, 30528,
       12179, 33209, 43473, 32947,  9074, 16173, 46245, 40162, 14022,
       43632, 38115, 23655,  1804, 43304, 34808, 18161, 29218, 17507,
       13231, 10350, 14440, 13903,  5826, 13978,  6298,  9776,  1407,
       33744, 45867, 38720, 11586, 41934, 34852, 14283, 14316, 37591,
       29787, 39067,  4058, 37850,  2976, 48478,  4173,  1349, 25403,
       33501, 27390, 10561, 45517,  3587, 35811, 37777, 33796,  6603,
       36327, 21382, 16636,  3664, 17550, 15777, 39377, 41688, 22133,
       20698, 21252, 42910,  9655,  3560, 32285,  3757, 41590, 44703,
        7855,  2990, 46531, 41005, 41249,   725,  2830, 22990, 15538,
       23739, 30857, 18403, 30967, 26706,  3879, 28260, 31160, 48087,
       48484,   528, 35648, 14087, 12261, 28076,  6274,  3519, 22153,
       25299,  8379, 48985, 48447,  1654, 48937, 42938, 41451, 36690,
        8423, 39866, 19180, 29161,  5029, 43529, 48807,  3913, 28596,
       27282, 38995, 17179,  9155, 44846, 44095,  9389, 47399,  3695,
        6594, 22202, 44586, 17707, 49475,  8263,  6636,  6569, 49564,
        3690, 26713, 17845,  4978,  6652,  8271, 41186, 20446, 42675,
       14744, 19857, 39698, 42313,  6535, 11081, 24421, 48367, 21229,
       25860,  4280, 13428,  4642, 33740, 39891, 35175, 35077,  9019,
        3377, 45605, 16640, 18628, 28903, 13038, 41302, 32757, 18185,
       20826, 25776, 30291, 19882, 48008, 35185,  3326,  7043, 38103,
       17728, 21659, 40842, 45311,  6770, 38810, 43637, 43694, 21453,
       45221, 18434, 41777, 25401,  5742, 45396, 42769, 34352,  1324,
       12324, 44517, 34816,  8208, 10904, 25292, 39772, 18054, 34882,
       19980, 35797, 37193, 25099,  5358, 12331,  5813, 36247, 46174,
       48521]), [9, 2, 7, 4])]
Collaboration
DC 0, val_set_size=1000, COIs=[6, 1, 7, 4], M=tensor([6, 1, 7, 4], device='cuda:0'), Initial Performance: (0.25, 0.04443984019756317)
DC 1, val_set_size=1000, COIs=[5, 0, 7, 4], M=tensor([5, 0, 7, 4], device='cuda:0'), Initial Performance: (0.262, 0.04436455535888672)
DC 2, val_set_size=1000, COIs=[8, 3, 7, 4], M=tensor([8, 3, 7, 4], device='cuda:0'), Initial Performance: (0.275, 0.04432411301136017)
DC 3, val_set_size=1000, COIs=[9, 2, 7, 4], M=tensor([9, 2, 7, 4], device='cuda:0'), Initial Performance: (0.225, 0.044532188177108765)
D00: 1000 samples from classes {4, 7}
D01: 1000 samples from classes {4, 7}
D02: 1000 samples from classes {4, 7}
D03: 1000 samples from classes {4, 7}
D04: 1000 samples from classes {4, 7}
D05: 1000 samples from classes {4, 7}
D06: 1000 samples from classes {1, 6}
D07: 1000 samples from classes {1, 6}
D08: 1000 samples from classes {1, 6}
D09: 1000 samples from classes {1, 6}
D010: 1000 samples from classes {1, 6}
D011: 1000 samples from classes {1, 6}
D012: 1000 samples from classes {0, 5}
D013: 1000 samples from classes {0, 5}
D014: 1000 samples from classes {0, 5}
D015: 1000 samples from classes {0, 5}
D016: 1000 samples from classes {0, 5}
D017: 1000 samples from classes {0, 5}
D018: 1000 samples from classes {8, 3}
D019: 1000 samples from classes {8, 3}
D020: 1000 samples from classes {8, 3}
D021: 1000 samples from classes {8, 3}
D022: 1000 samples from classes {8, 3}
D023: 1000 samples from classes {8, 3}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.07179966375231743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.062282080620527265) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.0866992349922657) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.10071668189764023) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.09029780164361) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.0706805988252163) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.09509955850243569) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.269, 0.12162477773427963) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.10274842362105846) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.0916736244559288) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.12655268445611) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.14859290887415408) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.404, 0.1201973113194108) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.11129003369808196) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.17445171085000039) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.18057729303836823) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.13035795422643423) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.451, 0.13108988019824028) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2068759422376752) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2312651706188917) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.1425810459293425) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.158301618501544) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2383120637461543) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.24315960404276848) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.1451124506518245) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.457, 0.15763706554472445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.2743811624888331) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.2798636357784271) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.1760368304941803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.17171229308471084) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.3013828208167106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.29754252534732223) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.1761757607832551) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.17199157116003336) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.3272228060346097) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.30223537323623895) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.19416647404991091) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.448, 0.17022600414045155) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.3739811360947788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.2888700136058033) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1000, COIs=[4, 7], M=tensor([0, 1, 4, 5, 6, 7], device='cuda:0'), Initial Performance: (0.0, 0.3502244997024536)
DC Expert-0, val_set_size=500, COIs=[1, 6], M=tensor([6, 1, 7, 4], device='cuda:0'), Initial Performance: (0.93, 0.006120357226580381)
DC Expert-1, val_set_size=500, COIs=[0, 5], M=tensor([5, 0, 7, 4], device='cuda:0'), Initial Performance: (0.896, 0.007846746120601893)
SUPER-DC 0, val_set_size=1000, COIs=[6, 1, 7, 4], M=tensor([6, 1, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 0, 7, 4], M=tensor([5, 0, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x762bfc6a61c0>, <fl_market.actors.data_consumer.DataConsumer object at 0x762bfc3cdcd0>, <fl_market.actors.data_consumer.DataConsumer object at 0x762c1c066e50>, <fl_market.actors.data_consumer.DataConsumer object at 0x762bfc3cdac0>, <fl_market.actors.data_consumer.DataConsumer object at 0x762c1c097dc0>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC 2 --> ['(DO3', '(DO4']
DC 3 --> ['(DO0']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.004847944553941488) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.006239216009154916) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.2804099201522768) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.36857542700693013) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.788, 0.020148791410028934) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.5, 0.05227834524214268) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.06620140628516674) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.004601156257092952) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.0060168980881571766) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.2654064565198496) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.3699344303701073) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.775, 0.01906551134586334) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.04979255723953247) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.057061853766441344) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.006172332348302007) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.007513790913857519) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.28568824915587904) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.3514050739137456) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.76, 0.028759520038962365) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.049195975422859195) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.0655620853304863) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005628434179350734) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.006341466660611332) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.2710462767910212) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.3722552032554522) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.799, 0.029645369440317156) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.504, 0.06950858837366104) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.451, 0.09241347587108611) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.00665931103238836) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006329266331158578) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.27489404047653077) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.37889712006412446) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.784, 0.031025056302547455) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.05432364654541016) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.0852816368341446) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO5', '(DO1']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004792127344757318) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.0070717412158846854) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.28446892846142874) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.4065714776692912) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.818, 0.021132924035191535) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.512, 0.06280938625335693) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.441, 0.08805636513233185) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004767677744850516) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005968466626480221) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.24538434442132712) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.4074127551652491) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.816, 0.030551764387870207) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.07452739143371583) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.448, 0.08874572482705116) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005279593093320727) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.00572655244357884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.24098527922853827) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.4294758947775699) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.828, 0.02430215822160244) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.06153342571854591) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.08006127738952637) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005746861964464188) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005226584147661925) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.2562052158638835) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.3964092186402995) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.824, 0.026351825654506684) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.06072342225909233) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.08325433014333249) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004367515132762492) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.006254730647429824) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2734184427689761) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.42799761629104616) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.802, 0.02786705820262432) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.474, 0.07952016705274582) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.11084738373756409) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO5']
DC 2 --> ['(DO1', '(DO3']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005815124213695526) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.00643373249284923) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.25433912333659825) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.44282886367873286) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.827, 0.03127430272102356) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.504, 0.07860765796899795) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.0906140220463276) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0045610538888722655) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005985826237127185) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.22444156565610318) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.4332660596817732) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.828, 0.029814839137718083) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.07732368764281274) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.08836006982252002) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.003794791681692004) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005663252059370279) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.21118854025937617) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.4136120694391429) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.83, 0.032338237591087816) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.587, 0.0570546365082264) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.08337299695611) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005253768019843847) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.924, 0.006580132929608226) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.21172833963669838) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.47529166941531004) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.813, 0.03869275423884392) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.503, 0.08167660582065582) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.419, 0.13119783893227577) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004307972558774054) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006451085262000561) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.2308136045895517) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.45201513696037) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.83, 0.028052161648869513) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.557, 0.056508253693580625) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.09919930925965309) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO3']
DC 2 --> ['(DO2', '(DO1']
DC 3 --> ['(DO5']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004681548445951193) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.00806490174587816) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.23136792396008968) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.4714118017926812) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.781, 0.03836263492703438) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.05985442698001862) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.10179519641399383) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004596972010098397) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.005975758355110884) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.2360338009018451) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.46530058535776336) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.805, 0.03787771324813366) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.058683076798915865) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.09278901210427284) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004148474030196666) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005479453390464187) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2512743160482496) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.4156490989997983) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.826, 0.030773506924510002) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.576, 0.06242850428819657) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.423, 0.11144185519218444) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0039388890825212) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005448428638279438) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.22870784074347467) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.4004938157279976) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.822, 0.03552147088944912) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.0605537936091423) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.11525509250164032) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004707122826948762) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006505914244800806) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.21939577488601208) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.46557823605265003) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.835, 0.029652138076722623) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.575, 0.06189932954311371) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.457, 0.09074907213449478) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO5', '(DO0']
DC 3 --> ['(DO3']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004243391573429108) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.006129249963909388) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.21535902482829988) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.48103798827901484) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.849, 0.020457207141444087) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.546, 0.07313626310229301) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.08647040829062462) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004703623773530126) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005120264714583754) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.26092368620447814) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.4467943084668368) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.813, 0.02479796438291669) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.548, 0.06582978248596191) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.07344273644685745) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005746824924834072) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005277362053282559) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.27226090972870587) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.4881845053957077) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.832, 0.024615292511880396) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.499, 0.0757724996805191) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.451, 0.11624371075630188) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004900179669260978) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006107185423374176) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.27250481223501266) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.4147351743505569) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.82, 0.03394982665777206) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.55, 0.07353431034088134) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.45, 0.12130160021781922) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005655342314392328) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005291938334703445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.2245028189532459) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.45688396788272073) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.824, 0.03823480886220932) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.548, 0.07075734823942184) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.09967214831709861) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO0']
DC 2 --> ['(DO3', '(DO4']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004220013424754142) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.0058636665102094416) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.23795646731369197) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.44507053734007057) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.835, 0.015808162346482277) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.05522851020097733) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.1130161827802658) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003983776971697807) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.005623312622308731) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.2525189238358289) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.42671380818029864) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.831, 0.025941067688167094) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.49, 0.0934396913945675) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.433, 0.13731053134799004) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004511418893467635) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.004506556056439876) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.21707919483073057) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.45363776085583957) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.821, 0.028319794729351998) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.07232345175743103) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.12488110157847404) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.005231027674395591) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005663825865834951) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.2471224521510303) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.3889651844010223) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.823, 0.026921942569315432) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.508, 0.06700079536437989) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.09712229153513909) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0050896634706296025) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.005191159303300083) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.25022593124210835) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.4497111236206256) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.817, 0.027560132429003716) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.07666334289312363) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.449, 0.10808965796232224) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO0', '(DO1']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005215530000627041) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005452047434169799) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.23723288665246217) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.39855883434810674) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.827, 0.019280304197221994) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.495, 0.06718754708766937) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.09587002757191658) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004328564718365669) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.005330973340198397) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.27259399625565855) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.4002134968675673) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.821, 0.019296824976801873) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.48, 0.07735363149642945) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.437, 0.11532918059825897) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0059902496945578605) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006125784302130341) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.289177996750921) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3666717611381318) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.813, 0.026661774300038816) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.09556299546360969) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.423, 0.1317994391322136) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004709334435407072) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.004661399153992534) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.24654095920547842) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.4194557261855807) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.816, 0.025812886774539946) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.497, 0.08489766550064087) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.10976928524672985) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005047003394458443) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.004273543436080217) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.23804673038981855) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.4141308653745218) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.814, 0.034463767260313034) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.504, 0.07407035207748414) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.414, 0.12291079716384411) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO4']
DC 2 --> ['(DO1', '(DO0']
DC 3 --> ['(DO2']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005522243721410632) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005019196080043912) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.2319697470087558) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.411998500822112) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.818, 0.033872278101742266) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.10163754051923751) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.112777549803257) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0053575994749553504) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.004589627446606756) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.21471698656957597) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.4495072907907888) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.824, 0.030260068491101265) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.09035997968912125) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.13379591742157937) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005618908529635519) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.005304968783631921) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.24206827799230815) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.3925026231560623) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.836, 0.024941806431859732) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.0816370880752802) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.458, 0.11074187129735946) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0053918164646020155) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.004729258120059967) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.19716570233367384) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.39711593235353937) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.836, 0.026786446750164034) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.498, 0.08351561796665191) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.12110879465937614) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004177084628492594) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.00437245655618608) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.22676919312030078) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3868720710540656) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.819, 0.03578856631461531) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.1087640233039856) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.422, 0.13018268248438836) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04443984019756317), (0.25, 0.07179966375231743), (0.294, 0.09029780164361), (0.405, 0.10274842362105846), (0.404, 0.1201973113194108), (0.451, 0.13035795422643423), (0.451, 0.1425810459293425), (0.461, 0.1451124506518245), (0.459, 0.1760368304941803), (0.468, 0.1761757607832551), (0.465, 0.19416647404991091), (0.5, 0.05227834524214268), (0.518, 0.04979255723953247), (0.536, 0.049195975422859195), (0.504, 0.06950858837366104), (0.52, 0.05432364654541016), (0.512, 0.06280938625335693), (0.475, 0.07452739143371583), (0.531, 0.06153342571854591), (0.52, 0.06072342225909233), (0.474, 0.07952016705274582), (0.504, 0.07860765796899795), (0.524, 0.07732368764281274), (0.587, 0.0570546365082264), (0.503, 0.08167660582065582), (0.557, 0.056508253693580625), (0.563, 0.05985442698001862), (0.606, 0.058683076798915865), (0.576, 0.06242850428819657), (0.577, 0.0605537936091423), (0.575, 0.06189932954311371), (0.546, 0.07313626310229301), (0.548, 0.06582978248596191), (0.499, 0.0757724996805191), (0.55, 0.07353431034088134), (0.548, 0.07075734823942184), (0.592, 0.05522851020097733), (0.49, 0.0934396913945675), (0.534, 0.07232345175743103), (0.508, 0.06700079536437989), (0.535, 0.07666334289312363), (0.495, 0.06718754708766937), (0.48, 0.07735363149642945), (0.459, 0.09556299546360969), (0.497, 0.08489766550064087), (0.504, 0.07407035207748414), (0.462, 0.10163754051923751), (0.464, 0.09035997968912125), (0.475, 0.0816370880752802), (0.498, 0.08351561796665191), (0.464, 0.1087640233039856)]
TEST: 
[(0.26325, 0.043459017485380176), (0.25, 0.06910176756978036), (0.28625, 0.08651740956306457), (0.405, 0.09793887826800346), (0.40425, 0.11450791484117508), (0.45, 0.12330296683311462), (0.4495, 0.13521357721090316), (0.46975, 0.1373603984117508), (0.46, 0.1669597494006157), (0.47575, 0.16677658635377884), (0.46975, 0.18435369455814363), (0.50825, 0.048732266262173654), (0.55575, 0.04468583409488201), (0.53225, 0.04748889464139938), (0.52575, 0.06592539261281491), (0.536, 0.04955914208292961), (0.52575, 0.05873150336742401), (0.51725, 0.06736673900485039), (0.56175, 0.05494419994950295), (0.545, 0.05470854887366295), (0.4595, 0.07829084923863411), (0.53075, 0.07109290400147437), (0.54175, 0.0721543387323618), (0.57675, 0.05317113724350929), (0.511, 0.07610358870029449), (0.56825, 0.05254613830149174), (0.5865, 0.05162105506658554), (0.604, 0.05159026426076889), (0.587, 0.05542050549387932), (0.59825, 0.05226885457336903), (0.587, 0.05403070621192455), (0.55425, 0.06920166790485383), (0.56425, 0.059352596312761303), (0.52175, 0.0709413385540247), (0.567, 0.06993653899431229), (0.55525, 0.0675658235102892), (0.607, 0.051989778846502306), (0.50725, 0.08801623910665513), (0.563, 0.06455669738352299), (0.54425, 0.05901004129648209), (0.554, 0.06836988416314126), (0.5235, 0.06146532768011093), (0.50475, 0.07281390842795372), (0.471, 0.09018132597208023), (0.515, 0.08061854669451714), (0.5095, 0.06835405603051185), (0.475, 0.09553141698241234), (0.48675, 0.08361262890696526), (0.50225, 0.07627918151021004), (0.52625, 0.07696875041723251), (0.49125, 0.10103114649653434)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.98      0.31      0.47      1000
           4       0.39      0.70      0.50      1000
           6       0.37      0.13      0.19      1000
           7       0.54      0.82      0.65      1000

    accuracy                           0.49      4000
   macro avg       0.57      0.49      0.45      4000
weighted avg       0.57      0.49      0.45      4000

Collaboration_DC_1
VAL: 
[(0.262, 0.04436455535888672), (0.251, 0.062282080620527265), (0.373, 0.0706805988252163), (0.463, 0.0916736244559288), (0.459, 0.11129003369808196), (0.451, 0.13108988019824028), (0.443, 0.158301618501544), (0.457, 0.15763706554472445), (0.461, 0.17171229308471084), (0.446, 0.17199157116003336), (0.448, 0.17022600414045155), (0.505, 0.06620140628516674), (0.466, 0.057061853766441344), (0.442, 0.0655620853304863), (0.451, 0.09241347587108611), (0.442, 0.0852816368341446), (0.441, 0.08805636513233185), (0.448, 0.08874572482705116), (0.462, 0.08006127738952637), (0.453, 0.08325433014333249), (0.431, 0.11084738373756409), (0.471, 0.0906140220463276), (0.467, 0.08836006982252002), (0.469, 0.08337299695611), (0.419, 0.13119783893227577), (0.434, 0.09919930925965309), (0.442, 0.10179519641399383), (0.447, 0.09278901210427284), (0.423, 0.11144185519218444), (0.434, 0.11525509250164032), (0.457, 0.09074907213449478), (0.475, 0.08647040829062462), (0.499, 0.07344273644685745), (0.451, 0.11624371075630188), (0.45, 0.12130160021781922), (0.474, 0.09967214831709861), (0.47, 0.1130161827802658), (0.433, 0.13731053134799004), (0.431, 0.12488110157847404), (0.446, 0.09712229153513909), (0.449, 0.10808965796232224), (0.463, 0.09587002757191658), (0.437, 0.11532918059825897), (0.423, 0.1317994391322136), (0.447, 0.10976928524672985), (0.414, 0.12291079716384411), (0.434, 0.112777549803257), (0.431, 0.13379591742157937), (0.458, 0.11074187129735946), (0.464, 0.12110879465937614), (0.422, 0.13018268248438836)]
TEST: 
[(0.25325, 0.0434291261434555), (0.25125, 0.06006230038404465), (0.37275, 0.067833630412817), (0.46325, 0.08763139560818672), (0.46075, 0.1062627123594284), (0.45225, 0.12552101576328278), (0.4415, 0.1515355521440506), (0.457, 0.1510611606836319), (0.45775, 0.1647727227807045), (0.44675, 0.16469195330142974), (0.45125, 0.16312419641017914), (0.5015, 0.06084761765599251), (0.489, 0.051991433694958684), (0.47075, 0.06270482715964318), (0.4585, 0.09297340935468673), (0.4575, 0.08150168201327324), (0.4665, 0.08626922303438186), (0.4675, 0.08906709915399551), (0.47025, 0.07966293212771415), (0.45725, 0.08191535270214081), (0.45525, 0.11063499638438225), (0.46775, 0.08923798532783986), (0.4765, 0.08462346442043782), (0.48075, 0.07977668386697769), (0.4485, 0.12446057137846947), (0.47975, 0.09273788765072823), (0.461, 0.09537399464845657), (0.48425, 0.08781673148274421), (0.4585, 0.10554959082603455), (0.44225, 0.1151141916513443), (0.47375, 0.08753051364421845), (0.48075, 0.08428009107708931), (0.50225, 0.07036658841371536), (0.45725, 0.12099094539880753), (0.4645, 0.12110106149315834), (0.48, 0.09623413154482842), (0.49175, 0.11043640032410622), (0.4485, 0.13755236425995826), (0.4575, 0.12157535427808762), (0.46625, 0.09372481435537339), (0.461, 0.10811614227294922), (0.471, 0.09610698363184929), (0.4565, 0.12359907877445221), (0.438, 0.14097003012895584), (0.45325, 0.11883650302886963), (0.43425, 0.12751285696029663), (0.437, 0.11945595020055771), (0.4505, 0.13713540068268776), (0.4675, 0.11147517067193985), (0.466, 0.12382578033208846), (0.446, 0.12887649983167648)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.97      0.11      0.20      1000
           4       0.41      0.72      0.52      1000
           5       0.59      0.07      0.13      1000
           7       0.44      0.88      0.59      1000

    accuracy                           0.45      4000
   macro avg       0.60      0.45      0.36      4000
weighted avg       0.60      0.45      0.36      4000

Collaboration_DC_2
VAL: 
[(0.275, 0.04432411301136017), (0.387, 0.0866992349922657), (0.451, 0.09509955850243569), (0.448, 0.12655268445611), (0.451, 0.17445171085000039), (0.464, 0.2068759422376752), (0.464, 0.2383120637461543), (0.47, 0.2743811624888331), (0.468, 0.3013828208167106), (0.467, 0.3272228060346097), (0.47, 0.3739811360947788), (0.468, 0.2804099201522768), (0.469, 0.2654064565198496), (0.461, 0.28568824915587904), (0.461, 0.2710462767910212), (0.462, 0.27489404047653077), (0.461, 0.28446892846142874), (0.462, 0.24538434442132712), (0.465, 0.24098527922853827), (0.466, 0.2562052158638835), (0.464, 0.2734184427689761), (0.461, 0.25433912333659825), (0.466, 0.22444156565610318), (0.475, 0.21118854025937617), (0.464, 0.21172833963669838), (0.469, 0.2308136045895517), (0.469, 0.23136792396008968), (0.468, 0.2360338009018451), (0.464, 0.2512743160482496), (0.461, 0.22870784074347467), (0.465, 0.21939577488601208), (0.468, 0.21535902482829988), (0.469, 0.26092368620447814), (0.47, 0.27226090972870587), (0.471, 0.27250481223501266), (0.471, 0.2245028189532459), (0.47, 0.23795646731369197), (0.469, 0.2525189238358289), (0.466, 0.21707919483073057), (0.465, 0.2471224521510303), (0.47, 0.25022593124210835), (0.468, 0.23723288665246217), (0.468, 0.27259399625565855), (0.471, 0.289177996750921), (0.472, 0.24654095920547842), (0.47, 0.23804673038981855), (0.474, 0.2319697470087558), (0.47, 0.21471698656957597), (0.472, 0.24206827799230815), (0.472, 0.19716570233367384), (0.478, 0.22676919312030078)]
TEST: 
[(0.27825, 0.043275490760803226), (0.38925, 0.08313762971758842), (0.45525, 0.09075938367843628), (0.461, 0.11947788047790528), (0.466, 0.16523100256919862), (0.4715, 0.1961016225218773), (0.4705, 0.22676054126024245), (0.47375, 0.25869140833616255), (0.4735, 0.28333983933925627), (0.47325, 0.3097177770137787), (0.47275, 0.3541496697664261), (0.4705, 0.2649703647494316), (0.47375, 0.25384714335203173), (0.463, 0.2727502326965332), (0.46575, 0.2600059432387352), (0.46975, 0.260648709833622), (0.4675, 0.27132990658283235), (0.4675, 0.23277173280715943), (0.469, 0.22864968085289), (0.472, 0.2430966297388077), (0.4715, 0.26106557273864744), (0.471, 0.2462024174928665), (0.4725, 0.21644935101270676), (0.475, 0.203016488134861), (0.4705, 0.20010558944940568), (0.47275, 0.22147659391164778), (0.4725, 0.22183109188079833), (0.46975, 0.22626259416341782), (0.47, 0.23780641609430314), (0.47175, 0.2163518345952034), (0.4735, 0.2078703859448433), (0.476, 0.20478933733701707), (0.47325, 0.2438149209022522), (0.472, 0.25918728578090666), (0.4755, 0.25912758451700213), (0.47375, 0.21200546830892564), (0.4725, 0.22557700020074845), (0.476, 0.24142177122831346), (0.4755, 0.20568173909187318), (0.47175, 0.2354703232049942), (0.4765, 0.23788091921806334), (0.47325, 0.22930063939094544), (0.47625, 0.26234212589263917), (0.47425, 0.2785904297828674), (0.4745, 0.23591991001367568), (0.47475, 0.22662948375940323), (0.47625, 0.21862055402994157), (0.4735, 0.20238212072849274), (0.4755, 0.23376237255334853), (0.47425, 0.18684880805015563), (0.475, 0.2150940477848053)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.34      0.97      0.50      1000
           4       0.00      0.00      0.00      1000
           7       0.00      0.00      0.00      1000
           8       0.84      0.93      0.88      1000

    accuracy                           0.48      4000
   macro avg       0.29      0.47      0.35      4000
weighted avg       0.29      0.47      0.35      4000

Collaboration_DC_3
VAL: 
[(0.225, 0.044532188177108765), (0.25, 0.10071668189764023), (0.269, 0.12162477773427963), (0.416, 0.14859290887415408), (0.441, 0.18057729303836823), (0.446, 0.2312651706188917), (0.463, 0.24315960404276848), (0.455, 0.2798636357784271), (0.467, 0.29754252534732223), (0.469, 0.30223537323623895), (0.468, 0.2888700136058033), (0.469, 0.36857542700693013), (0.466, 0.3699344303701073), (0.47, 0.3514050739137456), (0.465, 0.3722552032554522), (0.475, 0.37889712006412446), (0.473, 0.4065714776692912), (0.47, 0.4074127551652491), (0.471, 0.4294758947775699), (0.468, 0.3964092186402995), (0.469, 0.42799761629104616), (0.469, 0.44282886367873286), (0.468, 0.4332660596817732), (0.468, 0.4136120694391429), (0.474, 0.47529166941531004), (0.473, 0.45201513696037), (0.475, 0.4714118017926812), (0.473, 0.46530058535776336), (0.475, 0.4156490989997983), (0.471, 0.4004938157279976), (0.472, 0.46557823605265003), (0.469, 0.48103798827901484), (0.471, 0.4467943084668368), (0.468, 0.4881845053957077), (0.468, 0.4147351743505569), (0.472, 0.45688396788272073), (0.465, 0.44507053734007057), (0.471, 0.42671380818029864), (0.469, 0.45363776085583957), (0.474, 0.3889651844010223), (0.472, 0.4497111236206256), (0.472, 0.39855883434810674), (0.473, 0.4002134968675673), (0.471, 0.3666717611381318), (0.473, 0.4194557261855807), (0.466, 0.4141308653745218), (0.475, 0.411998500822112), (0.472, 0.4495072907907888), (0.474, 0.3925026231560623), (0.469, 0.39711593235353937), (0.471, 0.3868720710540656)]
TEST: 
[(0.23075, 0.04349978512525558), (0.25, 0.09680754977464676), (0.27325, 0.11629943352937698), (0.42225, 0.14138725328445434), (0.44, 0.17147793173789977), (0.4455, 0.21885153269767763), (0.4645, 0.2289725457429886), (0.4565, 0.2658118067979813), (0.46975, 0.28245870041847226), (0.472, 0.28643305253982543), (0.475, 0.2728337438106537), (0.465, 0.3481193963289261), (0.474, 0.3495753960609436), (0.4705, 0.33216763818264006), (0.467, 0.3530434309244156), (0.47125, 0.3570363402366638), (0.471, 0.3863753663301468), (0.4745, 0.3885590693950653), (0.473, 0.40946164667606355), (0.47425, 0.3781240234375), (0.473, 0.4085810979604721), (0.47175, 0.42390991163253783), (0.47075, 0.4139670776128769), (0.47625, 0.3960424124002457), (0.47525, 0.453007860660553), (0.47375, 0.42923498594760895), (0.4715, 0.44848649084568026), (0.4715, 0.44262175846099855), (0.47475, 0.3935658864974976), (0.474, 0.3814751430749893), (0.47425, 0.4378299715518951), (0.473, 0.45464008486270907), (0.47575, 0.421092888712883), (0.47225, 0.46327754032611845), (0.47325, 0.39512214696407316), (0.473, 0.4315939701795578), (0.468, 0.4239032734632492), (0.47125, 0.40942823255062105), (0.46975, 0.4361120647192001), (0.47375, 0.37529362964630125), (0.47, 0.43120787346363065), (0.47275, 0.3866634366512299), (0.4725, 0.3853541762828827), (0.47375, 0.34886554086208343), (0.4735, 0.40061887323856354), (0.47225, 0.39649495577812194), (0.47525, 0.39317357397079467), (0.47525, 0.429678138256073), (0.47675, 0.37232290518283845), (0.474, 0.37629795956611634), (0.47925, 0.3664578539133072)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.34      0.98      0.51      1000
           4       0.00      0.00      0.00      1000
           7       0.00      0.00      0.00      1000
           9       0.80      0.94      0.87      1000

    accuracy                           0.48      4000
   macro avg       0.29      0.48      0.34      4000
weighted avg       0.29      0.48      0.34      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [42]
name: alliance-3-dcs-42
score_metric: contrloss
aggregation: <function fed_avg at 0x76872a505c10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=42
Partitioning data
[[6, 3, 1, 0], [4, 9, 1, 0], [2, 7, 1, 0], [5, 8, 1, 0]]
[(array([44073, 21183, 23596, 47728, 22362, 44695, 31128, 11101, 38769,
        9468,  7705, 30033, 16760, 33704,   313,  4389,  8937, 40664,
       11672, 12356, 23451,   326,  9481, 31835,  9625, 38853, 40544,
       13754, 33212, 15185,   347, 14513, 38677,  1596, 13137, 27554,
       36188, 47513, 39889, 26364, 46971, 40041, 15983, 12491, 13390,
       18884,  5526, 42015, 31564, 19941,  8679, 39252,  7516,  1529,
          22, 22337, 32010, 20734,  8171, 26966, 24289, 36360, 15498,
       17372, 18655, 10058, 47115, 15052, 26940, 36841, 46052,   525,
       17596, 18621, 22795, 25168, 27678, 40685, 41088,  7964, 47396,
       31706, 33824, 13479, 35439, 48841,  8597, 37887, 15612, 15492,
        7392, 25819, 46429, 22384, 18257, 10532,   124, 35928, 12804,
       33731, 14231,  7823, 42460, 39637, 37574, 20717, 42186, 49537,
       22058, 31948,  9367, 34028, 37332,  3833, 25967, 48565, 29798,
        6810,  4143,  2888, 48753,  1360, 43697, 19959, 42690, 11363,
       34805, 20106, 25904, 43581,  3290, 38662, 46188, 29538,  2623,
       23876, 12238, 17317, 34912,  2674,  6749,  7215,  2411,  3048,
        4644, 19955,  9320, 37974,  7867, 45247, 18656, 45900, 34706,
       12550, 40128, 17680,  7265,  2503, 24261,  5223, 32339,  7092,
        2997, 37682, 29241, 11048, 14042, 49200,   154, 32198, 27385,
       34563,  8983, 30143, 13331, 35672, 25067, 35830,   327,  3567,
       40836,  1733, 30193, 29680, 11644, 28322, 45305, 10938, 22065,
       26240, 36788, 32082, 11466, 41641, 42778,  5921, 49082, 14257,
        3861, 30323,  9126, 23771, 48658, 49094, 26054, 39986,  7003,
        7968, 14894, 48607, 29123,  1228, 35732,  6125,  3227,  3780,
       43661, 49059, 38166, 17759,  3849,  6859, 10029, 39678, 33488,
         409, 15192, 48047, 36285, 37033, 49829, 10828, 24852, 35904,
       11875, 40462, 30074, 23787, 14749, 17151, 48446, 40797, 17603,
        3266,  2115, 36486, 12996,   125, 14889, 36282, 11192, 42141,
        3178, 42131,   315, 42949, 20520, 39924, 26609, 38931, 33925,
       32215, 37175, 37162, 30708, 48686, 20868, 39609,  3763, 45152,
       19465, 40314,  2525, 34407, 28717, 43813, 10869, 31626,  3320,
       39166, 22922, 39130,  7049, 41918, 11211, 26357, 31535, 39500,
       33422, 36212,  9304, 43123, 28472, 21070,  4745, 38317, 10967,
       32451,  9976, 10843, 25882,  8656, 15912,  4657,  7909, 37770,
        6557,  6167, 36664, 43283, 33371, 37187, 30838, 35009,   159,
       42647, 36781, 10254, 23995,  5389, 30683, 15175,  1030, 32392,
       21574,  7412, 25492, 41952, 46568, 29692, 21909, 37215, 35163,
       10468,  1573, 18452, 32145,  6071, 27270, 13552, 35124,  5610,
       44047, 13471, 35071, 11028, 29235,   895,  9694, 38702,  7230,
         367, 34682, 23322, 15844, 10611, 14125, 22701, 38879, 44859,
        5578, 49468,  9052, 48315, 26154, 14574,  4439, 11449, 39822,
       47819, 18889, 15083, 46402,  7906,  8781, 39871, 49855, 31141,
        9710, 42789, 12821,  1059, 46594, 39150, 20601, 21415, 20631,
       10746, 36930,  4449, 42206, 29177, 46880,  4428, 47217, 12601,
         258,  2723, 14578, 21524,  1265, 22030, 47413, 35804, 46109,
       31261, 14155, 26711, 16827, 29950, 47523, 42223, 39420, 29004,
       30479, 43744, 19187, 49582, 20496, 15954, 31479, 44103, 32043,
        5859, 30586, 45379, 47851, 27619, 40584, 12013, 42865, 13678,
       17449, 46232,  7460,  9609, 24082, 16705,  5870, 39300, 19352,
        6304,  3447, 12776,  3591, 35976,  8255, 37109, 15383, 10948,
        6200, 26126, 17841, 39220, 26472, 17018, 36745, 43554, 23977,
       41369, 26575,  3289, 21015, 29074, 36051, 19847, 17124, 39575,
        8330, 17879, 43034, 26859, 34756, 17814,  8507, 21156, 30737,
       12174,  4564, 25933, 21147, 15397, 25535, 46838,  9738,    74,
        8056, 42633, 19820, 14122,   334,  5065, 48601, 37863, 17565,
       22008, 41057, 28731,  7128, 12454, 36929, 21685, 43908, 46396,
       12773, 48122, 38084, 42606,  6751, 18908,  8812, 30135, 19522,
       30182,  9189,  2180, 15583, 45214,   354, 11750, 30154, 24725,
       24463, 47109, 15770, 48358,  7255, 24400,  8232, 10190, 23921,
       24620, 47266, 28671, 28615, 36641, 38577, 21971,  7379, 41922,
        8667, 29188, 37801,  4047, 11407,  2582, 35861, 48595, 38596,
       39953,  6064,  9690,  2143, 35576,  9646, 47616, 20994, 35910,
       13277, 38514, 22399,  2204, 21528, 38725, 38682, 48740, 10292,
       10755,  4648, 25150, 22072, 15867,  9658, 19481, 37588, 12982,
       22111,  1559, 49409, 47470,  4983, 42805,  5197, 37030, 47863,
       42918,  1304, 46674, 30227, 13908,  9138, 16726, 10644, 28910,
       14859, 44448,  3064, 24284, 31737, 25323,  1660, 31512, 41439,
       19533, 30457, 49356, 21094, 12127, 45470, 34794, 35850, 12276,
       26304, 10363, 36791, 12920, 15299, 34347, 17752, 35649,  4100,
       22212, 12451, 13895, 24288, 17309,  9318, 20079,  9595, 28937,
       32087,  9509, 36028, 30747, 19886, 33059, 13608, 47772,  3934,
        8107, 36879,  6779, 40410,  9878, 30570, 11824, 29347, 16766,
       20746,  1907, 20753, 46840, 24774,  5137, 47987, 26141, 13849,
       44146, 23151,  2286,   568, 19354, 49447, 47980, 24833, 31274,
        7147, 31268,  9946, 48820,  9889, 40250, 40713,  4334,  1731,
       36040, 35106, 26467, 48264, 47151, 42413, 37618, 13755, 37179,
       46237, 22564, 33126, 46544, 43655, 24527, 38347, 12628, 43935,
       41701, 38174, 10824, 41471,  7130, 23578, 23864,  4993, 40958,
       18311, 13940, 49438, 16810, 24224, 33198,   536, 25459, 16012,
       21631, 16853, 24520, 36774, 13462, 35366, 24999, 48845, 45228,
       49998, 30560, 32684, 27488, 14549, 34336,   917, 42658, 16723,
        5420, 26794,  5202, 36736, 23232, 30047, 14048, 32665, 16986,
       42437, 10910,   815, 40204, 40595, 39735,  6831,  2775, 39387,
       34809, 30344, 37468,  5969, 40275, 43245, 18009, 23386, 29944,
       29364, 47106, 44599, 48378, 12469, 29596, 26162,  2718, 44906,
         332, 27961, 22026, 28972, 31164, 47678, 12030, 40747, 46576,
       17485, 23478, 36858, 27026, 38344,   349,   628, 47320, 31837,
       45242, 34856, 47691, 16238, 10553, 30977, 47379, 20265,  8382,
        7727, 27104, 26430, 20759, 11989, 18857, 31521, 44294,  6165,
       21881, 23194, 15473, 13326,  5873, 10394, 25337,   752, 26842,
       38551, 40945, 36750, 40287, 32247,  4295,  9014,  9140, 26546,
       18218, 33596, 47991, 17196, 30743,  4957,  4859, 25763,  4759,
       37558, 39716, 28978, 40644, 28208,  4645, 24387, 48470,  4504,
       47802, 31654, 15594,  2490, 23964, 10215,  6085,  2707, 15886,
       29480, 47579, 25350, 10334, 22962, 38855,  9813, 21874,   165,
       13357, 19294, 30378,  3318, 36149,  6242, 44715, 16840,  7660,
       42476, 19945, 36055, 34106, 36163,  8857, 18291, 41238, 39739,
       21825, 48671, 22847,  4861, 41300, 46898, 10043,  6396, 20924,
       23786, 21144, 49420, 23825, 29819, 36836, 42399, 10686, 13051,
       38977, 13835, 31459,  6276, 47550, 14949,  4765, 21195,  2578,
       43292, 45096, 17490, 46200, 19860, 31083, 36623, 12836, 38224,
        3549,  8383, 12767, 32592, 15863, 44167,   940, 42755,  1988,
       14534, 11897, 37628, 19237, 49556, 41258,  3362, 14213, 38656,
       49856, 38187,  8623, 11416, 21469, 23684, 12010, 47711, 36254,
        3618, 40411, 15541, 17054, 11572, 46614,  9493, 34017,  2513,
       47560, 48516, 14194, 25748, 42678, 32194, 42205, 14441, 40022,
        7347, 41731, 37646, 36786, 12429, 39670, 18953,   695, 28456,
       16100, 34919, 18047,  6904, 31488,  2435, 14594, 37729, 29246,
       11783, 24521,   694, 35758, 18677, 37806, 33750, 28636, 40479,
       16230, 49018, 29540,  5533, 18955, 43375,   748, 31119,  8199,
       35343, 39179, 10732,  5699, 35209, 21119, 26269, 27749, 32267,
        1039]), [6, 3, 1, 0]), (array([ 4666, 12143, 39947, 36083, 20608,  5946, 45609, 19936, 21684,
       29799, 48902,  8772, 28174, 30790, 33066, 20342, 43758, 26854,
       43923,  7100,  7062, 42938,  8117, 23393,  7383, 41270, 34261,
       48008, 19957,  5143, 26417, 49207, 14303,  4773,  6475, 27914,
       20485, 30341, 24377, 49024, 48879, 24933, 14664, 13787,  2922,
       35630, 41927, 30616, 16936, 48429, 31799, 18014,  3566, 45311,
       16085, 10385,  2710, 41635, 29131, 32026, 22727, 44473, 14082,
       13292, 44790, 12213, 16155, 22139, 30881, 37234, 43974,  7154,
       27469, 27213, 36896, 24236, 46940, 10629, 25949, 38802, 31433,
       24827, 22619, 29613,  6311,  5364, 26897,  1557, 12797,   982,
       25346, 27663, 28414, 30384,  2976, 32803, 15909, 34784, 27787,
       38353,  3802, 28885, 35925,  9452,  1639,  3393, 41788, 29368,
       22480,  3985, 48533, 44254, 29922, 14385, 14923, 10426, 17179,
       17819,  8523,  5594, 14810,  1535, 47305, 41820, 48478, 49114,
        8378, 31993, 28260, 31514,  2444, 31848,  6594, 33222, 40844,
        8208, 16294,  3970,  8379, 26653,  1148, 26351,  3556, 46339,
        3957, 24241,  7099, 49758, 39061, 27817, 41669, 42969, 22519,
       20956, 23823, 47441, 41097, 48945,  9848,  5721, 48163,  4943,
        6844, 22133, 18912, 34384, 26638, 42418, 12898, 26540, 20192,
       14041,   420, 18821, 36843,  6437, 49352, 34808, 44846, 37871,
       11092, 29756, 34082, 25846, 48884, 13282,  3688, 39734, 15018,
       28649,  3383,  5625, 15105, 26475,  4497, 47928, 23474, 18669,
       38755, 39202,  3141,  1407, 27214, 36533, 34215, 27429,  3326,
       27596, 36914, 45170, 18060, 41547,  6421, 33094,  9771, 30719,
       15674, 46255, 42087, 22246,  1349, 36777, 48521, 18320, 12106,
       36247, 27417, 29083, 21771, 17845,   398, 10865, 36600,   945,
        6532, 36199, 15487, 35066, 31549, 22944, 35809, 18300, 41453,
       49645, 27647, 36976,  1366, 35522,  6144, 27180,  4865, 36734,
       44242, 28459, 22989, 48431, 39556, 23908, 47690, 27717, 29073,
        8323, 29438, 19525, 31909, 36807, 26530, 19486, 48672, 36223,
       11678, 12708, 10071, 25499,  3876, 13588, 43596,  1117, 24609,
       40931, 32273, 16143, 20231, 44013, 26439, 24893, 40315, 34995,
       22009, 32952,  4584, 10092, 23645,  5595, 37600,  9673, 10030,
         749, 35178, 14471,  5300, 42058, 46685, 31458,  8506, 14723,
       20517, 26150, 36887, 32778, 27569, 35203, 39721, 19076, 14954,
       18235, 26050, 41904, 25510, 45055, 18668, 45197, 29238,   443,
       47343, 11258,  1827,  4643, 26627, 13609, 16803, 40693, 23302,
       43325, 18982,  8402,  8754, 28215, 18681, 49833, 40324,  9277,
       41899, 11388,  9959,  4142,  1767,  2935, 44315,  2251, 21887,
       30285, 27581, 10717, 35251, 21067, 36878, 46335,  4665, 29197,
       19432, 16642,  8126, 15221, 23886,  2219, 41539, 11620, 21801,
       23624, 22285, 16152, 21478, 42704, 49181, 31638, 45059, 33547,
       36834,  2129,  5133, 21714,  9027, 30175, 23979, 26121, 16718,
       49427, 23325, 14868,  1437, 40885, 27369, 33746, 28901, 34370,
       26848, 47755, 18968, 42624, 44380, 17747, 37347, 32439, 16773,
       48568, 15978, 25285, 31517, 23961, 41505, 26681, 34774, 42911,
       15384, 19891,  5801, 32084, 45735, 25945, 43104,  8567, 33676,
        9513, 48868, 32041, 27040, 23798, 35676, 44669, 10762,  7629,
       48165, 26763, 43760, 43560, 31768, 36557, 45174,  9936, 14198,
       39930, 24453, 23840, 34402,  3819, 21093,  5394, 46438, 28235,
       15350, 20296, 25936, 11430, 31790, 34423, 48919, 43616, 12986,
         683, 37604,  5382, 10467, 28379,  7583, 44056, 48449, 13208,
       19826, 17600,  3510, 15501, 48921, 45302, 38687, 26466,  6934,
       38035, 36794, 44469, 46351,  5485, 23471, 31012, 49904, 31618,
       13962, 43652, 46852, 35852, 41156, 41436, 47419, 37115, 10781,
       29384, 18488, 31771,  4028, 13780,  7587, 38858,   238, 33505,
       20327, 23431, 41963, 32311, 46519, 28876, 26008, 25389,  9620,
        4354, 10145, 22656, 34595, 10747, 12311, 21418, 48269, 29829,
       44412, 28530, 27802,  2771, 18670, 30912,  2339, 30086, 12360,
       40297,  8191, 38658, 11196, 41105,  7738, 14167, 21283,  7019,
       30103, 22490, 32121, 49674, 42239, 29763, 40431, 33845, 26298,
       40185, 14721, 19575, 13323, 17726, 38549, 37145,  8149, 28008,
       25689, 49652, 36046, 33314, 15261, 35666, 25463, 14900,  1287,
       37301, 32474, 31969,  8189, 37260,  8559, 12123, 27538, 15306,
       46953, 16286, 44876, 18331, 49705, 35757,  3735, 33646, 43520,
       35406, 25544, 45856, 15934, 16774, 40936, 43797, 33292, 39902,
       22217, 49269, 29794, 37995, 25105, 34128, 49769, 44466, 45099,
        4803, 28054, 39912, 18238, 26093, 21215,  6969, 47370, 16309,
       25750,  9478,  6639, 13240, 33561, 38078, 49286, 32003, 18562,
       43988, 42130, 31890, 36086,  9251,  4927, 22586,  2630, 17275,
       31300,  4099, 33951, 31147, 23100, 32235,  5827,    94, 44824,
       24936, 17205, 24146, 18456, 33643, 46794, 49121, 37183, 25069,
       34618, 23524, 26742,  5195, 40964, 19095, 10642, 11515, 23311,
       39990, 27330,  6813, 18844, 48639, 16412, 37327, 34862, 41946,
       28268, 10249, 16956, 14358, 25619, 45988, 39208,  2039,  5635,
       45910, 16958, 17017,  8757, 30236, 24297, 26550, 10458, 19574,
        5895, 21152, 44183, 37512, 25682, 20463, 41862,  4949, 20475,
       21429, 28617,  1586, 38637, 26368,  1790, 43984, 43585, 15855,
       25048, 27260, 49381, 31552, 18649,   874, 48263, 20595, 37461,
       22545, 38463, 49777, 27700,  8628, 42335, 13056,  7349, 46786,
       17106,  2615, 23357, 38231, 13191,  4132,   561, 24075, 19760,
       42976, 43290, 39437, 18160, 15783,  6269,  9584,  2727, 11580,
        2227, 42709, 25007, 34913, 13922, 38157, 42912, 44936, 29283,
       20336,  8085,   947, 27460, 45188, 44319, 40590, 40194, 28133,
       45348, 24735, 36889, 14689, 12192, 29666, 40131,  7470, 25362,
       10855, 29380, 21674, 43002, 47483, 18767, 14488, 22326, 40861,
        8453, 17200, 29854, 36370, 29367, 28653, 32076,  7168,  6374,
       40928, 47805, 44307, 32149, 41355, 47912,  8376, 10323,  4387,
       19632, 19386, 46910, 23767,  9860,  7826,  2006, 24939, 40607,
        1119, 17692, 25198, 32484, 42110, 45123,  2895,   341, 17520,
       17237, 20682,  7547, 11280, 21231, 29378, 24445, 28681,  3940,
        3536, 31728,  5529, 39959, 22800, 30539, 14334,  6015,  3903,
       16838, 15554, 38233, 37049, 39396, 37504, 28311,  7093,  4524,
       21760, 12181, 25457, 19329, 20257,  2613, 18525,   317, 26124,
       41179, 35028, 39431, 34173, 32874, 49510,  4516, 22521, 29444,
        9945, 49656, 37519,  7360,  5642, 40635, 41019, 35927, 45615,
       31450, 10487, 29641, 20233, 31927, 28068, 46839, 24750,  3180,
       48729,  6663,  2478,  1376, 43564, 45106, 30487, 48851,  4281,
        3564, 19277, 44645, 34780,  6328, 23024, 26178, 13817, 26296,
       16559,  4176, 23126, 15236, 21522, 24661, 25422,  5346, 12944,
       42241, 32683, 20473, 30913, 28865, 44552, 21774, 33003,  8478,
       26599, 19662, 39927, 16346, 10031, 43160,  1329, 35599, 18999,
       28665,  8549, 35377, 21227, 37032, 44785, 26391, 39504, 37905,
       13656, 35883,  4574, 35509, 23522,   598, 35763, 43667,   129,
       38307, 27080, 45384,  5094, 24795, 34189, 40343, 36363,  3128,
       48618, 21439, 27703, 13613, 21782, 48071, 46046, 35626, 49423,
       40556, 36415, 34646, 32631, 33509,  5924, 19445, 12764, 49174,
       15044, 12916, 36862, 38478, 43838,  7647,  4348, 47274, 33542,
       19730, 29189, 37250,  2169, 41792, 44619,  6735, 30408, 27315,
       31556,  8569, 29492, 41414, 18211,  3184, 24592,  3042, 17093,
       46367, 30321, 23491,  1382, 48958, 16253,  7196, 36632, 15414,
       41456]), [4, 9, 1, 0]), (array([37315, 14106, 29220, 39412, 37615,  8641, 37821, 20651, 44177,
       33293, 46673, 25455, 25038, 31269, 17779, 48074,  8219, 47587,
       19204, 10417, 43190, 48493, 40189, 33550, 25478, 40256, 15303,
       26990, 13290, 16221, 39931, 34529,  9319, 15751, 40385, 48146,
       31926, 13671,  2557,  8121,  1107, 31941, 18837, 42588, 28403,
       41312,  5922,    54,  3639, 28834, 40148, 15477, 13837, 22618,
       38476,  2247, 15563, 48518,  5016, 32499, 44302, 20880,  7844,
         830, 34495, 15522,  9424, 28224,  4902, 22045, 32141, 39508,
        8341, 43116, 28935, 37403, 14926,  7410, 46491, 14404, 12448,
        8234, 43312, 17826, 36585, 44670,  1334, 18086, 12782, 30102,
       40213, 22287, 42144, 12141, 30874, 26551, 14299, 19343, 43873,
       37613,  4023,  8776,  4719,  7807, 37352, 47335,   218, 30542,
        7919, 34630, 12346, 35460, 27560, 28948, 33183, 30466, 17817,
         411, 31026, 11921, 30023, 33564,  7948, 31040, 13556, 49224,
       18964, 29150, 13727, 40371, 31882,  1110, 19163,  7178, 49397,
       19789,  9227, 18863, 38007,  1789, 48244,  6946, 24902, 35537,
       36726,  3963, 39550, 25901, 15638,  9577, 21836, 41538, 31041,
       30478, 24739, 29581, 39976, 14146,  4491, 40115, 38022, 24706,
       19287, 24904, 17980,  8543, 33081, 19231, 21521, 24978,  9660,
       12721, 47449,  9240, 27580, 12536, 47481, 35734,  9740, 21442,
       25379, 42821, 13408,  5131, 26696, 32103, 14768, 25200,  7452,
       39514, 17206, 13786, 17380, 10474, 19655, 30654, 37549, 31420,
        8032, 23692, 17070, 47492, 14120, 43162, 23237, 43513, 31361,
       17262,  7194, 47533, 48287, 17834, 17844,  6392, 46810, 45341,
       45021, 29637,  7633, 20957, 44218, 23493, 14317,  3254, 32047,
       30669, 35230,  9954, 25743, 35294, 45238, 11528,  6502, 37766,
       31058, 35022, 13448, 43631, 24429, 11991,  7576, 34373, 35862,
       46179,  5527, 49991, 11813, 12656, 19964,  4588, 33450,  9224,
       24908, 39518,  3135, 39755, 30761, 14400, 28945, 26702,   623,
       28446,  4157, 32296,  8681, 18258,  7182, 38611,  8533, 31990,
       47092, 48767, 27748, 39494, 43919, 32673, 11881, 27634, 10259,
       44578, 14503, 22782, 10496,  1898, 47665, 36133, 19849,  2969,
       12658, 45851, 40930, 47153,  6739, 37211, 43671, 18941, 21482,
       18411, 43491, 10077, 48649, 14680, 41700,  1650, 34487, 39378,
       35814, 13124,  4736, 14745,  8814, 29588, 33921, 33151,  3487,
       23621, 14144,  2392, 23506, 20186, 46454, 38252, 30987, 21025,
       30319,  2813, 34899, 13268,  4933, 46348, 16025, 14977,  1276,
       14319, 25213, 32660, 21001, 23860, 45027, 37200, 35108, 40300,
       46988, 23834, 43083, 40912, 19962, 33981, 43580, 25968, 41052,
        7894, 37601, 38955, 49407, 32335,  2000,  4790, 44222, 43515,
         440, 13376, 14333, 21108, 13272, 18491, 41624, 40723, 29137,
       44356, 32696,  5058, 38110, 35975, 47170, 43163, 22031,  1054,
       29916,  6218,  1365,  4238, 40604, 19250, 33397, 47241, 49942,
       27781, 26796, 15296, 33496, 41001, 13423, 45058, 38474, 20016,
       43544, 48826, 19956,  6326, 33349, 14095,  3348, 34584,  6748,
       27916, 24012, 40439, 49264, 31520, 25365, 26032,  1488, 47748,
       19794, 36513, 10386, 10562, 43629,  2554,  6726, 43791, 14983,
       23744,  5737, 10348, 24970, 23053, 34228, 12381, 16914, 39345,
       23292, 30216,  5560, 47455, 48853, 25788, 27915, 17956, 37908,
       30030, 34372, 27086, 43293, 28150, 12683,  3835,  5771, 13058,
       41000, 22267, 14556, 37884, 24547, 15081, 17023,  1490,  7204,
       18601,  8751, 41959, 29062, 23880, 24698,   178, 33855, 22566,
       19461, 45722,   181, 28639, 19326,  5890, 24982,  9778,  8817,
       21822, 35623, 13818, 33684, 32507, 47185,  4076, 34583, 25410,
        5317, 41843, 30935,  3651, 12763, 27930, 29669, 26259, 26343,
       41805, 26017,  8461, 44826, 24954, 15662, 39696, 45859, 41519,
       29044, 46463, 28761,   565, 33532, 10630, 13973, 27024,  7875,
       19226, 44282,  9066, 33625,  3404, 40472, 29485, 28526, 31522,
       37412, 44198,  9822,  6703, 36751, 30446, 33539, 38327,  9365,
       25596, 23514, 30676, 34358, 45613, 24398, 39769, 32319,  9127,
       15609, 38491, 26937, 22822, 46730, 15100, 23379, 47256,  5311,
       23317, 27009,  8801, 13701, 40822, 41133, 25363, 39974, 32083,
       23139, 48515, 29616,  6429,  7102, 45475, 13740, 11007,  1631,
       17746, 24008, 43089, 23164, 34364, 41336, 14311, 11264, 31765,
        1689, 31711, 13509, 16882, 28896, 27545,  7495, 24807, 42113,
       20057, 46679, 24220, 28476,  8475, 39674, 22421, 45977,  4533,
       35635,  3230, 30183, 20811, 39352, 12260, 19898,  8825, 44960,
       40075, 42171, 28187,  9809,  5831, 42669, 11767,  5112, 22316,
       46956,  6683, 13244, 10377, 17209, 32402, 35508, 32486, 48749,
       37937, 20088, 49494, 42925, 14129, 22145, 28297,  3025, 11640,
       26471, 34802,  5515, 19564, 26474, 16278, 34069, 16938, 35264,
       18438, 18907,  8938, 25196, 10655, 25799, 39622, 10090, 36015,
       43547,  8181, 25857, 49993, 22261,  4101,  3922, 20661,  5811,
       21398, 20764, 30083, 45332, 42143,  8977, 22372, 41051,  2568,
       40808, 22037,  1820,  6130,  6668, 28394, 24157, 45317, 44835,
       28163, 27194,  8573, 12849, 26331, 26456,   617, 43618, 22818,
       48128, 13534,  1020, 49973,  5146, 29120, 21280, 21720, 19526,
       45403,  8809, 49977, 15512,  4171, 47839, 27607, 28858, 35357,
       45329,  1320, 25611, 17002, 39490, 40615, 35070, 25697, 27512,
       44751, 21003,  6494, 30120, 29875, 40090, 42150, 43480, 43061,
       29066, 32847, 30166,  4609, 20353, 24228,  8307, 31158, 25135,
       24151,   962,   991, 41248, 37687,  3483, 31925,   206, 18415,
       36593,  5627, 45427, 39275, 22055, 14040, 16953, 31319, 23275,
        5552, 41663,  6959,  5010, 49507, 19858, 10399,  8522, 36386,
       45604, 47468,  9859,  1335,  4991, 25439, 14520, 42309,  5477,
       24543, 42622, 10617, 19749, 36311,  1960, 26045, 35470, 16625,
         179, 46554,   284, 10516, 13649, 44637, 49633,  8292, 44932,
       44350, 12131, 11176, 25489,  9619, 40752, 22993,  2391, 10265,
       47271,  6570,  8889, 14754,  9616, 39628,  2863, 13260,   407,
       11474, 28305, 27646, 47456, 18847, 40190, 36925, 42754,  6428,
       46287,  4096, 39904, 29295, 36491, 44037, 15581, 13436, 22109,
       24212, 28512, 40525, 30426, 17203, 10115, 29099, 25546, 37432,
       21217, 17107, 30895,  1338, 12785, 42934, 17076, 21721, 18630,
        9124, 16564,  9059, 11107,  1514, 34989, 17328, 44609,  2946,
       45358, 32885, 21308, 49439, 15003, 21892,  9181, 44770, 41628,
       47163,  7299, 38618, 42952, 43700,  7925, 11304, 48327, 41164,
       38943, 11122,  9966, 22692, 33229, 25037, 12363, 11255, 49671,
       46876, 12078, 48119,  6373, 44116, 29732, 32403, 32460,   557,
        6920, 22129, 33249, 47969, 47916, 48197, 37493,  9328, 12328,
       32033, 32629,  3053, 38479,  5955, 41740, 44027, 44662, 42784,
       17183, 10811, 38613, 18381, 24258, 14350, 12133, 36252, 35042,
       10847, 40241,  7788, 24360,  7183, 28063,  3859, 28400, 27751,
       15030,  7060,  2967, 46553, 48289,  3382, 10024, 45716, 48359,
       44082, 12660,  3842, 24021, 43811,  3918, 43951, 39104, 25531,
       31339, 34270, 31214, 24802, 28267,  6640, 42867, 15396, 26427,
        3665, 22716, 12435, 25106, 32659, 14023, 17100, 13864, 39926,
       13850, 39899, 36886, 17011, 34568, 45509, 39113, 42563, 35286,
       16892, 32669,  4726,  1473, 10690, 42456, 20537, 20831, 45232,
       36068, 45262, 44145, 30172,  5640, 11224, 48691,  8118, 13876,
        5414, 22458, 45982, 48852, 11276, 12270,  4906, 38617, 25190,
         843,  6954, 14528, 43219, 18603,  9696, 48986, 10326, 16214,
       24367]), [2, 7, 1, 0]), (array([17346, 10744, 37508, 24120, 21089,  9397, 47629, 28022,  8204,
       25333, 15019, 41531, 10119, 39649, 15499,  2963, 31064, 29603,
        8400, 25248, 41098, 46780, 23268, 17186, 37880, 21379,  5201,
       42776, 37589, 47524, 48403,  1411, 49638, 27445,  1545,  5305,
       36378, 22430, 46767, 16698, 11496, 11174, 11636,   784, 48625,
       31132, 26889, 33319, 43991, 45653, 32079, 26543, 12573, 26173,
        5057,  9035, 36308, 13352, 20122, 20615, 37395,  4067, 44154,
       30757,  4424,  3152, 17375, 13111, 10541, 15485, 35748, 39677,
       16643, 24953, 20647, 20020, 49715, 26010, 48645, 13252, 31631,
       44024, 47905, 18939, 39558, 33295, 28754, 22390, 17773, 21148,
       17425, 21961, 14681, 28099, 35390, 28340, 44632, 11753, 18262,
        8502, 20322, 23639, 48161, 34216, 16212, 48135, 25202, 10586,
         450, 24656, 42494, 48208, 26917, 46471, 42152, 25057, 49052,
       30140, 27968, 34757, 47334, 44211,  1331, 20672,  5288, 36127,
       32636, 33795, 45278,  2167, 19105,  7864,  1786, 10294,  4893,
       20939, 32889, 24128, 15915, 14696, 17670, 10740, 49644, 27093,
       11189, 47672,  1810, 24037, 45222, 49333,  1298, 28967, 44112,
       15133, 33768,  6807,   607, 10759, 11396, 18001, 20185, 28306,
       12791,  8276, 22424, 21534, 38851, 31543,  5547, 21075, 27671,
       24006, 45831, 47012,  3748, 33552, 46165, 16496, 28902, 32782,
       20776, 34859, 47781, 28756, 43467, 12326, 21689, 17619, 40681,
        8457, 14663,  4874, 29774, 19252, 18003, 21266, 37706, 20092,
       44033,  1717, 30310, 48477, 17354, 48048,    40, 46010, 19537,
        9987, 20484, 48139, 44229, 43455, 44832, 15544, 20495, 24130,
       48341, 38788, 40310, 39176, 31248, 42645, 45184, 43338, 29660,
       26738,  1033, 21436, 47621, 31240,  8768, 17882, 34403,  8671,
       34887, 27890,  8287, 11259, 38629, 44306, 32452, 36030, 26073,
       12581, 24452, 13190, 42384,  4817, 29816, 40528, 41403,  7668,
       25618,   240, 26980, 25104,  8668, 21036,  4594, 42275, 20962,
       25472, 44720,  9648, 29695,  7496, 15676, 40881, 40137, 26481,
       26437, 30148, 16435, 22255, 23572, 31663, 22920, 12258, 34814,
        3740, 15241,  6756, 10702, 15223,  4652, 10810,  9324, 41314,
       21905,  1771, 44384, 14402, 14883, 16676, 18312, 12172,  2586,
       41264, 42048,  7306,  2166, 18382, 17854, 20150, 45017, 34983,
       28807,  2754, 20492, 42207, 34096, 23962, 34712, 37823, 29493,
        5523, 37994,   777, 11141, 47793, 16738, 32322, 49393, 16381,
        1357, 19195, 34963, 23538, 33231, 49830,  8278,  8445, 16483,
       38458, 43713,  1391, 23839, 12353, 33554, 49184, 22635, 31370,
        5732, 34276, 21456, 27490,  4272, 13004, 12891, 38525, 12201,
       34521, 24093,  9262,  7384, 13388, 13883,  9226, 10096, 41288,
        1994, 37002, 49960, 35150, 40500,  1616, 10098, 32698, 11842,
       17899,  5054, 47212, 29903,  5911, 38524, 43348, 30788,  8045,
       28037, 36584, 26220, 38002, 24787,  4974, 34152, 35864, 14587,
       29661, 45777, 13227, 46001, 40093, 48678, 26923, 34523, 27621,
       49744, 23080,   741,  3065,  2434, 27034, 38436, 40789,  7156,
       22056,  3541, 14245, 16159, 42032, 16898, 20198, 35374, 43899,
       23560, 20209, 35736, 45966, 14711, 41078, 37483,  3881,   456,
       42693, 42325, 19557, 44659, 36553, 27692, 38408, 45626, 25450,
        1720,  1250, 15743,  7790,  8337,  1686,  9826, 25521, 38120,
       15276, 14176,  5360,  7934, 20219, 31996, 36939, 21759, 47992,
       13693, 10088, 30514, 23219, 45648, 32819,  1914,  5427,  8489,
       41225,  1506, 36765, 15927, 16846, 20594, 39507, 30762, 45650,
        9883, 26951, 24271, 40124,  6912, 47445, 22862, 42557, 32543,
       39766, 24281, 22758, 21823,  8968, 23625, 30164, 36893, 39079,
       48299, 12954, 30745, 35274, 18182,  2284, 32800, 14934, 13958,
       43913, 47960, 21579, 38857, 48164, 30949, 12917, 32168, 48496,
       27262, 11571, 40358, 42507,  4050, 21137, 14855, 29791, 14481,
       12977, 45677,  7448, 48915, 17112, 21407, 30902, 49151, 14776,
       25201,  8090, 41489, 29007,  3120, 25488, 34442,  8944, 18594,
       16925,   942, 17255, 12080, 46259, 23901, 41733, 37027,  6851,
       42753, 41445,  6161,  8174, 36237, 31445, 13218, 42292, 39749,
       37203, 25209, 13217, 30712, 41602, 12648,   140, 16543, 17995,
        7913, 10826, 19865, 26288, 45655, 38950, 44726, 25306, 48075,
       28027,  4858, 14145,  7597, 18495, 42652, 46547, 25210, 38919,
       15172, 22710, 30374, 21661,  4565,  2928, 10248, 28810, 31534,
       49435, 17223, 40178, 38482, 10251, 32305, 42833, 27794,  3212,
       48030, 19446, 15740, 33970,  2844, 48294, 14381,  1570, 23662,
       11966,  7288,   997, 32366,  1476, 39039,  8300, 34153, 12283,
       43186, 39940, 35465, 31749, 13924, 21150, 45488, 17750, 43167,
        5076, 26519,  1547, 45923,  4040, 37938, 32252, 12981, 48574,
       26905, 31241, 15655,  1037,   105, 21498, 12158,  6922, 11558,
       37799, 40762,  2795, 19581, 22395, 36537, 29410, 12841, 36868,
       24255, 31288, 20914, 18276, 24561, 11464, 18698, 26424,  8875,
        8600, 22522, 16128, 41463, 30281, 27868, 34778,  3399, 12927,
       45866, 16576,  7640, 36672, 33034, 23564,  6786, 19484, 43602,
        3257, 38929, 18321,   743, 22984, 34162, 38630, 39532,  3954,
       44411, 21627,  4379, 22396, 34923, 21328, 36039, 28818, 13332,
       23635,  2600, 19628,  1611, 15710,  4953, 11791,  3072,  6020,
        4360, 28098, 10711,  1464, 30439,  5301,   840, 20197, 18527,
       46859, 16089, 47226, 35587, 15195, 41110, 45466,  6453, 24263,
       38522, 41844, 16594, 33624, 37491, 41525, 46966, 24979, 16829,
       27871, 19539, 20244,  6122, 47968, 19869, 12183, 46078, 16385,
       49045, 20546, 26881, 46147,  2422, 33206, 33907, 30104, 14964,
        6731,  9549, 15248, 21564, 47981, 23854, 41274, 11963,  3689,
       21488,  2421,  5266, 45373, 19210, 26081, 28428, 47771, 15502,
        7530,  4523,  8833, 23866, 32251, 14309,  4468,  2258, 14407,
       41751, 24534, 13660,  1950, 27155, 34828,   637,  7757, 18765,
       13880, 49169, 46420,  2598, 31880, 12915, 37324,  5516,  5037,
       46548, 42674, 33242, 14567,  8160, 40654, 44001, 40807,  2824,
       44296,  7700, 17998, 32378,  7663, 13561, 34746,   371, 12461,
        2355, 23449, 19682, 15931, 16529, 43442, 11440, 22308, 22907,
       31726, 34381, 11238,  5702, 18605, 26007, 27638, 49530, 38683,
       22665, 21815, 44468, 27873, 37165, 39213, 46113, 30749, 35524,
       11305, 38586, 36180, 39723, 31246, 47401, 34547, 46434, 44866,
       47763, 17963, 27522, 34676, 18315, 38898, 20606, 34711, 15884,
       40516,   189, 38254, 49245, 20622, 23096, 31600, 27071, 33700,
       12922, 46782, 16002, 27426,  3361, 36286, 30078, 20694, 20916,
       33510,  2451, 40649, 17717, 31888, 20980,  1243, 38937, 21605,
        4030,  3713, 33336,  5194, 44470, 38518, 41848, 17947, 44742,
       45085, 28273, 35285, 36134,  2171, 39080, 42844,  1926, 43867,
        7537, 19527, 36573,  7972,  3979, 30904, 16553, 13043, 13225,
         415,  9384, 38052,  8102, 16225, 40802, 37881, 36004, 26764,
       19103, 44884,  2855, 40876, 19347,  9495, 20985, 18200, 49555,
        2932,  2413, 31648,  2652,  1999, 16420,   687, 20708, 40843,
       46294, 28408, 11721, 27179, 22080, 20611, 24589, 40612, 35059,
       18659, 42231, 28401,  2010, 14413, 28126,  4559, 39477, 20035,
        5719, 41467, 34998, 44143, 13594, 21647, 47799,  5216,  1216,
       25288,  8842, 14727, 19639, 33076, 36358, 47089,  9759, 15240,
       39372,  3900, 24721, 11762, 22628, 17763, 28467, 12993, 41234,
        9287, 26722,  6923, 25083, 31947, 17838, 16920,  2060, 34722,
       27147, 16939,  8098, 43944, 21533, 20105, 16371, 31004, 18680,
       14124]), [5, 8, 1, 0])]
Collaboration
DC 0, val_set_size=1000, COIs=[6, 3, 1, 0], M=tensor([6, 3, 1, 0], device='cuda:0'), Initial Performance: (0.242, 0.04443864643573761)
DC 1, val_set_size=1000, COIs=[4, 9, 1, 0], M=tensor([4, 9, 1, 0], device='cuda:0'), Initial Performance: (0.218, 0.044824156880378725)
DC 2, val_set_size=1000, COIs=[2, 7, 1, 0], M=tensor([2, 7, 1, 0], device='cuda:0'), Initial Performance: (0.243, 0.04442102932929993)
DC 3, val_set_size=1000, COIs=[5, 8, 1, 0], M=tensor([5, 8, 1, 0], device='cuda:0'), Initial Performance: (0.244, 0.044828352451324466)
D00: 1000 samples from classes {0, 1}
D01: 1000 samples from classes {0, 1}
D02: 1000 samples from classes {0, 1}
D03: 1000 samples from classes {0, 1}
D04: 1000 samples from classes {0, 1}
D05: 1000 samples from classes {0, 1}
D06: 1000 samples from classes {3, 6}
D07: 1000 samples from classes {3, 6}
D08: 1000 samples from classes {3, 6}
D09: 1000 samples from classes {3, 6}
D010: 1000 samples from classes {3, 6}
D011: 1000 samples from classes {3, 6}
D012: 1000 samples from classes {9, 4}
D013: 1000 samples from classes {9, 4}
D014: 1000 samples from classes {9, 4}
D015: 1000 samples from classes {9, 4}
D016: 1000 samples from classes {9, 4}
D017: 1000 samples from classes {9, 4}
D018: 1000 samples from classes {2, 7}
D019: 1000 samples from classes {2, 7}
D020: 1000 samples from classes {2, 7}
D021: 1000 samples from classes {2, 7}
D022: 1000 samples from classes {2, 7}
D023: 1000 samples from classes {2, 7}
D024: 1000 samples from classes {8, 5}
D025: 1000 samples from classes {8, 5}
D026: 1000 samples from classes {8, 5}
D027: 1000 samples from classes {8, 5}
D028: 1000 samples from classes {8, 5}
D029: 1000 samples from classes {8, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.07661269408464431) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07654814468324185) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.09640297073125839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.07325242793560029) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.07945466989278793) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.08400962515175342) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.278, 0.12523775127530098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.09349438312649727) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.377, 0.09021134239435195) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.09025510634481906) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.15705580693483354) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.14240596529841423) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.11596032911539078) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.412, 0.11271193671226501) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.19727019335329532) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.16480454625189303) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.13193382388353347) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.12182052563875914) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.20265718460083007) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.21888766702078283) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO5']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.12846777415275573) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.140342478916049) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.21342508929222823) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.24219384985789658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.12581581400334835) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.14166802729293704) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.2270521096549928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.27305388302356004) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.11846597637236118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.15023153780028223) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.24203452307730913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.26939495775196703) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.388, 0.12462650395929814) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.17387545125558973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.2579819469116628) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29477349990280344) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.39, 0.14789085798710586) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.19303536528348922) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.267256860435009) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29640268966881556) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[0, 1], M=tensor([0, 1, 2, 3, 4, 6, 7, 9], device='cuda:0'), Initial Performance: (0.816, 0.017217253228028615)
DC Expert-0, val_set_size=500, COIs=[3, 6], M=tensor([6, 3, 1, 0], device='cuda:0'), Initial Performance: (0.78, 0.015118347033858299)
DC Expert-1, val_set_size=500, COIs=[9, 4], M=tensor([4, 9, 1, 0], device='cuda:0'), Initial Performance: (0.932, 0.004831772923469544)
DC Expert-2, val_set_size=500, COIs=[2, 7], M=tensor([2, 7, 1, 0], device='cuda:0'), Initial Performance: (0.858, 0.010821685314178467)
SUPER-DC 0, val_set_size=1000, COIs=[6, 3, 1, 0], M=tensor([6, 3, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[4, 9, 1, 0], M=tensor([4, 9, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[2, 7, 1, 0], M=tensor([2, 7, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7686f430cd00>, <fl_market.actors.data_consumer.DataConsumer object at 0x7686d42ed670>, <fl_market.actors.data_consumer.DataConsumer object at 0x7686f41cbbb0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7686d42bfbe0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7686f46e4a60>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.786, 0.015468073666095733) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.004768962955102324) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.010674739077687264) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.2516883104862645) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8953333333333333, 0.009084465575714906) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.548, 0.054948179945349694) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.661, 0.03387542162835598) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.569, 0.04634813503921032) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.824, 0.013641494035720824) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.003621678251773119) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.012019084744155407) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2295039456691593) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8773333333333333, 0.013231892174730699) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.08114310747385026) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.537, 0.06659797533601522) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.526, 0.06663681499660015) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.013015482515096665) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.0037263114377856254) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.010430236659944057) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.23394385206513107) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8926666666666667, 0.009079982546468576) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.513, 0.07247323779761791) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.564, 0.055079680427908895) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.549, 0.05342588528990746) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.822, 0.013216698706150056) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003602453466504812) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.011099358022212982) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.23175660742027684) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.89, 0.011541034527122974) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.07242285184562207) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.05273679814487696) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.555, 0.06865730769187212) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.013859333828091621) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.004878069758415222) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.011533086605370045) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.2074290907504037) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.011397560190409422) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.0752728655487299) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.587, 0.058199121356010436) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.567, 0.06419695244729519) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO0']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO5', '(DO1']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.824, 0.014239510774612427) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.003922464041039348) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.011534373417496682) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.19940743063716218) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9133333333333333, 0.009128535816445947) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.0709782692193985) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.559, 0.06228951290994882) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.551, 0.07054143969714642) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.013960665985941888) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.004271521009504795) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.012114911548793315) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.20383855273411608) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.918, 0.011281540086725727) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.478, 0.10370629081130028) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.0717745293751359) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.503, 0.09354680283367633) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.01690524846315384) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.0041857201363891365) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.011141271702945233) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.20173543431656435) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.012371774936715763) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.502, 0.08499631501361728) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.07223502090573311) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.534, 0.0859801552593708) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.816, 0.016824354030191897) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.0043866105452179905) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.011757126912474632) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.18374579942575656) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9053333333333333, 0.016725242807801504) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.523, 0.07540106497146189) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.07408388556446881) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.527, 0.08407761539518833) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.848, 0.013320116698741913) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005573258614167571) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.012352812111377716) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.17683951625507324) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.016928032266286513) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.491, 0.0944339669458568) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.569, 0.07302026272192598) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.556, 0.07289013012871146) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO4', '(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.015789344251155853) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.004783874714747071) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.010736134499311448) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.20043626523250713) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.926, 0.008427273925549041) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.611, 0.06216573651880026) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.587, 0.06423997706919908) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.523, 0.10405378868803382) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.01405450600385666) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005783160982653498) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.011441873520612716) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.1743488983507268) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9173333333333333, 0.008534862621997794) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.55, 0.079763526879251) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.568, 0.07593440357130021) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.551, 0.10431250961124897) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.016871577858924867) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.004684101697057486) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.011387687891721725) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.1714669408411719) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.010311504742751519) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.08422738533467054) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.07213001731038093) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.532, 0.0888397358059883) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.013422531217336655) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005792750282213092) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.011190727412700652) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.19181926027685403) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9313333333333333, 0.016129510463642267) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.492, 0.11593622950464487) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.10149148103967309) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.502, 0.13491132146492602) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.014271321013569831) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.926, 0.008586689443793148) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.011823590189218521) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.1729250516691245) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9246666666666666, 0.013813959541420142) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.0997326543405652) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.552, 0.08048416977375746) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.517, 0.11669846718758345) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.856, 0.014085970044136047) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006036566483788192) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.012515565633773804) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.17260138144437223) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.009030056139764686) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.09006283657625318) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.608, 0.06526115321367978) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.529, 0.10062511906772852) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.0155494344830513) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.00570347900968045) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.012790222913026809) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.18676527057285422) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.009274573759796718) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.557, 0.08429299074411392) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.06440704779326915) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.564, 0.08446927832067012) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.824, 0.017221573770046236) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.004600636615417898) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.011351830840110778) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.18911121561122127) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9273333333333333, 0.01115560022989909) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.507, 0.09690398736298084) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.57, 0.08086539295688272) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.51, 0.10486183433094994) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.018716079238802193) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006267664054874331) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.011315119616687299) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.20683901525335385) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9233333333333333, 0.015534980519829938) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.09797205176949501) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.06582789347693324) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.506, 0.15266137167811394) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.017624513387680053) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.00422963770199567) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.010726156279444694) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.20542666118196212) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.014370778758117618) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.509, 0.10540636737295427) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.53, 0.08984749179985374) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.12516408349294214) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO3', '(DO1']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.01792551916837692) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.008309934738324955) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.01088819009065628) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1858858754802495) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9306666666666666, 0.01153667724194626) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.514, 0.09027147446572781) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.547, 0.09007221351657063) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.529, 0.10006304386258125) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.014879214391112328) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.00414121977891773) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.9, 0.010107918813824654) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.23509400180052034) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.015291367899083221) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.496, 0.11395496638491749) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.537, 0.08377053054561838) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.498, 0.13301856579445304) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.015556974492967128) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.004587641101330519) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.902, 0.011455381423234939) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.22592864259576892) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9293333333333333, 0.014379478057448675) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.484, 0.12427484838292002) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.09514181259693578) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.15659073431789874) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.015228253170847892) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006102676148060709) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.011547409057617187) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.21230367606901562) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9206666666666666, 0.014626517912528168) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.11650200103968382) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.08548429075628519) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.507, 0.11859650765359402) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.015009181782603263) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.006464168840902857) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.011836111336946488) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.20870438706548886) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9293333333333333, 0.014107050309559175) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.506, 0.10686076706647873) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.583, 0.07288937836140394) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.504, 0.10615664939582348) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.013443020537495614) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006042334435507655) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.012931561075150966) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.2206711252259556) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.008875570650833348) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.508, 0.11011136873811483) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.551, 0.08761541324108839) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.536, 0.09562272311002017) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.852, 0.014244434386491776) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.0054743235763162375) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.902, 0.011339520439505577) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.1984919493305497) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9313333333333333, 0.009306880799898257) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.08840753692761064) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.603, 0.0604972899928689) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.08276730830222369) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.848, 0.014884898737072945) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.007463585193734616) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.902, 0.010975351139903068) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.22850848406017757) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9066666666666666, 0.0211425470544685) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.476, 0.13292774024605752) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.534, 0.08325217089802027) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.497, 0.10874314897879958) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.015794778645038605) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005520447378978133) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.012982024490833282) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.2249711747399997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.924, 0.015982494924974163) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.494, 0.12828045364283025) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.547, 0.09555756538175046) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.15211309802159667) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.822, 0.013928773045539857) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.007448883677832782) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.011764124229550362) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.22582993899157736) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9273333333333333, 0.011847327115945518) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.493, 0.12512706047296523) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.0717351407930255) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.519, 0.12323012579977512) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO1', '(DO0']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.012884475991129875) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006184419428464026) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.012858592510223388) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.24037504448374966) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9266666666666666, 0.008728151777448753) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.5, 0.11386435122787952) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.581, 0.08232356176525354) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.536, 0.10251653277128935) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.015974538668990134) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.005157321892911568) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.012248161688446998) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.226450291645946) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9033333333333333, 0.029323305868659947) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.23094717656821012) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.14306190346431685) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.18905435354076325) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.015480090081691742) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006801649856381119) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.012210725009441375) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.25455594392685454) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9326666666666666, 0.012857391670967142) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.493, 0.13044753492623568) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.09723373773973436) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.503, 0.18017909172177315) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.015490354895591736) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005032081059878692) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.01335094666481018) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.24632610291906168) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.01219251955547952) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.491, 0.1351973922252655) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.09022055239928886) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.519, 0.15902361062914133) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.014972677141427995) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.004632492310367524) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.012039693892002106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.22705821118073072) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9193333333333333, 0.01781558349876044) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.505, 0.10547355611622333) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.571, 0.08449685824196786) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.548, 0.09871459309011697) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO1', '(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.014175437927246093) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006496990034589544) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.01298648278415203) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.2492927413065918) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9293333333333333, 0.015061625103776654) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.492, 0.13035341273248197) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.10932861536811106) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.508, 0.13351840350031852) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.86, 0.01349745163321495) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006780073177767918) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.010943134650588035) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2643220064287889) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.017790656293334903) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.499, 0.11842969939857721) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.517, 0.11072299544453562) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.494, 0.17257465424388646) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.86, 0.014151598751544953) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.009392059609701391) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.01127252072095871) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.23107010956085286) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9213333333333333, 0.02000085259353121) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.485, 0.13139041100442408) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.11451600052279537) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.491, 0.1807197926491499) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.015859414398670196) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.00695698195474688) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.011171017751097679) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.23953751826047665) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9173333333333333, 0.022189613354731894) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.496, 0.12234547436237335) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.10865935774589888) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.496, 0.1609031796604395) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.014396212503314017) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.005490517078433186) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.011208986774086951) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.25932483387639516) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9233333333333333, 0.016284143281479677) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.498, 0.11317221235483885) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.555, 0.1059761883233441) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.508, 0.12146633435785771) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.242, 0.04443864643573761), (0.354, 0.07661269408464431), (0.275, 0.07945466989278793), (0.377, 0.09021134239435195), (0.41, 0.11596032911539078), (0.408, 0.13193382388353347), (0.398, 0.12846777415275573), (0.394, 0.12581581400334835), (0.378, 0.11846597637236118), (0.388, 0.12462650395929814), (0.39, 0.14789085798710586), (0.548, 0.054948179945349694), (0.477, 0.08114310747385026), (0.513, 0.07247323779761791), (0.528, 0.07242285184562207), (0.517, 0.0752728655487299), (0.531, 0.0709782692193985), (0.478, 0.10370629081130028), (0.502, 0.08499631501361728), (0.523, 0.07540106497146189), (0.491, 0.0944339669458568), (0.611, 0.06216573651880026), (0.55, 0.079763526879251), (0.528, 0.08422738533467054), (0.492, 0.11593622950464487), (0.524, 0.0997326543405652), (0.527, 0.09006283657625318), (0.557, 0.08429299074411392), (0.507, 0.09690398736298084), (0.516, 0.09797205176949501), (0.509, 0.10540636737295427), (0.514, 0.09027147446572781), (0.496, 0.11395496638491749), (0.484, 0.12427484838292002), (0.516, 0.11650200103968382), (0.506, 0.10686076706647873), (0.508, 0.11011136873811483), (0.535, 0.08840753692761064), (0.476, 0.13292774024605752), (0.494, 0.12828045364283025), (0.493, 0.12512706047296523), (0.5, 0.11386435122787952), (0.455, 0.23094717656821012), (0.493, 0.13044753492623568), (0.491, 0.1351973922252655), (0.505, 0.10547355611622333), (0.492, 0.13035341273248197), (0.499, 0.11842969939857721), (0.485, 0.13139041100442408), (0.496, 0.12234547436237335), (0.498, 0.11317221235483885)]
TEST: 
[(0.2285, 0.04340292808413505), (0.35375, 0.07347347635030746), (0.27775, 0.07620753061771393), (0.3925, 0.0865502817928791), (0.4165, 0.11079292529821395), (0.403, 0.12526093649864198), (0.401, 0.12224474203586579), (0.3945, 0.11966084939241409), (0.38425, 0.1124050145149231), (0.38725, 0.11857743835449219), (0.3925, 0.13983969384431838), (0.55125, 0.05359725032746792), (0.4945, 0.07850523337721825), (0.5205, 0.07184809944033622), (0.54175, 0.07319018702208996), (0.512, 0.07554492631554603), (0.53125, 0.07192438986897469), (0.481, 0.1064438471198082), (0.51725, 0.08756507235765457), (0.538, 0.07543927499651909), (0.49925, 0.09272206097841262), (0.60675, 0.06125453169643879), (0.5445, 0.07986509791016579), (0.53675, 0.08581775227189065), (0.4875, 0.11851763784885407), (0.515, 0.09956333723664283), (0.52875, 0.08729590955376625), (0.5585, 0.08126983746886253), (0.5245, 0.09538524323701858), (0.526, 0.09678845009207726), (0.51275, 0.1108878446817398), (0.5215, 0.08890663793683053), (0.48125, 0.11508664053678512), (0.48525, 0.12423142874240875), (0.5185, 0.11308518728613853), (0.499, 0.10841302725672722), (0.50725, 0.11300374925136566), (0.54275, 0.09099241568148136), (0.488, 0.13989105457067488), (0.49725, 0.12920596066117288), (0.51025, 0.12259353500604629), (0.518, 0.1161139464378357), (0.44425, 0.21667232805490494), (0.517, 0.11989498835802079), (0.49725, 0.1302187687456608), (0.52625, 0.10646368172764778), (0.4975, 0.13396342486143112), (0.50225, 0.11761784547567368), (0.499, 0.1306954663991928), (0.511, 0.11555068066716194), (0.5135, 0.11365058988332749)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.44      0.90      0.59      1000
           1       0.55      0.90      0.68      1000
           3       0.74      0.12      0.20      1000
           6       0.82      0.14      0.24      1000

    accuracy                           0.51      4000
   macro avg       0.64      0.51      0.43      4000
weighted avg       0.64      0.51      0.43      4000

Collaboration_DC_1
VAL: 
[(0.218, 0.044824156880378725), (0.25, 0.07654814468324185), (0.252, 0.08400962515175342), (0.381, 0.09025510634481906), (0.412, 0.11271193671226501), (0.434, 0.12182052563875914), (0.467, 0.140342478916049), (0.461, 0.14166802729293704), (0.474, 0.15023153780028223), (0.469, 0.17387545125558973), (0.466, 0.19303536528348922), (0.661, 0.03387542162835598), (0.537, 0.06659797533601522), (0.564, 0.055079680427908895), (0.557, 0.05273679814487696), (0.587, 0.058199121356010436), (0.559, 0.06228951290994882), (0.535, 0.0717745293751359), (0.545, 0.07223502090573311), (0.536, 0.07408388556446881), (0.569, 0.07302026272192598), (0.587, 0.06423997706919908), (0.568, 0.07593440357130021), (0.578, 0.07213001731038093), (0.545, 0.10149148103967309), (0.552, 0.08048416977375746), (0.608, 0.06526115321367978), (0.589, 0.06440704779326915), (0.57, 0.08086539295688272), (0.6, 0.06582789347693324), (0.53, 0.08984749179985374), (0.547, 0.09007221351657063), (0.537, 0.08377053054561838), (0.523, 0.09514181259693578), (0.556, 0.08548429075628519), (0.583, 0.07288937836140394), (0.551, 0.08761541324108839), (0.603, 0.0604972899928689), (0.534, 0.08325217089802027), (0.547, 0.09555756538175046), (0.578, 0.0717351407930255), (0.581, 0.08232356176525354), (0.463, 0.14306190346431685), (0.54, 0.09723373773973436), (0.556, 0.09022055239928886), (0.571, 0.08449685824196786), (0.527, 0.10932861536811106), (0.517, 0.11072299544453562), (0.536, 0.11451600052279537), (0.576, 0.10865935774589888), (0.555, 0.1059761883233441)]
TEST: 
[(0.22, 0.043847873479127886), (0.25, 0.07381470426917076), (0.2505, 0.08048938992619514), (0.386, 0.08601839923858642), (0.4165, 0.10769178631901741), (0.44125, 0.11695740818977356), (0.4685, 0.13551646131277084), (0.4675, 0.13753553491830825), (0.4745, 0.14645577955245973), (0.47025, 0.1688867705464363), (0.47025, 0.18691782051324846), (0.61775, 0.03724771966040134), (0.53025, 0.06828998279571533), (0.54675, 0.055778385460376737), (0.5415, 0.05630069744586945), (0.56575, 0.060717743054032325), (0.53775, 0.0662358180731535), (0.5285, 0.07713835686445236), (0.5315, 0.07876431807875633), (0.52425, 0.07877751475572586), (0.54875, 0.0756613310277462), (0.57925, 0.0686671946644783), (0.56275, 0.07603461088240146), (0.5695, 0.07434715431928635), (0.5235, 0.11051916199922561), (0.546, 0.08444302546977997), (0.5865, 0.06694871374964714), (0.59, 0.06455717402696609), (0.54525, 0.08521276101469993), (0.5585, 0.07294602113962173), (0.53175, 0.09155942070484162), (0.533, 0.09708325439691544), (0.52025, 0.09087551382184028), (0.50275, 0.09827795004844665), (0.52025, 0.09677772635221481), (0.55875, 0.07721041557192802), (0.549, 0.09428239941596984), (0.59325, 0.0651555542498827), (0.52925, 0.0874189655482769), (0.5405, 0.09799024960398674), (0.572, 0.07229352051019669), (0.55475, 0.08595619443058967), (0.45925, 0.14957016146183014), (0.537, 0.10322712373733521), (0.5405, 0.10038451820611954), (0.5545, 0.09502101001143455), (0.50675, 0.11726193439960479), (0.51075, 0.11761569467186928), (0.52225, 0.1213184765279293), (0.54625, 0.1165437743961811), (0.5245, 0.116499406427145)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.49      0.84      0.62      1000
           1       0.52      0.82      0.64      1000
           4       0.64      0.26      0.37      1000
           9       0.56      0.17      0.26      1000

    accuracy                           0.52      4000
   macro avg       0.55      0.52      0.47      4000
weighted avg       0.55      0.52      0.47      4000

Collaboration_DC_2
VAL: 
[(0.243, 0.04442102932929993), (0.25, 0.09640297073125839), (0.278, 0.12523775127530098), (0.396, 0.15705580693483354), (0.42, 0.19727019335329532), (0.426, 0.20265718460083007), (0.421, 0.21342508929222823), (0.43, 0.2270521096549928), (0.427, 0.24203452307730913), (0.435, 0.2579819469116628), (0.429, 0.267256860435009), (0.569, 0.04634813503921032), (0.526, 0.06663681499660015), (0.549, 0.05342588528990746), (0.555, 0.06865730769187212), (0.567, 0.06419695244729519), (0.551, 0.07054143969714642), (0.503, 0.09354680283367633), (0.534, 0.0859801552593708), (0.527, 0.08407761539518833), (0.556, 0.07289013012871146), (0.523, 0.10405378868803382), (0.551, 0.10431250961124897), (0.532, 0.0888397358059883), (0.502, 0.13491132146492602), (0.517, 0.11669846718758345), (0.529, 0.10062511906772852), (0.564, 0.08446927832067012), (0.51, 0.10486183433094994), (0.506, 0.15266137167811394), (0.487, 0.12516408349294214), (0.529, 0.10006304386258125), (0.498, 0.13301856579445304), (0.486, 0.15659073431789874), (0.507, 0.11859650765359402), (0.504, 0.10615664939582348), (0.536, 0.09562272311002017), (0.552, 0.08276730830222369), (0.497, 0.10874314897879958), (0.487, 0.15211309802159667), (0.519, 0.12323012579977512), (0.536, 0.10251653277128935), (0.442, 0.18905435354076325), (0.503, 0.18017909172177315), (0.519, 0.15902361062914133), (0.548, 0.09871459309011697), (0.508, 0.13351840350031852), (0.494, 0.17257465424388646), (0.491, 0.1807197926491499), (0.496, 0.1609031796604395), (0.508, 0.12146633435785771)]
TEST: 
[(0.246, 0.04339280295372009), (0.25, 0.09244508910179138), (0.2815, 0.12015084260702133), (0.38875, 0.15123850136995315), (0.42325, 0.18991599303483964), (0.4235, 0.19383455294370652), (0.42825, 0.20367060768604278), (0.42975, 0.21929274678230284), (0.4355, 0.23072633934020997), (0.44425, 0.245837830722332), (0.442, 0.25503220057487486), (0.57275, 0.04740870699286461), (0.51575, 0.0702077072262764), (0.5295, 0.0541303386092186), (0.53825, 0.07045017245411873), (0.56775, 0.06620442767441273), (0.5605, 0.0688620561659336), (0.514, 0.09419377443194389), (0.53275, 0.09050024572014809), (0.5365, 0.0863530216217041), (0.55275, 0.07688376247882843), (0.51875, 0.10571548143029214), (0.544, 0.1041167422235012), (0.535, 0.09325699681043625), (0.50625, 0.1309227792918682), (0.52775, 0.11896595993638039), (0.55275, 0.09903512877225876), (0.56875, 0.08563885647058488), (0.53025, 0.10419950151443481), (0.5155, 0.1552866767644882), (0.498, 0.11898040035367012), (0.52925, 0.10156787812709808), (0.50375, 0.13308946096897126), (0.49375, 0.1576836569905281), (0.499, 0.12328780674934388), (0.50675, 0.10839207130670547), (0.54125, 0.09890957015752792), (0.54775, 0.08455502027273178), (0.51375, 0.11192142474651337), (0.485, 0.15351338082551957), (0.5235, 0.12578720909357072), (0.5305, 0.09918564805388451), (0.44975, 0.18600485819578172), (0.49625, 0.1760451267361641), (0.51225, 0.1587166481614113), (0.54775, 0.09994646829366684), (0.50075, 0.13591753846406937), (0.48525, 0.17199609655141831), (0.484, 0.18241932064294816), (0.49975, 0.15804979091882707), (0.51575, 0.11877618166804314)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.38      0.86      0.53      1000
           1       0.70      0.86      0.77      1000
           2       0.40      0.09      0.15      1000
           7       0.84      0.25      0.39      1000

    accuracy                           0.52      4000
   macro avg       0.58      0.52      0.46      4000
weighted avg       0.58      0.52      0.46      4000

Collaboration_DC_3
VAL: 
[(0.244, 0.044828352451324466), (0.392, 0.07325242793560029), (0.46, 0.09349438312649727), (0.474, 0.14240596529841423), (0.469, 0.16480454625189303), (0.469, 0.21888766702078283), (0.47, 0.24219384985789658), (0.477, 0.27305388302356004), (0.471, 0.26939495775196703), (0.477, 0.29477349990280344), (0.477, 0.29640268966881556), (0.472, 0.2516883104862645), (0.48, 0.2295039456691593), (0.473, 0.23394385206513107), (0.478, 0.23175660742027684), (0.475, 0.2074290907504037), (0.474, 0.19940743063716218), (0.475, 0.20383855273411608), (0.478, 0.20173543431656435), (0.48, 0.18374579942575656), (0.473, 0.17683951625507324), (0.481, 0.20043626523250713), (0.474, 0.1743488983507268), (0.48, 0.1714669408411719), (0.478, 0.19181926027685403), (0.474, 0.1729250516691245), (0.468, 0.17260138144437223), (0.476, 0.18676527057285422), (0.471, 0.18911121561122127), (0.474, 0.20683901525335385), (0.47, 0.20542666118196212), (0.477, 0.1858858754802495), (0.472, 0.23509400180052034), (0.474, 0.22592864259576892), (0.472, 0.21230367606901562), (0.477, 0.20870438706548886), (0.476, 0.2206711252259556), (0.473, 0.1984919493305497), (0.475, 0.22850848406017757), (0.474, 0.2249711747399997), (0.478, 0.22582993899157736), (0.476, 0.24037504448374966), (0.477, 0.226450291645946), (0.478, 0.25455594392685454), (0.478, 0.24632610291906168), (0.479, 0.22705821118073072), (0.477, 0.2492927413065918), (0.48, 0.2643220064287889), (0.477, 0.23107010956085286), (0.478, 0.23953751826047665), (0.478, 0.25932483387639516)]
TEST: 
[(0.2495, 0.044014592796564105), (0.40075, 0.07023401990532875), (0.46475, 0.08952063956856728), (0.47275, 0.135820614695549), (0.476, 0.15784210336208343), (0.47525, 0.20845930409431457), (0.47475, 0.2282492750287056), (0.48075, 0.2596552656888962), (0.47775, 0.25685111624002455), (0.48, 0.28122049593925474), (0.47975, 0.28348508703708647), (0.478, 0.24061121428012847), (0.4785, 0.2177070525288582), (0.47825, 0.22268052303791047), (0.481, 0.2217788327932358), (0.47975, 0.1968918514251709), (0.48125, 0.19045713752508164), (0.474, 0.1929316616654396), (0.4795, 0.1931508297920227), (0.48075, 0.17866988092660904), (0.47725, 0.1692274762392044), (0.48075, 0.19226678657531737), (0.4775, 0.16714797592163086), (0.47875, 0.16404814016819), (0.4815, 0.18369595658779145), (0.4765, 0.16411491245031357), (0.47525, 0.16305639600753785), (0.4795, 0.17826229214668274), (0.47775, 0.17799213111400605), (0.47875, 0.19721089792251587), (0.47775, 0.19475033086538315), (0.48075, 0.1758804417848587), (0.476, 0.2216276695728302), (0.48225, 0.21269438850879668), (0.4795, 0.20333164739608764), (0.4795, 0.1987374193072319), (0.47925, 0.21470627588033675), (0.47975, 0.19097436755895614), (0.4785, 0.2206770550608635), (0.47825, 0.21284138584136963), (0.4805, 0.21387801373004914), (0.48, 0.23309068655967713), (0.4795, 0.21673471367359162), (0.48, 0.24246602046489715), (0.47725, 0.23434279751777648), (0.47825, 0.21614742159843445), (0.4785, 0.23757255268096925), (0.481, 0.2503254734277725), (0.47825, 0.2181640015244484), (0.47725, 0.22616885375976561), (0.47975, 0.2465197763442993)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           1       0.00      0.00      0.00      1000
           5       0.74      0.94      0.83      1000
           8       0.36      0.97      0.52      1000

    accuracy                           0.48      4000
   macro avg       0.28      0.48      0.34      4000
weighted avg       0.28      0.48      0.34      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [104]
name: alliance-3-dcs-104
score_metric: contrloss
aggregation: <function fed_avg at 0x7f870b255c10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=104
Partitioning data
[[5, 9, 0, 3], [4, 1, 0, 3], [6, 2, 0, 3], [7, 8, 0, 3]]
[(array([29961, 11298,  5755, 34458, 42818,  8742, 20483, 38411, 20171,
         767, 11024, 14802, 11949, 32795, 30095, 15575, 40417, 30072,
       43979, 47688, 40721, 25662, 30618, 14004,  7435, 10429, 35189,
       38600,  2071, 35606,  6017, 37958, 20615, 33853, 13316, 35905,
       16623, 30098, 27844, 37451,  3495, 35602, 35712, 15606, 41318,
       16358, 43338, 22445, 24648, 18626, 23392, 11269, 35053, 38437,
        5853, 35652, 23623, 47167,  7926, 34183, 21561, 45581, 48303,
        8929, 37874, 26503, 46901, 23146,  9284, 17778, 18845, 38369,
       23385, 48701, 26747, 47024, 49638,  8851, 21610, 47555, 24819,
       11740, 21586,  9383,  7601,  7417, 41942,  1717, 43495, 11928,
       16282,  9286, 35534,  9594,  2410, 32468, 24981, 46606, 12239,
       15685,  3229,  7090, 25262,  5606, 40490,  3471, 45698, 35501,
       15849, 32919, 43387, 33138, 48454, 40561, 32926, 46684, 34008,
       44371, 27679, 43759, 30260, 26095, 43878, 34048, 43852, 47810,
       49472, 28460, 45728, 29896,  4002, 48730, 48139, 15130, 14677,
       45184,  1911, 25981, 33818, 39002, 30063, 28845,  3794, 37680,
       13624, 18913, 35510, 44435, 46074, 11405, 48750, 24196, 31145,
       19150, 28652, 30757, 21543, 27345, 16626, 41376, 40044, 12177,
       20122, 22679,  2877, 21910, 23234, 47988, 18167, 23724, 44904,
       36035, 46925,  3653, 35993, 46267, 22999, 37471, 47920, 44094,
       48537, 49927, 41082,  5736, 31791, 45206, 44106, 30085, 45503,
       13960,  8243,   607, 47054, 37366, 18764, 18319, 20207, 10703,
       29110, 23614, 17332, 42745,   988, 41062, 25248, 36132, 38202,
       49920, 42323, 47258, 31031, 19735, 49848,  2361, 48632, 11725,
       42533, 36653, 24295, 28736, 40765, 22390, 34763, 27359, 49052,
       22061, 34941, 27885, 20460, 26589,  5970, 30134, 31222, 40140,
        4494, 49428, 17773, 27956, 45583, 36952, 15775, 49431,  5288,
       19201, 46187, 44150, 47485, 36439,  1662, 18230, 33778, 15785,
       10375, 20172, 37050, 41580, 26822, 25010, 49779, 22366,  3032,
       18860,  2758, 32595, 23798,  8826, 13906, 38331,  8364, 32489,
       10622, 29458, 24537, 31605, 13078, 37547, 41169, 15142, 49369,
       22415, 42627,  6178, 18749, 23877, 29431, 21273, 40508, 13669,
       12793,  9425, 28907, 44714,  5801, 38378, 35057,  6150, 49360,
        2208, 38820, 49793, 38827, 41976, 18213,  5591, 37094, 19824,
       40040,  1893,  7441, 44009, 14008, 42591, 44088,   360,  5244,
       33432, 29115,   664, 27237, 33784, 19505, 12152, 39510, 34943,
       39025, 22959, 43997, 29551, 12680, 33224, 32266, 36031, 40931,
       24609, 42159,  3479, 19766, 28713, 24731, 40215,  6643,  5621,
        2302, 47511, 26289, 34502,  1078, 27837, 32635, 49112, 43695,
       13364,  7556, 23651,  4289, 45876, 17709, 30392, 43839, 40735,
       26393, 16533, 19934, 21537,  6357,  2477, 39041,  4749, 34049,
       48544, 13767, 28850, 39280, 39801, 14458, 24987, 13074, 26113,
       39731, 16818, 10231, 44410, 35203, 47426, 34147, 29672, 38314,
       23225,  3915, 28915,  9466, 16614, 46269, 23770, 32745, 17544,
       40722,  3061, 28899, 30178, 10751, 42704, 24683, 39557, 30175,
       29418, 47076,  7698, 15096, 21237, 15586, 29651, 24603, 36329,
       45735, 23129, 41054, 38561, 15901, 10523, 45463,  1572,  8716,
       24960, 34129, 44947,   512, 26447, 12116,   406, 39870, 32838,
       21688,  4675, 22815, 10161, 24042,   763, 18430, 46195, 47025,
       25965, 26880, 30503, 39572, 15566, 23359,  5881, 19474, 30771,
       32902, 47469,  2609, 23091, 41683, 31453, 13421, 47985,  5622,
        9947, 45791, 44996, 48751,  1115, 13491, 32701, 26852, 44332,
        7652, 27287, 40915, 34640, 26600,  1906, 47709, 40037, 36401,
       45048, 34485, 27899,  6201, 22346, 39894, 33928, 15252, 40693,
       35746,  5234,  6268, 16708, 29529, 19380, 21506, 32575, 48345,
       30758, 37242,  2880,  9990, 28931, 24581, 25624, 35750, 17196,
       47916, 19390, 33960, 24404, 45666, 31726, 11913, 44771, 43324,
       30114,  5719, 10197,  2341, 28972, 19286, 16238, 25920, 10911,
       45297, 27487,   185, 37867, 12634, 18893, 12764, 34783, 16825,
       43989, 23002, 39170, 23796, 16219, 35796, 29307,  7788,  3842,
       10847,  8741,  3292, 46576,   527, 31728, 21878, 27047, 45067,
        9819,  7731, 17082, 37962, 49359, 24356, 43289, 18763, 34568,
       29578, 38041,  6954, 11325, 32471, 24257, 24587, 35731,  3329,
       38044, 29927, 48986, 28724, 41762, 21469, 28272, 42711, 44645,
       10650, 16391, 43657, 46523, 10130,  8857, 20686, 28467, 38043,
        3789,  3689, 28570, 41855, 40703, 22557, 37358, 41146, 17352,
        5414, 13419, 30378, 22299,  1243, 12916, 41222, 48324, 31471,
       27301, 13260, 13966, 15258, 12922, 28833,  6833, 28515, 47651,
       13660, 26644, 10499, 10159, 18768,  5770, 41435, 38963, 39329,
       10275,  6242, 32905, 11372, 47525, 40802, 34259, 47906, 22167,
         557,  3277,  8079,  8018, 34722, 20454, 25212,  8759, 21439,
       41258,  1306, 13368, 41797, 42992, 26704, 13174, 35369, 34234,
         695, 44864, 44624, 27460, 15827, 41274, 24395, 48852,  2053,
       31600, 25748, 15483, 48147,  9193, 29053, 16048, 15304, 10954,
        1319, 32631,  2145, 11122, 15848, 39392,  9697, 17432, 41038,
       21092,    77, 28092, 49633, 10545, 41040,   555, 41745, 18358,
       41907, 49219, 24072, 19492, 49837, 35089, 31412, 36464, 17257,
       19294, 21288, 36955, 40432, 24365, 12944, 37068, 16225, 10782,
       31863, 30968,  4721,  2451, 35658, 21143, 31391, 38478, 49308,
       31976, 22970, 15542, 22449, 47925, 37049, 16552, 27855, 38828,
       44296, 33128, 11768, 21275, 22628, 42309, 21087, 16196, 17947,
       19664, 14867, 11975, 35039, 12430, 39574, 39763, 13401, 36606,
       40851, 23528, 34109, 20287, 30271, 48671, 35690, 19974, 37102,
       23851, 24764,   940,  7389, 12145, 38040, 41021, 42382, 15210,
       15444, 38442, 40378, 34204, 10293,  6496, 39211, 11205,  4558,
       18371, 21373, 23715, 34536, 47995, 45949, 21687, 47284, 38048,
       40651,  5493,  9904, 43551, 19922, 13329, 45552, 40587,  8380,
       36948, 36930, 25380, 33049, 21728, 23389,  9183,  1963,  4166,
       38095,  8356, 47146,  1405, 45885,  7426, 26964, 46094, 43813,
       34011, 37966, 20139, 30347, 49322, 16680,  6482, 44399, 42122,
       45631, 34967, 34517, 20630, 48200, 40249, 11628, 42800, 32172,
       15595, 28020, 27796,  7997, 31148, 40127, 24077, 43598, 46817,
       27734,   416, 46286, 15383, 41330, 25019, 21366, 37863, 33841,
       32180,  2337, 44530,  5329, 31698, 43402,  5478, 44084,   377,
       20050, 48306, 42357, 10135, 20639, 25947,  3340, 32520, 41016,
       32219, 18769, 12500, 20204, 32379, 34155, 21783, 20770, 45908,
       11067,  1895,    91, 10653, 33894,  4294, 43784, 35832, 30461,
        6061, 43336,     9, 25882, 34742, 16714, 11366, 48039, 20271,
       41372, 22611, 28373, 16817, 39151, 37217, 16405, 20529, 37408,
       17930, 18127, 26716, 16209,  4435, 27424, 10869,  5209, 14032,
       35715, 34997, 41705, 24733, 23270, 41364, 29423, 32304, 32725,
       12988, 32917, 27683, 45250, 48629, 24434, 15127, 42233,  5007,
       49190, 36951, 12971, 24522,  3190, 32621, 23590, 41159, 45508,
        3763, 21835, 44868, 24364,  5423, 15417, 13205, 24335,  1150,
       44029, 15640, 32293, 28731, 34317, 49982, 33525, 14064, 19617,
       20410, 17451, 41057, 47478, 22302, 17644, 39341,  6837, 39543,
       35931, 22759, 42752, 37910, 42482, 16872, 40690, 39384, 39158,
       48097, 21672,  2197, 37415,  8876, 38012,  9120,  4481, 29273,
       34822, 26564, 49384, 43704, 30753, 25492,  6732, 31567, 46882,
       44093,  5260,  8105, 12973, 34061, 33582, 32914,  1265, 20784,
       25235, 30817, 46013, 42134, 35935, 34055, 34649,  7493, 20631,
       41853]), [5, 9, 0, 3]), (array([39470, 41148, 10032, 35811, 15140, 44629, 29163, 13803,  6022,
       23743,    89, 12313, 43632, 11116, 43746, 13453, 41246,  7634,
       34410, 41688,  6298, 28577, 40718, 30970, 29138, 41337, 48860,
       28097, 20906, 10904, 41852, 23134, 14600, 15152, 27105, 15790,
         310, 27579, 45274,  9502, 47951, 40529, 16967, 45441,  9685,
       12591, 28414, 21676, 18300, 49154,  3587, 23252, 22491, 33099,
       14060, 11125, 24933, 12106, 36533,  4527, 13636, 10925, 36646,
        6879, 35791, 29787, 25636, 40848,  3219, 48513, 25216, 18450,
        2418,  5902, 22902,  4750, 24290,   520, 15434, 12084, 46803,
        7459,  9780,  4328, 11860, 43049, 25474,  6605,  1889, 21419,
       20450,  1904, 22246, 30153, 13303, 11626,  2691, 42418, 45100,
       45645, 40277, 28151, 41788,  3533,  1303, 42907,  6415,  4725,
       32280, 32491, 31306, 43044,  7818, 47622, 32634, 34306, 47922,
        2155, 39125, 38755, 31061, 24804, 49981,  1866, 25423,  7078,
        1851, 44095,  7295, 21946, 47312,  2999, 32446, 46528, 43386,
        7382,  3577,  7099, 10890, 44974, 31860,  7811, 28904, 36225,
       27847, 24900, 37075, 48309, 23354, 49480, 45595,  5029, 11020,
       40299,  4227, 19471,  3664, 30942, 30354, 10982, 41836, 38730,
       24967, 33827, 12590, 14105, 35429, 49690, 39803, 16672,   856,
       16961, 15909, 36755, 13145, 26943, 43627, 29770, 31256, 13646,
       34418, 26512, 44846, 18818, 16294, 33296, 43682, 22468,  9872,
       43188, 40088, 35304, 22606, 35023, 38776, 11226, 12826,  3089,
       29086,  7154, 11573, 15482, 45764, 40862, 46220, 14474, 34671,
       47796, 41648, 37639, 29830, 36466,  1315,  1333, 17038, 39209,
       47305, 27247, 20243, 18476, 23445, 33735, 33290, 22053, 14576,
       16514,   904, 13134, 49390, 22825, 11769,  3776, 39576, 40067,
        9441, 11279, 18821, 15643,  3695,  3112, 19458,  7536, 11423,
       25099,  5119,  4381, 48338,  6110, 49950, 27914, 44726, 44936,
       42318, 38841,  7607, 28045, 36844, 41445, 20162, 48845, 17244,
       37264,  2394, 19382, 28127, 31854, 10644, 34064, 15710, 15748,
       21151, 17752, 24446, 48656, 17694, 21407, 36546, 35011, 44671,
       38278, 40967, 49151, 27024, 45535, 44138, 24110, 33063, 25544,
        3452, 12373, 46866,  8189, 26491, 35453, 48584, 18438,  7884,
       34911, 43854, 42221, 11532,  8001, 34128,  4591, 31352, 38610,
       20988, 34246, 11558, 35605, 10795, 34530,  6226, 12977, 22818,
       22829, 19079, 14358,  4492, 11075,  1380, 12494, 43217, 22675,
       18654, 41627, 33951, 35357, 41217, 32988, 26008, 20307, 11571,
       36473,  4101,  2428, 12210, 11920, 34063, 22697, 37551, 26292,
       43297, 19939, 24678, 22490, 48574, 39455, 25331, 48806,  4200,
       46135,   834, 26375, 29536, 42085, 12231, 10049, 40075,  9263,
       14740, 17856, 47035, 24288, 25280,  7349, 34913, 26065, 45001,
       44526, 17861, 35051, 18890, 45569, 41742, 25975, 18927,  3414,
       10110, 25558, 41519, 18206, 12741, 39638, 23895, 38919, 24762,
       21422,  6429,  9407, 44499, 27794,  1578, 49121, 48915, 19500,
       17031, 23500, 34637, 11407, 21848, 46173, 37480,  6786, 42121,
        9717, 18702, 19031, 47568,  2849, 35644, 11971, 28532, 35086,
       26789,  2067, 44824,  8869, 22379, 43206, 10035, 40423, 30245,
       33230, 13402,  4858, 40520, 26080, 27402, 37260,  7706, 34162,
       33188, 47540, 15740, 42443,  9411, 16471, 42060, 11403, 42684,
       38630, 41849, 21871, 21118, 42113, 44674, 24975,  4696, 35567,
       26267, 41209, 25974,   840, 48441, 23020, 44960, 41579, 40518,
       20746, 33808, 43893, 46177, 22636, 14201, 16745, 43865, 24979,
       33196, 41092, 49438, 26405, 19226, 46993,  5627, 23709, 31101,
       33271, 14659,  9534, 25278, 24240, 43668,  6411, 39353,  9524,
       35827, 13468, 26509, 48269,  2511, 24003, 48263, 15069, 45483,
       25488,  2752,  3840,  9400, 37136, 17838,  3574, 39995, 20105,
        4651, 30695, 43700, 19066, 14778, 31693, 14909, 20682,  8392,
       35326, 45800, 41220,  2662, 32884, 20831, 42073, 16010, 18298,
       44884, 41677,  8882, 26592, 45737, 35446, 35279,  4165,  5341,
       24793, 44468, 30895, 40343, 14848, 15439, 13031,  9714, 11080,
       48778, 20537, 19639, 32592, 16567, 46380, 49387, 48119, 34831,
       25489, 18979, 13860, 13181, 25605, 49375,  7490,  8849, 35758,
       29732, 17383, 41774, 41806, 39236, 18302, 36119, 48136, 13223,
       45143,  2598, 26003, 32661,  6328, 20976,  3548, 30971,  4524,
        2365,  2171,  6190, 46409, 29468,  9934,  6512,  3337, 18900,
       19216, 31947,  3113, 15949, 21458,  3066,  6085, 47326, 36106,
       16100,  7674, 25337,  2027, 43002, 36786, 48856, 49660, 48470,
        3024, 27986, 11424, 45089,  1338, 28231, 12420,  1142, 11224,
       18231, 31648, 21564, 28897, 28035, 17884, 15900, 32091, 15509,
       34280, 38547, 35403, 39179, 46480,  2066, 20275,  9292, 24367,
       39196, 16733, 34746, 17764, 23478, 46137, 39589,  9119, 26840,
        4311, 25422, 30730, 32247,  3900,  1711, 20790,  7660,  5566,
       45546,  5477, 11671, 32460, 29497, 37729,  4030,  6354, 35286,
        8249, 43998, 32131,  1759, 37177, 36386, 23056, 38566, 32058,
       47865, 14407, 31481, 17763, 32016,  9920, 31048, 35281, 13902,
       38345, 32670, 13861, 18659, 18849, 38205, 44067, 33336, 31654,
       10526, 46987,  5924, 43664, 49276,  5858,   405, 35079, 32398,
       12344,  4571, 34828, 26027, 14329,  1424, 43771,  3644, 32968,
       49863, 43368,   417, 11078, 23972, 39121,  4192, 36389, 21338,
       27609,  5549, 18315,  6904, 33514, 34123, 42621,  2459, 16423,
       41800, 38419, 42272, 30101, 26083, 35529,  6929,  6124, 20566,
       27147, 44742, 35059, 23924, 14138, 33864, 38656, 36038, 27278,
        7002, 18704, 26913,  3852, 30605, 11937, 38344, 16370,  1477,
        9895, 41651, 30152, 29561, 13189,  9300, 21736,  9343, 32702,
        1002, 25307, 16536, 12598, 21909, 20522, 26064, 37924, 36970,
       24348,  1271, 43487,  6162, 41725, 25008, 26613, 26372,  6309,
       38880, 38894, 19221, 13941, 46659, 33914, 43554,  3795, 18111,
       11564, 27652, 22517, 34835, 32732, 11131, 17322, 41734, 44313,
       28091, 32098, 19871, 18169, 23120, 49368, 49336, 40788,  7584,
       37546, 37998,  1363, 22933, 49713, 30838, 22406, 38423, 30022,
       12289, 10383,  4177, 38127, 25256, 47526, 12475, 18222, 49481,
       24135, 36366, 36054, 14225, 36448, 18180,  4799, 30960, 34159,
       36027,  3785, 26336, 35083, 24174, 20215,  5741, 32963, 13857,
       34581, 49015, 23935, 29810, 18662, 31198, 33511, 15706,  2042,
        8330, 35583, 20151, 21739, 28991, 12681,  7835, 17904, 47892,
       34116, 18862, 41815, 40668, 32449,  1057, 19780, 41134, 33355,
       42056, 25055, 10784, 35428, 28529, 15341, 16628, 33642, 22223,
       10770, 34170, 16583, 15397, 24871,  4437, 47819,  8781, 45921,
       13986, 38761,  1803, 40954, 27761, 46085, 33714, 12105,  9166,
       11477,  3447, 24100, 16335, 14108, 13925, 20593, 20121, 31632,
       34345, 10611, 38781, 15739, 30756, 27698, 15753, 42153, 41271,
       19060, 33168, 43685, 38976, 36809, 36979, 40399, 22453,  8722,
       24384, 45251, 37176, 12966, 21326, 21851,  9700, 38879, 39300,
       33601,  8122, 22209,  8760, 10418, 34016, 32325, 36743, 15877,
         479, 43730, 20159, 47553,  2383, 34461, 14373,  2594, 33236,
        6466,  6669, 10244,  4669, 38066, 18072, 24551,  5425,  3110,
       36800,  8686,  1196, 35629, 33955, 24022,   691, 32844, 28523,
       24751, 49298, 31141,  7467, 32021, 28149,  4672, 48045, 33031,
       36216, 15345, 45439,  3043, 28513, 37204,  5797,  4310, 23995,
        9914, 16231,  2223, 29950, 49430, 49944, 22826,  6914,  5106,
       41102, 31261,   774,  9371, 37765, 13811, 33972, 24598, 22161,
       21287]), [4, 1, 0, 3]), (array([20177, 36044,   854,  9901, 32063, 44492, 31408, 40153, 32074,
       45741, 19281, 35219, 30128, 44023,  3504, 18722, 41297, 27687,
        3991, 33948, 11638, 31660, 35136, 46791, 34020, 14476, 30938,
       31356,  5723, 46377, 15986, 37887,  3341, 13188,  3098, 34142,
       45925,  8256, 42584, 23821, 24556,  8633, 10013, 30813,  3716,
       38061, 49082, 25815, 10961,  1191, 49708, 16478, 42881, 16760,
       10738, 10111, 30001, 29976, 46951,  1176, 15823, 37917,  5728,
        9275, 30910, 23606, 45585, 45863, 26469, 41969, 17235,  9625,
       42196, 30685, 22337, 29506, 17427, 30610, 10786, 24879, 26685,
       29108, 13018, 39203, 21941, 23945, 45978, 21612, 49041, 38665,
       13495, 47396, 13804, 39457, 38738, 14043, 46979, 38461,  1959,
       33733, 26243,   937, 49343, 42525, 29141, 17574, 13120, 31342,
        7092, 49412, 23087,  7881, 47552, 16475, 35830, 28254, 15513,
       11204, 43977,  4167, 11181, 32904, 23193,   355, 45878, 35117,
       49200, 27022, 47649, 44259, 44405, 37992, 16455, 32345, 40565,
       34612, 48886, 39122, 23208, 44028, 16945, 22256, 43268, 13866,
       10889, 35747, 40142, 20527, 42167, 39161, 18909, 20766, 40206,
       30682, 27177, 37021, 47821,  6941, 14345, 23542, 34092, 44417,
       41350, 30976,   409,  6517, 37033, 47499, 43869,  6035, 23144,
       28986, 38993,  1837, 14838,  2831, 16539, 24928,  4095, 27229,
        7084, 10600,  5402,  9727, 40710, 43155, 14236,  5381, 48723,
       29233, 25999, 10108, 25829, 20501, 42836, 18655, 28533,  6935,
       34740, 37609, 18868, 49771,  3943, 38707, 43481, 42975, 47150,
       47361, 49212, 32454, 46602, 18085, 38372, 18050, 14679, 34058,
       11903, 18618, 43666,  3311,  7252, 17204, 42762, 28668, 29931,
        9237, 35435,  9681, 34542, 18254, 34563, 31465, 34686, 35370,
       18420,  8063, 45524, 20254, 37219,  7927,  8603, 26619, 17536,
       11110, 22631, 14163,  8069,  8212, 32493, 45155, 42694,  8750,
        2291, 28399, 34488, 39540, 13639, 29612, 38352,  9798, 28397,
       13985,  9463, 33300,  7850, 36441, 22602, 47450, 12584,   271,
        1614, 14918, 37838, 14559,  5773, 10391, 48427, 14423, 25366,
        9168, 17954, 38546,  5506, 29135,  9900,  5535, 28705,  2080,
       22974, 37374, 25263, 13477, 12940, 29912, 22672, 18184, 25312,
       12473, 45023,  5550,  3455, 30021, 36006,  1492, 44193,  8676,
        6073, 38991,  6230, 25757, 44910, 13321, 49020, 13036, 43106,
        6836, 46652, 42014, 18485, 31401, 46877, 49870,  2567, 21486,
        1139, 24617, 24500, 29658,  7714, 12720,  5746,  7136, 34373,
         281, 24789, 11471, 40221, 36598, 44256, 41619,  4679, 37560,
       29988, 30108,  1523, 13934,  8037, 22280,    47,  9217, 31802,
       19130, 17695, 37586,  4014,  4109, 22104, 45550, 13071, 16028,
        6901, 15310,   288,  5012,   108, 14314, 24980,  7890, 27764,
       17980,  1067, 31704,   673, 40860, 36994, 43503,  6927, 33449,
       15895,  4719, 44730, 48270,  5118, 21590,  2812, 18027, 27049,
       46365, 40087, 46405,  6816, 42450, 25276, 33350, 20671, 34149,
       22441, 36920, 31063,  8776, 19944, 28890, 48498, 40687, 24699,
       42271,  5150,  2744, 35126, 30102, 32387, 42193,   648, 20797,
       35656,  4418, 41894, 36268,  3416,  6041, 31884,  7576, 43484,
       38556, 25295, 11478, 45451, 34698, 10398, 20323, 14195, 22603,
       37781, 18328,   800, 39389, 14793, 42472, 46299, 15109, 10217,
       38072, 44652, 22331,   483, 47674,  6317, 10833, 10725, 17325,
       40908, 30694, 42098, 41362, 22843,  1777, 33977, 47360, 33412,
       42185,  3982, 38981,  4012, 22486, 29058, 26893, 45864,  1354,
       24927,  1677, 36385, 43823, 30003, 19899, 30224, 11961, 35360,
        3372, 38705, 41662,   335, 32730, 37208, 13770, 26110,  7688,
       12945, 42803,   864, 49357, 23343, 39426, 21076,    24, 43116,
       45021, 37515,  5281, 11221, 48833, 37178, 44668, 24721, 42399,
       40680,  7328, 39628, 26776, 49869,  4490,  2504, 28363,  2574,
        4941, 27444,   165, 32403, 39650, 24169, 22924, 44464, 11989,
       20834, 12534, 15893, 39281, 16704, 46324, 11642,  8813, 23805,
       26656, 28866, 47201, 19127,  9542,  6374, 19444,  8365, 20523,
       19662, 25976, 32483, 45085, 44141, 32455, 40649,  3332, 22648,
       25668, 30487, 39716, 42755,  1466, 25419, 20039,  1999,  5657,
       37558, 22418,  2460, 41900, 19541,   989, 22329, 24476, 40479,
       25616, 33328, 18591, 20097, 28625, 32025, 48145, 34460, 25930,
        9983, 42678,  7444,  6400, 18706, 11297, 23847, 15622, 17026,
       44794, 13030, 21023, 33148, 15707, 46763, 26842, 12178, 30820,
        1381, 23684, 19277, 48209, 45176,   905,  1185, 31435, 37355,
       38479, 30324, 35418, 36993, 38235, 29881, 26824,  8774, 42952,
       21308, 42945, 35782, 31201, 38551,  3214, 32873, 14615, 21248,
       31700,  8155,  6054,  2932, 32194, 33380, 21297, 28619, 34131,
        4229, 32316, 16596, 48318, 12660, 27440, 37276, 48891, 21881,
       45827, 16521, 41352, 29257, 12133, 11944, 41664, 29944,  1664,
       34758, 22408,  8558, 42573, 34711, 32617,  3609,  5346, 39904,
        1926, 42010, 45373, 47328,   189,  9533, 44568, 17355, 17520,
       25191,  5428, 23024, 24445, 35121,  6711,  3204, 35996, 37279,
        2675, 26045, 33087, 34601, 22858, 13011, 13483, 37712, 20539,
        8098, 32002, 37674, 40648,  8810, 34919, 10893,  8912, 29884,
       44476, 26258,  9801,  8118, 16375, 32444, 20169,  5028, 38847,
       25789, 22026, 27686, 31521, 36558, 10989, 33654, 33076, 25807,
       46625,  8190, 48643, 33926,   129, 39883, 20449, 48635, 21869,
       48024, 28650, 38580, 22967, 46935, 17154,  9616, 42630,  1755,
       31105, 49490, 10215, 14056, 21467, 25820, 47691,   199, 10365,
        6687, 28487, 28921, 37189, 35269, 11451, 33416, 46873, 29940,
        5494,  3515, 25318,  7290, 36781, 29324, 30571, 11271, 21898,
       43626, 23295, 17530, 49970,  9163, 16014, 37135, 26688, 35794,
       48722, 22866, 48062,  9489, 40246, 35549, 41285, 17441, 34545,
        3905, 20017, 34756, 25922, 46204, 25378, 39482,  3490,  5800,
       17051, 25062, 48331, 26711, 21882, 26689, 47413, 46561, 14089,
       44381, 47643, 30896,  3016, 31606, 22914, 16070, 30687, 38626,
       40226, 22011, 47075, 25627,   174, 31886, 43254, 22555, 39166,
       42757, 15247, 33560, 23145, 48326,  6602, 11458, 26609, 43716,
       34361, 32312, 46950, 26210,  7734, 37209, 24707, 20153,  9034,
       25825,  7909, 11199, 39420, 21655, 28067, 33801,  4720, 19895,
       39979, 36074, 45454, 22922, 33952, 41910, 37105, 23483, 48144,
       37180,  3178,  9052, 38552, 33301, 32358, 43034, 30305, 25771,
       41013, 49901,  5183, 11236, 49054, 39822, 18393,  1487, 45479,
       36063, 16339,  9357, 23963, 43922, 17698, 20917,  3910, 30862,
       44550, 48377,   801, 17195, 36124,  9307,  6658, 40369, 43084,
        7765, 12147,  2738, 43500,  2770, 45152,  5720, 24991, 44543,
       46595, 12558, 44234, 32607, 21703, 40326, 27433,    26,  3218,
       45670, 44127, 32006, 20131, 46596, 31295, 35245, 44267,  9345,
       25673, 24839, 26097,   494, 43233, 36518, 48999,  8101,  3807,
       47313, 32264,  6402, 37697, 43283, 34579, 11294, 40097, 44110,
        1127,  3371,  6199, 44376,  9332, 37967, 40584,  5221, 34984,
       37675, 23461, 14558,   776, 13584, 15937, 24611, 26260, 12783,
       32583, 30986, 46712, 17413, 23314, 10070, 22007, 49146, 17835,
        6590, 42647,  6945, 18497, 22994, 46157, 21351, 12647, 17661,
       27753,  5870, 27759, 36230, 19518, 26907, 23278,  5389,  6674,
       12707, 11864, 42021,  7205, 24949, 24749, 37038, 20691, 41540,
       46229, 33915, 36969, 45850, 27263, 46232, 29050, 19381, 36714,
       20338, 29960, 10949, 15226,  1109, 39609,  2505, 26862, 40459,
       14942]), [6, 2, 0, 3]), (array([37228, 15148, 28398, 26615,  5771, 18916, 23880, 46397, 40005,
       45365, 28815, 21340, 47665, 13445, 10874, 45459, 24820, 37482,
          85, 10073, 12844, 24159, 19244, 20094, 24892, 47575, 35767,
       39667, 30939,  3071, 25568,  6734, 22723, 38575, 49335,  3988,
       48066, 14742, 16265, 31212,  5403, 38358, 43617, 46797,  4248,
       42366, 22806, 33191, 16706,  6655, 29535, 31321, 44353, 43150,
        2405,  9590, 40374,  1386, 19997, 29886, 11581, 14531, 11391,
       39081, 13935,  5325,  5976, 39164, 44081,  1583, 37489, 16995,
       27657,  5220, 33821, 40977, 43929, 28015, 15364, 22126, 26056,
       29614, 17720, 12271, 20618, 34904, 12633,  5307, 35120, 17913,
       26916, 49377, 42959, 41239, 26702, 41437,  1902, 10754, 21751,
       39304,  3817, 38104, 27451, 17398, 45292, 16344,  2969, 46154,
        2397, 46465, 38195,  1489, 42555, 43848, 41805, 37293,  3835,
       25283,  3637, 35008, 19159,  1483, 39400, 13799, 35462, 15067,
       32640, 37971,   662, 44578, 46954, 29789, 12794, 33330, 47924,
       44892, 13026, 10156,  6157, 20429, 29859,  9939, 49456,   727,
       25865, 42018, 16096, 46167, 27929,  7141, 33388, 32490, 48307,
       38367, 43556, 45682, 46027, 41959, 46266, 34165, 21060, 48767,
        7185, 11321, 25791, 16879, 27985, 19206, 28206, 21314, 29910,
       46549, 36718, 47657, 15332, 44935, 12397,  7197, 10496, 41347,
       30484, 39351, 45657,   652, 41526, 48732,  4427, 49048, 14066,
        7420, 27217, 29962, 19779, 34733, 13566, 15400, 15842,  1549,
       46672, 15932, 47817, 24930, 43036, 48166, 34584,  6827, 16031,
       44123, 25724, 40243, 44366, 42268, 26314, 18417, 49116, 26692,
       11553,  6335, 44086, 18620, 10818, 39246, 36270, 16654, 16138,
       12941,  3824, 22782, 24371, 33935,  8887,  3782, 47673, 12535,
       16996, 43305, 31389, 42453, 37159, 31529, 20223,  7829, 31653,
        4736,  6713, 41517,  6307, 37948, 37862, 48055, 45327, 18104,
       30433, 15964, 49445, 22440, 26951, 47527, 40491, 32243,  8972,
       13054,  9972, 14010, 32574, 48076,   766, 29414, 19195, 42794,
       19147, 46248, 22462, 20400, 13131, 19175, 38469, 39621,  5348,
       37703, 21312,  5701, 20455, 26337,  4891, 36733, 15721, 21377,
       17116, 25823, 37207, 44199, 20405, 35864,  8241, 40702, 33093,
        7334, 49571, 19851, 33562, 40252, 21830, 22920, 25207, 37583,
       14123, 29868, 29199, 40493, 48959, 13674, 30018,  1770, 32543,
         193, 39802, 44877, 11993, 20652, 18593, 11652,  8401, 41616,
       27864, 43149, 23436, 21253,  1383, 47854,  7118, 44205,  4287,
       44382, 24841, 39298, 49393, 36799,  7455, 26603, 22262, 47206,
       10682,  1250, 12115, 48690, 34152, 31104, 36534, 47791, 19733,
       26246, 34910, 23973, 30510, 12543, 34248, 36013, 48311,  3497,
       19416, 44304, 20089,  1794,  6995, 28384,  9418, 10475,  6418,
       27932, 32915, 26977,  8073, 40939, 16479, 42617, 43899, 24444,
       21936, 46552, 23766, 31170, 23839, 47978, 38525, 35139, 38949,
         442, 12041, 22584, 44458,  3159, 29762, 26714, 48678, 23258,
       35115,  2160, 28435,  5185, 40429,  7104,  3670, 15198, 24386,
       37416, 11977, 40598, 20540, 18171, 33287, 45619, 40458, 19479,
       13672, 46151, 20237, 18853,  3144, 40408, 41988, 19472, 48871,
       30808, 26094, 30411, 29371, 42050, 42842, 18623, 30380, 31423,
       32567, 40778,  6397, 31966, 10095, 34419, 30328, 32463, 33335,
       17345, 28728, 31121, 13355, 19887, 19141,  8526, 35562, 20006,
       19768, 16084, 24350,   880,  1702,  1357, 38276, 30844, 26988,
       49905, 16118, 25258, 21172, 25794, 48602, 23153, 10606, 45890,
        3584, 23215, 13406,  3353,  6508, 42676, 42388, 43278, 12233,
       10272,  2923, 45240, 41570, 34560,  1967, 10445, 23583, 11842,
       11501, 11010, 38315,  8852, 13967,  2395, 17013, 37335, 28874,
       23282, 15043, 45776, 28461, 42182,  1144, 48212, 33000, 27934,
        2169, 16632, 29060, 18508, 37083, 26427, 41588, 29487, 22962,
       44749, 11721, 29295, 33933, 21983, 38433,  1270, 15466, 22190,
       32251,  8549,  3665, 24700, 45615, 15288, 38112,  9171, 41300,
       35594,  9221, 46458, 18525, 27940, 19370,  7747, 32505, 39149,
       11178, 25144,  1329,   497, 19004,  3180, 31459, 21105,  2718,
       17088, 38335, 40644, 48105, 49091, 17620, 26300, 41960, 36363,
       28033, 22881,  3263, 15436, 48280,  7360,  9296,  3466, 21777,
       13850, 10855, 40589,  5984, 30829, 26879, 23157, 43965, 11475,
        7826,  2780, 10266, 15678,  8200, 43750, 24639,  8478, 20434,
       22200, 30667, 17237, 47379, 34022, 32190, 13751, 48508, 14450,
       43659, 38198, 28449, 31674,   663, 29274, 23419, 41574, 34537,
        7808, 42934,  4368, 26488, 13238, 24939, 36149,  8437,  9512,
        2258, 42298, 10353,  6665, 14708, 12785,  1044, 49656, 37265,
       12818, 45113, 22799, 23182, 19527, 11431, 16460,  7504, 38003,
       43184, 27323, 10607, 45936, 48246, 12079, 21227,  8072, 30811,
       22973, 27004, 35991, 30652, 23610, 46831, 24179, 16316, 10513,
       46303, 16554, 12083, 16230, 19814, 48850,  8695,  4314, 48729,
        5484, 11919, 46807,  1816,  5010,  2659, 22276, 47220, 29941,
       25160, 14080, 30002, 36446, 17133, 43219, 41818, 33438,  9004,
        2714, 46655, 33242, 37165, 36623, 30830, 11682, 13106,  8355,
       24881, 30158, 16146,  9375, 29977, 49723, 37181, 13668, 31358,
       43814, 11691, 30078,  4079, 35685,  6629, 42597, 24642, 24291,
       39530, 12836, 19084,   436, 45621, 24325, 47143, 19812,  3441,
       27981, 33732, 49734, 18192, 23738, 19804, 16002,  7792, 39380,
       26723, 29336, 40504, 30304, 34964, 27562, 24600, 46964, 38326,
       18614, 21413, 18100, 10323,  8444, 14509, 36909, 12181, 11029,
       47317, 26761, 21892, 42722,  2345,  6640, 16015, 34125, 14591,
       25070,  7203, 33453, 32139,  2559, 36870, 11188, 49983, 42825,
       25934,  9760, 37163, 25547, 26160, 43735, 43773, 48058, 18798,
       49500, 25971, 31649, 31603, 24330, 42084,  1098, 37817,  8599,
       16869, 15057, 34437, 37453, 38364, 14574, 13061,  3514, 11976,
         949, 31254, 39761, 11257, 11293, 20034, 28738, 14527, 37700,
        6060,  8164, 25745,   550, 27298, 23122, 14861, 18387, 46653,
        2164,  8161, 28838, 28028, 19787,  9737,  1554, 20961, 43742,
       12050,    21, 11938, 16366, 23417, 22778,  5848, 44926,  4288,
       49819, 29692,  9667, 26883,  9982, 44456, 17414, 18865, 34623,
       36418, 29839, 22364, 28433, 33964, 30634, 46841, 23027, 31788,
       23239, 37973, 23465, 44802,  3756,  7107, 27156, 32686, 19838,
         101, 42789,  9421, 28808, 27742, 44142, 29460,  1685, 13064,
       21103, 40511, 39689, 48540, 30624,  9222,  3003,  3376, 17144,
       31094, 14340, 28819, 38171, 39533,  4402, 10011, 25077, 39608,
        8067, 30386, 36273, 43800, 20877, 42997, 38699, 49599, 14148,
       35449,  4392, 17430,  9399, 41511,  8246,  8635, 11231, 25443,
       26906, 13463, 18771,  8422, 25006, 48849, 47973, 32524, 28813,
       35595,  3818, 11578, 25986,  2518, 30779, 28983, 25688,    74,
       33211, 13914, 49499, 28029, 23998, 30573,  6186, 18422, 46962,
       13313,  1778,  8381, 20601, 27467,  6980,  6394, 26302, 41269,
       17909, 31825, 33925, 17098, 21962, 25219, 45091, 22664, 12821,
       32541, 10958,  4791, 32982, 33393, 28034, 44562, 45053, 46347,
       11743, 30839,  5062,  1461, 22638, 29979, 42411, 44409, 34952,
        5915, 31676, 20007,  2359,  3758, 25782, 12631, 32113,  1427,
        6883, 15856, 38932, 37156, 16814,  4083, 25706, 15083, 43105,
       31366, 18954, 39789, 23763, 11040, 34590, 28984,  6343,  8822,
        9943, 18389, 47539, 38951, 43533, 27212, 31161, 15080, 38377,
       21510, 32350, 37773, 20849, 38897, 11409, 37304,  6724, 46198,
       39171]), [7, 8, 0, 3])]
Collaboration
DC 0, val_set_size=1000, COIs=[5, 9, 0, 3], M=tensor([5, 9, 0, 3], device='cuda:0'), Initial Performance: (0.25, 0.045359280467033386)
DC 1, val_set_size=1000, COIs=[4, 1, 0, 3], M=tensor([4, 1, 0, 3], device='cuda:0'), Initial Performance: (0.274, 0.04425249779224396)
DC 2, val_set_size=1000, COIs=[6, 2, 0, 3], M=tensor([6, 2, 0, 3], device='cuda:0'), Initial Performance: (0.254, 0.04541688692569733)
DC 3, val_set_size=1000, COIs=[7, 8, 0, 3], M=tensor([7, 8, 0, 3], device='cuda:0'), Initial Performance: (0.255, 0.044730136394500734)
D00: 1000 samples from classes {0, 3}
D01: 1000 samples from classes {0, 3}
D02: 1000 samples from classes {0, 3}
D03: 1000 samples from classes {0, 3}
D04: 1000 samples from classes {0, 3}
D05: 1000 samples from classes {0, 3}
D06: 1000 samples from classes {9, 5}
D07: 1000 samples from classes {9, 5}
D08: 1000 samples from classes {9, 5}
D09: 1000 samples from classes {9, 5}
D010: 1000 samples from classes {9, 5}
D011: 1000 samples from classes {9, 5}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {2, 6}
D019: 1000 samples from classes {2, 6}
D020: 1000 samples from classes {2, 6}
D021: 1000 samples from classes {2, 6}
D022: 1000 samples from classes {2, 6}
D023: 1000 samples from classes {2, 6}
D024: 1000 samples from classes {8, 7}
D025: 1000 samples from classes {8, 7}
D026: 1000 samples from classes {8, 7}
D027: 1000 samples from classes {8, 7}
D028: 1000 samples from classes {8, 7}
D029: 1000 samples from classes {8, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.321, 0.059396352589130404) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.0762955961972475) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.08767266011238098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.0864397220313549) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.06267942577600479) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.09520264573395253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.333, 0.12743896949291228) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.09229815077781678) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.07932132732868194) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.11468889297544957) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.15861733889579774) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1226446467190981) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.10350485199689866) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13766499603539706) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.17524761104583741) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1584977867305279) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.425, 0.12845924731343986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.15613945835083723) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.18912637266516685) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.19356412656605243) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.14465451043099165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.17646908596530556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.19739552516490222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.22081087283417583) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.1671894628610462) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1882399409338832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.18544992347434164) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.23137943388335408) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.1650114922504872) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.1933495959304273) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.22099166601337492) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2418985731303692) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.1857669574674219) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.21793028167169542) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.268153380241245) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.272056638228707) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.432, 0.21375495231803507) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.23726580816414208) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.27721165637299416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.26264312120061367) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[0, 3], M=tensor([0, 1, 2, 3, 4, 5, 6, 9], device='cuda:0'), Initial Performance: (0.7386666666666667, 0.019556183099746705)
DC Expert-0, val_set_size=500, COIs=[9, 5], M=tensor([5, 9, 0, 3], device='cuda:0'), Initial Performance: (0.864, 0.014043234599754215)
DC Expert-1, val_set_size=500, COIs=[1, 4], M=tensor([4, 1, 0, 3], device='cuda:0'), Initial Performance: (0.924, 0.007941799638792872)
DC Expert-2, val_set_size=500, COIs=[2, 6], M=tensor([6, 2, 0, 3], device='cuda:0'), Initial Performance: (0.862, 0.012280856512486935)
SUPER-DC 0, val_set_size=1000, COIs=[5, 9, 0, 3], M=tensor([5, 9, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[4, 1, 0, 3], M=tensor([4, 1, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[6, 2, 0, 3], M=tensor([6, 2, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7f86d474dca0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f86d46fe2e0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f86d4776490>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f86fc0af790>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f86fce70a30>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO0', '(DO5']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.916, 0.008797819319181143) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0027626071330159904) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.846, 0.013388146057724953) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.202284628899768) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.86, 0.011904631897807121) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.51, 0.05188334634900093) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.591, 0.0386452279984951) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.551, 0.0431800479888916) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.008682149488013238) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.004852463942952454) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.834, 0.015791516786441206) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.19819134156219662) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.882, 0.013182898362477621) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.04949211710691452) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.574, 0.05032752841711044) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.509, 0.07008931949734688) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.007154843011172488) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0032015770715661347) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.848, 0.01475045869871974) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.19185614731162787) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.88, 0.015705688549205662) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.515, 0.07234897427260875) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.596, 0.05381020018458366) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.08362297907471657) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.006733662298880517) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.004740340396761894) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.013733267890289426) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.18627288238983603) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8713333333333333, 0.019884441730876765) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.07831947584450245) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.48, 0.08501663907617331) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.1169239146411419) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.007685335980728268) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002926716686459258) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.016141982642933726) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1867575901262462) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8793333333333333, 0.01884974013765653) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.07386719399690628) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.596, 0.05418705374002457) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.12178006175160408) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO2', '(DO4']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00603665279597044) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.005650342805543915) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.848, 0.014217321373522282) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.15693664961494505) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.878, 0.019287356863419213) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.06848330770432949) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.06400193326175213) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.12357474705576897) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006272130399011076) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0034782493873499335) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.01647706372337416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.16303204258624465) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8566666666666667, 0.019315778482705354) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.08111459956690668) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.07120438142120838) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.11193278734385967) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.007432206010911614) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0031926923780702054) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.011944354344159365) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.482, 0.15962765615060925) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8813333333333333, 0.021738560996910867) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.51, 0.09280189917981625) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.574, 0.062284848622977736) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.1254464317448437) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006510346467373892) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002884585874155164) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.013499623343348503) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.19402825484983624) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8853333333333333, 0.014938272611858944) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.08005668370425702) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.056590872913599016) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.0902147321254015) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.91, 0.012887852136045694) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002823212784714997) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.015212585625238717) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.15432002197531983) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.87, 0.01992270176609357) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.0882925277352333) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.588, 0.06127103886008262) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.10593145179748535) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO0']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO4', '(DO5']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007147078015375882) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0027388650139328094) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.01395148017257452) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.1597617387995124) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9013333333333333, 0.010270345531404018) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.057141930758953095) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.692, 0.04349037130177021) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.524, 0.07781884890794755) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.012371458045992767) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003330631384276785) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.014094014093279839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.18074664761638268) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8926666666666667, 0.01577568422506253) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.495, 0.06900407017767429) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.657, 0.044877390325069426) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.501, 0.08592344632744789) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.008203834002139046) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.004222981085302308) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.01384442660957575) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.16830685949418694) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8993333333333333, 0.014792030582825343) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.573, 0.06975565332174301) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.645, 0.04767058899998665) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.507, 0.08684714521467686) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.00752336347696837) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0034084603029768913) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.014728436388075352) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.2156158124502981) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8773333333333333, 0.02000431376757721) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.0763988836556673) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.641, 0.052378553155809644) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.11195826586335897) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.007329028574982658) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0025791598302312194) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.015145707655698061) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.18677675103675573) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.015864197353832425) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.08993280877172947) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.63, 0.05874638655781746) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.11200674146413803) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO5', '(DO2']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.008424876491015311) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0036475600189296528) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.014754123521968723) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.484, 0.1497696012388915) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.908, 0.012891745950483407) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.09837621605396271) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.637, 0.05615497786179185) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.12294337707012892) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.010681920854141935) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.003480084926588461) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.011984552055597305) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.1565052929436788) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9113333333333333, 0.012893854146823288) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.509, 0.083429901227355) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.613, 0.06523317552357911) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.454, 0.14814233236759902) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006009857769822702) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.0044279661867767574) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.012297477589920162) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.17785728448512964) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8993333333333333, 0.01544627624284476) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.10032183369249105) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.07073476970940829) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.15145339602977037) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006644043601118028) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004181144768372178) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.013942571630701422) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.19283248333260417) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.015655061736343973) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.1021920265313238) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.543, 0.07918449993431569) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.443, 0.15043318097293376) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.007023573950398714) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0030606704250676557) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.856, 0.015406595140695572) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.16282831619982607) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.013997458492095272) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.08185077619552612) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.05846500688791275) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.13707699072360993) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO1', '(DO4']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006791485738707706) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0023929036326007917) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.01148803972825408) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.13632027061656118) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9013333333333333, 0.01521938026122128) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.532, 0.08530445443093777) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.595, 0.057420378103852275) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.1593458225093782) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005328749123029411) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0033827176720951685) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.012927127601578832) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.1436647316981107) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8666666666666667, 0.015083851849039395) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.588, 0.07023514124751092) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.631, 0.05608541110157966) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.11787673798203469) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005538581674918532) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0028014465735759584) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.013785000763833523) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.482, 0.16437361545301973) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.015978978397945563) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.573, 0.07981936053931712) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.636, 0.05829978094995022) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.1274201175644994) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.00893632118741516) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.002691277205129154) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.013688851121813058) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.17408827702887356) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.894, 0.01587187810242176) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.525, 0.07381827569007873) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.577, 0.06439052510261535) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.10882750642299652) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.006340943960938603) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0032707413439638914) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.011666942909359932) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.15272915404476226) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.88, 0.01894489412754774) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.07666555706039072) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.579, 0.06761926504969597) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.434, 0.15172454115748404) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO4', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.006675323863397352) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0030947608663700523) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.014396986618638039) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.1784473698316142) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.008999346004178127) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.05461609560251236) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.649, 0.04492214065790177) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.505, 0.08171650516986848) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.006436146337538958) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.003166094608430285) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.015902306402102113) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.16862346666213124) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8966666666666666, 0.017113448372731606) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.08117258333042264) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.06981517609907315) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.14127426079660654) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.009003866168568493) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0020901354488451034) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.013893921058624983) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.1363991097919643) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9026666666666666, 0.01046888702052335) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.05555423206090927) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.635, 0.052924154937267306) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.10555503931641579) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.007482806828280445) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0030701248918194326) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.014228018473833799) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.15100812367349864) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9126666666666666, 0.015825499405153095) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.58, 0.07473848024383187) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.635, 0.05298545780964196) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.1387817184496671) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.007145770341856405) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0042073328316328115) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.012795881800353527) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.1442554307663813) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9006666666666666, 0.022172521217959002) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.58, 0.07746736445371062) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.624, 0.05343718648143113) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.13929371204972268) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO3', '(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005837494179140776) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.0024728283006697892) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.014082595467567445) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.475, 0.14735017582913862) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.008540828635916113) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.622, 0.04948051387071609) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.647, 0.04571648895740509) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.495, 0.1095470377355814) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.007567205921106506) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0037586598920461255) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.011312005937099457) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.478, 0.1406161149572581) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.914, 0.012465392368535201) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.053051490128040314) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.639, 0.04415915369987488) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.12187251075357199) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.006583817577455193) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.002637824766279664) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.012312566634267568) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.15517189231328665) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.013931151605521639) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.622, 0.06317522004246712) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.593, 0.06040302117168903) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.1458447486460209) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006153874894138426) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.00438250995472481) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.013861162841320038) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.1451684135934338) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9126666666666666, 0.019625570172521596) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.599, 0.07651323808357119) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.549, 0.06985076327621936) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.1588041096404195) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007395956352120266) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003648213281529024) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.013247292183339596) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.15308491450268774) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.892, 0.018987711841861408) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.566, 0.0650518587026745) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.56, 0.05660489083826542) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.455, 0.12540598753094673) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO0']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO3', '(DO5']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00675099419511389) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0027380446848110297) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.012628548633307219) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.13948837914317846) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9173333333333333, 0.01168941122914354) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.056140568435192106) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.664, 0.041801796585321425) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.1033733970746398) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007528007957385853) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0029837142864125783) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.013141703844070434) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.1664905238985084) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9053333333333333, 0.01269388726229469) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.595, 0.05898658022284508) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.588, 0.05373974618315697) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.13498189368844032) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.009481785293959547) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0032466075153788554) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.013503954038023949) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.1453288573101163) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9113333333333333, 0.015040922812962284) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.06654856085404753) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.573, 0.05909930290281772) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.14173584230989217) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007408826069673523) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002910313053405844) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.011657439164817334) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.14767794376146048) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9146666666666666, 0.01592887438897742) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.057075161404907704) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.547, 0.06741560214757919) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.14708754047751427) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.0068932567005977035) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0026289214560529218) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.011924619436264038) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.158649131112732) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9126666666666666, 0.01999915239376666) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.58, 0.06179235304892063) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.06972173573076725) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.458, 0.14302597012370824) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.045359280467033386), (0.321, 0.059396352589130404), (0.405, 0.06267942577600479), (0.434, 0.07932132732868194), (0.459, 0.10350485199689866), (0.425, 0.12845924731343986), (0.434, 0.14465451043099165), (0.452, 0.1671894628610462), (0.465, 0.1650114922504872), (0.456, 0.1857669574674219), (0.432, 0.21375495231803507), (0.51, 0.05188334634900093), (0.554, 0.04949211710691452), (0.515, 0.07234897427260875), (0.47, 0.07831947584450245), (0.487, 0.07386719399690628), (0.54, 0.06848330770432949), (0.516, 0.08111459956690668), (0.51, 0.09280189917981625), (0.516, 0.08005668370425702), (0.482, 0.0882925277352333), (0.555, 0.057141930758953095), (0.495, 0.06900407017767429), (0.573, 0.06975565332174301), (0.517, 0.0763988836556673), (0.528, 0.08993280877172947), (0.528, 0.09837621605396271), (0.509, 0.083429901227355), (0.542, 0.10032183369249105), (0.539, 0.1021920265313238), (0.554, 0.08185077619552612), (0.532, 0.08530445443093777), (0.588, 0.07023514124751092), (0.573, 0.07981936053931712), (0.525, 0.07381827569007873), (0.577, 0.07666555706039072), (0.609, 0.05461609560251236), (0.555, 0.08117258333042264), (0.583, 0.05555423206090927), (0.58, 0.07473848024383187), (0.58, 0.07746736445371062), (0.622, 0.04948051387071609), (0.604, 0.053051490128040314), (0.622, 0.06317522004246712), (0.599, 0.07651323808357119), (0.566, 0.0650518587026745), (0.612, 0.056140568435192106), (0.595, 0.05898658022284508), (0.586, 0.06654856085404753), (0.606, 0.057075161404907704), (0.58, 0.06179235304892063)]
TEST: 
[(0.25, 0.044425843864679335), (0.315, 0.05723254606127739), (0.4005, 0.06019690066576004), (0.43425, 0.07564766851067543), (0.455, 0.09838825124502182), (0.426, 0.12078447866439819), (0.4355, 0.13630374038219453), (0.44775, 0.15698001247644425), (0.46175, 0.15570431262254714), (0.45275, 0.1750297738313675), (0.43225, 0.19910654628276825), (0.517, 0.04840267573297024), (0.564, 0.04539964936673641), (0.51775, 0.06805687613785266), (0.4715, 0.07698481196165084), (0.48425, 0.07363548289239406), (0.54625, 0.0671285502910614), (0.50325, 0.08036265036463737), (0.51525, 0.089060562312603), (0.52125, 0.07632073950767516), (0.49525, 0.08436713510751724), (0.58325, 0.05301331432163715), (0.5075, 0.06541303053498268), (0.57225, 0.06680126169323922), (0.521, 0.0738573904633522), (0.54175, 0.0860982222855091), (0.536, 0.0916092700958252), (0.526, 0.07980495163798332), (0.5385, 0.09726503983139992), (0.5225, 0.0999449874162674), (0.55775, 0.08019475163519382), (0.53575, 0.08415873473882675), (0.5865, 0.06853266173601151), (0.579, 0.07863743287324905), (0.53575, 0.07153590503334999), (0.55975, 0.07542950560152531), (0.6, 0.0525066974312067), (0.57, 0.07978815451264382), (0.58375, 0.053983713135123255), (0.578, 0.07268883799016476), (0.57075, 0.07384569936990738), (0.616, 0.04878663429617882), (0.59875, 0.053379642575979235), (0.61925, 0.0632165914773941), (0.60425, 0.07561728510260582), (0.579, 0.0641536696255207), (0.60975, 0.05324277028441429), (0.60725, 0.05451296405494213), (0.57825, 0.06380531451106071), (0.60225, 0.056444297522306446), (0.5715, 0.06201739613711834)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.62      0.90      0.74      1000
           3       0.44      0.77      0.56      1000
           5       0.47      0.13      0.21      1000
           9       0.94      0.48      0.64      1000

    accuracy                           0.57      4000
   macro avg       0.62      0.57      0.54      4000
weighted avg       0.62      0.57      0.54      4000

Collaboration_DC_1
VAL: 
[(0.274, 0.04425249779224396), (0.25, 0.0762955961972475), (0.251, 0.09520264573395253), (0.301, 0.11468889297544957), (0.395, 0.13766499603539706), (0.39, 0.15613945835083723), (0.431, 0.17646908596530556), (0.459, 0.1882399409338832), (0.462, 0.1933495959304273), (0.466, 0.21793028167169542), (0.462, 0.23726580816414208), (0.591, 0.0386452279984951), (0.574, 0.05032752841711044), (0.596, 0.05381020018458366), (0.48, 0.08501663907617331), (0.596, 0.05418705374002457), (0.536, 0.06400193326175213), (0.541, 0.07120438142120838), (0.574, 0.062284848622977736), (0.604, 0.056590872913599016), (0.588, 0.06127103886008262), (0.692, 0.04349037130177021), (0.657, 0.044877390325069426), (0.645, 0.04767058899998665), (0.641, 0.052378553155809644), (0.63, 0.05874638655781746), (0.637, 0.05615497786179185), (0.613, 0.06523317552357911), (0.561, 0.07073476970940829), (0.543, 0.07918449993431569), (0.565, 0.05846500688791275), (0.595, 0.057420378103852275), (0.631, 0.05608541110157966), (0.636, 0.05829978094995022), (0.577, 0.06439052510261535), (0.579, 0.06761926504969597), (0.649, 0.04492214065790177), (0.578, 0.06981517609907315), (0.635, 0.052924154937267306), (0.635, 0.05298545780964196), (0.624, 0.05343718648143113), (0.647, 0.04571648895740509), (0.639, 0.04415915369987488), (0.593, 0.06040302117168903), (0.549, 0.06985076327621936), (0.56, 0.05660489083826542), (0.664, 0.041801796585321425), (0.588, 0.05373974618315697), (0.573, 0.05909930290281772), (0.547, 0.06741560214757919), (0.565, 0.06972173573076725)]
TEST: 
[(0.27025, 0.043206459641456606), (0.25, 0.07365380853414535), (0.25025, 0.09170785903930664), (0.305, 0.110004026055336), (0.39225, 0.13228247344493865), (0.3865, 0.14950004374980927), (0.42625, 0.1695541403889656), (0.4605, 0.1806327423453331), (0.45975, 0.18581818532943725), (0.4595, 0.20804015803337098), (0.4505, 0.22729678165912628), (0.5985, 0.036444145917892454), (0.56425, 0.04927974770963192), (0.585, 0.051857920676469806), (0.473, 0.08334635320305825), (0.58275, 0.05234840978682041), (0.55775, 0.060655331030488015), (0.55, 0.06873160934448243), (0.5625, 0.06103827066719532), (0.6105, 0.05138237802684307), (0.591, 0.05778900954127312), (0.69425, 0.038100824624300006), (0.64925, 0.04073906065523624), (0.64525, 0.04639633031189442), (0.6115, 0.05229295037686825), (0.61725, 0.054467867821455004), (0.616, 0.05662575176358223), (0.5875, 0.06578255243599415), (0.58025, 0.06691091747581959), (0.53175, 0.07793269461393357), (0.58225, 0.05535239949822426), (0.5915, 0.05537011614441872), (0.63575, 0.05060394621640444), (0.61775, 0.057153070703148845), (0.59725, 0.05651558062434196), (0.5875, 0.06577482634782791), (0.6575, 0.04136195059120655), (0.56625, 0.0681123581007123), (0.64875, 0.04758562745898962), (0.62025, 0.05278485229611397), (0.60725, 0.05452183011174202), (0.64075, 0.0419049629420042), (0.64725, 0.04337911809980869), (0.60025, 0.055175087109208105), (0.5585, 0.0656243527829647), (0.586, 0.05219199933111668), (0.657, 0.04015386894345283), (0.59325, 0.053049692139029506), (0.571, 0.056933272004127504), (0.552, 0.06475321246683598), (0.555, 0.06753564298152924)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.52      0.92      0.66      1000
           1       0.95      0.33      0.49      1000
           3       0.51      0.79      0.62      1000
           4       0.55      0.19      0.28      1000

    accuracy                           0.56      4000
   macro avg       0.63      0.56      0.51      4000
weighted avg       0.63      0.56      0.51      4000

Collaboration_DC_2
VAL: 
[(0.254, 0.04541688692569733), (0.317, 0.08767266011238098), (0.333, 0.12743896949291228), (0.39, 0.15861733889579774), (0.423, 0.17524761104583741), (0.423, 0.18912637266516685), (0.413, 0.19739552516490222), (0.415, 0.18544992347434164), (0.428, 0.22099166601337492), (0.418, 0.268153380241245), (0.431, 0.27721165637299416), (0.551, 0.0431800479888916), (0.509, 0.07008931949734688), (0.486, 0.08362297907471657), (0.433, 0.1169239146411419), (0.437, 0.12178006175160408), (0.443, 0.12357474705576897), (0.447, 0.11193278734385967), (0.448, 0.1254464317448437), (0.475, 0.0902147321254015), (0.477, 0.10593145179748535), (0.524, 0.07781884890794755), (0.501, 0.08592344632744789), (0.507, 0.08684714521467686), (0.454, 0.11195826586335897), (0.482, 0.11200674146413803), (0.451, 0.12294337707012892), (0.454, 0.14814233236759902), (0.439, 0.15145339602977037), (0.443, 0.15043318097293376), (0.448, 0.13707699072360993), (0.459, 0.1593458225093782), (0.486, 0.11787673798203469), (0.475, 0.1274201175644994), (0.466, 0.10882750642299652), (0.434, 0.15172454115748404), (0.505, 0.08171650516986848), (0.464, 0.14127426079660654), (0.488, 0.10555503931641579), (0.472, 0.1387817184496671), (0.468, 0.13929371204972268), (0.495, 0.1095470377355814), (0.48, 0.12187251075357199), (0.461, 0.1458447486460209), (0.464, 0.1588041096404195), (0.455, 0.12540598753094673), (0.482, 0.1033733970746398), (0.478, 0.13498189368844032), (0.467, 0.14173584230989217), (0.466, 0.14708754047751427), (0.458, 0.14302597012370824)]
TEST: 
[(0.25275, 0.04452108883857727), (0.3225, 0.08423771148920059), (0.348, 0.12218378227949142), (0.39725, 0.15200227892398835), (0.41825, 0.16702659863233565), (0.416, 0.18177365124225617), (0.3995, 0.19000737953186037), (0.40175, 0.17578768062591552), (0.42075, 0.21132051086425782), (0.41375, 0.25228228414058684), (0.42475, 0.2621416676044464), (0.5615, 0.042539005890488625), (0.49775, 0.0698877067565918), (0.481, 0.08458694663643837), (0.438, 0.11819455188512802), (0.44575, 0.12024194267392159), (0.45875, 0.12238440009951591), (0.45725, 0.11254908755421639), (0.45825, 0.12689036849141122), (0.48175, 0.08951256284117699), (0.47875, 0.10548533707857131), (0.515, 0.07913729718327522), (0.5035, 0.08491598689556122), (0.504, 0.08830009055137635), (0.45875, 0.11215429413318634), (0.4825, 0.11180774921178818), (0.472, 0.1233478852212429), (0.47, 0.1504174976348877), (0.451, 0.15280825483798982), (0.45625, 0.15005446743965148), (0.465, 0.13731241723895074), (0.455, 0.15876861542463303), (0.46925, 0.11553596651554107), (0.4665, 0.13058608958125115), (0.48025, 0.1069278228878975), (0.456, 0.14876867628097534), (0.51275, 0.0808304697573185), (0.471, 0.14314982801675796), (0.49475, 0.11005405351519584), (0.472, 0.14529122573137282), (0.45775, 0.14143259835243224), (0.488, 0.11486515012383461), (0.48175, 0.12685845482349395), (0.4665, 0.14669293397665023), (0.45875, 0.16564061760902404), (0.4555, 0.12886491292715072), (0.4765, 0.10711622792482377), (0.46775, 0.14034500125050545), (0.465, 0.14751057544350624), (0.46225, 0.154000002682209), (0.457, 0.1486506912112236)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.62      0.93      0.74      1000
           2       0.66      0.05      0.09      1000
           3       0.35      0.85      0.50      1000
           6       1.00      0.00      0.00      1000

    accuracy                           0.46      4000
   macro avg       0.66      0.46      0.33      4000
weighted avg       0.66      0.46      0.33      4000

Collaboration_DC_3
VAL: 
[(0.255, 0.044730136394500734), (0.45, 0.0864397220313549), (0.465, 0.09229815077781678), (0.477, 0.1226446467190981), (0.477, 0.1584977867305279), (0.481, 0.19356412656605243), (0.477, 0.22081087283417583), (0.479, 0.23137943388335408), (0.48, 0.2418985731303692), (0.479, 0.272056638228707), (0.48, 0.26264312120061367), (0.478, 0.202284628899768), (0.473, 0.19819134156219662), (0.475, 0.19185614731162787), (0.48, 0.18627288238983603), (0.477, 0.1867575901262462), (0.48, 0.15693664961494505), (0.478, 0.16303204258624465), (0.482, 0.15962765615060925), (0.473, 0.19402825484983624), (0.48, 0.15432002197531983), (0.476, 0.1597617387995124), (0.476, 0.18074664761638268), (0.48, 0.16830685949418694), (0.481, 0.2156158124502981), (0.478, 0.18677675103675573), (0.484, 0.1497696012388915), (0.479, 0.1565052929436788), (0.472, 0.17785728448512964), (0.477, 0.19283248333260417), (0.476, 0.16282831619982607), (0.476, 0.13632027061656118), (0.479, 0.1436647316981107), (0.482, 0.16437361545301973), (0.481, 0.17408827702887356), (0.479, 0.15272915404476226), (0.475, 0.1784473698316142), (0.477, 0.16862346666213124), (0.479, 0.1363991097919643), (0.478, 0.15100812367349864), (0.473, 0.1442554307663813), (0.475, 0.14735017582913862), (0.478, 0.1406161149572581), (0.481, 0.15517189231328665), (0.469, 0.1451684135934338), (0.477, 0.15308491450268774), (0.477, 0.13948837914317846), (0.479, 0.1664905238985084), (0.479, 0.1453288573101163), (0.48, 0.14767794376146048), (0.481, 0.158649131112732)]
TEST: 
[(0.2535, 0.04368370050191879), (0.4385, 0.08277494287490844), (0.46125, 0.08807962861657143), (0.47575, 0.11717280966043472), (0.48025, 0.15088662540912628), (0.47625, 0.1845778265595436), (0.48075, 0.20741392266750336), (0.48275, 0.2172055230140686), (0.48175, 0.22952548241615295), (0.481, 0.25865569376945496), (0.47825, 0.2520137227773666), (0.47975, 0.19416546428203582), (0.47025, 0.18976876080036165), (0.47525, 0.184199372112751), (0.4805, 0.17898445546627045), (0.4805, 0.1793174769282341), (0.4815, 0.1520005999803543), (0.477, 0.15640242356061934), (0.48025, 0.1522474473118782), (0.4755, 0.18456343859434127), (0.47675, 0.14851689320802688), (0.478, 0.15255109119415283), (0.48525, 0.17273740637302398), (0.4835, 0.16049660021066667), (0.4825, 0.2069152603149414), (0.4825, 0.17946988999843597), (0.47875, 0.14411262571811675), (0.48175, 0.14825471848249436), (0.4735, 0.1654797681570053), (0.48275, 0.18584182864427568), (0.474, 0.15533064442873), (0.4795, 0.12919218891859055), (0.483, 0.13500113958120347), (0.484, 0.15800216990709304), (0.48425, 0.16705368828773498), (0.479, 0.14486952918767929), (0.4755, 0.1683585970401764), (0.48275, 0.16195842236280442), (0.48225, 0.1295547170639038), (0.47825, 0.14320941346883773), (0.4785, 0.13748773819208146), (0.48, 0.13920319044589996), (0.479, 0.1344702118039131), (0.4835, 0.14764766001701354), (0.47775, 0.13772258323431014), (0.48075, 0.14722880405187608), (0.48325, 0.13309265697002412), (0.4805, 0.15916637241840362), (0.48125, 0.13757544940710068), (0.485, 0.13823838418722154), (0.483, 0.14882621294260026)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.00      0.00      0.00      1000
           3       0.00      0.00      0.00      1000
           7       0.54      0.94      0.69      1000
           8       0.44      0.99      0.61      1000

    accuracy                           0.48      4000
   macro avg       0.24      0.48      0.32      4000
weighted avg       0.24      0.48      0.32      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [89]
name: alliance-3-dcs-89
score_metric: contrloss
aggregation: <function fed_avg at 0x7cdf017d1c10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=89
Partitioning data
[[3, 9, 1, 4], [0, 2, 1, 4], [8, 6, 1, 4], [5, 7, 1, 4]]
[(array([13581, 11353, 30571, 14064, 46596, 26097, 45402,  7200, 17497,
        3134,  2665, 24099, 41137, 17655, 33371, 39773, 48088, 45634,
       31198, 16586, 19455,  8179, 40587, 39868,  3000, 45217,  1966,
       26036,  8408, 29979,  6556,   367, 25315, 33443, 44913, 10209,
       21106, 41513,  7075, 23885, 32075, 21687, 40651, 18814, 45216,
       41725, 43262, 47902, 11805, 33980,  1625,  6198, 39135, 35154,
       19496, 13371, 24847, 43254,  2947, 15753, 27722, 13062, 27735,
       37370, 15706,  5876, 24538, 43744, 42590, 43843, 20932,   806,
       19913, 20469, 39287, 48131, 20770, 26022, 14779, 39980, 13253,
       14091, 12076, 15467,   685, 24348,  6422, 39865,  9332,  8162,
        2565, 19726, 37924, 39187, 45502, 21712, 34689, 43017, 17897,
       39165, 42970, 34999, 13205, 39101, 38879, 20371, 44856, 39398,
       43237, 32541, 36113, 35071, 15080,  1164, 38897, 41951, 39158,
       38444, 43428,  5725, 32404, 36159, 47666,  8434,   995, 13686,
       37546, 10101, 17155, 12872, 30809, 44110, 11047, 40433, 14936,
       20488, 43034, 17333, 49430, 22244,  4197,  7869, 18570, 48592,
        3927, 25640, 23526,  7259,   639, 16190, 27683, 19617, 39461,
       40097, 37057, 21551,  7997, 32938, 26132,  6014, 16364, 18400,
       22505, 19422, 12370, 14580, 13302,  7665, 39289, 14993, 31861,
       24922, 45250,  6368, 42997, 38891, 41639,  4672, 49143, 19631,
       36219, 28237, 25259, 26707,  5493, 14938,  6343, 46155, 40809,
       36124, 26357, 30213,  9024, 10534, 24606, 14232, 46595,  5553,
       47196, 18414, 11370, 43726, 45914, 45456,  1696, 40399, 49078,
       20534, 38390, 39885, 24082, 20058, 11359, 47062, 28786, 28554,
       11623,   314,  3910,  8988, 42198,  9300, 27673,  9429, 31449,
       35449, 26558,  8105, 46423, 33422, 27658, 21731,  3875, 12007,
       45901, 25257, 25563, 42646, 41705,  5502,  8386, 35427, 22226,
       24704, 15335, 39689, 26665, 26900, 22523, 42682, 42407, 19380,
       44809, 39319, 38402,  2506, 11918, 12086, 36029, 49360, 13889,
       10533, 12541,  5009, 10138, 24173,  3477, 37536, 36967, 30127,
       14099, 39572, 37857, 44646, 48769, 23990,  7838, 31744, 26597,
       47066, 47687, 24436,  2632, 49031, 42224, 49662, 11901, 41631,
       15141, 33248, 35504, 48431, 10373, 34034, 29148, 41611, 19601,
       26832, 21138, 28932, 40267, 28376, 26908,  6002, 34666, 36050,
        7040, 39993, 42739, 48126,  3974, 15252, 34772, 31918, 38601,
       39699, 38663, 14571, 44606, 42278, 33025, 35064, 35749, 36475,
       45465,  9701,  6158, 34006, 28495,  5079, 23169, 18439, 14572,
       38328, 27807, 39870, 30525, 40030, 46243, 45101, 33053,  6984,
        3929, 34957, 29105, 31637, 35852, 25973, 13346, 12040,  1000,
       36705, 38380, 40254, 45284,  6360,  3479, 27168, 32164, 20013,
        2199,  1115, 40082,  3316,  9302, 32439,   205, 41904, 34480,
       15817, 31378, 27957, 21571, 49029, 49405, 41284, 32879, 40559,
       33281, 16163, 40895, 26531,  8532, 17099, 36590, 18648, 28949,
       32902, 34232, 46698, 35607,  7302, 49074, 19674, 46651, 27752,
       12011, 20799, 14880, 26252, 11111, 17452,   316,   188,  6480,
       33886, 34903, 20787,  4407, 38324, 39611, 30080, 12288, 43171,
        6457,  3700,  3633, 17875, 11517, 18183, 49403, 40693, 10523,
        3876, 43522,  3023, 29628, 10056, 36741, 41535, 23783, 37126,
       19199, 44009,   269, 39331, 40742, 34474, 23657,  4587, 28259,
       32841, 17171, 13128, 48451, 10252, 36109, 49634, 41508, 36859,
        8841, 28304, 41395, 13642, 42885, 24048, 43614, 36165, 24731,
       47303, 43104, 26945, 30580, 10717, 10118, 23341, 41683, 46844,
       15993, 10078,   613, 17607, 24246, 34843, 41746, 34549, 41541,
       19805, 44280, 30733, 33024, 37349, 30678, 34423, 33679, 24893,
       28915, 37968,  4256,  1767,  6610, 46227, 20675, 10859, 26013,
       26674, 15586, 27821, 42452, 24320, 47365, 32937, 37849,  6241,
       14507, 34810, 12929, 10714, 13002, 36868, 20667, 20155, 44725,
       25459, 42976,  4528, 25689, 17275, 33261, 43932, 15386, 40822,
       19354, 31890, 20619, 47470, 29429,  5568, 48128,  6733,  9318,
       46679, 40145, 45126, 31352,   997, 21938,  3379, 29986, 42793,
       29963, 30198,  2731, 18456, 22060, 25046, 20368, 35106, 28001,
       35850, 30782, 23662,  5344, 16986, 12586, 10035,  6383, 19526,
        7992,  3734, 11708,  2259, 16310, 47279, 11804,  9327, 42282,
       19773, 46396,  7861, 35863, 38078, 29318, 45077, 22437, 17279,
       35567, 44541, 29874, 24119, 27215, 11522, 15905, 41472, 45613,
       49933, 22479, 35692, 35021, 38223, 38892, 27868, 46461, 42347,
       41502, 47900, 47178,  9411, 11971, 47588, 44341, 39422,  7723,
       34420, 28861, 29019, 36611, 42081, 10122, 40000, 46162, 20307,
       18752, 14007, 49594, 44767, 16066, 31720, 32736, 48595, 34629,
        7430, 31747,  4047, 15286, 13166, 31348, 31583, 26950, 17534,
       14061, 38175, 33748, 41289, 21407, 18654, 33624, 19046, 16745,
       15738, 26470, 18671, 47377, 41262, 27792, 12169, 35516, 22472,
       41520, 19769, 41723,  7145, 12185, 42163,  7683, 23929, 19324,
       23020,  6608, 18069, 32410,  4553, 13366, 13922, 20463, 37422,
       31282,  4011, 15934, 10655, 18160,  6623, 38577, 13296, 25323,
       29875,  1907, 44472, 24161,  6703,  9380, 47680, 30457, 36331,
       11824, 10079, 16992, 15583,  2469, 42085, 20796, 39906,  8346,
       31203, 34906, 14964, 20937, 22349, 31607, 18526, 48086, 17032,
       45316, 17718, 26740,  6831,  9232, 13108, 33866, 18961, 11558,
       21796, 19485,  6798, 38725, 15897, 44869, 19164, 45212,  2023,
       42007, 40258, 41215, 29023, 13417, 21153, 23203, 13849, 33561,
        2390, 49643, 46357, 28476,  9668, 31523, 26465, 34993, 14878,
       43711, 11782,  4565, 42802, 45704, 48539,  7130, 46514, 36251,
       15114, 46177, 32235, 48807,  6485, 18912,  2363,  3948, 48613,
       25232, 46061, 19568, 13834, 17632,   345, 28372, 36047,  9061,
        3802,  2300, 48351, 19118, 42313, 34562, 21733, 12737, 35915,
       28577, 40757, 24996,  7317, 17878, 14635,  3464, 46526,   505,
         449, 30585, 28407,   887, 11138, 21912, 10447, 15170, 19643,
       23367, 38964, 28040, 23792, 47239, 40642, 40309, 21187, 23022,
       32518, 42700, 37389, 44032, 36383, 25977, 44397, 19475, 19340,
       43139, 10736, 48605, 16216, 41814, 38351, 42750,  3533,  2842,
        6924,  6891, 36817, 25954, 18060, 41162,   632, 19952, 36566,
       40794, 29419, 37923, 36896, 33095, 47837, 43595, 14956, 26860,
       49027, 10865, 36976, 23755, 26892, 23211,  7296, 14036, 24265,
       28319, 25355,  8987, 46133,  7787, 23057,  7851, 42544, 42729,
       12026, 41301, 33796, 48478, 10808, 48824, 38598, 22259, 39656,
        6877,  4043,  9615, 44517,  1695, 19543, 22246,  3027, 49608,
       32494, 27535, 29835,  4305, 30630, 14670, 29737, 13466, 36160,
       47257, 15718,  8396, 23954,  1315, 42910, 48997, 42640, 35877,
       36652, 43032, 28614, 14777, 35180,  4158, 43203, 18043, 15267,
       44200, 38454, 24630, 15036, 13427, 36090, 30353,   669, 35573,
       27259, 26838, 35714, 30900,  4687, 32643,  2905,  9149, 43650,
       10765, 45775, 34217, 41451, 13691, 45644,  1791, 43860, 28600,
       29070, 18289, 42229, 38776,  5940, 41691, 31003, 27247, 29129,
       21676, 27322,   563, 15152, 39127, 26929, 25346, 14879, 41530,
       29184, 22990, 18300, 29827, 46778,  1516,  8784, 44729,  7382,
       49213, 22798, 22698, 39348,  3236,  9771, 22509, 14474,  1350,
       11957,  2178, 47120, 16033, 16897,  6270,  7743, 10633,  3522,
       12301, 33013, 12075,  2703, 36777, 37089,  9507, 14255,  4527,
        9713, 10900, 31143, 30499,  8137, 28799, 37996, 27891, 45234,
       35954, 41186, 41669,  1934, 24778,  5495, 27596, 15508, 31840,
       22580]), [3, 9, 1, 4]), (array([44167, 49992, 38943, 14754, 19493, 22993, 23796, 41606,  9059,
       38238, 10847,  2365, 33117, 47981, 47328, 16460, 30321,  3304,
       41745, 45412, 30160, 49303, 11424, 35788, 43829, 43012, 27434,
       26607, 37178, 21087, 33943, 44143,  7573, 14945, 38039, 34017,
       28354, 18271, 10637, 35121, 30709, 40125, 37022, 13324, 26922,
       18073,    49, 47486, 19041, 32602, 35858, 11963,  8971, 49490,
       42931,  2617, 47834, 41300,  6743, 28794, 44925, 27911, 37160,
       38036, 46738,  7643,  6276, 42476, 26776, 32206, 48254, 32056,
       41324, 46725, 14116, 22973,  3731, 29225, 14698, 26391,  1381,
       36858, 16137, 10690, 27278, 39329, 24113,  3335, 11734,  2720,
         871,  1935, 45163, 41355, 38345, 11356, 24720, 43875,  2714,
       10205,  4721, 37517,  1524, 17677,  4155, 30383, 44967, 36464,
       42446, 29003, 42456, 28972, 42183,  2574,  6190, 40796,  1950,
       34944, 24079, 41907,  4081, 47468, 21974, 26060, 25942, 17917,
       43649, 17762, 40639, 42628, 10340, 41235, 36353, 33258, 48556,
        5864, 48926, 15304,   733,  3789, 36106, 25212, 45344, 10275,
       32369,  8553, 10990, 17717,  2633, 42332,  1473, 25288, 38613,
       16531, 35286, 25190,  1926, 49517,  2962, 24096, 30428,  2401,
       28665,  4854, 34018, 11372, 28456, 29412, 27137, 20891, 49557,
       10577,  8199, 10301, 36357,   115, 12083, 47105,  5457, 32670,
       16186, 48246, 15077, 34083,  3815, 28921, 23825, 49656,    93,
       15946, 38478,  4653, 48327, 38043,  7093, 44175, 26361, 32107,
       10148, 27042,  4353,  2107, 25236, 25616, 30078, 14153, 26296,
       11224, 46945, 39606, 12420, 46287, 24939, 11709, 18080, 43479,
       32934,  3906,  4477, 40485,   965, 24644,  4552, 30172, 38499,
       24750, 40679, 47771, 41600, 27487,  8062, 20813, 20602,  1187,
       26783, 38179,  1759,  3184, 19778, 47865,  8535, 17115, 34036,
       14413, 39650,  4952,  2413, 11612, 40652, 29364,  2393, 11380,
       39646, 11873, 40348, 37878, 26128,  1527, 43781, 10567, 47826,
        3901, 37713, 13848, 10651, 29742, 36481, 16402, 43898, 49457,
       23918,   933, 27484, 36025, 39882, 26386, 10628,  6097, 23537,
       14946,  9597, 19868, 18040,  2626, 47932, 32792,  6488, 36213,
       28933, 35792,  6369, 37298, 38032, 42889, 36377, 25148, 38745,
       30665, 35031, 25929, 21867, 30333, 30746, 22281,  3193, 20216,
       32697,   522, 38459, 17739, 38054, 25381, 11934,  4525,  9503,
       42588, 13179, 20038, 33396, 46564, 39237,  1798, 35486, 21746,
       40527, 44995, 44427,  2927,  2202,  9151, 32071, 46261, 38001,
       40463, 39847, 48090, 46088,  4549,   513, 40333, 46943, 42774,
       14811, 21452,   403,  7761, 36759, 37645, 13270, 33644, 26176,
       13814, 32908, 39407, 13195, 42600,  8032, 17702, 10833, 34196,
       44658,  6843, 13130, 49730, 20700, 24706, 45460, 42928, 46326,
       40282, 35096,  4518, 20547,  9712, 45131, 19056,  6757,  7662,
       20656, 44335, 19932,    24, 31705, 33361, 11730, 17834, 20183,
       17086,  9249,   218, 31501, 37581,    47, 19815, 26795, 45533,
       46509, 14314, 16288, 49093, 12361, 42990, 12589, 26864, 25989,
        5871, 11348,  4370,  5889, 35616,  3259, 32406, 11962, 25695,
       12067, 13901, 21135, 36170,   196, 36726, 13770, 10742, 29625,
       38129, 27097, 10461, 37446, 49784, 15139, 29379,  7664, 49693,
       41925, 44008, 26999, 43523, 48101, 24409,  6946,   724,  7658,
       43747, 32419,  1139, 32067, 26572, 41775, 40087, 48735,  2126,
       18027,  8081, 10756, 42450, 34925,  9011,  9940,  3349, 42581,
       34734, 15065, 37315,  9790, 12148, 27980,  7477, 31007, 10204,
       31707, 42564,  3944, 27877, 11548,  4930, 35104, 41721, 21145,
       26442, 48951, 14088, 29907,  2136, 43041,  6772, 46957, 46584,
        3321, 42158, 19899, 10940, 36994, 14646, 27356, 13934, 10968,
       29637, 13819, 36581,  1789, 27746, 47939, 43853, 12276, 19202,
       34336, 32564,  9549, 32472, 35344,  1574, 43214, 29252, 22298,
        2727, 18048, 39652, 38756, 33207,  2173, 15363, 24385, 14502,
       42921, 26077, 23313, 18890, 27942, 39338, 29777, 17268, 21150,
       31019, 41529, 48820, 16471, 23332, 45769, 29188,  7198, 18078,
       30407, 18844, 27465, 10229, 45675,  1037, 10795, 35965, 20047,
       37702, 34364,  8547, 34850,  9898, 44841, 15885,  5171,  9476,
       37826, 16385, 41710, 45834,  2587, 21152, 45157, 12773,  8529,
       14379,  6814, 32258, 44524, 16077,  4492, 28643,  9717, 44109,
       10039,  7288, 45475, 31253, 12923, 16638, 30313, 21593, 10446,
        3346, 38086, 20168,  9878, 29044, 38420, 42475, 10953, 41248,
       22089,  6074, 42734, 23260, 14376,  7338, 46827, 26550, 19939,
       46544, 49381, 46859,  4100, 22553, 20725,  4132, 49045, 26930,
       28655, 13852, 34163,  8965, 16947, 16594, 48709, 40971,  5811,
       42448, 46900, 33951, 33126, 47529, 24446,  2038, 42239, 29146,
       27447, 33611, 39195, 49348, 47088,  9857, 46993, 40518,  7326,
       49629, 29226, 41706, 26787, 48667,    79,  5422,   840, 46866,
         261, 11631, 44204, 34128, 17112, 29901, 14859, 47310, 31957,
        5157, 28626, 48234, 24284,  6226, 40449, 28127, 21309, 34118,
        7261, 30804, 19185, 45972, 27089, 31959,  8944, 48681, 24228,
       40152, 25993, 23127, 39996, 37666, 10074, 33742,  9400, 12080,
        2072, 45011, 24151, 35635,  4265, 16777,  6349,  7349, 14549,
       15756,  8450, 29828,  1901, 37743, 29512,  4496, 43274,  3414,
       31079, 44591, 11696, 25050,  7605, 12100, 31288, 32462, 28298,
        4993, 42710, 43480, 36400, 12620, 23275, 36663, 10479, 12025,
       28420,  2200, 23213,  8475, 38908, 29029, 23210, 44423, 26295,
       48639, 33314,  6813, 19989, 18527, 36813, 42413,  8127, 24484,
       41963, 24008,  1464, 19571, 30062, 26870, 44537, 15984, 41209,
       27262, 19181, 32743, 20206, 27282,  8913,   272, 47975, 20340,
       42072, 30307, 14305, 38033,  1268, 11582, 17197, 41219, 31446,
       10661, 45293, 20515, 12313, 36317, 46076, 29852, 28823, 15012,
       34350, 28289,  9650,  8405, 16155,  9816, 26844, 14388, 10951,
        2695,  7059, 31733, 40188, 21781, 23013,   399, 37577, 29111,
       18404, 12997, 28084, 28903, 32969, 39413, 25579, 46947, 43228,
       35806, 31953, 29698,  5491, 46531, 31804, 38668,  6879, 31998,
       12417, 27029, 48884, 17953, 48447, 27220, 27390, 19331, 11860,
         660,  8049, 30881, 47139, 49669, 34477, 39772, 25098,  9141,
       41766, 19345, 12489, 40116, 37172, 34114, 48801, 42603, 16880,
       41270,  8184,  7811, 24810,  2647, 18365, 38603,  9568, 29525,
       31549, 48202, 35185, 17039,  1804, 39377, 21629, 16672, 32803,
        7110, 43758,  7269, 14584, 40027, 16749, 32818, 36420, 13510,
       10282, 10966,  3684, 18560, 27946, 23739, 49685,  4987, 43810,
       48902,  6911, 35432,  1904, 28830, 43231, 19264, 40949,  4642,
        3326, 41337, 41056, 22443, 22617, 13764, 16772, 25403, 24594,
       27489, 10154, 24846, 21659,  8930, 36442, 20343,  2155, 33279,
       44760, 42979, 46998, 28809, 15744, 31277, 38995, 11872, 31709,
       25931, 46650,  1952, 14810,  6339, 33482,  1992, 49114, 27630,
       36514, 41103, 28755, 28751, 22949, 36731,  3784, 42907, 25871,
       14284,  2673, 20011, 19943, 19704, 10264,  4297, 14051,  4447,
       22018, 20409, 42982, 18813, 37853, 44614, 31932, 32533, 33226,
        5067, 27455, 10134, 45199,  1007, 23200, 49318, 11021,  1425,
       27221, 36740, 42418, 30449, 17725, 24084, 32413, 24785, 34956,
       21194, 20342, 34328,  6605,  6128, 47079,  7383, 27649, 14492,
       44007, 16890, 39177, 24804,  2225, 20608, 43584, 10904, 33266,
       34182, 21597, 12846, 24776, 22144,  2672, 45114, 39467, 17483,
       11249,  8114, 45400, 15624, 42636, 26631, 19407,  9260, 20521,
       10131]), [0, 2, 1, 4]), (array([48247, 20762, 49551, 29868, 44236, 29075, 15260, 16466, 14891,
       30357, 24692, 25521, 34712, 33441, 16422, 38587, 39802, 23438,
        6651, 34491, 38686,  4283,  9599, 13227, 46892, 31423, 27507,
       39094, 13425, 20455, 49985,  9265, 45024, 28396, 23408, 15743,
       45062, 38416,  1162, 46488, 34473, 10856,  9675, 48757, 19979,
       42333,   716,  4298,  4823, 13582,  1512, 39184, 26055,  8222,
       24554, 46033,  1763, 39254, 49681, 20491,  1751, 34441, 41761,
       49861, 19851, 36221, 26463, 10923,  4955, 34369, 49389, 20650,
       29782, 12426, 19748, 28599, 44384,  8329, 16132,  8495, 39507,
         460,  9722, 42469, 32562,  4204,  1370, 48541,  4780, 49899,
       15250, 42693, 18565, 19509, 17588, 21157, 37318, 27864, 43913,
       40391,  8138, 25501, 31489, 26131, 49388, 16727, 23205, 15046,
       39336, 19887, 45779, 31456, 20822, 39715, 17894,  9192, 22268,
       11327, 42713, 30026, 48984, 13874,  8384, 44180, 21036, 41629,
       33398, 18444,  6153, 37643,  2434, 24435, 47597, 39702, 42254,
       35593, 40345, 29371, 47342, 11180, 13702,  4758, 19666,  8830,
        8248, 12513, 10443, 26147, 38460, 14872, 25855, 38221, 38925,
       20617, 47395, 43492, 37633, 47506, 38065, 31778,  6825, 19605,
       16724, 45104, 20479, 13992,  6773, 27458,  9023, 26784,  5283,
       45912, 13788, 20288, 48905, 15811, 29056, 19991, 45620,  3215,
       33815,  3518, 13386, 28491, 24876, 43109,  2979, 13576, 10836,
       34210, 45447, 19836, 35224, 14642, 10822, 25869, 39317, 45240,
       40702, 30955, 16086, 34656, 27715, 35367, 45868, 15049,  3832,
       12319, 10956, 30844,  5730, 10234, 42933, 23312, 31725, 36390,
       27615, 14693,  6846, 46938, 19398, 46888, 13004,  6789, 31841,
       43183, 16178, 12861, 25618, 17394, 17220, 40062,  8526,  4652,
       26004, 16044, 17335, 19720, 38694, 49851, 27842, 21499,  9346,
       12646, 32213, 27988, 43982,  6912, 11674,  2918,   164, 41870,
       14162, 17035,  7041, 21424,  2267,  2939,  2019, 28678, 30130,
        6918, 47347, 43070, 22534, 45970,  5318, 31382, 43426, 23520,
       19128, 10113, 16507, 24418, 13123, 37574, 48201, 38197, 26151,
        7392, 13796, 32637, 36360, 31616, 27927, 49289, 18801, 46134,
       38061, 26243, 35099, 40653, 39874, 30176, 35114, 44078, 18816,
        3976, 43452, 34652, 34432, 37992, 22654,  7137, 20324, 19511,
       48298, 38865,  4208, 20098, 44612, 17248, 24184, 36192, 41184,
       47734, 26324,  2494, 35499, 25985, 31558, 14039, 41004, 12214,
       47071, 48400,  7215, 36586, 17680,  1031, 26066, 15497, 28378,
        9653, 40274, 36808, 15651, 43488, 44194, 39007, 30427, 16546,
       12321,  5745, 30367, 38438, 27450, 37796,  5679, 46124, 28139,
       12765, 30626,  6572, 11644, 35270, 45585, 10792, 26429, 34804,
       35900, 23821, 49804, 23025, 27134, 33425,  3655, 33012, 32793,
       34362, 25396, 48723,  6367,  6395,  6810, 11319, 34322, 38537,
       48460,   588, 14830, 19834, 37926, 21917, 25417,  5907, 28990,
       39252, 41448, 34505,  3018, 20767, 28986, 31465, 18851,  1766,
       17282, 48229, 12445, 31933,  2729, 20993, 25729, 43439, 16760,
       40811, 17000, 24117, 25085, 48710, 17075, 11150,  7143, 17952,
       30100, 16404,  9303, 40929,  3716, 47464,  5025, 44708, 40164,
       46743, 40357, 45019, 31099, 48932, 44566, 12122, 38993, 28956,
       15294, 45958, 28021, 44518, 49892, 43645, 20930, 47752, 29223,
       47728, 13797,  9468, 24198, 35703, 49305, 20293, 41909,  9808,
       49311,  3070, 41809, 46913, 22100, 26122, 49708, 18336, 45383,
       22345, 17788, 37481, 25067, 41074, 45823, 15220,  1374, 23881,
        7675, 43538, 24882, 49776, 37487, 25747, 36089, 44696,  2965,
       13683, 43298, 20561, 19924, 30741, 38606, 49210, 35208, 46180,
       29677, 43577, 48665, 18407, 22101, 23912,  7976, 31228, 16129,
       41593, 21200, 43481, 32534, 28374,  2851, 21104, 17970,  4858,
       45628, 31935, 43145, 12893, 49416,  3937, 40279, 37844, 48740,
        4099, 24466, 20856, 48182, 18744, 15748, 42438,  7005, 14204,
       27419, 45500, 47749,  3062, 22984, 40537,  1578, 49383, 13996,
       13505, 30136, 23837, 46173, 47616, 19609, 48728, 14048, 10387,
        7529, 10912, 22829, 37494,  9538, 22306, 32580, 16985, 47845,
       11850, 22884, 21675,  6899, 48917, 34442, 29693,  3452, 40815,
       38646, 20376, 28163, 24137, 47035,   206, 35048, 18263, 37512,
       12071, 36641, 42237, 17815, 44889, 34701,   942, 49658,  7777,
       33963, 35364, 17662,  2445, 38822, 13144, 14110, 22227, 33164,
       21218, 47987, 30783, 45535, 39737,  5917, 33762,  8693, 36158,
       36237, 30083, 15299, 24986, 26074,  4459, 34909, 14904,  5144,
       16588, 18425, 28417, 14005,  5197,  4557,  5816, 34426, 15183,
       49616, 45099, 45899, 18178, 18388, 49932, 28532, 25926, 41964,
       36298, 15609, 20931, 32438, 35081,  2656, 15988, 32474, 39811,
       42287, 32418,  9781, 12280, 33392, 49494, 44309,  4533, 22788,
       44308, 17106, 17856, 14068, 27716, 17334, 40453, 19898, 15855,
       47362, 18246,   364, 13234, 35953, 24774,  8576, 13858,  1551,
       42709,  6353, 41627, 38192, 29953, 35666,  2802, 10248, 19351,
       44411,  1021, 35366, 45013, 19490, 10453,  3781, 41307, 20444,
       47633,  8977, 38028, 44395, 39277, 26794, 15832,  2883,  1694,
       15496, 34376, 39745, 32681, 14512, 19265, 26115,   325, 16682,
       19313, 49080, 46359, 14209, 48669, 19598, 32011, 21181,  5635,
       37239, 30553, 13952,  2268, 40341, 43167, 10188, 20746, 28175,
       18518, 33838, 22037, 45210, 27178, 32941, 43684,  1605, 48954,
       14288, 39912, 37213, 42341,  1020, 25377,  4949, 16223,  9095,
       39109, 32083, 34791, 49716, 17454,  9012, 21180, 35508, 37533,
       34233, 16855,   250, 30949,  3444, 21870, 40034, 12800,  7286,
       32042, 44601, 44225, 12805, 30681, 29489,  9202,  9074, 10615,
       38787,  5754, 46153, 14667, 34431, 22149,  5756, 46823, 43746,
        2056, 40121, 24108,  2911, 22183,  7043, 33666,  6894, 39005,
       13464, 27003, 33369, 16240, 13472, 29396, 44857, 10207, 27394,
       30268, 38511, 20192, 35563, 13145,  4438, 13498,  7100, 35517,
       26653,  3577, 19852, 12803, 24674, 49622, 38354,  3635, 35843,
       40665, 21949,  5909, 38805, 32776, 32095,  2123, 33480, 49077,
       33370, 21853, 31579, 11967,  8279,   951, 18065,  5063, 17338,
        3145, 34950, 31755,  5237,  6039, 49002, 44324, 40848, 27572,
       47928, 36915, 12179,  9967,   764, 45618, 42160, 46968, 43049,
       27935, 37719, 20904, 12645, 45397, 37977, 10067, 11481, 12590,
       45408, 14853, 20108, 24997, 26706, 13927, 27790,  3296,  2787,
       43172,  9782, 28913, 38762, 40562, 41810, 38755, 20435,  9155,
       14905,  3288, 10604, 32179, 40793, 47576, 45111, 41665, 14935,
       45340,  2088, 25620, 17877, 15990, 40842, 47664, 36167, 41573,
       12723,  1882, 36536, 25402, 38213, 13469,  7625, 31939, 22683,
       43974, 24419, 48878, 34699, 14152, 41417, 20363, 15737, 19441,
       38517, 49751, 16587,  3776, 46119, 19032, 39701,   162, 35228,
       12453,  2144,  3509,  8236, 28174, 12858, 35522, 45076,   816,
       23445, 21222, 28414, 19098, 11759, 47295, 28654, 27888, 22205,
        9153,  1069, 17556, 41382, 34038, 21041, 17077, 25250, 36096,
       41191,  6387, 11997, 16007,  3377, 49207, 47312,  8688, 34384,
       37741, 32960,  9761, 42312, 14649, 29939,  5180, 43683,  7642,
       40259, 17049, 37688, 12224, 46470,  4763, 45935, 11626, 40473,
        3587, 49984, 20659, 14104, 21176, 32535, 30061, 37342, 26638,
       19702, 19569,  8447, 27313,  6780,  8254,  5624,  6844, 23937,
       33015,  3189, 10105, 10136, 37294, 35956, 45170, 44172, 14377,
        6710,  4781, 43509, 25371, 21929, 22837, 14737, 47570, 27206,
       12416]), [8, 6, 1, 4]), (array([41098, 10258,   686, 36782, 17797, 34541,  4850,  1847, 40471,
       24160, 16797, 47065, 27893, 47289, 30393, 25230, 29432, 45728,
        6873,  6259, 19856, 36851, 44738,   426, 18346, 25465,  9301,
       11852, 13911, 33311, 47544,  3002, 22473, 16812, 48245, 10632,
        7160, 28877,  2794, 31264, 13257, 18764, 11314, 28661, 12326,
       26987, 14965, 46264, 18230, 26125, 33116, 48625, 21128, 25698,
       31047, 26024, 27771, 46720,  6958, 46842, 37252, 36556,  3017,
       11819, 18082,  7476, 30016, 28902, 27879, 14632,  4529, 44260,
       44781, 33295, 18729,  6760, 10397,  3421, 49327, 49680, 38529,
       43467, 18170, 44899, 45436, 23119, 30349,    70, 48645, 21278,
       49797, 47218, 20836, 33702,  8325, 31949, 20445, 34095,  2314,
         359, 11519,  1545,  6131, 17855, 37486,  5962, 31129, 49980,
       39002, 11770, 38695, 33435, 44706, 30622,  9196, 49753, 29110,
       31901, 36956, 22893, 34803, 11185, 12573, 22427, 18748, 13067,
       32511, 41499, 21780, 38854, 25311, 48673, 30662, 20292,   875,
        7094, 14227, 11662, 32184, 36224, 23335, 21077, 48383, 46112,
       45991, 44700, 29304, 29474, 46246, 47496, 28218, 45503, 11365,
       27598, 46494,  1656, 22483,  5393, 27292, 40351,  1849, 36603,
       24650, 41583, 25418, 37096, 36171, 48083,  5305, 23947, 12701,
        7472, 30235, 39228,  7157, 23976, 18061, 43703, 48883, 32530,
       22158, 20075, 45469, 22321, 24006, 29061, 25870, 36899,  5681,
       35165, 41046, 41069, 23146,  1786, 33306, 17346, 29558, 39929,
        8198, 30735, 23150, 44551, 21046, 28693, 39103, 27397, 44290,
       16653, 14875, 43999, 48573, 28442, 23949, 30684, 42337, 25057,
       43071, 39660,  1519, 11201, 33873, 27679, 16436,  2984,  7181,
       27945, 45553, 40091, 32719, 37946, 44555,  5500, 49720, 39587,
       15207, 35660, 49938,  6436,  5379, 21767, 41793, 20020, 21494,
       25610, 28995, 12912, 37813, 36669, 18985, 47625,  4686,  9201,
       11334, 21083, 19346, 29196, 16031, 42284, 16537,  6449, 13525,
       12099,   570,  2045, 11410, 21927, 30167, 42612, 20741, 25283,
         514,  6808,  3146, 31822, 10666,  3398, 29350, 33495, 15539,
       28715, 15081,  3272, 42593, 39425,  1071, 15283, 30030,  7819,
       13749, 10127, 43128,  8311, 32354, 22901, 38104, 11391, 37449,
       29026, 40837, 21967, 30447, 14947,  9082, 28132,  5774, 23880,
       37836, 40892, 42751, 37671, 37624, 41973,  5126, 26752, 49377,
       28150, 18095,  9789, 15679, 17989, 45861,  9795, 10887, 25553,
       40821, 44286, 47856,  8516, 35982, 33755, 32307, 11338, 17129,
       37902, 11846, 28965,   913, 34308,  1915, 37159,   318,   320,
       22725, 10734, 48467, 17997, 26991, 10730,   797, 22128, 43322,
       22934, 39395, 28945, 27568, 24916,  3955, 42668, 37496,  3715,
       45365, 18139, 45061,  8984, 22324, 16666, 15317,  5525, 37935,
       46211,  1135, 27036, 23928, 36364, 21718, 39107, 24091,  7372,
       17886, 16876, 24129, 16198,  4706, 23196, 18556, 10184, 20699,
       12925, 12994,  6752,  2616, 10695, 11796, 45874, 28853, 37458,
       12864, 47777,  9214, 29957, 39498, 11088, 32875, 19973,  7506,
       29727, 31719, 16843, 23547, 16149,  6699,  7609, 12838,  4903,
       12320,   329, 36352,  7939, 30996, 22526, 36136, 25724, 34782,
       28201, 25464, 16040, 18528, 12437, 17254, 43305, 48948, 17636,
       44101, 28243,  3637, 22198, 16022, 46391, 10269, 47372, 47673,
       36589, 46348,    11,   131, 26163, 15136, 38798, 18828, 10481,
       35158, 31036, 38500,  7023, 16589, 39485, 19250, 19361, 38955,
       21773, 32208,  7185, 48041, 34260, 36306, 10538, 43512, 33773,
       42936,  4809, 44151, 48490, 45087, 44694,   152,  3649, 23842,
       26291, 19200, 42909, 14398,  8904, 19794, 45758,  6759, 10034,
       14758,  9496, 37482, 36677, 31807, 40295, 26032, 37472,  4253,
       36691,  8202, 47039, 25598, 14537, 37327, 46213, 33112, 23895,
       47349, 22054, 29473, 25697, 10902, 21706, 49554, 49121, 16332,
        2286,  6928, 33686, 34595, 33114, 42666, 34097, 18698, 35401,
       31737, 43786, 33615, 12592, 36774, 19575, 43641, 42065,  9080,
       44993, 46649, 26652,  2339, 12894, 29409,  8480, 21234, 12560,
       39771, 34043, 44474, 48141, 10569,   396, 41254, 46956,  8119,
       20736, 35530, 38858, 16829, 31251, 20761,  8135, 29485,  1559,
       29494, 23936, 26915, 45602, 34263,  3078, 18331, 37468, 14764,
       25718, 29283, 45485,  1090, 22833, 30446,  7673, 38493,  4696,
       44936, 17126, 43178, 42860, 33693, 41579, 26127, 27268, 46889,
       21529,  4423, 41550, 40248, 34090, 15749, 28381, 47779, 16989,
        8191, 20866, 12903,  2455, 41563, 22217, 18594, 33250, 16412,
       22419,  9766, 39474, 11707, 22165, 31655, 41338,  9263, 38347,
       16549, 42925, 10022,  8771,  3827, 11109, 18649, 21020, 46463,
       35691, 15702, 42912, 42758, 15506, 21959, 18228,  7048,  7722,
       19018, 37309, 37385, 31830,  5207, 14378, 30234, 35338, 34081,
        3315, 49795, 18551,  4200, 28096, 35705,  6121, 18117, 12247,
       24247,  8794,   257,  3085, 23006, 17479, 33640, 28539, 36085,
       34493,  2184,  8884,  6068,  1052, 35674, 18093, 32121,   432,
       15632, 48138, 13899,  3020, 36343, 29230, 27627, 12038, 29203,
        6020, 19869, 36975, 40880, 31945, 20042, 36872, 30458, 40972,
       25619, 25910,  1304,  9809, 46099, 15557,  5810, 23500, 46893,
       30104, 24923, 28876, 47213, 44272, 11645, 19143, 11243,  3366,
       46547, 40713,  2039, 11075, 34068, 18625, 29974, 31138, 20840,
       35651,  3273, 37012,  1394, 43416, 19565, 13122, 23714, 30530,
       34349, 18303,  8432, 22372,  7284, 29633,   262, 49186,  2511,
        6639, 45977, 31599, 46078, 49849, 45317, 10362, 42318, 33970,
       32584, 46406, 45487, 38491, 19628, 18639, 25614,  7214, 27994,
        9442, 18333,    99,  9000, 27547, 32284, 28100, 31290, 37987,
       35538, 41286, 20137, 27050, 48892, 41897, 20730, 10888, 48367,
       47441, 27135, 31769, 27494,  1925,  1001, 46150, 30300, 33027,
       31775, 14397, 10766, 21188, 27631, 21327,   712, 29743, 32253,
       26417,   520, 44834, 27812, 23421, 45820,  2654, 46331, 18161,
       48211, 23072, 31954, 18637, 44850, 23187, 42080, 45782, 38146,
        8338, 27500, 38228, 15155, 35415, 40631,  3354, 24920, 17402,
       18384, 32553, 18056, 32634, 44963, 34751,  4104, 21382,  9147,
       43335, 41675, 44641, 21021,  7563, 21626, 27613, 19462,   489,
        5434, 24582,  5173, 40974, 44587, 37903,  9724, 35526, 23354,
       33129, 47534,  9887, 19256, 19178, 28260, 20906,  4741, 44942,
        9550, 47149, 10303, 36968, 43299,  3519, 33004,  7404, 17713,
       45773, 47644, 40531, 10179,  7295,  6168, 12810,  3217, 27718,
       47611, 37193,  7619, 14937,  9937, 11982, 13210, 30734, 20655,
       15909,  6603, 47907, 17199,  5931, 27469, 45515, 28892,  2106,
       38728, 29101, 25879, 13416,  4773, 39217,  3695, 21946,  7207,
       16527,  9529, 11116, 23252, 14359, 18493, 22137,  5581, 18130,
       48591, 18606,  8792, 47571, 20778, 31281, 44565, 44202,  4195,
       43044, 47738, 32018, 35276, 11994, 11250, 32094, 31515, 26586,
       17727, 45957,  2835, 15203, 19180, 32620, 36302, 43294, 32026,
       18734, 31053,  4695, 37778, 39768, 26819, 45962, 35450, 45496,
       15004, 17623, 27429, 23362, 48738, 33232, 46382, 24829,  2216,
        9962,  4998, 42315, 33971, 13513, 37963, 18011, 18110, 14521,
       24823,  7536, 48889, 43529, 17500,  4359, 17852,  3141, 26715,
       46446,  5298, 43337, 32590, 18374, 30801, 25798, 33401, 42662,
        1903,  4156, 30627, 37234,  7712, 47374, 29071, 43870, 17819,
       14726,  6298,  8131, 16073, 35514, 40705, 16328, 34936, 11757,
       44125, 23869, 30044, 29922, 10890, 26023, 42296, 33523, 15454,
       14704]), [5, 7, 1, 4])]
Collaboration
DC 0, val_set_size=1000, COIs=[3, 9, 1, 4], M=tensor([3, 9, 1, 4], device='cuda:0'), Initial Performance: (0.25, 0.04431799054145813)
DC 1, val_set_size=1000, COIs=[0, 2, 1, 4], M=tensor([0, 2, 1, 4], device='cuda:0'), Initial Performance: (0.252, 0.044353610038757325)
DC 2, val_set_size=1000, COIs=[8, 6, 1, 4], M=tensor([8, 6, 1, 4], device='cuda:0'), Initial Performance: (0.276, 0.04447247278690338)
DC 3, val_set_size=1000, COIs=[5, 7, 1, 4], M=tensor([5, 7, 1, 4], device='cuda:0'), Initial Performance: (0.218, 0.04446711611747742)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {9, 3}
D07: 1000 samples from classes {9, 3}
D08: 1000 samples from classes {9, 3}
D09: 1000 samples from classes {9, 3}
D010: 1000 samples from classes {9, 3}
D011: 1000 samples from classes {9, 3}
D012: 1000 samples from classes {0, 2}
D013: 1000 samples from classes {0, 2}
D014: 1000 samples from classes {0, 2}
D015: 1000 samples from classes {0, 2}
D016: 1000 samples from classes {0, 2}
D017: 1000 samples from classes {0, 2}
D018: 1000 samples from classes {8, 6}
D019: 1000 samples from classes {8, 6}
D020: 1000 samples from classes {8, 6}
D021: 1000 samples from classes {8, 6}
D022: 1000 samples from classes {8, 6}
D023: 1000 samples from classes {8, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO2']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.375, 0.06283778327703476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07227671495079994) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.07495411774516106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.349, 0.08856414425373077) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.284, 0.06856693160533905) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.08170339134335518) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.08742304396629333) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.09984482651948928) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.08577430075407028) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.357, 0.09504514583945274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.11441597338020802) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.10870000609755516) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.435, 0.11007066434621811) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.11734175483882427) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.16358603571355343) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.13449031338095666) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1351322904229164) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13958969017863274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.17408120495080948) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.1502700263261795) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO4']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.15643265687674285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.419, 0.1466355331838131) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2705200811526738) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.1623449475169182) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.1696251774523407) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.432, 0.15292382569611074) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2703526836093515) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.190651739038527) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19223802105896176) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.425, 0.16991872733086347) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3325524968125392) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.1890639338158071) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19927251303382218) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.418, 0.17540518701449037) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3120443267803639) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.2167533227801323) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.21156952209956945) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.424, 0.16317588014900683) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.3092682206723839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.2174901111423969) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[1, 4], M=tensor([0, 1, 2, 3, 4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.9213333333333333, 0.01489187490940094)
DC Expert-0, val_set_size=500, COIs=[9, 3], M=tensor([3, 9, 1, 4], device='cuda:0'), Initial Performance: (0.94, 0.005162857327610254)
DC Expert-1, val_set_size=500, COIs=[0, 2], M=tensor([0, 2, 1, 4], device='cuda:0'), Initial Performance: (0.848, 0.010477451875805855)
DC Expert-2, val_set_size=500, COIs=[8, 6], M=tensor([8, 6, 1, 4], device='cuda:0'), Initial Performance: (0.95, 0.00401176818087697)
SUPER-DC 0, val_set_size=1000, COIs=[3, 9, 1, 4], M=tensor([3, 9, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[0, 2, 1, 4], M=tensor([0, 2, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[8, 6, 1, 4], M=tensor([8, 6, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7cdee0098d30>, <fl_market.actors.data_consumer.DataConsumer object at 0x7cde6c726d00>, <fl_market.actors.data_consumer.DataConsumer object at 0x7cde6c663af0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7cdee0144b50>, <fl_market.actors.data_consumer.DataConsumer object at 0x7cdef438a6d0>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO0']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO5', '(DO1']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0047706635007634755) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.01041794351860881) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.00325043027382344) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.19675794168934227) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9353333333333333, 0.006139374636113644) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.578, 0.040150375038385394) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.06082550072669983) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.661, 0.03446072705090046) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.005158813979476691) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.009789427660405636) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.0038513190567027776) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.18715722899883985) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9126666666666666, 0.009859571090744187) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.522, 0.07899183051660658) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.11284744812175632) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.587, 0.055970601191744206) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004515942841768265) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.010221544578671455) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.978, 0.00238488234532997) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.19004722152650355) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.004904053071203331) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.06881707756593823) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.08265771237388253) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.666, 0.03863025961816311) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005613019225653261) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.012310305383056401) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.97, 0.003284999325638637) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.1896155283227563) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9546666666666667, 0.006037599925417453) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.545, 0.07031739469617605) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.514, 0.0917750150617212) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.596, 0.054603885393589735) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005276892591267824) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.010082008957862853) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.962, 0.0031639105447102338) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.17440937259793282) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.008555108592729083) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.08561411194875837) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.11617983718030155) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.543, 0.07075341831240803) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO3', '(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004721710234414786) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.011243025295436382) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.968, 0.003830968350172043) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.1742000042386353) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.004005238536124428) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.616, 0.04958767895400524) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.539, 0.08626542606204748) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.639, 0.04522704666107893) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004735699371201918) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.010346661314368248) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.968, 0.0032240601847879588) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.17512721762992442) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9653333333333334, 0.003746584088852008) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.07840453071892262) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.517, 0.09901384473964572) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.616, 0.0693327429741621) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004662863769102842) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.011621295731514693) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0036477351526846176) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.1814727083668113) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.005827468290925026) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.581, 0.06336008703708648) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.08562813726067543) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.664, 0.06193355330824852) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004155539496801794) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.01079650841280818) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.98, 0.0029428511010482906) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.20235344663821161) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9626666666666667, 0.005293737368270134) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.538, 0.0930487177465111) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.10794780334830284) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.567, 0.127961903039366) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005398307101568207) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.010023294851183891) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.964, 0.0047245778286014685) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.19524706594645977) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.004979846794313441) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.07305939577519893) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.534, 0.08754919857531786) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.602, 0.05763985227793455) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO5', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005343723821104505) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.011394322343170643) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0033513319279882127) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.1814849591460079) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.003433726811315864) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.07721328299492597) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.567, 0.09342693614959717) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.617, 0.06611411319673062) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005399119133828208) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.01045804612711072) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0037079757340834477) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.202594157487154) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9666666666666667, 0.006101525621888868) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.498, 0.10529488201439381) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.502, 0.1319527153247036) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.562, 0.09111920173745602) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004846374093554914) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.906, 0.010649649741128087) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0036308392771752553) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.20844588031619787) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9666666666666667, 0.0033616375752414268) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.07280790096521378) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.544, 0.0885180748924613) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.594, 0.06692984121292829) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007192786574596539) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.011877630174160004) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.978, 0.003084256582224043) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2073123925291002) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9626666666666667, 0.004416641721201207) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.546, 0.07174137147516012) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.08588995896279812) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.613, 0.06229135006293655) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006382935307803564) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.01159126470796764) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.968, 0.003055915651959367) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.1892870243564248) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9593333333333334, 0.005397575005605177) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.522, 0.08402158041670918) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.52, 0.12152655327506363) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.582, 0.08814234642498195) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO5', '(DO2']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004831325579667464) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.01340717605687678) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.97, 0.003137284678959986) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.19514055089280008) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9666666666666667, 0.0040977541752702865) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.567, 0.07923582360520959) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.512, 0.11903440678678454) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.605, 0.08266832549218088) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005036171514540911) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.011943142995238304) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.964, 0.0034815409818547776) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.18095124150440098) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9686666666666667, 0.003917469347652513) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.569, 0.06619568140245974) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.09733372228592634) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.61, 0.07299964257702231) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006008627880131826) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.01133092362433672) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.964, 0.0035805569217191077) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.18319724938832224) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9633333333333334, 0.00423187719968458) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.579, 0.0668623319733888) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.533, 0.09850268649309873) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.667, 0.05623241355642676) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005845830831909552) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.010878276005387306) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.002480643164482899) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.18733695867657663) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.004277501560747623) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.596, 0.05340447559952736) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.06708136002719402) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.661, 0.054133607059717176) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0057763065373292195) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.012334959141910077) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0027965314679313453) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.424, 0.20925925024226308) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9413333333333334, 0.007133567857556045) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.0763701436072588) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.1072237135414034) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.629, 0.07476177839934826) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO2', '(DO4']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005287780998041853) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.010968589503318072) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.97, 0.00418542588109267) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.18335829075053334) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9613333333333334, 0.004785639219839747) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.07857540204375982) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.513, 0.11001726940646768) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.551, 0.09043670325912535) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0060436589515302334) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.9, 0.011783539474010467) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0029345193755580114) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.16970533192530274) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.006285489876521751) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.515, 0.08728450075164437) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.09684364195913077) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.582, 0.09095291589945555) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004890602736268192) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.012990336870774626) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.003232886853278615) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.1891079869195819) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9533333333333334, 0.009855155017464767) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.0874578001908958) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.0894492720682174) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.587, 0.08774027011264116) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005923324336181395) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.9, 0.010827648118138314) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.003444293127889978) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.19304091520607472) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9633333333333334, 0.005588571593985156) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.08095429955795408) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.11285853629605845) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.604, 0.08131147778406739) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005096034704009071) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.906, 0.012078858584165573) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0035305140011478216) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.424, 0.2047099668160081) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9613333333333334, 0.00548827275033788) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.0908003948070109) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.09339395074732601) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.645, 0.06194397136569023) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO0', '(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.0055803070301190015) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.906, 0.012952676430344581) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.97, 0.004706341173645342) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.42, 0.17759370117634535) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9593333333333334, 0.004741650212245683) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.574, 0.06048858869075775) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.551, 0.06994326411187649) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.679, 0.047192604914307594) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005671343136113137) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.012610778953880072) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.004749333252788346) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.431, 0.20286498376354575) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.005259777340882768) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.06538878157548607) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.08078369503840804) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.663, 0.05722401812300086) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.006060729684773832) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.012140422262251377) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.003832106222398579) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.20118578038364648) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9573333333333334, 0.008410251050928005) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.08764000544324517) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.518, 0.10779216948151589) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.557, 0.08245937080029399) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006138464628020301) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.011761645786464214) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.982, 0.0027828424316467134) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.2117225462719798) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.007631102205227459) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.55, 0.06460698195546866) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.08848584064282478) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.618, 0.06477590750530363) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.0056222518582362685) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.013829996263608337) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.964, 0.005101366559574672) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.421, 0.20260038238018752) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9573333333333334, 0.010383074207763722) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.09569798475201241) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.519, 0.09764743153750896) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.591, 0.07018029753898736) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO0', '(DO2']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005961465483880602) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.01237553982436657) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.962, 0.004942461077735061) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.1866189674511552) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.004634215086291078) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.597, 0.050757387757301334) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.554, 0.06620817925035953) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.629, 0.05507452230155468) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.007550845143618062) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.013305531987920404) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0035593299449101323) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.21111816514655948) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9686666666666667, 0.005334221178153105) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.07250874641165138) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.07624581395089626) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.599, 0.06499919269979) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.007184782354335766) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.011581866767257452) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.966, 0.0037492511251039106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.23483694146573544) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9706666666666667, 0.005764930379495733) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.58, 0.0657120813280344) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.13071938444999978) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.594, 0.07269348249258474) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00647790633700788) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.012063592381775378) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0034321029524726327) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.2036259067468345) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9573333333333334, 0.008341274495598705) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.07919008653610944) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.5, 0.14952289086487144) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.602, 0.08355058274196926) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005721950601786375) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.012800234001129866) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0035535880346724297) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.2310500474870205) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9666666666666667, 0.004241303093149327) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.049007970416918394) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.533, 0.09130894058290868) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.681, 0.04925927948439494) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO1', '(DO0']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005905628051608801) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.010837903097271919) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0032956503274617715) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.22581859016045927) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9706666666666667, 0.0031391743227529027) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.05662774062156677) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.10008100389502943) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.652, 0.05497814234346152) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.0073761267675436105) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.904, 0.010928294960409403) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.964, 0.004891477301945997) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.20113463890925048) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.0068928662340864925) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.605, 0.04604472628235817) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.08134433601703495) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.655, 0.04385789845883846) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006049418300855904) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.013413702264428139) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.966, 0.004637111934134737) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.2100509679019451) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9673333333333334, 0.0035585389533662236) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.621, 0.04688850197196007) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.08763833487033844) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.659, 0.04736839071661234) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005530505603179335) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.013207299116067588) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.004968442344988944) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.422, 0.2328846785314381) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9653333333333334, 0.0055562257348065035) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.578, 0.0686673525813967) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.497, 0.08844858890399337) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.621, 0.04894052214920521) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00803335881640669) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.014058625204488634) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.003473102212999947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.422, 0.20936419904232026) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.00462224300801366) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.578, 0.053649373061954976) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.553, 0.06974884440004826) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.588, 0.053136806651949885) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04431799054145813), (0.375, 0.06283778327703476), (0.284, 0.06856693160533905), (0.394, 0.08577430075407028), (0.435, 0.11007066434621811), (0.458, 0.1351322904229164), (0.452, 0.15643265687674285), (0.463, 0.1696251774523407), (0.471, 0.19223802105896176), (0.471, 0.19927251303382218), (0.47, 0.21156952209956945), (0.578, 0.040150375038385394), (0.522, 0.07899183051660658), (0.543, 0.06881707756593823), (0.545, 0.07031739469617605), (0.52, 0.08561411194875837), (0.616, 0.04958767895400524), (0.534, 0.07840453071892262), (0.581, 0.06336008703708648), (0.538, 0.0930487177465111), (0.549, 0.07305939577519893), (0.534, 0.07721328299492597), (0.498, 0.10529488201439381), (0.563, 0.07280790096521378), (0.546, 0.07174137147516012), (0.522, 0.08402158041670918), (0.567, 0.07923582360520959), (0.569, 0.06619568140245974), (0.579, 0.0668623319733888), (0.596, 0.05340447559952736), (0.534, 0.0763701436072588), (0.534, 0.07857540204375982), (0.515, 0.08728450075164437), (0.534, 0.0874578001908958), (0.552, 0.08095429955795408), (0.524, 0.0908003948070109), (0.574, 0.06048858869075775), (0.583, 0.06538878157548607), (0.52, 0.08764000544324517), (0.55, 0.06460698195546866), (0.518, 0.09569798475201241), (0.597, 0.050757387757301334), (0.535, 0.07250874641165138), (0.58, 0.0657120813280344), (0.549, 0.07919008653610944), (0.628, 0.049007970416918394), (0.6, 0.05662774062156677), (0.605, 0.04604472628235817), (0.621, 0.04688850197196007), (0.578, 0.0686673525813967), (0.578, 0.053649373061954976)]
TEST: 
[(0.254, 0.043263681530952454), (0.37075, 0.060598327726125716), (0.28775, 0.06587039145827293), (0.39, 0.08190187042951584), (0.42925, 0.10396925833821297), (0.45375, 0.12755469673871994), (0.44575, 0.1482144956588745), (0.45525, 0.16008145165443421), (0.46375, 0.17392877840995788), (0.4635, 0.18666653883457185), (0.46075, 0.1964863825440407), (0.545, 0.043243968591094015), (0.512, 0.08384702825546264), (0.5395, 0.0723514252603054), (0.53775, 0.07365215638279915), (0.526, 0.08832099935412407), (0.58575, 0.05315384685993194), (0.53525, 0.07750741454958916), (0.568, 0.06546259154379368), (0.53, 0.09649547675251961), (0.5485, 0.07476558949053287), (0.546, 0.07528075683116912), (0.50375, 0.10950871014595032), (0.552, 0.07530734086036682), (0.54225, 0.07190029701590538), (0.53525, 0.08440418237447739), (0.56775, 0.07870145137608052), (0.573, 0.06669993956387044), (0.5845, 0.0652776440680027), (0.58475, 0.05683228604495525), (0.5455, 0.07579393541812897), (0.54625, 0.0800555944442749), (0.5365, 0.0851449559032917), (0.531, 0.08864018812775612), (0.5565, 0.08208417508006095), (0.544, 0.08890382313728333), (0.58525, 0.058871463939547536), (0.569, 0.06755060820281505), (0.5355, 0.08643457534909248), (0.55625, 0.06648569709062577), (0.51475, 0.09683361282944679), (0.59275, 0.054255784064531326), (0.53125, 0.07532059583067895), (0.57975, 0.07023816226422787), (0.543, 0.08361101627349854), (0.6065, 0.05394096340239048), (0.59375, 0.059401971749961376), (0.59925, 0.046292796529829504), (0.61775, 0.04764485440403223), (0.563, 0.07068328064680099), (0.58625, 0.05704132179915905)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.57      0.82      0.67      1000
           3       0.70      0.21      0.32      1000
           4       0.54      0.95      0.69      1000
           9       0.72      0.37      0.49      1000

    accuracy                           0.59      4000
   macro avg       0.63      0.59      0.54      4000
weighted avg       0.63      0.59      0.54      4000

Collaboration_DC_1
VAL: 
[(0.252, 0.044353610038757325), (0.25, 0.07227671495079994), (0.293, 0.08170339134335518), (0.357, 0.09504514583945274), (0.416, 0.11734175483882427), (0.395, 0.13958969017863274), (0.419, 0.1466355331838131), (0.432, 0.15292382569611074), (0.425, 0.16991872733086347), (0.418, 0.17540518701449037), (0.424, 0.16317588014900683), (0.507, 0.06082550072669983), (0.493, 0.11284744812175632), (0.526, 0.08265771237388253), (0.514, 0.0917750150617212), (0.486, 0.11617983718030155), (0.539, 0.08626542606204748), (0.517, 0.09901384473964572), (0.529, 0.08562813726067543), (0.487, 0.10794780334830284), (0.534, 0.08754919857531786), (0.567, 0.09342693614959717), (0.502, 0.1319527153247036), (0.544, 0.0885180748924613), (0.545, 0.08588995896279812), (0.52, 0.12152655327506363), (0.512, 0.11903440678678454), (0.562, 0.09733372228592634), (0.533, 0.09850268649309873), (0.557, 0.06708136002719402), (0.54, 0.1072237135414034), (0.513, 0.11001726940646768), (0.527, 0.09684364195913077), (0.522, 0.0894492720682174), (0.516, 0.11285853629605845), (0.541, 0.09339395074732601), (0.551, 0.06994326411187649), (0.541, 0.08078369503840804), (0.518, 0.10779216948151589), (0.546, 0.08848584064282478), (0.519, 0.09764743153750896), (0.554, 0.06620817925035953), (0.524, 0.07624581395089626), (0.499, 0.13071938444999978), (0.5, 0.14952289086487144), (0.533, 0.09130894058290868), (0.546, 0.10008100389502943), (0.541, 0.08134433601703495), (0.557, 0.08763833487033844), (0.497, 0.08844858890399337), (0.553, 0.06974884440004826)]
TEST: 
[(0.267, 0.04317044323682785), (0.25, 0.06941601613163947), (0.3085, 0.07784869140386581), (0.37125, 0.09040846940875054), (0.41275, 0.11196270486712456), (0.39575, 0.1334228486418724), (0.41875, 0.14155897158384323), (0.43075, 0.1480341517329216), (0.4325, 0.16426597940921783), (0.4275, 0.16917636078596116), (0.4355, 0.1590513065457344), (0.50925, 0.061352430537343024), (0.48925, 0.10882103943824768), (0.54625, 0.0802739462852478), (0.513, 0.08792097440361976), (0.5005, 0.11410763037204742), (0.553, 0.08338555574417114), (0.53175, 0.10020771926641464), (0.5475, 0.08554427090287209), (0.50175, 0.10655573499202728), (0.5425, 0.08707777822017669), (0.55525, 0.09336366739869117), (0.5105, 0.1308963561654091), (0.57275, 0.08754033151268958), (0.54975, 0.08511855168640614), (0.5345, 0.11930327534675599), (0.521, 0.11868322989344597), (0.56875, 0.09447703745961189), (0.5495, 0.09787661528587341), (0.579, 0.06477341634035111), (0.5305, 0.10925927463173866), (0.52175, 0.10632297503948211), (0.5405, 0.0929761565029621), (0.52775, 0.08595525333285332), (0.5285, 0.10912152609229088), (0.54825, 0.09077377638220788), (0.55975, 0.06986329019069672), (0.5555, 0.07931144100427627), (0.51975, 0.10582257121801376), (0.55375, 0.0872310956120491), (0.53025, 0.09382881888747216), (0.5595, 0.06453852531313896), (0.53275, 0.07302348479628563), (0.516, 0.12672645059227944), (0.4985, 0.14287871637940408), (0.546, 0.08833512544631958), (0.53725, 0.09665869188308716), (0.531, 0.08051728692650795), (0.5485, 0.08658380374312401), (0.506, 0.0860782590508461), (0.55175, 0.06947213757038116)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.93      0.23      0.37      1000
           1       0.76      0.93      0.84      1000
           2       0.54      0.11      0.18      1000
           4       0.40      0.94      0.56      1000

    accuracy                           0.55      4000
   macro avg       0.66      0.55      0.49      4000
weighted avg       0.66      0.55      0.49      4000

Collaboration_DC_2
VAL: 
[(0.276, 0.04447247278690338), (0.436, 0.07495411774516106), (0.452, 0.08742304396629333), (0.473, 0.11441597338020802), (0.472, 0.16358603571355343), (0.459, 0.17408120495080948), (0.478, 0.2705200811526738), (0.477, 0.2703526836093515), (0.474, 0.3325524968125392), (0.474, 0.3120443267803639), (0.475, 0.3092682206723839), (0.661, 0.03446072705090046), (0.587, 0.055970601191744206), (0.666, 0.03863025961816311), (0.596, 0.054603885393589735), (0.543, 0.07075341831240803), (0.639, 0.04522704666107893), (0.616, 0.0693327429741621), (0.664, 0.06193355330824852), (0.567, 0.127961903039366), (0.602, 0.05763985227793455), (0.617, 0.06611411319673062), (0.562, 0.09111920173745602), (0.594, 0.06692984121292829), (0.613, 0.06229135006293655), (0.582, 0.08814234642498195), (0.605, 0.08266832549218088), (0.61, 0.07299964257702231), (0.667, 0.05623241355642676), (0.661, 0.054133607059717176), (0.629, 0.07476177839934826), (0.551, 0.09043670325912535), (0.582, 0.09095291589945555), (0.587, 0.08774027011264116), (0.604, 0.08131147778406739), (0.645, 0.06194397136569023), (0.679, 0.047192604914307594), (0.663, 0.05722401812300086), (0.557, 0.08245937080029399), (0.618, 0.06477590750530363), (0.591, 0.07018029753898736), (0.629, 0.05507452230155468), (0.599, 0.06499919269979), (0.594, 0.07269348249258474), (0.602, 0.08355058274196926), (0.681, 0.04925927948439494), (0.652, 0.05497814234346152), (0.655, 0.04385789845883846), (0.659, 0.04736839071661234), (0.621, 0.04894052214920521), (0.588, 0.053136806651949885)]
TEST: 
[(0.27075, 0.04346053540706635), (0.4375, 0.07203164887428283), (0.44975, 0.083502028465271), (0.4725, 0.10839265322685242), (0.47325, 0.15421055006980897), (0.45525, 0.16559282863140107), (0.48025, 0.25170764434337617), (0.478, 0.2528487293720245), (0.475, 0.3110364720821381), (0.478, 0.291153617978096), (0.48075, 0.29257084810733797), (0.65075, 0.03473335035890341), (0.57775, 0.05612831474840641), (0.67625, 0.036001020565629004), (0.59775, 0.05280169703066349), (0.54925, 0.0685217472910881), (0.661, 0.042865015551447866), (0.59925, 0.06690811142325401), (0.65825, 0.05319992955774069), (0.554, 0.11129038086533546), (0.602, 0.05436218243837357), (0.61825, 0.06320214177668095), (0.54425, 0.09098027554154396), (0.59725, 0.06808465860784053), (0.6225, 0.06069057524204254), (0.577, 0.08675322063267231), (0.61925, 0.08026429748535156), (0.6055, 0.06795064190030098), (0.66925, 0.05432072388380766), (0.654, 0.052044874265789985), (0.607, 0.06899335218966007), (0.548, 0.08566457769274712), (0.56325, 0.08559983152151107), (0.57125, 0.08205619180202484), (0.6005, 0.08002034893631935), (0.63275, 0.05999587127566337), (0.673, 0.04346350386738777), (0.64475, 0.05469322739541531), (0.5435, 0.08280533233284951), (0.6145, 0.06256044673919678), (0.57225, 0.06802670842409134), (0.6255, 0.05184641809761524), (0.59675, 0.05994515892863274), (0.6045, 0.06688458511233329), (0.59325, 0.08148088769614696), (0.67875, 0.04581339180469513), (0.65425, 0.05094937239587307), (0.659, 0.04192335591092706), (0.66025, 0.046637317430228), (0.617, 0.046770817771553994), (0.6125, 0.049424240984022615)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.62      0.95      0.75      1000
           4       0.51      0.96      0.67      1000
           6       0.90      0.27      0.42      1000
           8       0.93      0.27      0.41      1000

    accuracy                           0.61      4000
   macro avg       0.74      0.61      0.56      4000
weighted avg       0.74      0.61      0.56      4000

Collaboration_DC_3
VAL: 
[(0.218, 0.04446711611747742), (0.349, 0.08856414425373077), (0.393, 0.09984482651948928), (0.407, 0.10870000609755516), (0.407, 0.13449031338095666), (0.415, 0.1502700263261795), (0.416, 0.1623449475169182), (0.416, 0.190651739038527), (0.406, 0.1890639338158071), (0.414, 0.2167533227801323), (0.43, 0.2174901111423969), (0.416, 0.19675794168934227), (0.427, 0.18715722899883985), (0.431, 0.19004722152650355), (0.427, 0.1896155283227563), (0.423, 0.17440937259793282), (0.423, 0.1742000042386353), (0.425, 0.17512721762992442), (0.43, 0.1814727083668113), (0.428, 0.20235344663821161), (0.423, 0.19524706594645977), (0.431, 0.1814849591460079), (0.427, 0.202594157487154), (0.432, 0.20844588031619787), (0.429, 0.2073123925291002), (0.425, 0.1892870243564248), (0.427, 0.19514055089280008), (0.419, 0.18095124150440098), (0.43, 0.18319724938832224), (0.43, 0.18733695867657663), (0.424, 0.20925925024226308), (0.423, 0.18335829075053334), (0.436, 0.16970533192530274), (0.43, 0.1891079869195819), (0.427, 0.19304091520607472), (0.424, 0.2047099668160081), (0.42, 0.17759370117634535), (0.431, 0.20286498376354575), (0.433, 0.20118578038364648), (0.428, 0.2117225462719798), (0.421, 0.20260038238018752), (0.427, 0.1866189674511552), (0.432, 0.21111816514655948), (0.43, 0.23483694146573544), (0.433, 0.2036259067468345), (0.432, 0.2310500474870205), (0.429, 0.22581859016045927), (0.425, 0.20113463890925048), (0.432, 0.2100509679019451), (0.422, 0.2328846785314381), (0.422, 0.20936419904232026)]
TEST: 
[(0.2305, 0.0433762149810791), (0.34325, 0.08504497039318085), (0.39575, 0.09558765774965286), (0.4115, 0.10409454986453057), (0.40925, 0.12879934746026994), (0.41825, 0.14445116937160493), (0.41375, 0.15593986171483992), (0.4215, 0.18018277913331984), (0.41, 0.17879825294017793), (0.424, 0.20379313588142395), (0.42875, 0.2091999084353447), (0.417, 0.1886175980567932), (0.428, 0.17805876791477204), (0.43425, 0.18351318836212158), (0.434, 0.18330194503068925), (0.432, 0.16893582582473754), (0.4255, 0.16768063336610795), (0.42725, 0.16848324078321458), (0.4345, 0.1776248568892479), (0.429, 0.19860230892896652), (0.434, 0.1907758429646492), (0.433, 0.17993150317668916), (0.43025, 0.19852049899101257), (0.435, 0.20405472922325135), (0.43175, 0.19978830927610397), (0.43, 0.18478451442718505), (0.43325, 0.19077866053581238), (0.4215, 0.1749006912112236), (0.42875, 0.17710783499479293), (0.43475, 0.18327917802333832), (0.42825, 0.2002059596180916), (0.427, 0.17552132058143616), (0.43425, 0.1665228054523468), (0.4335, 0.1908985652923584), (0.4305, 0.19080831986665725), (0.427, 0.20243965542316436), (0.42225, 0.17406210672855377), (0.43125, 0.1975720461010933), (0.43125, 0.19725602746009827), (0.4335, 0.20700787568092346), (0.427, 0.19830165088176727), (0.42675, 0.18236369353532791), (0.432, 0.2033377845287323), (0.42875, 0.2245523248910904), (0.434, 0.1965936090350151), (0.42975, 0.22244282281398772), (0.42775, 0.2167014696598053), (0.42825, 0.1945838725566864), (0.4345, 0.2005729353427887), (0.428, 0.22658540666103363), (0.426, 0.20485936135053634)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           4       0.00      0.00      0.00      1000
           5       0.36      0.94      0.52      1000
           7       0.56      0.76      0.65      1000

    accuracy                           0.43      4000
   macro avg       0.23      0.43      0.29      4000
weighted avg       0.23      0.43      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [145]
name: alliance-3-dcs-145
score_metric: contrloss
aggregation: <function fed_avg at 0x7f58b0646c10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=145
Partitioning data
[[0, 3, 6, 8], [1, 4, 6, 8], [9, 7, 6, 8], [5, 2, 6, 8]]
[(array([33451, 31000,  3114, 10896,  4060, 19644, 44165, 35381, 42784,
       22062, 10061, 13549, 40945, 48788,  6015,  4859, 39301,  7347,
       13833, 47754, 38289,  3570, 44806, 46113, 42829, 15677,  1338,
       12492, 41880,  4273, 13712, 17232, 13627, 44568, 19945, 26018,
       25242, 36106, 19444, 10954, 21869, 26049,  3713, 35996, 38185,
       30267, 36163, 49050, 33058, 48847, 19778, 26060, 27080, 18572,
       24534, 32538, 42931, 12461, 28068, 31207, 15870, 32062, 42630,
       18211, 22129, 34856, 34017, 32376, 16380, 45186, 16763, 12659,
        4498, 24700, 20267, 44970, 24887, 26724,  6512, 15309, 37328,
       47473, 38618, 37022, 45344, 40635, 32078, 41055, 35269, 22674,
        3204, 26846,  6687,  4079, 46872, 34168, 31837, 47243, 30517,
       32885, 20015, 27415, 39236, 27280, 11812, 38222,   115, 36007,
        4940, 44668, 11734, 38433,  2720, 28080,  4619, 16596, 30029,
       16316, 19015, 17486, 48359, 34434, 11768,  9868, 48986, 16553,
       32479, 13401,  7504,  2365, 25027, 32233, 10326, 32967,  8249,
       38796, 13861, 44443, 18953, 42992, 30913, 14341, 44875, 45347,
       15735, 12911, 14482, 45102,  8858, 22844, 43777, 29295, 12178,
       24349, 33419,  9755, 37729, 25082, 17357, 17866, 26632, 11263,
        5994,  7655, 12149,  1147,  5194, 38545,  7320, 21881, 25362,
       43992, 12499, 34944, 14116,  7970, 12257, 24111, 35973, 38566,
       13714, 29860, 20217, 43757, 40211, 44580, 27768,  7299, 46367,
       33603,   564,  3441, 34917, 38466, 24837, 30895,   695, 17093,
       17994, 23581, 49717, 36052, 20398,  9257,  1522, 36737, 24735,
       30233, 20924, 22684, 25995, 47520,  3136, 14732, 41490, 48147,
        5589,  5828,  9671, 43951, 36149, 30963,  3582, 13368, 25178,
       20813, 16305, 41164, 41851, 21144,  7925, 47324,   405, 39329,
       48924,  4869,  1493, 31284,   392,  9096, 43542, 23746,  9287,
       48958, 15102, 40022, 20454,  5858, 46987,   527,  7603, 39211,
       37554, 16438,  3351, 22761, 37429, 41601, 27304, 17024,  4266,
       30346, 20784, 35960, 10085, 32844, 30318, 30305, 38093,  5507,
       41853,  5741, 29074, 38304, 39761,  1109, 25215, 23465, 12391,
       24733, 23974,  2738,  3365, 15247,  4025,  9034, 39289,  5586,
       33717, 37236, 19808, 47654, 21147, 42233, 13593, 33098, 48742,
       15421, 36249, 20252,  9830,  1951, 37194, 39543, 46349, 33582,
       20017,  7469,  3818,  5006, 18366, 14171, 35007, 45948, 43836,
       38982, 28237, 33633, 41053, 19577,  4558, 28996, 20496, 18188,
        8943,  8105, 22523, 18348, 41159, 46423,  6872, 13064, 21913,
       13581, 30839, 43110,  6205,  8101, 48088, 28513,  7049, 17530,
       38761, 20648, 16301,  5086, 11476, 39045, 14018, 34984, 24276,
        3756, 46977, 10463, 48019, 45117, 23695, 26485,  9394,  5578,
       48062, 25997, 38127, 17567, 47788, 27585, 15383, 47830, 31242,
       10565, 25062, 21373, 26556, 42633, 20625, 17920, 16714,  4472,
        1573,   878,  6716, 14536,    80,  9052, 46529, 27906,  1919,
       41521, 23700, 13688, 49386,  3749, 49233, 14924, 21873, 34647,
        3514, 36990, 21652, 37513, 32145, 35130, 27796, 12155, 38699,
       22008, 20338,  9817, 47997, 48402, 36511, 47603, 28835, 10439,
       43065,  6089, 15298, 36828, 41910, 19871, 49582, 37514, 34581,
       11434, 49574, 30159,  3524, 31632, 41578, 35923, 34822,   287,
        6724,  9345,  3110, 26064, 12904, 32264, 37105, 29109, 29096,
       44093, 48132, 17478, 26482, 34741, 31415, 46855, 35759, 41285,
       23278,  8067, 21832,  7224, 39235, 26036, 18442, 15614, 41845,
       11738, 39921, 13053, 10948, 23541, 11894, 38100, 17251, 33874,
       29865, 28050, 48849, 27673,  7343, 45217, 35323,  2353, 36269,
       42738,  3125, 42445,  3222,  8381, 37912, 23295,  2425, 43013,
       11294, 40651, 45590, 48346, 44456, 36946, 28725, 20965, 24384,
       26160, 14706,  3758, 46341, 42189,  8069, 40039,  1327, 45135,
        5398,  5964, 32918, 15338, 45169, 27346,  3780, 37378, 11392,
       13753, 40261, 21580, 14476, 17680, 46073, 22550, 17788, 31601,
       36903, 14906, 47161, 11651, 17566, 38268, 24354,  3122, 44654,
        9941, 23582, 48400, 26618,  2571, 43264,  6262, 41870,  9928,
       14342,  6506, 34552, 18103, 21497, 16387, 19275, 37013,  4217,
       12396, 35332,  3724, 14212, 14131,  2288, 11675, 20788, 42791,
       20972, 23708,  8991,  2112, 32694,  3892, 12168, 22926, 37481,
        8891, 22085, 35340, 23780,  1192,  3593, 10938, 12508,  8436,
       28690, 13120, 29767, 30323, 46307, 11917, 48977, 14608, 23135,
       46320, 20586, 35831, 25622, 48888, 23008, 15534, 31353, 24478,
        6938,  6949,  7825, 19139, 40202, 27648, 26228, 46473, 23208,
       36310, 21097, 15497, 48619, 39343, 41491,  3111, 25469,   899,
       41668, 45730, 49771, 15351, 26211, 10026,  2684, 36712, 40514,
       18736, 48532, 43392, 33696, 49406, 34965,   143, 19239, 34035,
       28277, 15650, 44012, 41603,  4391, 41309,  1563, 32869, 22889,
       22640, 42447, 31112, 36943, 24728, 49265, 15185,   248, 23086,
       42108, 45862,  5000,  5019, 15402, 17936, 37579, 40153, 19959,
       24000, 29560, 13683, 17172,   368,  5474,  9575, 26566, 30626,
       40357,  8612, 39913, 46322,  4082, 27450, 23516, 38751, 38508,
        2503, 43298, 32793, 25841,  8206,   819, 20934, 16539, 17857,
       49952,  9924, 31447, 18282, 21037, 22044, 12003, 30982, 21588,
        4421, 21192, 13635, 15424, 41549, 43074, 35900, 18992, 20141,
        8309, 11044, 48446,   117, 42099, 40264, 31616, 30625, 25442,
       15651, 25291, 39148, 46612, 10600, 15971, 39068, 35123, 34264,
       18126, 37956,  9681, 23596, 49829, 47115, 34804, 34643, 16822,
       17204, 33069, 46550, 22082, 15231, 37748, 13077, 36625, 24401,
        5095, 16927, 37822,  5921,  7064,  9762, 25255, 44428, 12632,
       10733, 21970, 49962,  5644,  8926, 10234, 31594, 13633, 39336,
       16430,  6250, 10442, 32464,  1914, 41732,  4092, 17442, 23845,
       36146, 46888, 31386, 42050, 46821, 15328,  4233, 16512, 16473,
       45318,  2923, 29746, 12418, 24811, 40109,  3885, 35013, 48943,
        2070, 26109, 25002, 29477, 12452, 40983, 21427, 28153, 33240,
       30299, 20954, 30438,  2462, 11591, 37042,  2586,  5675,  4397,
       28239, 46142, 14693, 13425, 36526, 12481, 17007, 30273, 27271,
        8006, 39502,  8123,  5716, 12457,   627, 21414, 11993,  8512,
       16435,   485, 47708, 10682, 11948, 13109, 31919, 49782, 48213,
       38453,  1250, 30406, 32915, 12605, 11244, 46714, 13729, 37147,
       14132, 11005, 33942, 35687, 22776, 28807, 41041, 18397, 13241,
       41689,  1383, 31526,  5487, 15635,  3614, 27820, 28125,  6989,
       14642, 18645, 16156, 40008,  1141, 39578, 23440,  8854, 43109,
        8574, 49291,  1967, 29340, 21579,  7443, 10680,   888,  8483,
       25902, 44458,  3215,  6219, 37994, 29147, 49182, 39069, 47770,
        6176, 26055,  8668,  2285, 14753, 35300, 30357, 10956,   546,
       32987,  5078,   909, 37308,  6679, 49778, 19502, 31832, 43872,
       23893, 49011, 23219, 40939,  2754, 21246,  9282, 42699, 27661,
       32619,  3319, 27854, 33356, 38008, 14934, 17277, 34910,  5910,
       44213, 47877, 11577,  2732,  8406, 22888,  1439, 16049, 41335,
       12194, 39577, 29493, 25111, 33407, 18141, 34892, 25047, 38105,
       43725,  5999,  7496, 45372,  9708, 37653, 27561, 32281, 31697,
       40429, 37587, 12801, 30807,  1080, 31778, 35661, 32567, 29027,
       15746, 37002,  6627,  6862, 28687, 21565, 16125, 34577, 25557,
       35268,  3800, 37642, 15795, 36940, 40641, 12442, 29175, 49827,
        2065, 31121, 19132, 24451, 33888, 48253, 12861, 37318, 28599,
       16856, 40483, 17210, 39032,  3169,  4723, 33869, 45442,  7013,
       29850, 20913,  6192, 47960, 48838,  8358, 18349, 45375, 28103,
        9960]), [0, 3, 6, 8]), (array([28634, 35692,  6709, 46330, 11418, 23607, 40152, 35799, 48975,
       35004, 29536, 28526, 13032, 37570, 20336, 45210,  4591,  9809,
       49561, 38257, 24191, 23213, 34263, 16736, 35174, 48631, 10153,
       47314, 29828, 36158, 11308, 42294, 18223,  3366, 30902, 49503,
        3822, 41061, 10271, 36033, 25071, 30676, 19310, 39405, 26905,
       18932, 21154,  1251, 49491, 25799, 24807, 49871, 33650, 41248,
       29121, 41607, 29963,  8350, 43711, 28615, 42281, 42637, 17539,
       17397, 47616, 13317, 24484,   644, 49376, 49269,  2455, 25105,
       22900, 23357,  4696, 40145, 29120, 27073, 21540, 22349, 22013,
       33870, 35357, 41654, 15299,  8825,  9251, 47190,  4379, 25827,
       18066, 19565, 32235, 28183,  3735,   119, 14717, 41739, 46421,
       12676,  2727, 11242, 45979, 30208,  2390,  7709, 42287, 22659,
       40178,  4624,  4909, 36101, 32736, 38047,  9095,  4557, 32099,
         493, 30804, 19574,  2180,  7352, 43145,   617, 39800, 32603,
       10714, 42684, 26241, 43935, 43207, 14101, 36991, 38157, 10248,
       45810, 17534, 21398, 49314, 30492, 46649, 36376, 41257, 18063,
       41576, 29791, 23601, 49416,  5356, 19500, 48806, 36872, 13402,
       39352, 37989, 39192, 49354, 12169, 25975, 20494, 40013, 47839,
       19667, 28127, 12049, 46359, 23058, 42410, 31147, 30716,  6922,
       28018, 34349, 34519, 29049, 48528, 30712, 44472, 21528, 10895,
       44965, 13763, 44359, 24263, 43006, 15710, 37401,  6623, 47606,
       38682, 16128, 31985, 17111, 13774, 14040, 39892, 24446,  9480,
       33392, 47151, 30019, 36679, 18134, 44097, 35805, 36334, 17772,
       32472, 28105, 48675, 30049, 14851, 16638, 47026,  4840, 27404,
       44309, 24280, 17312, 39240, 28896, 46966, 49795, 22867, 12628,
       46547, 42318, 28326, 35041, 40220, 10868, 26180, 37702, 10522,
       28248, 47836, 19046, 49933, 12412, 27814, 21877,  9565, 25479,
       10420, 33808, 44411, 12424, 17474, 26362,  9442, 39177, 43897,
       43934, 11794, 30474,   145, 41766, 18834, 11567, 27044,  5256,
       49645, 36345, 49740, 48787, 19197, 40228, 38247,  1953,  3217,
       24548, 30950, 45881, 15170, 33357, 22886, 20147, 33272, 27303,
       40375, 34574, 10766, 20730, 32533, 48429,  1471, 14036, 38386,
       41302, 22247, 33610,  2951,  2363, 15917, 44752, 18434, 32754,
       46947, 25414, 14816, 37775, 38660, 32930, 44978, 32882,  8068,
       14555, 42863,    89, 37996, 15723, 48591, 19045,  4413, 43758,
       24067, 41625, 20406, 25781, 33037, 27028, 40530, 36896, 33073,
       35839, 25474, 28658,  1385, 41270, 21048, 37757, 47573,  2216,
       11765, 46884, 38805, 34752, 48513, 22981, 22480, 14535,  8792,
       14974, 47149, 18455, 11565, 49352, 33222, 21949,  2673,  6378,
       38701, 28084, 22385, 14303,  6387, 43420, 49719, 37853, 47141,
       30794, 17083,  3695, 25292, 44002,  9010, 37563, 13828, 23994,
       21260, 37903, 27214, 47399, 49981, 31588, 43304, 47155,  6612,
        3908,   660, 43923, 36420,  5212, 25672, 39451,  2515, 18114,
        9141, 21607, 10015, 19704, 43928, 19170, 30044, 22606, 42303,
         581, 43202,  2442,  6940, 33002, 48689, 17770, 10629, 40835,
       45022, 22098, 43502, 15024, 24524, 44900, 13134, 17634, 48478,
       23187, 19980, 39932, 28160, 38296,  3688, 32066, 13391, 27342,
       31735, 28797, 20828, 11190,  5462, 40642, 27014, 22202,  2691,
       29353, 16669, 26118, 28561, 31699, 22173, 22284, 22902,  9550,
       27964, 37886, 40798, 46526, 46452, 48016, 27074, 49727, 30153,
       47922, 45555,  6331, 13995, 44994, 49027, 32119,  8405, 48228,
       18912, 14893, 29922,  9776, 10913, 32450, 17800,  8289, 30156,
        4400, 28803,  8404, 10888, 36665, 46077, 33743, 41777, 14382,
       11033, 48985, 16640, 15530,  4317, 16481, 49355, 33862, 37639,
       24498, 29172, 20142, 29265,  4642, 10536, 17260, 22360,  1659,
       38395,  1663, 18821,  3866,  7984, 32063, 19092,  6391, 40823,
       33623, 26383, 39120, 27660, 25000, 16508, 30724, 11875, 20310,
       28188, 26569, 16177, 20802, 32709, 49404, 11719, 49279, 10108,
       36525, 12666, 42261, 36060, 10327, 13403, 45140, 31128, 26748,
       30496, 15994, 15479, 10985, 11657,  3712,  4440, 48771, 17751,
       37654,  8617, 30302, 39994, 32338, 28943, 22113, 28644, 31405,
       45900, 49655, 30953,  6095, 22947,   234,  9278, 17925, 37351,
        4201, 38723, 43624, 13600, 38572, 29339, 46263, 18330, 30222,
       28194, 36984, 41652, 29408, 36816, 28481, 10400, 26872, 19707,
        1584, 41466, 45978,  6745, 13719, 40627,  1628, 21956, 38959,
        4415,  4810, 22347, 44959, 17766, 22157,  5785, 45266,  3942,
       44492, 31315, 17776, 13101,  6804, 29680, 32359, 46342, 18610,
        3683, 16429,  9855, 34942, 10345, 43482, 33528, 37625, 45944,
       11394,  3991, 47874, 15986,  7720, 48325,  6722, 32739, 32759,
       45420, 30193,  2631, 29360, 24424, 39889,  6454, 42013, 44258,
       12356, 21846, 33244,  1956,  8002, 24552,   807, 19834, 33420,
       38310, 28171, 47567, 40364, 40031,  8008, 16287, 23066,   862,
       33953, 39057,  3912,  9045, 11468, 29506, 12221, 34754, 45823,
       20800,  8466, 26771, 36666, 33405, 39823, 43611, 23412, 31270,
       15179,  2625, 39007, 21560, 33543,  1761, 28325, 42778, 30130,
       34644, 49853, 49516,  9877, 34031, 39680, 47240, 20268,    19,
        7232, 15064, 27803, 35672, 14973, 24510, 15810, 11310, 18494,
       33366, 29889, 15612, 17663,  4911, 48947, 26921, 14660,  9406,
       43210, 23686, 33411, 32768, 44675, 34823, 42888,  6749, 16442,
        9850,   680, 12879, 28722, 14951,  9785, 15039, 29973, 41707,
       32904, 44597, 28506, 26727, 31629, 48463, 16687,   770, 36296,
        7143,  4118, 18338, 22910,  3742, 31118,  4637, 43166, 13162,
        1176, 39678, 44602,  6046, 39639, 34694, 45355, 39637, 44393,
       30741, 26240, 45208,  8027, 47214, 11808,  2441, 40988, 37384,
       16246, 13591, 40137,  6508, 36949, 26603, 13958, 25386, 43755,
       46431, 38724, 44570, 36739, 16902, 41954, 41980, 33752, 44981,
       38133,  5906,  5378, 36151, 29479, 11148, 47206, 24878, 33410,
        4446, 27842, 16395, 13404, 18132, 31530, 19706, 27339, 10876,
        1862, 39707, 18996,  6775, 38301, 47894, 14158, 27857, 45095,
       33029, 41256, 12600, 10102, 30844, 47758, 40691, 26220, 37064,
       31372,  6474, 49330, 44057, 49257, 40491,  9387,  3268, 31695,
       25258, 45017, 22661, 47119, 46008,  6403, 24692, 40276,  8879,
       37420, 48602, 25238, 29919, 37798, 49025, 41678, 22712, 22649,
        1138,  2212, 34359, 22214, 47723, 22255, 12762,  7118,  4652,
        5297,  4313, 20239,  4848,  2803,  8222, 26463, 26898, 19238,
       49220, 46313, 15727, 38443, 11593, 49712, 45486, 10848,  5245,
       39334, 43705, 42032, 29782, 27982, 20000, 46015, 10763, 31628,
       20650, 34208, 42325, 38120, 14684, 43334, 15323, 23149, 29877,
       10346, 27976, 39383, 49446, 40136, 41207, 49960, 49144, 13695,
       17922, 27955,  2749, 34135,  6825,  8214, 45161, 15405, 13868,
       35540, 48805,   291,  9786,  4191,  9490, 15759, 16381, 11965,
       47342, 15429, 49042,  5463,  2663, 27536, 43409, 43161, 23342,
        8250,  3330, 31313, 32259, 42265, 40019, 35619, 36267, 18548,
       40805, 42749, 32611, 38237, 18822,  1617, 36579, 19622, 23617,
       38362, 40493, 32077, 29734, 18112, 36966, 48286,  4611, 43855,
       36295, 12207, 30388, 37832, 45839, 39106, 41957, 27243, 26703,
       48757, 42004,  5630, 22862, 41753, 17482,  1151,  4794, 29674,
        2666, 35177,  2109,  3740, 36933,  6139, 16792, 47000, 41543,
       23723, 43678, 14123, 28295, 35912, 45892, 15046, 29078, 43662,
       23538,  1702, 26156,  7507,  7108, 30123, 23438, 45853, 42024,
       48541, 22635, 41676,  1357, 17628,  4602, 22455,  6964,  2454,
       41555]), [1, 4, 6, 8]), (array([36549,  8103, 41592, 46390, 14059, 48503, 20804, 21775, 41996,
       19199, 15690, 30525,  6571,  9911, 39672,  4364, 11549, 22475,
       15337,  3097, 26585, 46221, 23358, 27589, 12315, 25476,  3006,
       47437, 35558, 39434, 46775, 24343, 48084, 40168, 28949, 36683,
         615, 20693, 49193, 20825, 13711,  7657,  7583, 28379, 46599,
        3755, 26150, 43566, 35749, 12483, 31324, 23819, 27569, 14545,
       31983,   438, 38651,  1893, 46848, 17905,  6111, 38328,  1504,
        4665, 34049,  6480, 37201, 37536, 18019, 20683, 16592,  7854,
       12008, 14467, 40257, 38414, 39830,  1049, 21470, 46296, 39440,
       21159,   672, 25094,  9382, 11173,  5591,  6129, 49880, 39884,
       11332,  1868, 24712, 41994, 19670, 49123,  2526,  3941, 43560,
       47175, 37058, 27675, 47641, 14326, 15616,  8295, 47086,  4768,
        2585, 41477, 14077, 24149, 12588, 20851, 19111, 40540, 38226,
       44678, 29186, 48292, 23383, 21520, 19076, 19432, 24987,  8728,
       35203, 32724, 10806,  3677, 22663, 44182, 39174, 26347, 16205,
       22497, 32273, 19976,  1263, 47036, 17747,  5650, 15159, 30785,
        3014, 18937, 29580, 24211, 35807, 41357, 35197,  5009, 33729,
       49670, 45146, 41329, 32133, 37780, 33856, 25648,   881, 22233,
       27287, 49370,  8323, 28709, 38789, 48654, 43882,  8561, 23141,
       46788, 43008, 46059, 18749, 12279, 41044,  2818, 40052, 40237,
       15993, 21289, 43901, 14083, 17163, 33547, 44871, 45844, 21010,
       34774, 15637,  6275, 27959,  4416, 13138,  2985, 20187, 17108,
       30161, 24494, 37040, 36059, 42101, 27058, 41316,  5532, 20613,
       14205, 37126, 11504, 24718, 39418, 43218, 15252, 19687, 46346,
       17109, 17709,  1706,  9715, 43696, 31905, 17435, 42154, 49087,
       40387,  7494, 42385, 29651, 15221, 28871,  1437, 38277, 39115,
       41181, 40229, 46744, 16488, 43141, 38280, 29349, 44901, 28141,
       23633, 43558, 48353, 21516, 45008, 15976,  2961, 49310, 14254,
       23310, 49162,   163, 20063, 15881,  9462, 33605, 21113, 43833,
       15539,  1927, 42951, 27824, 23773, 30066, 20971, 24090,  9581,
       23356, 47321, 46694, 47027, 32660,  7604, 20618, 36303,  4883,
       42386, 15999, 46283, 37679, 40426, 37159, 45630, 39354, 12365,
       11877, 25868, 36256, 30348,  9454, 49640,  4241, 10368, 28296,
       40837, 16175,  6748, 19089, 33374, 15324, 19691, 32073, 37884,
       36452, 21086, 30845, 24536, 25572, 10879, 13975, 47387, 18144,
        6119, 18443, 36936, 33936,   289, 23331, 29904, 29618, 23506,
       23330, 18975, 20879, 44278, 31073, 49058, 27913, 18620, 39246,
       34357, 32382, 16600, 17377, 26861,  9805,  7546, 20773, 28249,
       39168, 46455, 38616, 34440,  6600, 37474, 37660, 14556,  2370,
       45493, 31082, 22647, 45725, 10162, 14044, 38611,  3544, 35341,
       22965, 36977, 19037, 37573, 44115, 47856, 41755, 38196, 17499,
       18041, 26788, 25164, 46025, 17462, 22198, 31653, 35589,  6695,
        7385, 12925, 15290,  9294,  9891, 11781,  9329, 41397, 42366,
       17119,  3844, 31777, 29644, 35284, 14897, 26699, 45459, 32354,
       35290, 21268,  7544, 20010, 28812, 45939, 39307, 13482,  6301,
         916, 34518, 32973, 18022, 37019,   237,  6864, 23501,  4871,
       14641, 17436,  1908, 31461, 18997, 18350, 15047, 49695, 30163,
       16251, 26692, 13347, 32148, 22625, 33678, 34228, 12527, 13124,
       31597, 16633, 26086, 40579, 42895, 37211, 14983, 35812,  8481,
       25896, 18079,   152,  6051, 22612, 20166, 29669, 21493,  6792,
       49183, 36815,  2563,  6278, 47607,  4485, 31718, 38920,  5771,
       32545, 13514, 17649, 44471, 27672,  1197, 33341,  4928,  3251,
       44353, 49037,  5168,  5542, 19306, 23484, 25227, 21660, 28156,
       49903, 25957,  7482, 18313, 35056,  7314, 42368, 23456, 37100,
       13703, 12547, 36205,  7611, 22130, 49809, 31746, 20738, 22934,
       19600, 42485,  4386, 30670,  3157, 14236, 13796,  5796, 24897,
       20098,  7638, 27087,  3841,  2369, 21941, 11363, 36176, 32170,
         921, 39808, 40313,  2972, 46306,  6179, 29931, 19421,  3341,
       22266, 28061, 36716, 27610,  5723,  1453, 24989, 46850, 13441,
       24156, 10920, 20628, 38769, 39759,  4430, 44077, 18265,  7968,
       24215,  6521,  9245,  8983, 15631,  8171,  1819, 12550, 22290,
       38401,  2596, 49119,  1760, 32240, 17272, 11233,  1393, 19984,
       27872, 45836, 44244, 31794,  9482, 31421, 16675, 36061, 48079,
       31984, 38409, 46385, 13365, 12004, 12739,  8950, 44314, 40016,
       31142, 46193, 10256, 45863,  8253, 19588, 49776,  2179, 38397,
       21030,  3563,  7481, 12996,  5679, 42079, 33309,  6395, 37356,
       46273, 37682, 13261, 40927, 27611, 26340, 46492, 36913, 44068,
       45474,  1259, 16440,  2831, 23054, 38088, 21091,  2638, 23745,
       40436, 39969, 34304, 20986, 28042, 29456, 33551, 22525, 27367,
       13634, 27473, 11638, 14828,  2100, 34157, 47545, 29538,  5599,
        1550, 29958,  9468, 31133, 42527,  1246,  4605,  5177, 16606,
       39922, 46885,  4077, 14163, 49854, 19482,  6107, 42391, 15026,
       26701,  6962,  4786, 27577, 25612, 42358,  4606, 27503, 25935,
        5878, 46915, 42614, 25340, 36348, 25569, 41684, 47135, 37796,
       41399, 13619, 15048, 41350, 28859,  8633, 26122, 38391,  9974,
       26247, 18033, 22670, 40675, 34953,  7092, 24950, 40438, 26619,
        7315, 40047,  7001,  1741, 32856, 15458,   210, 19189, 26413,
          23, 16242, 31312, 31181, 33018,   721, 24213, 44011,   242,
       20254, 18050, 21146,  9339,  7267, 48526, 40568, 24895, 29935,
       14395, 39000,  9219, 43661, 29279, 12850, 43155, 30644, 12302,
       28776, 32339, 21438,  7348, 16639, 22068, 23030,  3403, 45385,
       32385, 38845,  2517, 20008, 38721, 45582,  1766, 25723,  4946,
       38137, 38227, 35098, 49459, 16059, 49804,  1837,  6768, 43538,
       32812, 20561, 35342, 30916, 46670, 22600, 36229, 49043, 44444,
       13582,  9818, 44927, 49551, 35468, 37258, 19124,  3881, 21755,
       26195, 16388, 42963,  5648, 30660,  2987, 30071, 43400, 17894,
       15698, 27515, 15268, 37329, 45536, 36985,  7834, 45743,  7131,
       15803, 44504, 36911,  4272, 11616,  4300, 39462, 48273, 43014,
       36650, 29030, 11091, 17531,  7366, 47094, 26643, 41165,  7996,
       30262,  3860, 47791, 22504, 30948, 18312, 16700, 46413, 19557,
       48311, 44528, 17933, 39367, 31981,  7440,   602, 20508, 47646,
       25749, 42897, 28115,  6238, 48158, 14821,  4955, 20081, 21298,
        3829,  6995,  5253, 29934, 20530, 13565, 13015, 40919, 40881,
       21671, 24716, 23031, 35419, 43159, 25669, 39419, 34893, 40292,
       11906, 48965, 46152,  5251, 44304, 35388, 48475, 23294,  8007,
        7886,  4761, 45024, 38881, 23281, 43540, 15022, 24286, 11207,
       32861, 27978,  2426, 49631, 39838, 26639, 35169, 14369,  8273,
       28855, 23045,  8952, 47713,  3165, 10245, 43381,  9378, 31625,
       19713, 23148, 21931, 49378, 38255, 13606, 48320, 38045, 20946,
        6337, 46012, 27858, 34212, 13504, 41264, 36443, 27699, 34171,
        5891, 27783,  9192, 36013, 13674, 42870, 21664, 48107,  7716,
       12585, 11149, 39089, 31917, 22518, 47596,   566, 41835, 40409,
       31795, 22462, 35496,  1964, 40470, 18461, 17405,  8852, 37643,
        9975, 26967,  9883, 47827, 35321, 33418, 35181, 47245, 25508,
       46028, 30018, 10120, 36097, 25042,  4283, 35295,  8672,  3384,
       48737, 15491, 20290, 32183, 15241,  2640, 46938, 16090, 22696,
        8468, 45309, 32783, 18444, 44681, 28928, 42933, 18892, 49542,
       46892, 19725, 32897, 44691,  6182, 48120, 24578, 45138, 45994,
       13131, 21596, 27507, 49372, 23728, 16942, 18795, 12951,    92,
       14464, 11461, 33104, 40575,  2176,  8211, 38382, 27252, 22616,
       47609, 24039, 26579, 20100, 20089, 38158, 19472, 34636, 49677,
       38214]), [9, 7, 6, 8]), (array([ 2260,   993, 34763, 37911, 39124, 49703, 43397, 20416,   681,
       21586, 13411, 43886, 11735, 13679, 44223,  1486, 10033, 41400,
       33552, 47211, 20083, 13103, 41587, 15403,  9174, 18552, 43330,
       18732,  3810, 36540, 22470, 45331,  6094, 44268, 15987, 25722,
       25670, 26479, 32255, 45260, 24899, 33136,  7072, 45991,  2657,
       10175,  5049, 12832, 35748, 27188, 20076, 20809, 35438, 45903,
       36952, 42537, 49548, 31303, 42538,  3008, 21889, 49399, 24663,
       39694, 31243, 25065, 38272,  5755, 49327, 43466,  4960, 18685,
       46017, 10897, 14405, 33423, 24226, 37790, 43063, 44312, 10709,
       30684, 32137, 37537, 42924,  5431, 30543, 26738, 39869,  5372,
       37740, 36587,  4319, 15849, 12567, 11953,  2532, 30356, 11495,
       37046, 15686, 12205, 11153, 24440, 19965, 10484,  9295, 23162,
        9729, 20477, 39539,  1691, 22478, 35640, 12480, 48566,  9450,
       30567, 14221, 48139, 41210, 17811, 22742, 49826, 48754, 49288,
        7518, 12716,  7873, 49859,  7699, 14477, 20922, 23233, 19786,
       34252,  2305, 48991, 49431, 23207, 26313, 30723, 19900,  9987,
        4572, 22830, 38997, 49579, 27701, 19437, 24287, 27844, 11314,
       27671,  8223, 24532,  4362,  5208, 39920, 27225,  9815, 38975,
        4153, 28090,  2242,  5780, 34634, 19735, 42686,  5905,  6982,
       13551, 46621, 21465, 15591, 20394,  7545, 31949, 36629, 31155,
       37151,  8730, 34941, 48181,  5962, 30028, 21352,  9337,   624,
       18038, 40014, 40721, 34303, 26028,  1134, 13930, 36246, 13345,
       38437, 19319, 12532, 16610, 39647,  3660,  7773, 19515, 11396,
       31190, 15861, 44740, 43796, 33707, 26728, 46720,  3616, 12135,
        5960,  3966, 35879, 29321, 25658, 18856, 25127, 46246, 21286,
        3249, 17293,  6778, 35410,   534, 39973, 48645, 21598, 21726,
       37486, 24714, 34074, 33313, 27462, 30105, 31781, 39446, 24952,
       44899, 12728, 11347, 15985,  8851, 16934, 47621, 42648, 22966,
       20556, 40288,  8186, 42986, 38741, 38287, 33771,  1812, 46642,
       49956, 21711, 29637, 42345, 36510, 13433, 35126, 17826, 12536,
       13777, 27546, 26630,  7807, 36293, 33829, 37305, 28343, 19337,
       24739, 46516, 25587, 10433, 20695,  1129, 37353, 47133, 31026,
        6976, 27430, 41208, 31999, 18485, 17239, 43122, 22726, 41856,
       33030, 46895, 13655,  8333, 12589, 29243,  4431, 13415,  7878,
       39887, 32850, 19163, 35157, 46214,  4471, 33580,  9446, 31721,
       38007, 41970, 26279, 15326, 44736,  6542, 49549,  7853,  2550,
       45692,  2983, 42602, 48014, 14017,  5707, 16232,  9144, 46527,
       44344,  3569, 35880, 24789,  3118,  7194, 33621,  6744, 17682,
       33932, 22797, 11766, 41890, 24313, 13757,  6488, 14768,   538,
        8264, 18204,  8534,  2377, 43118, 49099,  1500, 38451, 38806,
       26572, 30524, 22734, 26837, 18756, 20880, 19110, 24097, 23347,
        9989, 21316, 12978, 14915, 12669, 35320, 26234,  2291, 47508,
       15940,  7480, 33513,  6477, 31813, 12703, 25939, 40917, 47255,
       14655, 25830, 37578, 39935, 16352,  4375,  1067, 43450,  2091,
       16202, 22667, 35022, 49536, 34149, 31307, 32711, 44186, 43127,
       24643, 36095, 49020,  9350, 37821, 16327,   963, 36184,  7397,
       25650, 41273, 25149,  3423, 24272, 28708, 47903,  6317, 18564,
       21399, 42094, 10440, 36496, 21650, 25195, 36137, 17363, 36355,
       35477,  5781, 37840, 26956, 49274,  7477, 40716, 30493, 21145,
       32693, 37754, 28095, 23209, 24919, 15584, 23181, 17267, 12533,
       15977, 41498,  2033, 42891,  9933, 30791,  2626, 26629, 27764,
       22045,  2467,  8065, 39928, 37361, 26265,  1800,    54, 41599,
       22275, 16102, 39976, 16176, 29942, 42993, 35360, 24399, 44622,
       42532, 33929, 16165, 36242,  6106, 12161, 44652, 39983, 35255,
        1288,  1139, 12607, 31275, 41538,  5012, 31035, 42746, 31020,
       19153,  7929, 43421, 48885,  3476, 24717,  4824, 29253, 37284,
        1402,  4678, 13410, 44405, 11412, 15343, 23517, 45149, 28692,
       49364,  8431, 31342, 23230, 41320,  7137,  1194, 23764, 24176,
       25829, 13389,   347, 19000, 42509, 16008, 29802, 10203, 45453,
       21311, 33704,  3098, 37574,  9360, 48167, 22986, 25638, 33848,
       33536, 32606, 43258, 30526, 24864, 36188, 24743, 21264, 25519,
       11733, 43353, 49061, 27332, 36561, 20930, 21763, 34861, 14830,
       49200, 39200,  3976, 37077, 49747, 18653, 40519, 42291, 12014,
        4875, 40640, 23413, 24282, 27385, 30976, 19456, 18826, 48542,
       31685, 11303, 11429, 16097, 31598, 20830, 27464, 38738, 38747,
       30001,  8009,  4126, 40544, 15211, 31639, 31413, 37970, 49834,
       18571,  7795, 17697, 26139,     0, 20921, 15812, 21747, 12906,
       44163, 15038, 14345,  7073, 39630, 26387, 31808, 41476, 43459,
       26841, 46921, 23762, 46317, 36023,  2915, 15833, 42436, 34242,
       48203, 41064, 34045,  7226, 41621, 25681, 19184,  3290, 17555,
       21200, 19608,  6582,  1085,   755, 33466, 25526, 40437, 44999,
       28088, 46404, 33785, 33402, 13806,   361,  3986, 14027, 18965,
       28588, 33060, 10053,  1131, 28956, 48658, 37759,  1447, 43958,
       12034, 12218, 32949, 48072, 41326, 48047, 12031, 27229,  8589,
       25139, 44772, 12374, 30212, 42484, 36638, 24423, 19420, 34139,
       43986, 23734, 44014, 48022, 45958, 29327, 25815, 18542,  1248,
       41140, 34539,  6252, 20641, 15107, 30666, 27928, 25985, 11358,
       23375, 44128, 47653, 25795, 46066, 17075,  8108, 33924, 14133,
       35506,  5508, 23323, 18794, 33436, 31577, 23927,  7422,  1917,
       49928, 25493,  5122, 16489,  1637,  7393,  7321, 13184, 34716,
        8699, 42562, 44274,  3804, 31798, 32035, 49698, 40850,  7070,
       28490, 33541, 23876, 15489,    22,  2810, 48373, 35600, 37992,
       37933,  7967, 13916, 32628, 24397, 33381, 23563,  6246, 45759,
       33759, 22282, 49361, 16108, 17962,  7281,  1763, 46540, 21699,
       24085, 49090, 15619, 35150, 22010,  5255, 14208, 13350,  8270,
       33398, 12189, 13967, 41647, 25721, 13810, 11866, 20228,  7112,
       17854, 47999, 36636, 24608, 37681, 20467, 46048, 18295, 48125,
       12125, 44807, 40415, 18274, 31565, 18045,  5916, 32038,  3121,
       20395, 26214, 16558, 10572,  9722, 49625, 38239, 11117, 23858,
       41251, 29537, 20749, 35678,  5452, 48920, 16342, 47112, 15617,
       48872, 13738,  2658, 18367, 31187,   793, 38458, 10068, 34404,
       49108, 34834, 26563, 36555, 25780, 36024, 32092, 32356, 21953,
       10354, 17824, 49216, 10228,  9545, 36708, 12258, 10708, 12866,
        5222, 42822, 10395,  4514, 32370, 31553,  3333,  7646, 43913,
       16676,  5667,  4380,  3324, 43191,  2974, 34419,  3269, 11042,
       24631,  5280, 15835, 29762, 35035,  9154, 43035, 33158, 20082,
       21434,  6788,  6631, 27669, 38524,  9913,  1853, 29371, 10688,
       46668, 27640,  1404, 23509,  8401, 31489, 18327, 31066, 13545,
       27747,  1014, 34310, 27902,  8717, 43175, 32234, 16042, 44438,
        9306, 46707, 16830, 19618, 47842, 48124, 18675, 22589, 46292,
       47236, 36744,  5977,  1325, 26927,  1775, 20118, 37808, 35489,
       32275, 25072, 43985, 14281, 34295, 21140, 11200, 14211, 11674,
        2344, 36804, 49180,  4807, 35621, 43101,  6667, 34021,  4863,
        2471, 37060, 18530,   736, 34527, 21884,  5687, 24688, 48794,
       44362,  7046,  4974, 40547, 31052,  6977, 42449, 11798, 30770,
       32501,  1673, 42842, 26239, 14804, 42847, 48374, 10817, 23707,
       13388, 26306, 35808, 20500, 45461,  1468, 18853,  2847, 15688,
       42763, 11141, 43807, 47992,  6632, 18728, 27864, 22000, 49746,
       38374, 41447,  5777, 39015, 27490, 33815, 39098,  3353, 12426,
       47814, 15334, 23838, 19131, 29530, 22471, 40139, 20052, 27386,
       47506,  3950, 46764, 14260, 46854, 18104, 17246, 18501, 31196,
       27988]), [5, 2, 6, 8])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 3, 6, 8], M=tensor([0, 3, 6, 8], device='cuda:0'), Initial Performance: (0.259, 0.04455009317398071)
DC 1, val_set_size=1000, COIs=[1, 4, 6, 8], M=tensor([1, 4, 6, 8], device='cuda:0'), Initial Performance: (0.222, 0.04445403122901916)
DC 2, val_set_size=1000, COIs=[9, 7, 6, 8], M=tensor([9, 7, 6, 8], device='cuda:0'), Initial Performance: (0.222, 0.04478240823745728)
DC 3, val_set_size=1000, COIs=[5, 2, 6, 8], M=tensor([5, 2, 6, 8], device='cuda:0'), Initial Performance: (0.331, 0.04419129967689514)
D00: 1000 samples from classes {8, 6}
D01: 1000 samples from classes {8, 6}
D02: 1000 samples from classes {8, 6}
D03: 1000 samples from classes {8, 6}
D04: 1000 samples from classes {8, 6}
D05: 1000 samples from classes {8, 6}
D06: 1000 samples from classes {0, 3}
D07: 1000 samples from classes {0, 3}
D08: 1000 samples from classes {0, 3}
D09: 1000 samples from classes {0, 3}
D010: 1000 samples from classes {0, 3}
D011: 1000 samples from classes {0, 3}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {9, 7}
D019: 1000 samples from classes {9, 7}
D020: 1000 samples from classes {9, 7}
D021: 1000 samples from classes {9, 7}
D022: 1000 samples from classes {9, 7}
D023: 1000 samples from classes {9, 7}
D024: 1000 samples from classes {2, 5}
D025: 1000 samples from classes {2, 5}
D026: 1000 samples from classes {2, 5}
D027: 1000 samples from classes {2, 5}
D028: 1000 samples from classes {2, 5}
D029: 1000 samples from classes {2, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO1']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.06674954384565353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06847400909662246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.08236931937932968) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09157281869649887) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.445, 0.06579716289043426) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.08552074305713177) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.08714920091629029) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.307, 0.12057992538809777) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.08181295055150986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.10531247013807296) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.10465574797987938) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.16387051099538802) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.45, 0.11633282428979874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.385, 0.123072186216712) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.1520442082285881) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.16512881445884706) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.14339558928459883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.415, 0.1408000374212861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.1840640387907624) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.20105758833885193) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO3']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.441, 0.16513536177948118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.414, 0.16343627533689142) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.2134882289879024) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2007472256422043) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.16899262822791933) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.1821278923470527) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.2760615418329835) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.20005470670759679) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.19086242514848709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.17739180225878953) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.30164285308122635) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.2237542724609375) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.2003168370425701) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.19552786941453815) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.34175975008215753) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.2044250428378582) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.454, 0.20181888235360385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.18588235468231143) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.350616532756947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.258475079447031) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[8, 6], M=tensor([0, 1, 3, 4, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.5806666666666667, 0.01833649750550588)
DC Expert-0, val_set_size=500, COIs=[0, 3], M=tensor([0, 3, 6, 8], device='cuda:0'), Initial Performance: (0.908, 0.0079437797665596)
DC Expert-1, val_set_size=500, COIs=[1, 4], M=tensor([1, 4, 6, 8], device='cuda:0'), Initial Performance: (0.926, 0.006831320371478796)
DC Expert-2, val_set_size=500, COIs=[9, 7], M=tensor([9, 7, 6, 8], device='cuda:0'), Initial Performance: (0.948, 0.003796410886570811)
SUPER-DC 0, val_set_size=1000, COIs=[0, 3, 6, 8], M=tensor([0, 3, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[1, 4, 6, 8], M=tensor([1, 4, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[9, 7, 6, 8], M=tensor([9, 7, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7f5890104d00>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f58900b5b50>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f5890532370>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f5840269220>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f589041b160>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO4', '(DO1']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.912, 0.007255169728770852) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0029795912271365524) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.0045947363637387755) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.22327490542829037) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9333333333333333, 0.005606268229583899) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.565, 0.04837567429244518) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.686, 0.029215000674128532) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.675, 0.03301486298441887) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.0066611340269446375) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.005049905013758689) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.004344784004613757) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.2142805718034506) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9413333333333334, 0.007252848835157541) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.0941788628231734) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.568, 0.06170943382382393) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.644, 0.043320400526747106) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.916, 0.007122744332998991) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.004799086853861809) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.0038941760836169125) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.19172411385178567) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.944, 0.006126068588967124) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.07650182315707207) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.581, 0.0594856814481318) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.664, 0.039232474178075794) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.892, 0.00941608534520492) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.0035221209339797496) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.0048598562225233765) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.19290498964488506) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9393333333333334, 0.008792648356718322) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.10919212925457396) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.572, 0.06705497713014484) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.592, 0.06012545660883188) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.0073305097930133345) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.004458538534585386) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.004412585165817291) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.17890833443403245) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.01016126552845041) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.09538781544100493) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.601, 0.055500910259783265) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.644, 0.04414730958640575) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO1', '(DO2']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.00803844388294965) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.0063755959856789555) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.003585925376508385) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.418, 0.1971931256055832) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.00552258104008312) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.532, 0.09081789335375652) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.602, 0.056846493992954494) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.637, 0.05266123470664024) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.007247998108621687) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003137999368598685) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.004189463436603546) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.19292931766808033) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.006503363677921394) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.08346897722873836) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.608, 0.05801606637984514) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.621, 0.056947760611772535) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.007024829853791744) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.005164401960908435) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.00485375245474279) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.18448201805353165) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.007859257991658524) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.522, 0.07666051962226629) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.605, 0.05444455160200596) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.623, 0.05508329942077398) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.008629055072553456) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0026695921670179816) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.00457021311018616) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.17770866037905217) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9466666666666667, 0.0060620840893437465) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.05365818162634969) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.695, 0.033079365253448484) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.712, 0.0331192337423563) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.007868318121880293) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.0033809929564595225) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.95, 0.005135999568738043) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.18113979667425156) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.011012319475179538) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.512, 0.10998370692739264) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.62, 0.059017643082886935) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.592, 0.06421335177309812) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO2', '(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.916, 0.008498148101614788) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.003890503357164562) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.005309731833986007) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.16740202176570892) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.008848294707480818) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.519, 0.09799581409804523) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.594, 0.0688250780235976) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.594, 0.06436445288732648) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.008820113993017003) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0025241107041947546) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.005064508631941862) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.17672490406036376) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9413333333333334, 0.010399381905732297) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.0855368856433779) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.618, 0.06320865912828594) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.633, 0.05447662391699851) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.008812251524999738) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.004152388790622354) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.004645481526036747) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.422, 0.18074643567204476) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.012188790275818368) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.09961614182335325) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.593, 0.07453652673587204) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.605, 0.06407698018103838) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.008109999543521554) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0029881861123722047) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.006588086205942091) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.18230143877863883) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9446666666666667, 0.012205786568447593) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.546, 0.07350270325224846) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.627, 0.06429205311834812) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.599, 0.06095473539270461) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.008234774322248995) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0033620255910791455) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.005719500411069021) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.412, 0.17182778790593148) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.010970826714986894) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.0939593766424805) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.606, 0.06833540823310613) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.593, 0.06765483326651156) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO0', '(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.01027793958131224) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.004170293899253011) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.005804122090456076) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.417, 0.16921214136481286) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.005234690303836638) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.584, 0.056852304585278034) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.658, 0.04567189763486385) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.693, 0.036004826188087466) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.008753772970987483) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.004506397960241884) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.004725708593381569) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.16903848883509637) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9233333333333333, 0.011231525167822838) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.052939574971795085) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.06234895914793014) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.638, 0.0586801427602768) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.008857181477127597) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.0063792277921456845) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.004323130341013894) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.17517433831095697) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.0077729324313501515) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.07042117165774107) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.07738338910788298) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.611, 0.05958739610016346) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.007260899603832513) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.005035180082428269) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.004412174826225964) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.174420688778162) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.008484351339449253) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.08422682508267462) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.582, 0.07170489295572043) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.588, 0.06922905408870429) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.009151501273270697) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.006077860769815743) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.004726699147955514) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.1634615177512169) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9373333333333334, 0.009889073949617645) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.07864994550682604) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.593, 0.07209483430162072) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.633, 0.06315413703396916) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO4', '(DO0']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.009234819649136625) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003164304011501372) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.004618087823881069) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.16163127398490906) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.008866382135039506) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.10061211348557844) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.62, 0.060813128292560574) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.572, 0.07708702034130692) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.005898641213774681) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0032643814629409462) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.95, 0.005250280672218651) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.417, 0.16220841838419436) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9433333333333334, 0.01187743338434302) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.496, 0.12948678012704476) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.581, 0.08894001220818609) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.582, 0.0880837842868641) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.006728767290711403) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.004367491361394059) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.005234330665785819) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.17361586628854275) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9326666666666666, 0.012851616136641193) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.508, 0.1124705424355925) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.575, 0.07991236016154289) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.561, 0.0882981654945761) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.914, 0.010256552762584761) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003219772583222948) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.006498096485738642) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.1643364751636982) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9433333333333334, 0.009637509045239615) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.512, 0.10261902518407441) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.594, 0.060747210122644904) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.593, 0.07291917092725635) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.008130029774270952) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0028795646261423826) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.962, 0.003723117890767753) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.409, 0.15691612316668033) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.012178355479180027) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.09933858882600907) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.555, 0.08753508174419403) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.56, 0.09258980934449937) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO4', '(DO0']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.00690991960465908) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0031816782969981434) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.005749669378623366) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.18941594794392586) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9513333333333334, 0.0052775735805432) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.571, 0.06920946408808232) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.685, 0.03889874935150146) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.697, 0.03872319565713406) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.007841447999235243) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002928189545869827) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.005226674173027277) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.417, 0.1842923045605421) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.009424331613229394) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.08164496971014887) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.629, 0.07697792779747396) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.571, 0.07308937680721284) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.007606329925823957) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003858725355588831) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.0035041017755866053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.417, 0.16322849947214127) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.011462695758360496) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.0932103862897493) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.638, 0.04207170046120882) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.609, 0.06675453826971352) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.008228651753626763) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0030829539518454113) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.005055188960395754) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.16473433858156203) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9593333333333334, 0.007648542989954876) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.53, 0.10236603848729282) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.05879679002985358) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.599, 0.07205332627519966) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.00899857029458508) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.0052104766361735525) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.005368783237878233) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.1900050077587366) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.009437635940101852) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.515, 0.12679754999047146) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.09408282973803579) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.563, 0.08535240093618632) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO4', '(DO0']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.007058689777273685) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002895028971717693) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.005209033152088523) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.1702455208748579) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9626666666666667, 0.004156149773315216) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.554, 0.08491135327517986) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.643, 0.06528881345316767) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.639, 0.05414872235804796) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.008443412756547333) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.0057410756080353165) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.95, 0.005075333470478654) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.409, 0.17765034222602843) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9586666666666667, 0.007310486656069164) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.548, 0.09766761643067003) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.07957890331745147) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.581, 0.08499469693750143) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.00903574850037694) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.0031317403831053526) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.004959973522694781) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.1787271652072668) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.00656079203200837) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.557, 0.08727582570165396) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.658, 0.04611155703663826) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.678, 0.045733988687396046) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.008670681800693273) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.003807851926307194) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.005386390796134947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.20416346794366835) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9546666666666667, 0.006311112513280629) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.544, 0.07666600264701992) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.631, 0.049570929549634456) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.632, 0.05875406300276518) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.008962234919890762) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0024879420686629602) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.0039110625343164425) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.2052458892017603) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.01237318904949666) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.523, 0.10916674043051898) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.0755607700832188) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.564, 0.090324730431661) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO4', '(DO2']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.010184163443744183) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002604419375071302) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.962, 0.003782491224032128) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.20062419433891773) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.005029515923544144) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.594, 0.06279932164959609) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.638, 0.04725382515415549) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.616, 0.06097680120170117) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.008266059630550444) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003391576462658122) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.0037218317920342087) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.1983733761012554) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9566666666666667, 0.005477143038141852) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.525, 0.09419346514716745) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.663, 0.04884389697760343) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.668, 0.04980186225473881) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.008274158077314495) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.004093959836638532) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.005367971709027188) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.417, 0.19242627930641173) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.009607504976955169) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.538, 0.08398757932335138) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.623, 0.05828639390319586) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.657, 0.0459105466529727) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.00936586746503599) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0030112212628591807) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.004695828396594152) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.1889318780750036) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9586666666666667, 0.007078632651968898) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.541, 0.09582243512762943) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.596, 0.08041674907878041) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.614, 0.0691677539292723) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.00948144078720361) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0035154482075777198) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.005137842012860347) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.208373410820961) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9586666666666667, 0.00715331280293564) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.546, 0.10750049879495055) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.613, 0.056294351994991304) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.643, 0.061876045003533366) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.259, 0.04455009317398071), (0.269, 0.06674954384565353), (0.445, 0.06579716289043426), (0.451, 0.08181295055150986), (0.45, 0.11633282428979874), (0.447, 0.14339558928459883), (0.441, 0.16513536177948118), (0.452, 0.16899262822791933), (0.453, 0.19086242514848709), (0.448, 0.2003168370425701), (0.454, 0.20181888235360385), (0.565, 0.04837567429244518), (0.516, 0.0941788628231734), (0.521, 0.07650182315707207), (0.488, 0.10919212925457396), (0.533, 0.09538781544100493), (0.532, 0.09081789335375652), (0.542, 0.08346897722873836), (0.522, 0.07666051962226629), (0.562, 0.05365818162634969), (0.512, 0.10998370692739264), (0.519, 0.09799581409804523), (0.531, 0.0855368856433779), (0.521, 0.09961614182335325), (0.546, 0.07350270325224846), (0.534, 0.0939593766424805), (0.584, 0.056852304585278034), (0.604, 0.052939574971795085), (0.583, 0.07042117165774107), (0.543, 0.08422682508267462), (0.554, 0.07864994550682604), (0.521, 0.10061211348557844), (0.496, 0.12948678012704476), (0.508, 0.1124705424355925), (0.512, 0.10261902518407441), (0.535, 0.09933858882600907), (0.571, 0.06920946408808232), (0.562, 0.08164496971014887), (0.518, 0.0932103862897493), (0.53, 0.10236603848729282), (0.515, 0.12679754999047146), (0.554, 0.08491135327517986), (0.548, 0.09766761643067003), (0.557, 0.08727582570165396), (0.544, 0.07666600264701992), (0.523, 0.10916674043051898), (0.594, 0.06279932164959609), (0.525, 0.09419346514716745), (0.538, 0.08398757932335138), (0.541, 0.09582243512762943), (0.546, 0.10750049879495055)]
TEST: 
[(0.26825, 0.04347268170118332), (0.26725, 0.06421797105669975), (0.4395, 0.06346339270472527), (0.4565, 0.07869957354664803), (0.4475, 0.11173120200634003), (0.457, 0.1374445474743843), (0.4525, 0.15964710181951522), (0.458, 0.1622095382809639), (0.46325, 0.18300087195634843), (0.46175, 0.19399894100427628), (0.45875, 0.1961120474934578), (0.5495, 0.04977467487752438), (0.511, 0.09559043541550637), (0.53275, 0.07373991534113884), (0.4925, 0.10782868951559067), (0.52675, 0.10007699501514435), (0.54525, 0.08847769132256508), (0.56825, 0.08243420594930649), (0.541, 0.07296912284195423), (0.57525, 0.053261602565646174), (0.51475, 0.11220608416199684), (0.521, 0.09829267263412475), (0.54275, 0.08748745228350162), (0.51925, 0.0988691494166851), (0.54225, 0.0745389994084835), (0.54025, 0.09505030313134194), (0.57575, 0.05897627727687359), (0.6065, 0.051163242787122726), (0.5665, 0.07047172498703003), (0.5405, 0.08699801802635193), (0.545, 0.07823972263932229), (0.51725, 0.1035067440867424), (0.49925, 0.12686356884241104), (0.51675, 0.1075433152616024), (0.51675, 0.10322817954421043), (0.52925, 0.09920853719115258), (0.55925, 0.07340634950995445), (0.5565, 0.0847692058980465), (0.519, 0.09383122235536576), (0.532, 0.10344324374198914), (0.51325, 0.13027625054121017), (0.55625, 0.08620090058445931), (0.53575, 0.10015857902169227), (0.547, 0.09514812397956848), (0.555, 0.07665798342227936), (0.51725, 0.10717447999119759), (0.59425, 0.06350753706693649), (0.53175, 0.0955282930135727), (0.54525, 0.08230336892604828), (0.5365, 0.09769800618290901), (0.531, 0.10983981063961983)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.89      0.06      0.12      1000
           3       0.67      0.23      0.34      1000
           6       0.56      0.89      0.69      1000
           8       0.47      0.95      0.63      1000

    accuracy                           0.53      4000
   macro avg       0.65      0.53      0.44      4000
weighted avg       0.65      0.53      0.44      4000

Collaboration_DC_1
VAL: 
[(0.222, 0.04445403122901916), (0.25, 0.06847400909662246), (0.256, 0.08552074305713177), (0.33, 0.10531247013807296), (0.385, 0.123072186216712), (0.415, 0.1408000374212861), (0.414, 0.16343627533689142), (0.442, 0.1821278923470527), (0.462, 0.17739180225878953), (0.471, 0.19552786941453815), (0.463, 0.18588235468231143), (0.686, 0.029215000674128532), (0.568, 0.06170943382382393), (0.581, 0.0594856814481318), (0.572, 0.06705497713014484), (0.601, 0.055500910259783265), (0.602, 0.056846493992954494), (0.608, 0.05801606637984514), (0.605, 0.05444455160200596), (0.695, 0.033079365253448484), (0.62, 0.059017643082886935), (0.594, 0.0688250780235976), (0.618, 0.06320865912828594), (0.593, 0.07453652673587204), (0.627, 0.06429205311834812), (0.606, 0.06833540823310613), (0.658, 0.04567189763486385), (0.6, 0.06234895914793014), (0.557, 0.07738338910788298), (0.582, 0.07170489295572043), (0.593, 0.07209483430162072), (0.62, 0.060813128292560574), (0.581, 0.08894001220818609), (0.575, 0.07991236016154289), (0.594, 0.060747210122644904), (0.555, 0.08753508174419403), (0.685, 0.03889874935150146), (0.629, 0.07697792779747396), (0.638, 0.04207170046120882), (0.604, 0.05879679002985358), (0.526, 0.09408282973803579), (0.643, 0.06528881345316767), (0.558, 0.07957890331745147), (0.658, 0.04611155703663826), (0.631, 0.049570929549634456), (0.584, 0.0755607700832188), (0.638, 0.04725382515415549), (0.663, 0.04884389697760343), (0.623, 0.05828639390319586), (0.596, 0.08041674907878041), (0.613, 0.056294351994991304)]
TEST: 
[(0.218, 0.04341068071126938), (0.25, 0.06590802815556526), (0.252, 0.08200803625583648), (0.3275, 0.10096185243129731), (0.392, 0.1177846822142601), (0.42225, 0.1349279899597168), (0.42825, 0.15713550812005997), (0.44525, 0.175697412610054), (0.46675, 0.1712297403216362), (0.47075, 0.19024631458520888), (0.4625, 0.18031305837631226), (0.685, 0.029159952238202096), (0.569, 0.06417778657376766), (0.566, 0.06076032149791717), (0.56425, 0.07154972219467164), (0.60275, 0.05791234554350376), (0.58725, 0.06116499620676041), (0.612, 0.06290311573445798), (0.5895, 0.055748546198010444), (0.7015, 0.03283318953961134), (0.6085, 0.06168393960595131), (0.57725, 0.07114417135715484), (0.626, 0.06543067048490048), (0.5965, 0.07789581063389778), (0.63275, 0.06461484941840172), (0.61425, 0.06950139153003693), (0.64875, 0.04644225464761257), (0.59075, 0.0650510419011116), (0.559, 0.07674079266190528), (0.58125, 0.0718227561712265), (0.60175, 0.075723264336586), (0.6235, 0.06268754447996616), (0.57775, 0.09083449873328209), (0.5715, 0.08459611204266548), (0.60075, 0.06183120602369308), (0.55525, 0.09020967984199524), (0.674, 0.039512124843895435), (0.62025, 0.07999990133941173), (0.63025, 0.04362553545832634), (0.59975, 0.06202509140968323), (0.53925, 0.09796790614724159), (0.632, 0.06947692255675793), (0.5645, 0.08198828357458114), (0.63875, 0.046837043061852456), (0.62175, 0.05312574401497841), (0.5645, 0.08045463493466377), (0.62725, 0.0501638792604208), (0.6555, 0.04893200145661831), (0.5995, 0.05871354511380195), (0.58525, 0.080818444699049), (0.61125, 0.05651152065396309)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.90      0.28      0.43      1000
           4       0.60      0.57      0.59      1000
           6       0.62      0.68      0.65      1000
           8       0.56      0.91      0.69      1000

    accuracy                           0.61      4000
   macro avg       0.67      0.61      0.59      4000
weighted avg       0.67      0.61      0.59      4000

Collaboration_DC_2
VAL: 
[(0.222, 0.04478240823745728), (0.442, 0.08236931937932968), (0.397, 0.08714920091629029), (0.462, 0.10465574797987938), (0.472, 0.1520442082285881), (0.475, 0.1840640387907624), (0.472, 0.2134882289879024), (0.468, 0.2760615418329835), (0.469, 0.30164285308122635), (0.479, 0.34175975008215753), (0.474, 0.350616532756947), (0.675, 0.03301486298441887), (0.644, 0.043320400526747106), (0.664, 0.039232474178075794), (0.592, 0.06012545660883188), (0.644, 0.04414730958640575), (0.637, 0.05266123470664024), (0.621, 0.056947760611772535), (0.623, 0.05508329942077398), (0.712, 0.0331192337423563), (0.592, 0.06421335177309812), (0.594, 0.06436445288732648), (0.633, 0.05447662391699851), (0.605, 0.06407698018103838), (0.599, 0.06095473539270461), (0.593, 0.06765483326651156), (0.693, 0.036004826188087466), (0.638, 0.0586801427602768), (0.611, 0.05958739610016346), (0.588, 0.06922905408870429), (0.633, 0.06315413703396916), (0.572, 0.07708702034130692), (0.582, 0.0880837842868641), (0.561, 0.0882981654945761), (0.593, 0.07291917092725635), (0.56, 0.09258980934449937), (0.697, 0.03872319565713406), (0.571, 0.07308937680721284), (0.609, 0.06675453826971352), (0.599, 0.07205332627519966), (0.563, 0.08535240093618632), (0.639, 0.05414872235804796), (0.581, 0.08499469693750143), (0.678, 0.045733988687396046), (0.632, 0.05875406300276518), (0.564, 0.090324730431661), (0.616, 0.06097680120170117), (0.668, 0.04980186225473881), (0.657, 0.0459105466529727), (0.614, 0.0691677539292723), (0.643, 0.061876045003533366)]
TEST: 
[(0.23925, 0.04380939894914627), (0.4305, 0.07938512188196183), (0.4055, 0.0837293835580349), (0.463, 0.1001882000863552), (0.472, 0.1443197346329689), (0.477, 0.17273092937469484), (0.4735, 0.20166757559776305), (0.474, 0.25924735152721406), (0.47175, 0.28262863409519196), (0.47725, 0.32329213321208955), (0.47725, 0.3337722965478897), (0.67625, 0.03117661914974451), (0.65525, 0.03914665672928095), (0.67725, 0.03646316882222891), (0.581, 0.05993762762844562), (0.6755, 0.04116712801158428), (0.6575, 0.05237245854735374), (0.63575, 0.05448779053986073), (0.65225, 0.049914846688508985), (0.71625, 0.030767568081617354), (0.6085, 0.05999754670262337), (0.598, 0.060082985326647756), (0.6445, 0.05002840439975262), (0.6195, 0.05965303549170494), (0.61975, 0.05607009264826775), (0.6095, 0.06316820484399796), (0.712, 0.0319782198369503), (0.62775, 0.054079586014151575), (0.61825, 0.057560755014419554), (0.59925, 0.0644918974339962), (0.64125, 0.058532107070088386), (0.5745, 0.07737548702955246), (0.574, 0.0864815345108509), (0.57575, 0.08465290421247482), (0.60925, 0.06837239947915077), (0.56875, 0.09042377425730229), (0.7215, 0.035697387211024764), (0.5815, 0.06950854766368866), (0.59375, 0.06746444204449653), (0.60975, 0.0669940188229084), (0.56025, 0.08163899314403535), (0.64625, 0.05100127559155226), (0.58125, 0.08411893859505654), (0.67875, 0.04458537305891514), (0.64475, 0.05517242027819157), (0.5715, 0.08853838548064231), (0.6315, 0.05960036888718605), (0.68425, 0.04856315588951111), (0.67125, 0.044961512751877306), (0.627, 0.0653349150121212), (0.65, 0.06359750798344613)]
DETAILED: 
              precision    recall  f1-score   support

           6       0.70      0.84      0.77      1000
           7       0.84      0.61      0.71      1000
           8       0.52      0.94      0.67      1000
           9       0.81      0.21      0.33      1000

    accuracy                           0.65      4000
   macro avg       0.72      0.65      0.62      4000
weighted avg       0.72      0.65      0.62      4000

Collaboration_DC_3
VAL: 
[(0.331, 0.04419129967689514), (0.257, 0.09157281869649887), (0.307, 0.12057992538809777), (0.398, 0.16387051099538802), (0.394, 0.16512881445884706), (0.405, 0.20105758833885193), (0.408, 0.2007472256422043), (0.413, 0.20005470670759679), (0.419, 0.2237542724609375), (0.413, 0.2044250428378582), (0.404, 0.258475079447031), (0.398, 0.22327490542829037), (0.404, 0.2142805718034506), (0.411, 0.19172411385178567), (0.408, 0.19290498964488506), (0.419, 0.17890833443403245), (0.418, 0.1971931256055832), (0.407, 0.19292931766808033), (0.407, 0.18448201805353165), (0.413, 0.17770866037905217), (0.413, 0.18113979667425156), (0.416, 0.16740202176570892), (0.402, 0.17672490406036376), (0.422, 0.18074643567204476), (0.415, 0.18230143877863883), (0.412, 0.17182778790593148), (0.417, 0.16921214136481286), (0.41, 0.16903848883509637), (0.414, 0.17517433831095697), (0.402, 0.174420688778162), (0.415, 0.1634615177512169), (0.415, 0.16163127398490906), (0.417, 0.16220841838419436), (0.401, 0.17361586628854275), (0.413, 0.1643364751636982), (0.409, 0.15691612316668033), (0.414, 0.18941594794392586), (0.417, 0.1842923045605421), (0.417, 0.16322849947214127), (0.408, 0.16473433858156203), (0.404, 0.1900050077587366), (0.405, 0.1702455208748579), (0.409, 0.17765034222602843), (0.403, 0.1787271652072668), (0.408, 0.20416346794366835), (0.403, 0.2052458892017603), (0.413, 0.20062419433891773), (0.4, 0.1983733761012554), (0.417, 0.19242627930641173), (0.413, 0.1889318780750036), (0.406, 0.208373410820961)]
TEST: 
[(0.3205, 0.04325020852684975), (0.2555, 0.08785385882854461), (0.29475, 0.11601749670505523), (0.39875, 0.15733955764770508), (0.3965, 0.1590762510895729), (0.403, 0.1939288192987442), (0.40575, 0.19325960141420365), (0.40525, 0.19291542088985444), (0.412, 0.21559283995628356), (0.412, 0.19505343741178513), (0.411, 0.25018797516822816), (0.40175, 0.21473255121707915), (0.4, 0.20545728492736817), (0.414, 0.18379663813114167), (0.41125, 0.186723356962204), (0.41975, 0.17385013657808304), (0.4135, 0.19159697329998016), (0.40675, 0.18618313026428224), (0.40675, 0.1781819943189621), (0.41, 0.17237879014015198), (0.41325, 0.17569975984096528), (0.41375, 0.1608375635743141), (0.40025, 0.17253344470262527), (0.4135, 0.17356922018527984), (0.4125, 0.17569153940677643), (0.4115, 0.16455136144161225), (0.4165, 0.16152953416109084), (0.41625, 0.16157008588314056), (0.419, 0.16659407728910447), (0.4145, 0.16530023223161697), (0.411, 0.15852338790893555), (0.41075, 0.15524127626419068), (0.41675, 0.15622621220350266), (0.40975, 0.16534712994098663), (0.42025, 0.156771759390831), (0.417, 0.1522460241317749), (0.41275, 0.18216633719205858), (0.4135, 0.17718098616600036), (0.4165, 0.1581594750881195), (0.40375, 0.1601212573647499), (0.40975, 0.18242869037389756), (0.40725, 0.16292774707078933), (0.41225, 0.17012973946332932), (0.41325, 0.17081906461715698), (0.41525, 0.19424452698230743), (0.40975, 0.19567395651340486), (0.41725, 0.19292908865213393), (0.4135, 0.19079034024477004), (0.419, 0.18648632431030274), (0.4135, 0.1835009788274765), (0.40825, 0.19804582500457762)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.32      0.91      0.47      1000
           5       0.62      0.73      0.67      1000
           6       0.00      0.00      0.00      1000
           8       0.00      0.00      0.00      1000

    accuracy                           0.41      4000
   macro avg       0.24      0.41      0.29      4000
weighted avg       0.24      0.41      0.29      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [72]
name: alliance-3-dcs-72
score_metric: contrloss
aggregation: <function fed_avg at 0x719a927c5c10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=72
Partitioning data
[[7, 9, 1, 2], [6, 8, 1, 2], [4, 3, 1, 2], [0, 5, 1, 2]]
[(array([ 2102, 37010, 13182, 10158, 47275, 19636,  1365, 30216, 31980,
       36407, 45523, 26661, 26983, 44664,  5047,  5597, 35724, 26606,
       14517, 47983, 18828, 43060, 47323, 24898, 36346,  3578, 40182,
        3284, 13516, 13280,  6320, 20701, 22128, 10684, 23000, 17462,
          68, 29789, 31528, 24853,  5774, 34010, 34440, 10045, 24682,
       25310, 45343, 18922, 26291, 29644, 16477,  7598, 31321, 19599,
       15067, 47898, 19909, 28764, 44892, 35176, 40465,  7291, 29962,
         440,   589, 43892, 40340, 16350,  6999, 14202, 34511, 12474,
       30876, 48837, 15999, 42355, 35082, 12844,  3251,  7717, 23621,
       43124, 41850, 24698, 38512,  4809,  8984, 14489, 49610, 37131,
       21051, 10573,  6726, 45708, 16224, 17876, 47372,  5313, 29047,
        6822, 21787, 20581, 24995,  6653, 45054, 11484, 35108, 37019,
       18417,   152, 44440, 31364,  7055, 24013, 18528, 45480, 21394,
       15472,  2207, 32660, 14261, 23682,  7798, 12320, 27529, 10347,
       45747, 44567, 12941, 44135, 31871, 35812, 16828, 43449, 28977,
        5101, 27043, 15013, 35220, 31589, 38616, 43435, 24327, 44661,
       48858,  8521, 29910, 35336,  1742, 12272, 16996, 15712, 40234,
       32211, 40360, 25701, 15324, 21553, 23561, 23627,  8076, 36895,
       10886, 18405, 34015, 37482, 39949, 17537, 18017, 32646, 38826,
       44718, 31156, 10666, 13268, 19159, 39246, 15292, 27697, 21396,
         641, 19940, 12864, 11068,  2366, 26567,  3213,  6239, 28695,
        8887, 38155, 49071,  9279,  4729, 38955, 29272, 30330, 22846,
       26032, 20094, 14144, 49592,   994, 37222,  2801, 34107,  5200,
       19821, 24944, 44262,  1806, 32282, 41319, 15720, 20681,  5241,
       39259, 47092, 17942, 14242, 41129, 11621, 25283, 30163, 49363,
       30030,  1445, 42117, 12744, 49760, 16022, 21876,   789, 40061,
       28832,  7450, 13818, 32061, 40942, 42457, 41784, 32633, 46301,
        3157, 38042, 15271, 23292, 30845, 10041, 11553, 41280,  9606,
       16482, 31378, 47743,  9626, 40599, 48816, 23169, 11312, 22643,
       29228, 38056, 35813, 29306,  2199, 18344, 11876, 30080, 43946,
       27591,  7698, 37993, 12427, 35984, 16601, 23336, 10163, 10144,
       17466, 10231, 14892, 14250, 27551, 20552, 39581, 11258,  6149,
       32040, 17359, 47574, 12555, 16684, 32552, 11472, 23720,  2935,
        1232, 13969, 41765,  8089, 31722, 42407, 43960, 37249, 25303,
       36291, 20947, 14224, 18020, 28189,   102, 39174, 16809, 14913,
       36413, 48332, 33856, 25387, 40043, 45465, 42278, 33813, 23303,
       36434,  8821, 40015, 26229, 44678, 35184, 24712, 30850, 11292,
       15694,  8125,  5332, 45547, 48284,  1765, 44692, 30966, 33437,
       21408, 22156, 19193, 16744, 28846, 46646, 19122,  1638, 40413,
       36475, 12835, 13380, 40059, 21330, 49398, 42638,  2637, 41614,
         668, 45906, 27184, 32945, 38280, 26574, 35667, 32820, 18703,
       12385, 23426, 42106, 24152, 25407, 16054, 40387,  5591, 32112,
       28053, 15263, 16999, 35498, 16092, 30678, 12562, 40790, 19053,
       41495,  8841, 25091, 38267,  7461, 13370, 13881, 42385, 43346,
       16773,  5306, 49823, 39841, 46059, 41630, 47148, 14599, 16170,
       37600, 13869, 36577, 34773, 16725, 10448, 14424,  3175, 23908,
        3143, 20481, 37400, 42208, 36210, 30932, 11889, 44881,  9091,
       16262, 11173, 26647, 14197, 40052, 25752, 26262,  3702, 13529,
       36179, 33417, 16699, 34024, 22802, 23023, 22464, 34474,  9959,
       17945, 48210, 28684, 42851, 37730, 30420, 24938, 26468,  2058,
       10943, 47121,  1987, 16535, 27784, 36918, 24718, 34290,  6708,
        8749, 20176, 12288, 25988, 17097, 42973, 30351, 35379, 16688,
       34485, 24086, 20718, 17526, 12709, 22296,  5668, 37780, 46190,
        3648, 14825, 21696, 34078, 28046, 22240, 29148, 13475, 36384,
       36031, 36649, 33868, 18567, 19120, 37285, 41408, 26620, 28549,
        8146, 34427, 47175, 48577, 23877, 40152, 22533,  7992,  6371,
       11075, 27228,  6813,   743, 20619, 28526,  7513, 46904, 41948,
       16755, 16089, 11791, 34400, 14912, 41695, 37592, 26290,  2469,
       10882, 41392, 37929, 41847, 40076, 18321, 39902, 13323, 43636,
       12169, 49286, 41262, 49085, 39391, 19866, 46741, 48746, 25050,
       12629, 22938, 37844,  2431, 11804, 33034, 24292, 27174, 12064,
        8920, 49196,  4696, 33748, 43847, 10236, 30463, 36334,  9327,
       28347, 38596, 46207,  7358, 32994, 31395, 31208, 49993,  3830,
        7841,  8244,  4654,  3059, 10627, 21593, 40449, 32418, 10522,
       46259, 22747, 36672, 37611, 49426, 25279,  5816, 44886, 38919,
       28559, 49221,  6971, 43297, 49060, 20453,  2286, 10747, 21585,
       24010, 25201, 25488, 10790,  3467, 12373, 31407, 12814,  4145,
       11403, 13523, 44475, 38270, 11130, 47616,   427, 29019, 19667,
       38204, 32011, 21283, 13332, 46674, 18844, 39405, 45289, 15669,
       19446, 35335, 36891, 24008,  2727, 21215, 45599, 43240, 21871,
       33105,  1946, 24024, 16223, 18246, 34673, 45105, 28187, 28688,
       16275, 35666, 30050, 30696, 12917,  5146,  2769, 43068, 35757,
       22145,  9252,  8570, 29315,  5257, 10569, 31552, 28393, 22788,
       21850,  3273, 24194, 44448, 23263, 33292, 37616, 21829, 22636,
       48275, 22833, 47279, 22552, 35058,  1389, 39118, 49151, 20865,
       32681, 16956, 22553,  5769, 22659, 12981, 40145,   848,  8124,
       17091, 42130, 29264, 19454,  1565, 41338, 35207, 31599, 19164,
       49411,    96, 38892, 43932,  7649, 42734, 41663, 26794, 21423,
       44235, 49849, 31300, 20336, 13858, 14844, 15950, 31957, 14717,
        5282, 30120,   226, 47495, 27730, 14538,  8938, 38464, 11822,
       28552, 48234, 18926, 17586, 31297, 16613,  1541, 45687, 28532,
         834, 11574, 34349, 37460, 23607,   262, 39333, 10953, 34145,
       20368, 40220, 13468, 38493, 25910, 22399, 46514,  8191, 43600,
       41087, 29522, 24828, 14423, 29042, 21654, 40327, 48651, 11630,
       20125, 12992, 30767,  9903, 31467, 25161,  6390, 47675, 37581,
        7783, 34566, 19998, 44053,  5603, 20149, 38186,   463, 22803,
       48090, 25695, 32387,  1777, 24457, 48388, 47730, 33699, 32570,
       48594, 30795, 27328, 36778, 36583, 12180,   403, 20876, 45550,
       28705, 35230,  1415, 27430, 45109, 44216, 41764, 10480, 27816,
       49156, 18149, 32420, 26992, 26990, 34293, 19874,  4896, 48012,
        4700, 30531, 45428,  5634,  9580, 33851, 27325,  4168, 34187,
       14588, 43856,  5384,  4220, 43823, 31307,  6558, 47846, 41469,
       42428,  4511, 14814,  8077, 39436, 29612, 35764, 33503, 15813,
       31090, 42155, 38830,  4398, 40231, 49342, 13179, 36245, 42158,
        7868, 16739,  6146,  9001, 27091, 42815, 44842, 25263, 36657,
       22280, 45229, 22006, 25530, 25831, 29715, 39765,  1789, 16123,
       26321, 21650, 15477, 12748, 14063, 23670, 39648, 36465, 44758,
       26982,  1995,   689, 46877, 40860, 12860, 37647, 35880,  3416,
        3760, 43469,   933, 30005, 36355,  5781, 45238, 21500, 42020,
       10935,  8148, 37947,  6616, 37280, 19070, 17344, 21102,  6917,
       18184, 21018, 15820, 27973, 46082, 18287, 22770, 21392,  4656,
       18241, 34181,  1676, 40221,  3484, 31475, 10018, 34608, 45074,
       35319,  9827,  1077, 38502, 36094, 35482, 39390, 28788, 28664,
       48870, 30705, 32166,  6502,  1497, 14834, 43287, 10474, 22375,
       43593,   281, 19714, 23241, 49328, 42564, 18756, 45342,  4242,
       48348, 45855, 49449, 11923,    41, 42234,  7480,  6388, 34497,
       38457, 11606, 38480, 24780,  9455, 15731,  6799, 25130, 46089,
        8713, 20138, 17538, 22864, 44328, 39594, 21208, 12462, 12584,
       49101, 21643, 27186, 22279, 20696, 14620, 48694, 30505, 27746,
       24064, 35823, 23388, 43041, 44121, 37374, 27542, 49395, 35673,
       24254, 44910, 14739, 35226,  6448, 37941, 20737,  1852, 15124,
        1530]), [7, 9, 1, 2]), (array([36373, 22530, 35781, 29310, 42872, 10211,  7403,  1317, 37261,
       41534, 34020, 12031, 12369, 30864, 31558, 30212, 44814, 46252,
        7091, 43390, 12321, 43174,  3716, 42712, 43691,  9653,  3041,
       47580, 46974, 10176,  6938,  5263, 11999, 49512, 21132, 25793,
       18103, 14162,  6025, 18870, 46317, 23370, 17282, 12604, 42584,
       33608, 17925, 43734, 26054, 23152, 34056, 12597, 12445, 15294,
       10600,  5661, 45693, 19184, 19308, 29687, 33173,  3926, 19763,
       28011, 42975, 19489, 16696, 17523, 41533, 48983, 47953, 25169,
       29241, 23087, 43431,  1876, 27626, 17287, 18021, 14660, 43761,
       16960,  1047, 16499, 36463, 36913, 21475, 32248, 39630, 42077,
       31142, 23366, 40206,  2796, 12874, 11392,   836, 13776, 13965,
        7064, 44224, 42859, 35672, 36522, 18615, 30816, 12796, 18554,
        3192, 33988, 18330, 18665, 34612, 20146, 16242, 41552, 16862,
       35891, 23579, 11675, 40316, 24992, 39357, 30001,  7967,  9423,
       46009, 37933, 46334, 20775, 42723, 27678, 45420, 37084, 27051,
       14830, 46050, 35600, 16782, 33436, 40437, 33758, 30644,  9275,
        8436,  7968, 49655,  8474, 34552, 35091, 28370, 21447, 46587,
       11066,  8363, 10517, 46765, 25761, 45597, 42577, 44096, 23593,
        2607, 25729, 13162,  5409, 22295, 33405, 49129,  6566, 29931,
       28504, 49289,    22, 18420, 38771,  7321, 34148, 48203, 27183,
        7172, 17143,  2898,  6442,  9323, 33791, 20008, 10757, 34762,
        3290,  8803, 23956, 29971, 25747, 22282, 33948, 44417, 24852,
       27117, 45300,  8679, 41785, 16611, 44518, 23987, 47858, 44489,
       36111,  1596,  5497, 16778, 45253, 42250, 32397, 30906, 25851,
       44708, 42748, 45719, 23771, 29339,  2276,  2777, 32759, 21125,
       34139,  6145, 12048, 29194, 12151, 24307, 14444, 36998, 18025,
       32080,  4678, 27741, 23373, 41196, 45385,  9112,  3912, 15193,
        4951,   355, 29550,  4143,  7632,  7036, 27367, 41867, 28507,
       32356, 45898, 44057, 32370, 39419, 27386,   958, 41218,  4779,
       41447,   897,  7029, 48805, 36214, 14442, 37692, 40669,  9786,
        1162,  5365, 39069,  9882, 46462, 20719,  5480, 22409, 30844,
       42270, 37953, 33264, 22920, 44043, 23125, 14281, 35617, 16044,
       17034, 23214, 16775, 23625, 48120, 12443, 21679, 24185, 24984,
       23838, 49897, 46543, 15948, 12206, 43858,  6788, 41957,  8414,
       40671, 48864, 28180, 24138, 33613,  4995, 40008,  5091, 34135,
       44594, 13369, 49899, 27727, 33861, 31367, 13587, 22661, 46223,
        7007, 38850, 34910, 48393, 49479, 42506,  1429, 47780, 42840,
        6916, 46710, 41712, 19251, 41008, 47400, 12513, 10840, 43644,
        5910,  3904, 14693,  7083, 23669, 16430, 47052, 28006, 12557,
       27243, 33756, 13576, 48297,  5820, 35825, 45649, 16607, 35707,
       38862,   507, 28066, 23440, 10537, 39038, 11244, 38162, 20727,
        5255, 27067, 44054, 47380, 12792, 42917, 41485, 24765, 39971,
        8657, 37711, 41292, 13481, 27550, 44420, 39362, 40495, 40292,
       37420, 32714, 37643,   609, 32783, 49521, 11490, 13095, 46985,
       38136, 36131, 24455, 28842, 29477, 49681, 27715,  3516, 36547,
       31182,  9188,  5941, 35885, 31714, 34892, 25756, 24083, 25754,
       22465, 24456,  2344, 38754,   259, 21134, 36498, 12249, 27858,
       37633, 17290,  9063, 26583, 46111, 13197,  2635, 42847, 11832,
        3305,  8531, 34319, 49887, 28707, 35540, 47776,  2381,  5985,
       37550, 43713, 49625, 29055, 27561, 38276, 47713, 28012, 30398,
       35150, 47376, 29788,  1794, 16086,  3258,  9669, 23960, 14613,
       46479,  5948, 25924, 21635,  1239, 45325, 36526,  7156, 18112,
       24588, 38734,  4955, 14369, 34695, 17799,  6134,  6219,  8267,
       47021, 20822, 36882, 25573, 30303, 42720,   786, 29877, 44701,
        7310, 41250,   291, 31188, 45243, 24328, 42884, 38797, 25533,
       32271, 47770, 37959, 41256, 15043,  6366,  6453,  3085,  4100,
       25071, 40655, 35129, 10133, 32679, 36813,  4354, 49503, 12903,
       39097, 18671, 18123, 13973, 22564, 28175, 31969, 24607,  8932,
       35041, 38719, 11427, 27194,   396,  2862, 42413, 47745, 33181,
        1559, 17515,  7261, 18649, 39476, 37799,  7019, 47060, 18181,
        6074, 17030, 35705, 34287, 40582, 11698,  6241, 25611, 30782,
       17366, 38785, 49594, 30673, 47839, 41627, 11664, 22358, 13277,
       40505, 15710, 35688, 47618,  4500,  6639, 27773, 31830, 22982,
       29932,  3773, 33257, 16197, 12743, 19018,   482, 27321, 38637,
       30865, 25206, 31539, 46544, 29318,    79, 34138, 11767, 44210,
        7428,  2584, 24678, 43638,  6668, 23933, 27089,  7378,  8977,
       49748, 33207, 18625, 26008, 12451, 35799, 30810,  6670, 28655,
        1304, 32883, 44198, 40987, 49356, 28784, 10450, 29546,  6515,
       24161, 40118, 41051, 26576, 32580,  2570, 13558, 22388, 39769,
       19869, 39692, 26320, 23529, 44168, 43948, 19983, 17893, 17647,
       30951, 23116, 37551, 49345, 40554, 28086, 36237, 40713, 22013,
       40386, 24923, 35339, 44571, 35933,  4858, 20856, 33085, 20680,
       32042, 10022, 44620, 22450, 37588, 13615, 12923, 44423,  2582,
       40880, 47178,  4299, 33897, 36703, 43605, 46865, 16310,  4562,
       32492, 24224,  7587, 44109,  3924, 46953,  7659, 29473,  6933,
       38333, 17593, 19341, 42753,  7198, 21529, 29283,  5076, 41378,
       21080, 49066, 12470, 20114,  4444, 43797, 24191, 44433, 28127,
       13509, 40615,  4200, 46162, 44217, 11707, 45322, 49383, 29901,
       30489, 31703, 39204, 12123,  9602, 19002, 39622, 28027, 15748,
       35567, 36593, 28220, 26168,  5149, 13217, 42735, 42448, 46928,
       23280, 23423, 31193, 37468, 21538, 24986, 40046,  5195, 39674,
        5247, 17128, 39679, 11683, 15776, 41532, 41964, 16989, 34530,
       28844, 26712, 41579, 49362,  8812,  5552,  9442, 36950, 49676,
       45866, 23139, 14336, 25912, 31394, 34968, 38234,  6583, 12721,
       44774, 21189,  3610, 11241, 21711,   425, 49870, 47589,  7074,
       21591, 32480, 34609,  1929, 12742, 19439, 17134, 32071, 42792,
       24636, 40032, 13154, 26181, 27743, 16472,   171,  9089,  4367,
       11992, 22322, 31343, 23843,  8182, 28443, 11137, 17786,  3440,
       45893, 34200, 32153, 34088, 40452,  3669, 21115,  8032, 18882,
        4730, 18477, 13639, 13637, 19916, 14640, 11342, 14918,     6,
        1793, 23138,  1936, 45709,  7951,  5151,  5506, 28518, 18242,
       27580, 31469, 35841, 12519, 27204,  7542, 13340,  9408, 47816,
       33793, 34827, 32377,  4703, 13542, 15522, 36756, 33881, 25830,
       38509, 16406, 13955, 43692, 30969, 37503, 40515, 46541, 25398,
       45710, 24016, 16530, 17682,  8178,  9802, 43476, 23315, 22676,
       35031,  4604,  1492, 48712, 14195, 23919, 42147,  4755, 16306,
       47411,   808, 15929, 42696, 39233, 13970, 12346, 38676, 19695,
       27594, 35259, 18496,  9900,  7056, 19977, 46125, 42661,  9440,
       31410, 42480, 27744, 31979, 35104, 39646, 40371, 46686, 23696,
       23692, 12945,  8616, 43607, 42345, 17571, 21940, 34463, 28348,
       44831,  5818, 38096, 49450, 42185, 48417, 41401,  7661, 33021,
        4376, 20633, 45818, 33396, 36169,  5333, 35958,  3119, 12958,
       38060, 27738, 12697, 22988, 19108, 19504, 48885, 26398, 14899,
       26541,  4776, 48323, 33042,  6976, 35168, 20318, 40305, 38766,
       48364, 27966, 37278, 40087, 17382, 30067, 14017, 27823, 44736,
       19359,  2875, 48973,   544, 44660, 22373, 49678, 45451, 26070,
       39312,  6660, 32582, 20712, 18143, 22843, 49693, 32278,  4811,
       17949, 20073, 13362, 14926, 15560,  6593, 43179,  4633, 12471,
       19220, 31035, 40487, 16165, 20029, 46589,  5226, 38016, 49736,
       46475, 13226, 15589, 42988, 16352, 17092, 32526,  8676, 41157,
        3769, 32430,  5609, 30287, 14317,  5922, 25222,  3856, 20850,
       14619]), [6, 8, 1, 2]), (array([44851, 11250, 43147, 48478,  2710, 43494, 44071,  9885, 11859,
       22133, 30037, 41178, 31893, 39244, 31615, 30425, 36096, 14263,
       15919, 45315, 12333, 23879, 26854,  1832, 10166, 29052, 43386,
       41093, 33586, 20137,  5051, 33279, 13896,  6103, 31775, 35250,
       45555, 10224, 26423, 23654, 36690, 15036, 43228, 49402, 24737,
        7602, 27276, 10282, 43473, 32413, 28219, 21389,  4104, 13513,
        5631, 39615, 31256, 21659, 46011, 13708, 14041,   378, 39451,
        5880, 26047, 32756, 11649, 28654, 48143, 28074,   946,  4193,
       43033, 40840,  6180,  2654,  8260, 34331, 16181, 41582, 30561,
       34727, 14656, 35601, 44525,   153, 23200, 16151, 29125,  2695,
        9352,  1866, 35228,  3723, 29862, 22173, 45122, 23455,  9761,
        5486, 12132, 11565, 43049, 42303, 14810,  4129,  6560, 38701,
       14447, 46251,  5625, 27135, 33703, 13778, 46591, 35634, 37288,
       31075,  4384, 42811,  3089, 10728, 41665, 23252, 27166, 15790,
       31365,  2999, 37294, 19510, 42640, 10006, 31627, 42138, 43584,
       10877, 39293,  3948, 46372,  2856, 27258,  2245, 40862, 29291,
       18705, 28002, 23814,  9260,  3433, 33215, 19528, 41305, 19516,
       32201, 33027, 35526, 39853, 34752, 28881, 42629, 36428, 10134,
       49154, 38161, 10207, 19452,  1795, 28903,  5143, 25893, 40591,
       37627, 33370, 44729, 15918, 14852, 12672, 25992,  8899, 46506,
       27455, 20192,  1212, 33740,  4057, 28755,  3116, 22536,  2418,
       33669, 43203, 28583, 24171, 17800, 27644, 33378, 48425, 45132,
       17045,  5669,  7407, 19614,   904, 32523, 17594, 47311, 14996,
       32459,  2216, 14090,  2144,  5957, 48889,  2900, 29278, 36536,
        8981, 18115, 29613,  7634, 29070, 26975, 45397,  9593, 21041,
       22765, 35954, 33408, 45189, 14060, 27631,  7531, 38124, 30734,
       10604, 48502, 46490, 39284, 19178, 37469, 24846, 19112, 44586,
       13385, 19315, 14262, 42096, 48521, 14316,  9147, 12642,  9943,
       43922, 18414, 36926, 15210, 13330,  8722, 12033,  2564, 13254,
       29561,  9371,   691, 25384, 12016, 24332, 14543, 13991,  5190,
       17455, 32504, 19830, 42585, 26859,  7253, 45128, 31934,  9300,
       46712,  7235,  3177, 38485, 19449, 27114,  9670,  3109, 33132,
       40839,  7852, 42837,  7804, 43123,  3568, 11387, 40446, 18949,
       13925, 14171, 49083, 40370,  9841,  1265,  4674, 17449,    36,
       49280, 30424, 35794, 11623, 22842, 15523, 27115, 32358, 28162,
       23197, 24364, 11836, 21501,  3875, 17821, 40365, 44852, 30772,
        5586, 37304,  9152,  1030, 29096, 38891,  6162, 18222, 10468,
       34136, 33956, 37804, 22866, 48638, 27722, 44354, 47132, 16301,
       42482,  5848,  4310,  9702, 30459, 20367, 49466, 15766, 35016,
       20194,  1098, 21103, 22110, 18329, 49563, 45897, 16120, 19276,
       11103, 36601, 44381, 10209, 43955,  7539, 45789, 11970, 48629,
       18127, 19799, 10255, 11222, 27698, 43551, 11805,  6584, 20153,
       36425,  4994, 25559,  1655, 12342,  9429, 36930,  2432, 31141,
       10506,  4346, 27021,  3807, 12750, 49140,  8330, 39921, 37939,
        6229, 24749, 19853, 22568, 22891, 48452, 46374, 20058, 14018,
        1963, 10587, 28423, 47892, 49408,  6306,  7161, 33358, 36474,
        6905,  6169, 16808, 10929, 36080, 14330, 31868, 39980, 21544,
       32982, 17047, 30960, 29139, 42357, 38992, 35124, 17578, 38066,
       48007, 13793, 28563, 12528, 12425, 21147,  2383,  7517,  5797,
       22223, 23189,  9792, 16604,  6684,  5963,  7153, 36990, 42373,
        8467, 31094,  5007, 13907, 25899, 22660,  2562,  8028, 16796,
       15321, 39158, 33976, 46286,  3709, 49078,  3190, 24652, 45319,
       44543, 14800,  6567, 14014, 44957,  1120, 42650, 10653, 15444,
        8680, 13393, 11668, 47933, 19875, 12546, 27275, 25713, 31460,
       27665,  5183, 39438, 44390, 36828, 37769, 44859, 33978, 32732,
        4108, 46503,  7519,  6135, 37204,  4739, 26877,  7357, 49234,
        6226, 19609, 39058, 44311,  5102, 14498, 24520, 46893, 23463,
       14927, 18890, 12755, 39911, 38710, 43818, 47349, 35753,  2825,
       12335,  1548, 45187,   168, 14967, 42666, 39040, 27802, 18117,
        7370, 23151, 11308, 33426, 40060, 22627, 35406, 28626, 34675,
       19063, 30923, 27994, 49230, 25596, 14593, 34233, 48806, 33838,
        4517, 14668,  2587, 28045, 49186, 37743,  6064,  6967, 18480,
       48899,  4769, 42031, 25061, 47113, 11473,  7821,  3452, 34940,
       14546, 21369, 21137, 33050, 30439, 43098, 23439, 45195, 26424,
       25459, 43918,   714, 36490, 26074, 38037, 33112, 23533, 42347,
       22762,  7390,  8696, 13849, 42171, 26344, 43646, 49392, 37689,
       17219, 23032, 22151,  4983, 44841, 36298, 44601,  6121, 31534,
       25069,  7777,  9070, 20245, 49539, 34453, 19125, 47566, 19546,
        7728, 36844, 24228, 49232,    65, 43164, 18698, 39828, 23332,
        5685, 22043,   432,  7827, 21893, 47423, 20994, 20273, 42860,
       29209, 46796, 22529, 42498, 27402, 44575, 31694, 17409, 32348,
       33643, 31209, 29986,    44, 27792,  6464, 25105, 45202,  5747,
       42292, 41710, 22818, 13994, 37461, 20162,   753, 38902, 27745,
       26532, 24385, 36388, 15577, 40472,  9765, 47001, 28538, 23564,
       18238, 17255, 16766, 39234, 36400, 17126, 48185, 39964, 32886,
       41489, 48003, 32462, 49759,   761, 26470, 31523, 24528,  6544,
        1551, 14209, 38650, 36251, 19598, 24462, 24425, 28694, 41849,
       22217, 25614, 33114,  9863, 45877, 46026, 36379, 43195, 47280,
       15425, 27447, 28081, 48681, 10157, 48840, 20844, 23601, 20222,
       37698, 33907, 16992, 12551, 24932, 41138, 31607, 14425, 37981,
       35674, 20381, 49873,  6673, 17701, 27576, 28192, 21604, 13207,
       34845,  2600, 24979, 14966, 44436, 28643, 35037, 34364, 49959,
       40484,  3505, 49075, 33742,  9781, 13317, 41230, 47288,  8346,
        3851, 49121, 13845, 48074, 48733, 41174,  3488, 15065, 25528,
       20538, 42706,  8157,  2044, 25089, 23537, 13286, 41333, 42120,
       29087,  8234, 33564, 24927,  2343, 47172, 33170, 18646, 39412,
       41667, 28667, 19932,  5717, 42168, 41084,  3245,  4493, 39901,
       11171, 32310,  1787, 17767,  6073, 16349, 28802, 28426, 20959,
       19039, 34873, 20457,   709, 14435, 30834, 26287, 15926, 10391,
       26605, 18162, 47152,  6685,   963,  6856, 14840, 38365,  2133,
        1609, 29229, 41599, 35853, 33379, 23123, 35310, 14473,  3629,
        5707,  9463, 18503, 35148, 11382, 44730, 45252, 33350,  6757,
       16215, 38546, 40134, 28497,  6236, 35834, 16616,  8019, 15326,
       37934, 23064,   630, 43019, 33448, 43545, 35653, 14127,  6069,
        7332,  9368, 38001, 29907, 24134, 14021, 47248, 31884, 19611,
        5340, 30096, 16232, 49858, 48370,  7664,  7340, 24962,   779,
       48300, 31002, 41273,  3982, 40348, 15649, 26730, 11270, 21332,
       25195, 33519,  7662, 38058, 36112, 38592, 40084, 39580,  8819,
       39269, 36293, 43680, 42217, 46053, 22780,  5926, 29354, 45360,
       17883, 37713, 32067,  2846, 25587,  1219, 44238, 12287, 39960,
        1918, 24496, 22960, 47452, 22035, 22432, 41990, 13506, 13814,
       29925,  6469, 30229, 23716, 48608, 28410, 18027,   790, 18234,
       42394, 36726, 35860, 12067,  2354, 46410, 39540,  6488, 36138,
       48790, 40244,  4634, 37488, 21987, 28862, 14459, 29126, 37615,
        8112, 11629,  7919, 15563, 13757, 36883, 16319, 43569, 39112,
       28686,    63, 26629,  1475, 46284, 33277, 35645,  3692, 32257,
       14720,  5638, 35353, 11385, 25491, 26215,  9660, 30025, 37895,
       25174, 42889, 15285, 23217, 46373,  6041, 22667, 17769, 32030,
       11030, 16693, 35157, 49573,  6467, 27470, 41562, 38705,  9992,
       14478,  1189,  4356, 49725,  4091,  8990, 42172, 38750, 11454,
       44574, 40661,  8740, 37932, 23158, 28641,  7449, 48194,  9404,
        9989]), [4, 3, 1, 2]), (array([38010, 31880, 23847, 43277, 35763, 14567, 39230, 48930, 48136,
       30383, 33530, 30645,  7547, 45927,  5404, 11939,   765,  9860,
       48788, 11721,  6400, 42151, 31757, 46303, 33933, 37879, 27609,
       18299, 40851, 40548, 34125, 43289, 22200,  5743, 39396, 10777,
       49387,  7731, 12375, 23240, 41664, 12944, 27425, 42587, 31412,
       47805,  4854, 23419, 48852,  8355,  6428, 32058, 22608, 13650,
       49559,  8660, 10796,   405, 14456, 42754,  2277, 26144, 32508,
       42878, 39739, 24764, 30324, 37181, 26704, 26488, 48908, 17107,
        3362, 46614,  9119, 47754, 20236, 30537, 29280, 29364, 38639,
       34012, 35263, 29173, 49303,  2287, 21360, 16225,  2066, 29844,
        5103,  4599, 36470, 16191,  5249, 13860, 30428, 29940, 48556,
       12328,  3197, 44616, 44884, 33542,  3842, 26391, 23021, 27000,
       28302, 37504,  8258, 43924, 43996, 27440, 34032, 30304, 23767,
       49528,  3049, 24543, 46041, 40287, 35603, 39966, 37061, 17221,
       28469,  5163,  9616,  7509,  1878,  7225, 15571, 46380, 29634,
       15304, 23352, 13457, 26656, 41410, 11114, 30959, 32107, 19445,
       38943, 43442, 11633, 10713, 37189,  4165, 42332,  4477, 24491,
        7203, 28179, 16075, 29515, 43483,  9657, 43475, 12270,  1205,
       33328, 44940, 16634, 17233, 39584, 48324, 29257, 30052, 10729,
       43368,  1178, 45304, 24844, 38387,  6132, 46872, 10146, 14782,
       28584, 44176,  7464,  5174, 35391,  4774, 15900, 34787, 17453,
        3607, 28033, 39138, 14867, 22264, 10987, 43814, 41243, 10955,
       38753, 28014, 19188, 37558, 37034, 12284, 25420, 46019, 41546,
       12052, 33662, 21639, 46369, 48554, 44303, 37102, 47332, 18591,
       42517, 17465, 20393, 33380, 42103, 28636, 44721,  2171, 18106,
       15115,  8655,  8549, 39392, 18271, 29620, 49628, 28833, 37945,
       39196, 33128, 28675,  7930,  2692, 48257,  3292, 47468, 22769,
       29927, 47324,  4552,  2683, 21288, 37062,  9435, 47018, 27671,
       23503, 22895, 45018, 26785, 32999,  9205, 24452, 42820, 32386,
       23432, 34635, 48817, 22387,  5593, 42813, 41501, 31561, 24702,
       11947, 19896, 10104, 33835, 12371, 14185, 10760, 31555, 34941,
       27805, 27141, 19889, 15968, 40783, 21780, 41875,  8683, 29796,
       28299, 43653,  6978, 32944, 32530, 39714, 47440, 44119, 49541,
       12485, 39458,    83, 33702, 26236, 10366, 40503, 49333, 16236,
       42384, 35842,  3002,  4255, 13990, 40269, 28144, 20292, 42691,
       36332, 27422, 44441, 27066,  1991, 23121, 44229, 21810,  5367,
        6575, 39225,  8269, 40205,  5729, 49185,  4548, 24796, 26845,
       47672, 38424, 29268, 27956, 19524, 48454,  6232,  5201, 44665,
       10319, 21957, 12545, 47059, 42987, 30701, 33142, 28854,  7417,
       34007,  4495, 37531, 44046, 49467,  8601, 36350, 33965, 23668,
        8768, 22840, 37676, 13252,  8960,  1538, 37216, 21059,  8954,
        3604, 14759, 21804, 43404, 14533, 38784,  5970, 10194, 25589,
       25517,  1567, 44312, 28845,  7189, 27150, 40905, 38417, 28843,
       28221, 26171,  1943, 20562, 22834, 44106, 15898, 18732, 29996,
       28550, 15602, 49565, 19012, 39147, 39748,  6436, 33989, 47768,
       36246, 18971, 10175, 12069, 28877,  2108, 29398, 26827, 36854,
       42731, 40004, 28069, 26954, 14908,  6815, 47223, 39539, 44345,
       38513,  1204, 31264, 36439, 29818, 15320, 13249, 41997, 11155,
       24226, 14734, 46667, 36487, 44085, 47485, 31973, 35949, 15403,
       15709, 45422, 20076, 19222,  1873, 33889, 16361, 35386, 48181,
       48083, 28286, 10181, 34207,  5510, 14530, 34220,  6082,   784,
       18981, 46222, 46621, 32044, 32804,  4740, 16767, 29603, 17259,
       18894, 21586,  3426, 48797, 32516, 26998, 26676, 32798, 20059,
       47038,  8511, 22650,  7113, 11267,  5760, 24867, 10555, 12573,
       25170, 33723, 40831, 24819, 14748, 12198, 43189, 28595, 13121,
       35254,  3015, 24363, 21834, 42178, 13218, 13843, 23100, 25044,
        2851, 28362, 36715, 30582, 38484, 15114, 40024, 21027, 26636,
       27905, 18518,   991,  3822, 41289, 37136, 27129,  9565, 12311,
       31147,  9209, 14302, 16272, 49163, 15740, 27871, 40537, 14061,
       26787,  4553, 48655, 11163, 41189, 45077, 22048, 23635, 13118,
       40407, 31102, 17312, 31072, 40250, 13505, 45769, 44225,  7124,
       20674, 46406, 33252,   261,  6709,  1631, 25874, 47774, 43369,
       22379,  6161, 44529, 49080, 28344, 21971, 24003,  8529, 34830,
       16410,  8216, 17580, 26471,  6851, 30135,  8090, 37264, 11242,
        4250, 41550, 37491, 34206,   772, 25209, 36594, 21280, 48269,
        5632, 18754, 38818, 14849, 24119, 23662, 44097, 12389, 33314,
       16180, 41455,  5224, 45787, 11196, 49616, 13296, 16087, 38549,
       48898,  8174, 18879, 42684,  2828,  3238, 28617, 39965, 13543,
       40358, 32311, 45859, 25306,  5805, 12964, 33910, 42402, 34063,
       16929, 24746, 24450, 41063, 13999, 48709, 34862, 43954, 48631,
       21720, 21094, 48528,  2259, 22884, 37620, 23203, 22067,  3661,
        2180, 42119, 25558,  9918, 46588, 41666, 11799, 27273, 21795,
        3922, 23379,  4528, 43006,  2280, 27130,  2545, 25881, 21407,
       13327, 18776, 25314, 11692,  1494,  7707,  2731, 11082, 44385,
         466, 35827, 42531, 41502, 16029,  5157, 36181, 29133, 42124,
        6895, 25608, 34809, 26241,  7381, 35651, 20056, 16563, 43145,
        4443, 36445,  8556,  5951, 10476, 28183,  9007, 17861, 21150,
       46598, 14518, 27607, 30840, 35291,  4648,  9400, 32488,  5627,
       15042,  3935, 34548, 46330, 14519, 35605, 14311,  4661, 19930,
       41133,  6120, 39275,  6889, 12893, 27084, 44589, 35953, 26467,
       44643, 38725, 17460, 34358, 46794, 18744, 47040, 20811,   250,
       24206, 17700, 32472, 47701, 37687, 15814, 18066, 12299, 17492,
       10630, 31138, 21985, 36591, 25567, 29188, 17471, 13766, 38878,
       27510, 22270,  3808, 12703,  2202, 22953, 31684, 48282,  4189,
       41498, 10860, 19729, 47133, 35493, 20403,  2619, 11825, 35765,
       13698, 26279, 32711, 45489, 30239, 24699, 15584, 28648,  2436,
       37560, 10406, 44351, 48232,  2784, 46563, 36494,   701, 12549,
       22974, 41143, 45681, 19157, 46642, 44797, 44327, 43826, 39729,
       34482, 39222, 36116, 20990, 32687,  9217, 36616, 27076,  5417,
       49008,  2090,  1050,   796, 28158, 15751, 48354, 35466, 27546,
       22587, 10926, 23264, 40213, 15837, 37008,  3101, 41856, 29636,
        9519, 13643, 21224, 35824, 15789, 37613,  3608, 36658, 29243,
       39308, 40941, 32151,  6499, 27750, 37485, 38989,  4109, 29079,
       33677, 35862, 34789, 44203, 16757, 13626,   778, 32951, 15910,
       15369,  4122, 37127, 29812, 44732, 13418, 31550, 47958,  8340,
       49666,  2719, 36009, 48589, 26449, 22215,   724, 30470, 26757,
       36377, 10866, 13435, 30396, 41454, 16312, 29682, 14715,  1675,
       19296, 34254, 29658, 10898, 12819, 10885,  4588, 39360, 36410,
       42765, 46795, 30377, 13727, 47171, 48205, 34268, 18464, 49248,
       39110,  7151, 34925, 30572, 19453, 11713, 18612, 10160,  7336,
       31454,  9712, 12582, 31687, 17902, 34508,  8173, 35785,  9420,
       36130, 18733, 31057, 40917, 46389, 25989, 36651, 18248,  7688,
       29470, 28291, 47853, 48270, 29389, 26976, 11194, 25030, 48598,
       47492, 49224, 24978, 48365, 39693, 38565, 45335, 30680, 47481,
       14601, 48745, 19806, 21630, 15847, 14174, 43106, 46088,    55,
       28031, 19130, 31707, 29269, 19435, 11197, 10152, 14088, 17850,
       27121,  9876,  5012, 49093,  2993, 29005, 31492, 43285,  8482,
       37121, 38389, 14135, 49329, 46870, 13524, 15922, 20061, 18385,
       39055, 44652, 42328, 24977, 25296, 39881, 12044, 41422,  8726,
       30746, 27326, 33457,  5576,  3259,  9933, 41512,  9448, 38129,
       37797,   646, 21668, 34685, 45367,   885,  3845, 15297, 16906,
       17117]), [0, 5, 1, 2])]
Collaboration
DC 0, val_set_size=1000, COIs=[7, 9, 1, 2], M=tensor([7, 9, 1, 2], device='cuda:0'), Initial Performance: (0.25, 0.04502433729171753)
DC 1, val_set_size=1000, COIs=[6, 8, 1, 2], M=tensor([6, 8, 1, 2], device='cuda:0'), Initial Performance: (0.26, 0.044506120443344115)
DC 2, val_set_size=1000, COIs=[4, 3, 1, 2], M=tensor([4, 3, 1, 2], device='cuda:0'), Initial Performance: (0.249, 0.04467034757137298)
DC 3, val_set_size=1000, COIs=[0, 5, 1, 2], M=tensor([0, 5, 1, 2], device='cuda:0'), Initial Performance: (0.248, 0.04460585105419159)
D00: 1000 samples from classes {1, 2}
D01: 1000 samples from classes {1, 2}
D02: 1000 samples from classes {1, 2}
D03: 1000 samples from classes {1, 2}
D04: 1000 samples from classes {1, 2}
D05: 1000 samples from classes {1, 2}
D06: 1000 samples from classes {9, 7}
D07: 1000 samples from classes {9, 7}
D08: 1000 samples from classes {9, 7}
D09: 1000 samples from classes {9, 7}
D010: 1000 samples from classes {9, 7}
D011: 1000 samples from classes {9, 7}
D012: 1000 samples from classes {8, 6}
D013: 1000 samples from classes {8, 6}
D014: 1000 samples from classes {8, 6}
D015: 1000 samples from classes {8, 6}
D016: 1000 samples from classes {8, 6}
D017: 1000 samples from classes {8, 6}
D018: 1000 samples from classes {3, 4}
D019: 1000 samples from classes {3, 4}
D020: 1000 samples from classes {3, 4}
D021: 1000 samples from classes {3, 4}
D022: 1000 samples from classes {3, 4}
D023: 1000 samples from classes {3, 4}
D024: 1000 samples from classes {0, 5}
D025: 1000 samples from classes {0, 5}
D026: 1000 samples from classes {0, 5}
D027: 1000 samples from classes {0, 5}
D028: 1000 samples from classes {0, 5}
D029: 1000 samples from classes {0, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.295, 0.06421255218982697) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.06314386874437332) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.279, 0.0991387677192688) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.08198602849245071) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.07024952399730683) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.0713477740585804) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.1353405278623104) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.08995045691728593) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.43, 0.0827924791276455) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.07941373744606972) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.1567333093881607) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.112021663159132) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1102066800892353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.09507753160595894) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.384, 0.17660580858588218) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.1585041108801961) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.1454920856282115) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.1163012564405799) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.392, 0.18973852957785128) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.20079846078529953) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.14613931557536125) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.129363997079432) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.20482386314868928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.22117121162824332) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.1731434683352709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.13845351364091038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.19699945250153542) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2411895220540464) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.17792725160717965) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.16328691100515424) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2086139908656478) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.25990544859599324) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.19964710243418812) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.15808263261057437) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.22641488587111236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.2669675034582615) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.20615564992837607) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.16297005883976817) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.20922902297973633) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.30454038487095386) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[1, 2], M=tensor([1, 2, 3, 4, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.9046666666666666, 0.016284945487976075)
DC Expert-0, val_set_size=500, COIs=[9, 7], M=tensor([7, 9, 1, 2], device='cuda:0'), Initial Performance: (0.954, 0.004131161715835333)
DC Expert-1, val_set_size=500, COIs=[8, 6], M=tensor([6, 8, 1, 2], device='cuda:0'), Initial Performance: (0.966, 0.0030926317796111107)
DC Expert-2, val_set_size=500, COIs=[3, 4], M=tensor([4, 3, 1, 2], device='cuda:0'), Initial Performance: (0.832, 0.01434116494655609)
SUPER-DC 0, val_set_size=1000, COIs=[7, 9, 1, 2], M=tensor([7, 9, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[6, 8, 1, 2], M=tensor([6, 8, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[4, 3, 1, 2], M=tensor([4, 3, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x719a78094d30>, <fl_market.actors.data_consumer.DataConsumer object at 0x719a347be3d0>, <fl_market.actors.data_consumer.DataConsumer object at 0x719a8407da30>, <fl_market.actors.data_consumer.DataConsumer object at 0x719a781ab580>, <fl_market.actors.data_consumer.DataConsumer object at 0x719a347be310>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO3', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0033980816118419173) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0024440920911729335) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.824, 0.015772972837090494) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.2342720573921688) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9206666666666666, 0.006532009847462177) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.03164990919828415) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.629, 0.03126419228315353) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.05702872969210148) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0036306695081293585) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.002469161670655012) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.01500162521749735) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.461, 0.22585041603539138) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9473333333333334, 0.0055488315007338924) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.06811768766492605) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.583, 0.05317370940744877) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.484, 0.0855667448490858) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.005045612627640366) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0023038949016481636) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.013863591134548187) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.22248578786943107) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.842, 0.01598854755156208) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.525, 0.09553511832468212) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.05419876852631569) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.522, 0.06860925761982799) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.00446990036778152) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0022895154217258094) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.015077739864587783) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.20786977837933227) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.00717313617374748) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.493, 0.08895004431903362) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.06511205031350255) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.10712109696865081) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005024509149603546) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0022679184284061195) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.014565882176160812) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.21783116027945654) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.007183964517588417) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.54, 0.05564727675914764) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.644, 0.05166293140500784) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.49, 0.08914563383162022) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO4', '(DO2']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0037572639640420674) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0027482688101008535) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.014751524299383163) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.22665385506127494) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9406666666666667, 0.004961357213867207) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.06249915143102407) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.06434778579324484) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.499, 0.09504013584554195) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004093221357092261) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0023393341177143155) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.013592292383313179) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.19934571213577873) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.0052493077703984455) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.07801805401593447) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.586, 0.07280712508410216) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.498, 0.11751716638728976) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0038962990492582322) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.0030406520403921606) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.015454698875546456) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.18839369577239268) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9333333333333333, 0.008510831554109852) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.515, 0.08541444387286902) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.542, 0.07436015915870667) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.492, 0.11745183837413788) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0040089594256132845) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002653262811829336) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.014654034227132797) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.17519795182486997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9433333333333334, 0.007099039304923887) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.08574182801693678) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.585, 0.060111343786120416) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.503, 0.09874783969670534) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004376491677947342) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0029732126740273086) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.013090789601206779) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.1810966370932292) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9246666666666666, 0.017747584943464366) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.505, 0.11257244109781459) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.08157790548168123) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.502, 0.10186465864814818) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO0', '(DO4']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003523614834062755) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002621759484289214) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.844, 0.014473602168262005) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.19168656682793517) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9326666666666666, 0.008420553654238272) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.09058350846171379) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.564, 0.07078828132152558) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.10925193313509225) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.0038060998069122435) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0023623564711306244) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.852, 0.013366555139422417) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.18507524378760717) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9233333333333333, 0.014593029611821597) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.49, 0.121004280641675) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.573, 0.07292697011120618) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.13481951542198659) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004556591367349028) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0028982829868327824) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.01380006168782711) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.19200169570033904) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9413333333333334, 0.005810957183750967) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.08111246076598763) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.639, 0.051656551614403724) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.528, 0.100939069673419) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005045347821898759) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003304243272403255) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.01511412426829338) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.18826940369908698) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9513333333333334, 0.007355272040721805) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.533, 0.0881090503782034) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.617, 0.06611057057231665) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.521, 0.12598039589822294) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004167975670658052) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0036086857857881116) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.014824498385190964) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.18595719704066868) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9433333333333334, 0.0057406992008909585) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.08532779841125011) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.635, 0.05725370268523693) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.511, 0.10581097766011953) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO0', '(DO4']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0037743596136569977) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003082541250158101) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.846, 0.01756852647662163) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.19287468804570382) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9366666666666666, 0.005847816829569637) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.502, 0.08094518481194973) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.623, 0.06119288608804345) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.11467272156476975) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0039493554318323736) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003895713170291856) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.017605435371398925) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.18748181337339337) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.011247732939819495) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.519, 0.09125504956766962) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.07244885835051537) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.13170856783911586) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004284653214737773) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0031837519644759595) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.015459607869386672) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.1905169287408935) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.009643198859201221) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.0747865769341588) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.648, 0.05546321687847376) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.509, 0.12133233544975519) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0037842460656538606) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0031138357816962526) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.01451371893286705) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.2056128339874558) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9246666666666666, 0.014786153183483369) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.499, 0.10234207760356366) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.614, 0.05510805934667587) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.513, 0.11298157614679076) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0041816447200253605) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002721622707787901) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.0134898976162076) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.20762071078945882) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9466666666666667, 0.010422717523579195) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.07905636341683567) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.055197979211807253) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.512, 0.09094810184463858) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO0', '(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003958697587251663) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0034688201203243807) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.015101105466485024) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.2048022494863253) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9393333333333334, 0.009776780922414522) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.514, 0.08119957749545574) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.61, 0.055383605379611255) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.501, 0.09820001811161637) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003936549761332572) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0039726073495985476) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.017176494061946868) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.20458398317650425) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9466666666666667, 0.007643200038233772) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.08056055999919773) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.05340589573979378) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.481, 0.10490619199350476) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.003544794725254178) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0033699131881585343) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.016279458723962307) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.21749386052333283) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.032271591377881124) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.492, 0.1112338242023252) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.601, 0.07702237487304955) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.1616975296912715) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0036680198749527333) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0028196943774819374) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.844, 0.01411986082792282) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.20074991777935064) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9086666666666666, 0.016493837496743557) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.08588456021994352) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.65, 0.048890207262709734) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.505, 0.10754291750257834) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.004049154611537233) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0037248861049884) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.846, 0.017444445699453352) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.464, 0.22260115210036746) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.011239158971778428) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.07337252509593964) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.668, 0.04023201197385788) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.51, 0.09203284767642617) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO4', '(DO2']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0035348944440484046) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.003481355596682988) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.84, 0.0184531192779541) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.24189309939614032) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9513333333333334, 0.005648412330464149) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.07885288519784808) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.675, 0.042682175800204274) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.502, 0.1087528660222888) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003214682884514332) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003101039182947716) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.844, 0.017130906142294407) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.464, 0.23626320242031942) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9553333333333334, 0.006484840566078977) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.528, 0.062265766579657794) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.743, 0.023892608888447284) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.511, 0.08194273386895656) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004289393246173859) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0028302520862198434) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.846, 0.015660884767770767) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.23059221093705856) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.006075730863104885) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.06224084614217281) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.729, 0.027399124044924974) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.542, 0.07623174326121807) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.0031747734453529118) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0029883257311885247) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.836, 0.014958126693964005) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.2516959501315723) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.007840244396604249) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.08985263564158232) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.641, 0.05518775253091007) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.491, 0.12025964923761785) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.003103701747953892) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0036648743142723107) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.016888775400817395) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.23143416827183683) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9366666666666666, 0.00888217558323716) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.09729155131056905) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.648, 0.049311789125204085) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.12999647017009555) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO0']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO5', '(DO4']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.003903418921865523) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003933245499822078) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.856, 0.01609081466495991) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.26128938074200414) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9573333333333334, 0.004540428179937104) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.07017456824332476) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.721, 0.03115208412706852) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.518, 0.09095592759549617) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003474051611497998) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.004652446879474155) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.013846699625253677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.26109080733917656) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.00935711364626574) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.509, 0.08348628800176085) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.711, 0.029809182032942774) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.499, 0.09798082663118839) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.978, 0.0032912740390747784) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003461919475492323) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.844, 0.019041052371263505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.27651394243194954) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.007181918115374477) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.09090738374739886) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.619, 0.05815072724968195) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.10739746342599392) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0037871312545612454) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003969347386577283) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.846, 0.01727407355606556) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.2924723530140473) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.006061801285327723) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.07382503886520862) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.667, 0.03795251111313701) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.496, 0.10208019261434674) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.976, 0.0033410421535372734) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0035541984063092967) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.01613167442381382) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.28074361326603686) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.009327807897595752) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.07196726285666227) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.694, 0.0350923736281693) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.512, 0.10172197180241346) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO0']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO3', '(DO4']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.003874020580202341) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0038543434783423437) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.84, 0.01722351945191622) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.25532456835196354) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.00541644820481694) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.515, 0.09015937693789601) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.679, 0.03387440799176693) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.506, 0.1016776006296277) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.976, 0.0035509319929406045) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.004172981913259719) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.018393637653440237) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.2900503463211353) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9546666666666667, 0.00755244493400581) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.501, 0.09990512064099312) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.616, 0.05054008059203625) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.5, 0.12168184988200664) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.0032534945178776978) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.004427278266721259) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.84, 0.018601149551570415) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.2904121321028797) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.006836481797508895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.489, 0.10440791282802821) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.677, 0.03632718528062105) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.496, 0.12051008284837007) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.003224316034466028) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.004756908407791343) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.834, 0.017525179594755174) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.2740468690164271) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9446666666666667, 0.009099947684366876) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.493, 0.0932939423955977) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.642, 0.0437360645532608) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.488, 0.11447733897715807) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.976, 0.0030394189991056917) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003148140225093812) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.84, 0.0160735115557909) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.2792776488934178) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9413333333333334, 0.009772557075339137) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.505, 0.0972928980179131) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.057582054818049073) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.11170317053049802) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04502433729171753), (0.295, 0.06421255218982697), (0.447, 0.07024952399730683), (0.43, 0.0827924791276455), (0.458, 0.1102066800892353), (0.448, 0.1454920856282115), (0.472, 0.14613931557536125), (0.477, 0.1731434683352709), (0.47, 0.17792725160717965), (0.477, 0.19964710243418812), (0.477, 0.20615564992837607), (0.626, 0.03164990919828415), (0.524, 0.06811768766492605), (0.525, 0.09553511832468212), (0.493, 0.08895004431903362), (0.54, 0.05564727675914764), (0.542, 0.06249915143102407), (0.528, 0.07801805401593447), (0.515, 0.08541444387286902), (0.524, 0.08574182801693678), (0.505, 0.11257244109781459), (0.521, 0.09058350846171379), (0.49, 0.121004280641675), (0.542, 0.08111246076598763), (0.533, 0.0881090503782034), (0.516, 0.08532779841125011), (0.502, 0.08094518481194973), (0.519, 0.09125504956766962), (0.535, 0.0747865769341588), (0.499, 0.10234207760356366), (0.517, 0.07905636341683567), (0.514, 0.08119957749545574), (0.524, 0.08056055999919773), (0.492, 0.1112338242023252), (0.521, 0.08588456021994352), (0.518, 0.07337252509593964), (0.543, 0.07885288519784808), (0.528, 0.062265766579657794), (0.547, 0.06224084614217281), (0.529, 0.08985263564158232), (0.527, 0.09729155131056905), (0.542, 0.07017456824332476), (0.509, 0.08348628800176085), (0.52, 0.09090738374739886), (0.521, 0.07382503886520862), (0.531, 0.07196726285666227), (0.515, 0.09015937693789601), (0.501, 0.09990512064099312), (0.489, 0.10440791282802821), (0.493, 0.0932939423955977), (0.505, 0.0972928980179131)]
TEST: 
[(0.25, 0.04384468254446983), (0.3, 0.06179358550906181), (0.45025, 0.0672670782506466), (0.438, 0.07863861978054047), (0.46075, 0.10416459986567497), (0.4575, 0.13624865448474885), (0.471, 0.13515578013658525), (0.473, 0.1617032497525215), (0.46975, 0.16537044155597685), (0.477, 0.18476930224895477), (0.475, 0.19292769503593446), (0.60125, 0.03270898659527302), (0.50275, 0.06811997276544571), (0.52075, 0.10075044268369675), (0.48525, 0.09254746308922768), (0.5425, 0.0555120849609375), (0.5475, 0.0591168519705534), (0.53025, 0.08049439841508865), (0.52025, 0.08568061408400536), (0.51375, 0.0876079825758934), (0.504, 0.11788861098885536), (0.509, 0.09124821588397027), (0.48925, 0.1228357949256897), (0.5375, 0.08500879579782486), (0.52775, 0.08770390677452088), (0.52475, 0.08671983674168587), (0.52375, 0.0786741783618927), (0.51375, 0.08957513627409935), (0.52725, 0.07369453856348991), (0.4945, 0.10078331178426743), (0.525, 0.0769853218793869), (0.507, 0.07949566188454628), (0.51575, 0.08013507968187332), (0.50025, 0.10684731104969979), (0.52, 0.08302492028474807), (0.5305, 0.0708915656208992), (0.55425, 0.07664216960966587), (0.54225, 0.061740417569875715), (0.5615, 0.06173695605993271), (0.52425, 0.0870945763885975), (0.52425, 0.09383079448342323), (0.54375, 0.06827394081652165), (0.511, 0.08111446386575699), (0.51525, 0.08843655809760094), (0.51625, 0.0726080153286457), (0.53725, 0.06988689261674881), (0.5085, 0.08797504422068596), (0.4965, 0.09733954441547393), (0.4945, 0.09939267909526825), (0.50125, 0.09084600058197975), (0.50175, 0.09604019752144813)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.55      0.87      0.67      1000
           2       0.44      0.95      0.60      1000
           7       0.82      0.03      0.06      1000
           9       0.75      0.15      0.26      1000

    accuracy                           0.50      4000
   macro avg       0.64      0.50      0.40      4000
weighted avg       0.64      0.50      0.40      4000

Collaboration_DC_1
VAL: 
[(0.26, 0.044506120443344115), (0.378, 0.06314386874437332), (0.453, 0.0713477740585804), (0.447, 0.07941373744606972), (0.478, 0.09507753160595894), (0.479, 0.1163012564405799), (0.487, 0.129363997079432), (0.485, 0.13845351364091038), (0.486, 0.16328691100515424), (0.489, 0.15808263261057437), (0.483, 0.16297005883976817), (0.629, 0.03126419228315353), (0.583, 0.05317370940744877), (0.578, 0.05419876852631569), (0.578, 0.06511205031350255), (0.644, 0.05166293140500784), (0.58, 0.06434778579324484), (0.586, 0.07280712508410216), (0.542, 0.07436015915870667), (0.585, 0.060111343786120416), (0.558, 0.08157790548168123), (0.564, 0.07078828132152558), (0.573, 0.07292697011120618), (0.639, 0.051656551614403724), (0.617, 0.06611057057231665), (0.635, 0.05725370268523693), (0.623, 0.06119288608804345), (0.58, 0.07244885835051537), (0.648, 0.05546321687847376), (0.614, 0.05510805934667587), (0.58, 0.055197979211807253), (0.61, 0.055383605379611255), (0.584, 0.05340589573979378), (0.601, 0.07702237487304955), (0.65, 0.048890207262709734), (0.668, 0.04023201197385788), (0.675, 0.042682175800204274), (0.743, 0.023892608888447284), (0.729, 0.027399124044924974), (0.641, 0.05518775253091007), (0.648, 0.049311789125204085), (0.721, 0.03115208412706852), (0.711, 0.029809182032942774), (0.619, 0.05815072724968195), (0.667, 0.03795251111313701), (0.694, 0.0350923736281693), (0.679, 0.03387440799176693), (0.616, 0.05054008059203625), (0.677, 0.03632718528062105), (0.642, 0.0437360645532608), (0.578, 0.057582054818049073)]
TEST: 
[(0.2605, 0.04348890659213066), (0.38275, 0.06072431656718254), (0.45925, 0.06845111122727394), (0.451, 0.07635300573706627), (0.479, 0.09213218602538109), (0.482, 0.1134857217669487), (0.4865, 0.12708065676689148), (0.48525, 0.1371110547184944), (0.48575, 0.1597578677535057), (0.4875, 0.15451923394203185), (0.48675, 0.16020039916038514), (0.6265, 0.0305072038769722), (0.56475, 0.0536544955521822), (0.59, 0.05415658275783062), (0.55475, 0.06721534122526646), (0.632, 0.05195160290598869), (0.58425, 0.05989135429263115), (0.585, 0.06864629247784615), (0.55275, 0.07010131132602691), (0.59025, 0.05826881787180901), (0.55375, 0.08234436245262623), (0.5705, 0.070565639346838), (0.5805, 0.07405681113898754), (0.6375, 0.05184421683847904), (0.6015, 0.06674524880945683), (0.6235, 0.05704377602040768), (0.60925, 0.06144089387357235), (0.59325, 0.0682449286878109), (0.637, 0.05397245901823044), (0.6095, 0.05406999865174294), (0.57775, 0.05483533902466297), (0.607, 0.05169597154855728), (0.60575, 0.05095751386880874), (0.59325, 0.07382448303699493), (0.653, 0.046394489720463755), (0.67525, 0.03895931012928486), (0.684, 0.04119873770326376), (0.733, 0.026547886323183775), (0.70925, 0.02950775280967355), (0.64, 0.05198474171757698), (0.6405, 0.04794853056967258), (0.7175, 0.030758955020457505), (0.7015, 0.030611893817782402), (0.62225, 0.055703392803668976), (0.667, 0.03805186318606138), (0.694, 0.03364272180944681), (0.68275, 0.034733730651438234), (0.6235, 0.04913447779417038), (0.6825, 0.03601222848147154), (0.646, 0.04285887584090233), (0.574, 0.05637816563248634)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.61      0.89      0.73      1000
           2       0.46      0.93      0.62      1000
           6       0.88      0.18      0.30      1000
           8       0.87      0.29      0.44      1000

    accuracy                           0.57      4000
   macro avg       0.71      0.57      0.52      4000
weighted avg       0.71      0.57      0.52      4000

Collaboration_DC_2
VAL: 
[(0.249, 0.04467034757137298), (0.279, 0.0991387677192688), (0.306, 0.1353405278623104), (0.354, 0.1567333093881607), (0.384, 0.17660580858588218), (0.392, 0.18973852957785128), (0.403, 0.20482386314868928), (0.403, 0.19699945250153542), (0.404, 0.2086139908656478), (0.422, 0.22641488587111236), (0.416, 0.20922902297973633), (0.488, 0.05702872969210148), (0.484, 0.0855667448490858), (0.522, 0.06860925761982799), (0.48, 0.10712109696865081), (0.49, 0.08914563383162022), (0.499, 0.09504013584554195), (0.498, 0.11751716638728976), (0.492, 0.11745183837413788), (0.503, 0.09874783969670534), (0.502, 0.10186465864814818), (0.486, 0.10925193313509225), (0.489, 0.13481951542198659), (0.528, 0.100939069673419), (0.521, 0.12598039589822294), (0.511, 0.10581097766011953), (0.472, 0.11467272156476975), (0.475, 0.13170856783911586), (0.509, 0.12133233544975519), (0.513, 0.11298157614679076), (0.512, 0.09094810184463858), (0.501, 0.09820001811161637), (0.481, 0.10490619199350476), (0.477, 0.1616975296912715), (0.505, 0.10754291750257834), (0.51, 0.09203284767642617), (0.502, 0.1087528660222888), (0.511, 0.08194273386895656), (0.542, 0.07623174326121807), (0.491, 0.12025964923761785), (0.479, 0.12999647017009555), (0.518, 0.09095592759549617), (0.499, 0.09798082663118839), (0.488, 0.10739746342599392), (0.496, 0.10208019261434674), (0.512, 0.10172197180241346), (0.506, 0.1016776006296277), (0.5, 0.12168184988200664), (0.496, 0.12051008284837007), (0.488, 0.11447733897715807), (0.482, 0.11170317053049802)]
TEST: 
[(0.2515, 0.043640657246112824), (0.267, 0.09514398372173309), (0.29675, 0.12978019261360169), (0.3455, 0.14937431919574737), (0.3785, 0.16722273707389831), (0.389, 0.17958619666099548), (0.39425, 0.19564056634902954), (0.3885, 0.18772674351930618), (0.39475, 0.1980825604200363), (0.409, 0.21704438626766204), (0.412, 0.19847762775421143), (0.4755, 0.05869686889648437), (0.487, 0.08520139080286027), (0.51775, 0.06663380415737628), (0.4775, 0.11229557129740715), (0.48925, 0.09100178670883179), (0.4975, 0.09550153416395188), (0.49575, 0.12125733298063278), (0.48375, 0.12075014361739159), (0.5055, 0.10005222770571709), (0.49325, 0.10713991352915764), (0.47525, 0.11294872453808784), (0.48825, 0.14422966504096985), (0.49775, 0.10783399671316146), (0.487, 0.13419355762004853), (0.5015, 0.11074204966425896), (0.479, 0.11474476879835128), (0.47125, 0.13267142939567567), (0.50175, 0.12649444657564163), (0.4925, 0.11803148210048675), (0.49475, 0.09714839774370193), (0.503, 0.10224951100349426), (0.482, 0.10791091865301132), (0.4625, 0.16944504922628403), (0.50025, 0.10854318624734878), (0.49825, 0.09500641229748726), (0.482, 0.1115657805800438), (0.509, 0.08480873695015907), (0.52175, 0.07807542783021927), (0.4775, 0.12652646720409394), (0.482, 0.13292506235837936), (0.49875, 0.0945279900431633), (0.49075, 0.1023588642179966), (0.476, 0.1101923246383667), (0.49375, 0.1049485966861248), (0.4965, 0.10616686511039734), (0.49225, 0.10371843457221985), (0.49, 0.12317038160562516), (0.4805, 0.12432126104831695), (0.47775, 0.1182467648088932), (0.479, 0.11637894663214683)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.84      0.91      0.87      1000
           2       0.33      0.95      0.49      1000
           3       0.76      0.04      0.08      1000
           4       0.53      0.02      0.03      1000

    accuracy                           0.48      4000
   macro avg       0.62      0.48      0.37      4000
weighted avg       0.62      0.48      0.37      4000

Collaboration_DC_3
VAL: 
[(0.248, 0.04460585105419159), (0.388, 0.08198602849245071), (0.468, 0.08995045691728593), (0.465, 0.112021663159132), (0.476, 0.1585041108801961), (0.46, 0.20079846078529953), (0.457, 0.22117121162824332), (0.463, 0.2411895220540464), (0.468, 0.25990544859599324), (0.472, 0.2669675034582615), (0.467, 0.30454038487095386), (0.459, 0.2342720573921688), (0.461, 0.22585041603539138), (0.469, 0.22248578786943107), (0.471, 0.20786977837933227), (0.469, 0.21783116027945654), (0.469, 0.22665385506127494), (0.459, 0.19934571213577873), (0.472, 0.18839369577239268), (0.469, 0.17519795182486997), (0.467, 0.1810966370932292), (0.463, 0.19168656682793517), (0.465, 0.18507524378760717), (0.458, 0.19200169570033904), (0.466, 0.18826940369908698), (0.471, 0.18595719704066868), (0.469, 0.19287468804570382), (0.467, 0.18748181337339337), (0.466, 0.1905169287408935), (0.469, 0.2056128339874558), (0.474, 0.20762071078945882), (0.474, 0.2048022494863253), (0.467, 0.20458398317650425), (0.468, 0.21749386052333283), (0.474, 0.20074991777935064), (0.464, 0.22260115210036746), (0.469, 0.24189309939614032), (0.464, 0.23626320242031942), (0.472, 0.23059221093705856), (0.472, 0.2516959501315723), (0.473, 0.23143416827183683), (0.472, 0.26128938074200414), (0.476, 0.26109080733917656), (0.473, 0.27651394243194954), (0.473, 0.2924723530140473), (0.473, 0.28074361326603686), (0.472, 0.25532456835196354), (0.473, 0.2900503463211353), (0.469, 0.2904121321028797), (0.467, 0.2740468690164271), (0.468, 0.2792776488934178)]
TEST: 
[(0.23875, 0.043380969196558), (0.3875, 0.07882354038953782), (0.4575, 0.0859538711309433), (0.45425, 0.10616131201386451), (0.46675, 0.1488635730743408), (0.45475, 0.19176117146015167), (0.45075, 0.21221389746665956), (0.46, 0.22604575574398042), (0.46225, 0.24731591022014618), (0.46675, 0.25240520119667054), (0.4685, 0.2846794567108154), (0.45925, 0.22638060283660888), (0.4595, 0.2144902248978615), (0.463, 0.2106477062702179), (0.46625, 0.19691637206077575), (0.4645, 0.20649341452121733), (0.46425, 0.2146002334356308), (0.456, 0.19245067256689072), (0.46625, 0.17885563123226167), (0.4625, 0.1658924304842949), (0.4645, 0.17261056798696517), (0.4595, 0.1829408413171768), (0.4635, 0.1758371878862381), (0.45575, 0.18540890425443649), (0.46525, 0.1812574293613434), (0.46725, 0.1764280591607094), (0.46525, 0.18354951149225235), (0.4615, 0.17981398177146912), (0.4635, 0.18302695137262345), (0.465, 0.19605257046222688), (0.469, 0.20021688801050186), (0.4695, 0.19489072424173354), (0.464, 0.19575902569293976), (0.46575, 0.2048219558596611), (0.47175, 0.19307102876901627), (0.46725, 0.21349846374988557), (0.4655, 0.23139722049236297), (0.46375, 0.2266002777814865), (0.47, 0.21998527586460115), (0.46875, 0.23724731194972992), (0.4695, 0.22047961306571962), (0.4675, 0.25022402322292325), (0.46925, 0.24889170300960542), (0.4685, 0.2631823717355728), (0.468, 0.2817345240116119), (0.46775, 0.26941506135463716), (0.46675, 0.24458424413204194), (0.469, 0.2802550332546234), (0.46675, 0.28136488103866575), (0.46475, 0.2622545260190964), (0.4675, 0.26698194789886476)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.59      0.90      0.71      1000
           1       0.00      0.00      0.00      1000
           2       0.00      0.00      0.00      1000
           5       0.39      0.97      0.56      1000

    accuracy                           0.47      4000
   macro avg       0.25      0.47      0.32      4000
weighted avg       0.25      0.47      0.32      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [95]
name: alliance-3-dcs-95
score_metric: contrloss
aggregation: <function fed_avg at 0x7469f589dc10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=95
Partitioning data
[[2, 3, 8, 9], [0, 1, 8, 9], [4, 6, 8, 9], [5, 7, 8, 9]]
[(array([46451,  2561, 21623, 18088, 24694, 25512, 35140, 16671, 31785,
       37613, 18952, 26960, 37745, 26051,  8019, 41312, 44181, 19057,
       28834, 28377, 33205,  4293, 43484, 25415, 24165, 33699, 13141,
       33849, 36141,  7008, 21765,  2993,  4008, 26765, 29673, 35157,
       11882, 33204, 43422, 29048,  1670, 35466,  4426, 17883, 12140,
       30849, 25626, 31704, 18477, 27191,  4109, 30690, 37972, 19039,
       25982, 12992, 22618, 17188,  9629, 18496, 12782,   288,  6628,
        8234,  4888, 31677, 48501, 20285, 47632, 47846, 31809,  5686,
       28934, 37247, 21859, 47563, 43545,  9386, 41362,  2702, 48404,
        3673, 22307, 29779, 11385,  5384, 39594, 13071, 41763, 19950,
       12871, 18586, 30680, 34608, 22864, 37280, 27484,  6230,  3068,
       20864, 12346, 16906, 44340, 36883, 24199, 36726, 32257, 19694,
        2193, 44053,  3331, 14926, 46907, 32106, 39339,  6583, 16747,
       11089, 43476, 43503, 15589, 44542, 33621, 45359, 31428, 28443,
        6836,  9038, 19359,  2393,  2693, 19811, 30239, 18888, 37811,
       23642, 43961,  8871,  2033, 40716, 30466, 36450, 42328, 48784,
       24773, 38434, 33535,  8485, 20490, 21168, 27131, 21446, 18101,
        6944, 11486, 30572,  3786, 11171,  8165, 47727,  3253, 19163,
        9827, 21819, 38565, 37015,  7363, 34130, 40965, 42605, 22931,
         849, 42894, 22275,  7178,  3814, 30110, 45533, 33851,  2535,
       13300, 46457,  7009, 39675, 17329,  3338, 30096, 26265, 20563,
        2093, 36469, 34219, 28281, 27973, 32956, 26730, 41604,  8219,
       32457, 12287, 21746, 40849,  9740, 36622, 18138,  5219,  3791,
         586, 46887, 10767,  1676, 17273, 20570,  6069, 37198, 29354,
       46837,    18,  5012, 30454,  4604, 14472, 16593, 37848, 40583,
        7410, 49700,  6305,  2041, 47273,  9896, 13985, 45131, 26001,
       18234, 40115, 36860, 18485, 27499, 46516, 28158, 49725,  5151,
       43738, 39051,  9678, 40480,   924, 17267,  1050, 30013,  2521,
       36431, 26797,  1553, 41117, 44322, 49819, 11960, 29478,   861,
       31676, 34334, 38449,  5945, 30754,  6135,    21, 35525, 18942,
       32215, 13204,  8104, 22273, 42504, 38068, 21103, 13167, 46496,
       44635, 29607,  6847, 36990,  1496, 26485, 26126, 22914, 26675,
       41446, 46561, 46109, 47075, 32813,  6492, 14032, 34042, 17661,
       42133,    91,   806, 48957, 40852, 37410,  4628, 40086, 34379,
       20593, 17831,  6208, 42646,  8255,  7065, 43017, 36772, 28808,
       31460, 37399, 41116, 32371, 20704, 43773,  9564,   774,  9152,
       47410, 14941, 31380, 17047,  5733, 22278, 20425, 48614,  4010,
       47067,  7584, 37095,  1070, 41323, 32914,  7075, 28423, 49982,
       27525, 46290,  7872, 16904, 40226, 47439, 41993, 11359, 39095,
       44267,  9305, 47217, 20770, 25649,  3081, 19580,  7128, 10575,
        4982, 30476,  6556, 30571, 39552, 49233,  5445, 24022, 15154,
       27090, 10135, 25705, 12639, 16131, 20639, 27628,  7213, 33964,
        7180,  6782, 20330, 25218, 15836, 15388,  9107, 42354,  2930,
       48798, 45850, 48561, 17155, 41511, 28712, 24903, 35215, 25492,
       30122,  2562, 39543, 34649, 32520,  5423,    26, 10137, 49443,
       23417, 20163, 49563,  5601, 45053, 42783, 17545, 41493,  2903,
       17912,  1965, 16932, 39325, 14264, 29082, 49463, 42192,  7086,
        9930, 35307, 26598, 34710, 25953, 44973, 18259, 30270, 38066,
       24775, 11564, 48262, 16635, 24949,  6185, 38444, 43438, 13093,
        5534, 21712, 22226, 44530, 19577, 12241,  2525,    33, 38679,
       41053, 15411, 12155, 12076,   494, 21373, 13813,  8288, 44276,
       19727, 42445, 15967, 32379, 35293, 16754, 47014, 17430,  2011,
       46121, 47970, 27250, 23763, 42122, 40985, 33642, 30960, 33915,
       16308, 31974, 44868,  2581,  6433, 48494,   342, 12758, 31204,
        1864,  2565, 37204, 43257,  7389, 25443, 38149, 14232, 49835,
       14419, 37562, 18233, 46424, 15974, 15618, 22682, 38835, 23736,
       10848, 47380,  4292, 19768, 28782, 14369, 29647, 10876, 15237,
       43408, 14804, 13633, 21165,  8120,  8057,  2732, 16820, 16676,
       47430, 27728, 49852, 38754, 22696, 38666, 20650, 32887,  3516,
       47635, 49446, 10245, 31340, 38239, 25374,  1087, 16304,  5807,
       35115, 29371, 30745, 11676,  1957, 11149, 14464, 15835, 30990,
       25096, 47894, 40389, 43344, 34387, 34743,  2325, 19131, 42050,
       48552, 24085, 17418, 11180, 47259, 47202, 16473, 32421, 47094,
        5073,  8919, 26925, 27958, 22492,  7508, 34093, 14969, 35361,
       32644,  8429, 19979,  8892, 31183, 35671, 34641, 44373,  6909,
       33005, 29674, 35730,  6948,  4380,  9188, 38443, 36803, 38162,
       40780, 15046, 23405, 18892, 39114, 36560, 11693, 31332, 36405,
        2229, 28435, 49679, 24608, 13641, 36670, 32051, 42515, 44458,
        5949, 10445, 16457,  6397, 43100, 14497, 47971, 21919,  3980,
       42135, 14790, 44681, 10088, 26188,   410, 34471,  9651, 42371,
       32876, 45045, 41753, 23960,  2817,  2018, 11886, 31732,  9743,
        5297, 42203, 16257, 32760, 37953, 30438,  2534,  3832, 12418,
        2798, 23672, 21451, 14010,  3380, 40990, 20461, 46737,  6773,
       25093, 34814, 33869, 11309, 44236, 47291, 30164, 37072, 27842,
       17653, 48393, 42236, 24878,  9684, 42389,   192,  3359, 20620,
       35995, 26981, 12125, 11345, 29903,  8471, 21172, 21812, 13565,
       39627, 44649, 38881,  6830, 45718, 21457, 42693, 26105, 38817,
       18122,  1688, 24185, 36985, 11266, 33992, 30018,  5732, 29476,
       37641,  7876, 34296, 19081, 26579, 17962, 45830, 12951,  8031,
       35540, 11042,  2743, 19648,  2918, 23534, 14832, 40500, 29316,
       46386, 15239, 38436, 20228, 22920,  9527,  2298, 26149, 18660,
        8526, 48562,  3310, 32676, 15780, 40136, 36742,  3753, 47181,
       20596, 31097,  2754, 39278, 26510,  7149, 48611, 24787, 49897,
       23258, 34893, 13874, 31490, 24343,   369, 15752, 33563, 29352,
       35262,  7698, 21690, 26489, 28709, 37139, 37730, 39934, 34943,
        4321, 39856, 24188, 38494, 49997, 44996, 43039, 26384, 44188,
       15652,  1829,  5861, 30056,  9869, 18361, 11677, 49502, 45740,
       12385, 38928, 28065, 17974, 15817, 43910, 32605, 30796, 19625,
       23770, 44871, 33486,  1633, 37780,   706, 10559, 34587, 15857,
        9097, 18789, 38740, 40381, 22614,  2020, 39879, 23397,  9382,
       27791, 34903, 16910, 23569, 26610, 43376,   798, 26875, 33432,
       37499, 20799,  8650, 27237,  2192, 41363, 42638, 48435, 31911,
        5938, 21658,  3179, 27040,  5662, 13669, 16078, 27168, 10118,
       16174, 10309, 13199, 42025, 19674, 18423, 33248, 47700, 17716,
       25009, 12700,  2935, 29334, 24603, 34502, 18778, 40229, 11828,
       37444, 26942,  5591,  1075, 17941, 11388, 39583, 15142, 12718,
       14599, 39984, 39713, 41495,  2845, 34078,  1353,  8743, 48713,
       12948,  9026, 29721, 34811, 23726,  8245, 27420, 38405, 23299,
       41245, 47765, 34586,  7414, 25604,  2761, 15750, 24331, 37314,
       49057, 29820, 14179, 24214,  3381, 26769, 40819, 25897,   668,
       31742, 11111,  6984, 13000, 18860,  8807, 28560, 43341, 18785,
       17968, 21645, 17470, 17139, 35085,  4215, 43619, 11931,  7708,
       46039, 25091, 31039, 21766, 49634,  2495,  9432, 44829, 18337,
       21209, 39876, 15150, 21121, 49601, 28907, 49417, 41267,  4735,
       19353,  6571, 31106, 29186, 26021,  3672,  5310, 47159, 26511,
        3312, 42632,  1987, 17163, 47419, 44155, 38370,  9213, 29148,
       13665, 38644,   768, 38561, 28591, 22196,   188,  1796, 41282,
       44669, 24114, 24182, 44469, 13343,  1262, 19640, 11485,  1652,
       27570, 49072,  3316, 16454,  5275, 31862, 20430, 13919, 23243,
       41503, 21632,  6644, 47084, 30404, 16735, 33593, 12775,  1201,
       32221, 31324, 11678, 22995, 47636, 29043, 25433, 23291, 27695,
       34290]), [2, 3, 8, 9]), (array([32290, 12297, 10399, 24605, 13194, 11963, 30605, 31086, 32220,
       45004, 33249,  1831, 19630, 17456, 47253, 36924, 32190, 39601,
       28515, 33732, 16297,   116, 26412, 14898, 37324, 12294, 20454,
       40649,  9287, 10776, 32703, 43996, 28080, 39396, 20265, 27128,
       40343, 19066, 47220, 49799, 18745,  7444, 34964,  3304,  4746,
       40906, 48257, 12820, 38185, 47404, 22112, 31991,  9696, 28179,
       14486, 10867,  8086, 33919, 40328, 16316, 11509,  1715, 24802,
       28921, 17178, 32857,  4906, 36007, 25615, 40401, 22167, 37841,
       10989, 34279,   983, 48054, 15707, 13382,  5642, 22190,  1147,
       25616, 40418, 20398,  9657,  4411, 40542, 41324, 17395, 47689,
       46046, 22118, 21903, 12139,  8733, 37758, 30837, 45349,  5472,
       26662,  4141, 34123, 21467,  9157, 21000,  6240, 34009, 35758,
       24447, 26495, 19942,  5924, 38581, 17080, 38748, 28653,  9004,
       27431, 24639, 44946, 48059, 41609,  7731,  3842, 20574, 25016,
        6479, 44925, 22724, 44742, 28791, 11643, 18368, 49050, 43564,
        4192, 40890, 49735,  6425, 41266, 35763, 38791, 38326, 24641,
       12037,  4348, 28650, 40830,  1144, 48430, 38683, 11958, 23822,
       40319, 27749, 10787, 20257,  6165,  7801, 29917, 30888, 45088,
       14056, 40746,  1811, 23924, 15949, 24349,  5174, 40245, 42073,
       30609,  4271, 29893, 46137, 46343, 27155, 17432, 11937,  3066,
       35524, 15620, 34537, 13420,   822,  1514, 17303,  6682, 34855,
       37962, 47267, 27375, 49245, 15594,  9373, 41179, 16392, 37265,
       34125, 20816, 22449, 25644, 28216, 13467, 48355, 23854,  8759,
       44975, 28408, 35042,  3918, 44031,  7592,  7965, 24488, 16160,
       10637, 42175, 39012, 34173, 25820, 32263, 44319,  4060,  6008,
       26846, 33999, 11229, 42945, 44875, 45484, 39875,  1338, 41258,
        2435,  9390, 22123,  1270, 16011, 37881, 36415, 30595, 10990,
       31412, 24549,  9619, 40190,   115, 18854, 26311, 33207, 38037,
       25852,  3992,  6515, 42954,  7289,  7019, 27509,  4360, 30103,
       25915,  7102, 19382,  2994, 17534,  2597, 27404, 35070, 38958,
       46993, 18413, 25910, 26637, 18716, 23463, 48322, 33097,  2569,
       14247, 40283, 39828, 29536, 13722, 13852, 23391, 23248, 43847,
       42282, 14048, 17479, 21080,   275, 44818, 41241,  4496,  3851,
       23425, 31251, 11037, 41568, 31079, 17553, 43655,  7442, 17701,
       16813,  2582, 17539, 24068, 23901, 43015, 24762, 46956, 42976,
       11831, 19351, 12150,  6922,  9668, 10276, 18160, 28245, 26467,
       49589, 30486, 42143, 22983, 31338, 44185, 35344, 28822, 20731,
         354, 33939, 36086,   815,  8232, 46140, 31102, 30166,  6895,
       47362, 46213,  3212,   676, 49411, 34720, 31445,  2084, 34249,
        2318, 29765,  3327,  8197, 23198, 26008, 14851, 38333, 47824,
       28098, 25463, 24269, 45016, 26471, 41254, 42758, 28175, 42318,
       28192, 45268, 15867,  8350,  2180,   330, 13156, 34301, 25105,
       30119, 21593, 21540,  9043, 46173, 12348, 32319, 21992, 24725,
       42697, 33237, 38691, 21003, 28220, 35335, 39405, 11776, 24044,
       12303, 37145,  4464, 29791,  2185, 12523, 16774,  2067, 28810,
        6411,  9889, 23263, 40000, 17205,  9535, 12516, 49435, 47035,
       38086, 40595, 27814,  1377, 24041,  4332, 15779, 31523, 38515,
       22859,  2841, 32226,  8566,  5822,  6356, 40024, 13509, 34721,
       43453, 48791, 46649, 39195, 18583, 46238, 16850, 42834, 39404,
       42307,  8221, 43711, 46731, 31539, 18639, 39805, 29546, 19181,
       32886, 41398, 33252,  4385, 28089,  1574, 19450, 47745, 13088,
       20856, 35531,  5572, 42294, 43859, 32994, 22659, 46240, 10535,
       18890, 35134,  7778, 33569,  4185,  8189, 36542, 43132,  9318,
        3907, 28383, 29143, 29555,  9380, 47453, 29675, 17802, 29252,
       20897, 46461, 12047,  8735, 40394, 23789, 18961,  7352, 15710,
       21270, 44824, 37512, 49354, 42305, 41403, 48168, 39915, 25580,
       42870, 10475,  2464, 17624, 46854,  4848, 40671,  9324, 36267,
       15648, 24039, 19720,   139, 35388, 26863,  7941, 26328, 11141,
       19309, 23031,  6044, 42280,  6379,  2857, 41889, 19413,  4758,
       14891, 24905, 48877,   602,  3039, 40169, 49851, 38322, 46582,
       34094,  9648, 40191, 35416,  8348,  1673, 36579, 48774, 12442,
       25501, 26170,  5360, 21842, 12024, 35345, 11598, 23319, 23267,
       32574, 10171,  4812, 10572, 30035, 26977, 24131, 45062, 39999,
       11577, 19887, 37104, 42521, 31313, 32178, 43913, 31950, 21267,
       39658, 45907, 11529,  7045,  8952, 26369, 31121, 48225, 21596,
        9055, 44888, 18378, 49976, 43400, 24140, 13562, 22849,  9109,
        4304, 36882, 20491, 21830, 21042, 10962, 18840,  3430, 45461,
       45243, 46944,  8544, 13582,  8982,   460, 15011, 16942, 29786,
       36628, 27241, 11674, 29055, 30472, 39590, 42810, 40192,  3518,
       25002,  1671, 32896, 27056,  9326, 25176, 17168, 44205, 20530,
       11010, 32322, 43409, 49967,  4575, 28918, 42162, 13388, 15914,
        5794, 48821,  3898, 21157, 39207, 37659, 20182, 10620, 36892,
       40926,  6004, 15007, 11576,  5948,  3469, 30294,  1391, 21377,
       35864, 21484, 38809, 29287, 46128,  6160,  5787, 47886, 25888,
       34522, 38421, 21036, 27261, 28115, 23399, 35938, 34712, 46048,
       49789, 11265, 14872,  9469, 31644,  9645, 13810, 41727, 46615,
       36874, 26131, 15346, 22064, 37858, 21295,  3170, 43858, 18327,
       20509, 25669, 36636,  5339,  6250, 13015, 23307, 39605, 10483,
       31498, 47628, 49757, 25124, 19547, 21741, 17696,   567, 41042,
       17516,  6192, 49378, 16167,  3067,  7737, 17601, 17327, 41867,
       48286,  7468, 37524, 26856, 36262, 32014, 15276,  9488,  7104,
         860, 34894, 19020, 21608, 35917,  4066, 27692, 31456, 41059,
       12277, 17705, 21823, 33078, 35894, 25364, 39298, 47376, 26694,
       40368, 35736, 18236,   915,  4603, 10829, 30276, 40735, 27524,
       38777, 21883, 15901, 29169,  7793, 36456, 34796, 26152, 17240,
       13609,  4371, 46629, 47663, 25960, 48949, 24234, 28285, 47511,
        6178, 39453,  5519, 11717, 23444, 17765, 19601, 23229,  5801,
        3407, 21537, 33786, 23749, 34538, 18943, 24615, 33930, 17435,
       48827,  2529, 46833, 43121,  3671, 13586,  5013,   225, 13024,
        5332, 18947, 27622, 26597, 22152, 12182, 37993, 32595,  4665,
       34483, 21866, 39801,  8428,  8517,  8575, 29514, 14695, 20187,
       42681, 22296,  7457,  4701, 19377,  9511,  1223, 41905, 43281,
       40068, 23287, 20943, 41348,  6152, 19593, 13705, 16744, 14100,
       28046, 26447, 48051,  7691, 26508,  5671, 37141, 30412,  7042,
       33900, 29094, 41640, 35498, 19661, 22433, 13577,  4289, 37737,
       47690, 21065, 14077, 29013,  9106,  2379, 38056, 17731, 19289,
        8811, 27439,  6286, 19410, 46345, 29609,  1034, 31638, 27198,
       25883, 32841, 37490, 33668, 10421, 32778, 49238, 40157, 11104,
        1706, 38602, 29335, 42387, 17201, 33671, 21555, 11213,  8951,
         360, 30073, 44157, 10071,  1767, 35330, 38516, 32717, 24152,
        6038, 27566, 31869, 32163, 46298,   657, 40742, 37538, 48743,
       28959, 10533, 15027, 48868, 42278,  7746, 13148, 41634, 32158,
       33856, 43377, 35206, 35961, 10859, 37454, 17406, 21648, 29349,
       35047,   666,  3023, 49694, 37540, 46774, 27675,  3923,  4587,
       47637, 37031,  3697, 36401,  3097, 35657, 20472,  2351, 34158,
       30179, 33639, 40083,  9739,  8159, 29938,  5978, 47944,  9244,
       27588, 48734, 14626, 27589,  9461,  4704, 33839, 13904, 36288,
       35138, 20925,  1774,  8644,  4252, 46656, 16909, 25001, 30678,
       12383, 11533, 37535, 16347, 28166, 40722,   884, 21583, 45860,
       47083, 27997, 17021, 11373, 46781,  6171, 42458, 30215,  3643,
       33982, 39522, 37600, 47448,  2437, 10252, 13889, 48751,  8126,
       31115]), [0, 1, 8, 9]), (array([24658,  3862,  9010,   693, 29770,  6021, 16461, 25898, 18340,
        6527, 37075,  6586, 21753,  2691, 41953, 40195, 39146, 39656,
       46415, 21838, 41770, 14996, 18559, 33434, 39009, 49727, 39772,
       31349,  5940, 15656, 14815, 23455, 42403, 29663,  3587, 42512,
       35624, 25949, 29466, 17033, 45773,  6924,  4053, 43683, 14415,
       30815, 48478, 15503, 41023,  7574, 28489, 23029, 39740, 26653,
       18099, 16254,  5289, 17585, 25441, 38995, 12179, 27853, 20575,
       41518, 36142, 46153, 39878, 40510, 27923,  7407, 38353, 35113,
       28226, 15582, 44473,  9041, 33215, 36960, 25017, 30907, 30911,
        2712, 12103, 26602, 13186, 22536, 20810, 33027, 29629, 25803,
       17937, 33223,  3690, 46526, 20409,  4296, 41305, 33080, 18699,
       25529, 28040,   930, 33735, 23420, 36225, 42303, 21978, 15737,
       24769, 17632,  2327, 38123, 39177, 22750, 20798, 36003, 18279,
         434, 28803, 28654,  3252, 12334, 43222,  1256, 23460, 41003,
       25992, 13412, 18690, 15802, 15428, 42544, 20782,   951, 27455,
       39209, 33501, 45571, 12416,  1158, 18293, 41822,  3089,  3687,
       42695,  3776, 47126, 28334, 20608,  7404, 19627, 44780, 24967,
       22318, 10150, 23057,  4763, 10661, 47716, 44558, 14704, 21341,
       49595, 34365, 27466, 26894, 37827, 31290, 18734,  7775,  4660,
       41699,  2830, 44760, 35634, 43793, 40028, 35250, 13025, 22018,
       34051, 30522, 14181, 32786, 30790, 48812,  1348, 19098, 38660,
       27860, 16195,  5619, 27193, 22361, 18566, 38852, 31143, 30499,
        7382, 25785, 26844, 14937, 44978, 45512, 18836, 25941, 45586,
       38386, 10350, 38103,  5491,  5347, 28100, 17728, 24883, 40665,
       24265,  8523, 37281, 37124, 35180,  8404, 19833, 49597, 36923,
         677,  2216, 17613,  4043,  6475,   543, 25834, 10561, 10970,
       25620,  2155, 23743, 31470,  8131, 25147, 46644,   925, 33266,
       11586, 46416, 29102, 35429, 30225, 31840,  6844, 21155, 31301,
       24242, 38698, 49412, 33953,  8474, 28925, 32859, 31792, 29751,
       16367, 24531, 31270, 19511,  3048, 40041, 31984, 33630,   854,
       21435, 17562, 39680, 19308, 42679, 26197, 28607, 43269, 27346,
         204, 45918,  1342, 36922, 16687, 38771, 16685, 49771, 36913,
       15830, 18282, 35535, 37270, 38061, 31135, 10792,  7025, 32880,
       37940, 11602, 31672, 46271, 21072, 36069, 32869,  8603, 48897,
       31570, 29284,   645, 36602,  9256, 18988, 14864,  9978,  2774,
       42391, 20146, 41916,  6582,  7034, 25892,   488, 45443, 38508,
        3262, 22746,  4824, 37380, 29217, 40396, 31080, 31928, 15497,
       17847, 21146, 23154, 48619, 17427, 41969,  7524, 14065, 49285,
       44590, 10704, 32648, 35891,  3266, 38738, 14188, 11751, 22290,
         103, 44194, 49364, 28776, 26190, 43026,  4926, 33967, 27022,
       38475,  9029, 20293,  9521,  1842, 25964, 23780,  8873, 23050,
        8564, 26387,  4751, 15962, 31508, 25511, 14443, 48400, 34033,
       31312, 14984, 35455,  9381, 33425, 49129, 18396,  4550, 36164,
       10649, 49798, 36232, 44018, 48401, 23230, 26335, 41668,   680,
       30350, 14638,   132, 43182, 47792, 35225, 36373, 16789, 26192,
       39415, 46991, 34805, 16389, 36638, 19870, 13619, 33733, 27688,
        8460, 24298, 22082, 11634, 29009,   655, 31852, 16998,  9953,
       48277,  7264, 26415, 34926, 16287, 38583, 49928, 12459, 23896,
       20853, 24732, 28663, 10600, 19665, 19281,  9924, 46622,  2650,
       19420, 40313, 20025, 29327, 13878, 24424, 23917, 36515, 16562,
       41558, 23216, 14513, 11911, 30089, 14411, 38665, 42641, 23539,
       18224, 41272, 49061, 39544, 17342, 21264, 18090, 27830,  7422,
       42145, 42724,  8956, 28490, 46995, 25835, 13291, 15039,  5443,
        4570, 32958, 23135, 38773, 15498,   437, 20687,  4752,  2925,
        7959,  5474,  9323, 37457, 49404,  3698, 10117, 43074,  9706,
       21719, 10444, 27450,  3567, 21192, 21972, 31652, 44984, 22471,
       18693, 24414, 47021, 29446, 44685, 12426,  2670, 20727, 36371,
       48911, 29175,  2923, 15795, 36390, 49358, 44659, 15676,   888,
        5245, 14823, 46636, 49712, 29530,  1300, 12867, 30639, 19817,
       16435, 33145, 33029, 49389, 13684, 24860, 19618,  1822, 16572,
       24286,  4507,  1174,  1032,  7484, 16147, 10810,  4892, 28295,
       49543, 48164, 29344, 33537, 10864,  9693, 10923, 46029, 19123,
       13115, 23321, 36856, 20298, 35078, 42571, 21546,   870, 27621,
       12206, 27057,  9674, 41314, 19167, 46787,  7534, 47960, 19658,
        9483, 15241, 26214, 18959, 40344, 48127, 34582,   987,  1138,
       48281, 46008, 44438, 28537,  7012, 31872, 18141, 48133, 21164,
        1375, 19219, 44807,  4611, 43612, 29661, 12041, 33372, 11591,
       42202, 34683, 31745, 40408, 49778, 31085, 21515, 30647, 39776,
       16902, 21679, 27124, 43623,  2471, 10524, 39702, 13197, 18125,
       43713, 28287, 18797, 43357, 10511, 23080, 47694, 46985, 22583,
        5643,   892, 47562,  3765, 10068, 40429, 16156, 32611, 37044,
       36949, 17935, 14123, 21427, 19733, 35171,  1236, 33756, 39072,
       21312, 30504, 36942, 32726, 14901, 38625, 17783,   244, 36214,
       26004, 26402, 32351, 15416,  7614, 19605, 28706, 22760, 16084,
        3330,  4210, 38274, 41002, 34212, 28204, 21611, 47854, 12207,
       32265, 15022,  9913, 38619,  4469, 19214, 26938, 35912,  7329,
       49501, 34104,  7310, 41507, 41674,  7281,  2220, 12725, 10975,
        9160, 40110,  4988, 31069, 17685, 23567, 32037,  8766, 12258,
        4916, 26454, 15617, 26608,  6418, 17020, 26239, 28642,  2272,
        2974, 48905,   170, 39956, 23669, 34004, 10817, 25399, 49568,
       18700,  1014, 35536, 12487, 47019, 20786,   795, 22551,  8926,
       38089, 10997, 14046, 10016, 39094, 32234,  1732, 17891, 16532,
       34815, 40179, 16620, 19464, 38008, 42677, 13139, 23765,  9796,
       28012, 30529, 32622, 26209, 24408, 29990, 17404, 29565, 14458,
       46955, 18000, 22898, 48140, 43946, 14510, 32851, 37126, 31014,
       27146, 16302, 13963, 22925, 20765,  7461, 12816,   756, 39330,
       44511, 16317, 24066,  4136,  9358,  1378, 43787, 47876, 44919,
       29972, 15694, 10312, 29926, 12085, 33028, 49793, 21738, 35625,
        8838, 21562, 16601,  2508, 46190, 33806, 33896, 22637, 13358,
       31845, 44410, 30733,  4340, 46269, 33149, 43137, 42764,  2853,
       12008,  2956, 47124,  4131, 19801, 17918, 21219, 41128,  3090,
       44723, 28318,  4169,  1744, 28601, 33208, 18786, 46687, 41409,
       39743,  6784, 15085, 37427, 24372, 19424, 16255, 12835, 37433,
       15337, 37793,  4115,  2199, 24055, 38300,  3129, 16480,  9911,
       10351, 37071, 17798, 42560, 27380, 21129,  5203, 44380, 12902,
        3876, 44056, 18244,  5637, 12405, 46852, 23284, 45993, 49931,
       25051,  5172, 40507,  4541, 40315, 31089, 46861, 29472,   749,
        8850, 49065, 36381,  8539, 30823, 44332, 30392, 11232, 15096,
       16362,  1277, 49662, 21221, 35521, 40750, 29073, 41396,  3479,
        2548, 41395, 36837, 23341, 45755, 16373, 16923, 36875,  3477,
       22150, 23550, 17313, 29418, 35974,    14, 28534, 33858, 22220,
       46090, 22694, 28572,  1638, 25991, 19824, 13767, 45431, 20995,
       19709, 23647,  7920, 42554, 24133,  9211, 49461, 17042,  3705,
       10144, 28879, 19122, 33794,  2001, 41906, 27365, 41126,  2767,
        4111, 10257, 46186, 34645, 10827, 48442, 39894, 26627,  6834,
       14083, 37201, 43942, 36640,  9036, 49945, 42406, 16092, 40885,
       42343, 24731, 47296, 38687, 36866, 23471, 38153, 28781, 14170,
       38324, 16725, 17176, 16917, 21817, 44289, 43696, 45183,  4028,
       17869, 48461,  3793, 30615, 30985, 39183, 49520, 22346,  4223,
       37363, 48178, 14220, 23303, 24391,  8762, 42349, 23403, 15690,
       42213, 39955, 29384, 18488, 20462, 48033, 37132, 17973, 10208,
       35618]), [4, 6, 8, 9]), (array([18202, 17830, 41371,  4579, 25845, 41499, 38106,  6723, 19999,
       10715, 11826, 47524, 17931, 39475, 35543,  5108, 37364, 34846,
       18921, 30290, 27892,  4002, 47659, 18245, 30698, 29077, 21318,
       25606, 39381, 16436, 10597,  1072, 37632, 25886, 33259,  3060,
       38860, 49541, 29615, 40657, 36350, 47977, 24909, 46422, 20809,
       19071, 26700, 33878,  7750,  8756, 15527, 17375,  1274,  8591,
        9999, 49118, 31409,  2305, 34188, 39229,  2013, 43330,  4009,
       19024, 42745,  7406, 33800, 46393, 19362, 11185, 18521, 24585,
       49508, 41680, 11328, 30099, 45417,  4463,  4692, 41034,  1152,
       47158,  1993, 12198, 29321, 10242, 31038, 22470, 18985, 25065,
       14184, 21879, 27947, 14246, 22225, 12728, 44756, 24380, 47783,
       24663, 33629, 19096, 18626, 31059, 29040, 21179, 39403, 46903,
        5100, 48882, 42293, 48736, 18059, 21077, 34790, 24703, 22021,
       20484, 10549,  1588, 26560, 24309, 17921, 17354,  8407,  1111,
       32427, 21678, 39456, 46289, 38807, 11043, 26735, 32877, 48673,
       12839, 27828, 25284, 11145, 13190, 41475, 22671,  5613, 15488,
       45440, 47090, 35155, 20747, 38305,  1662, 44388, 22430, 15549,
       40044, 30140, 33989, 34444, 18721, 46980, 35547, 18598, 45674,
       27835, 27804, 44223, 30684, 37568, 44284, 32862, 40538, 29251,
        9772, 13142, 33172, 32835, 26917, 47485, 19925, 25919,  5175,
       27805, 16798, 15312, 20315, 30085, 44337, 32764, 22590, 40014,
        8928, 38839, 41817, 44377, 18685,  4316, 43543, 49267,  4662,
        2059,  9132, 27401, 19595, 10831, 19757, 18212, 47160, 36532,
        3794, 11507,  3714, 30718, 35510, 26010, 15301, 31129, 46716,
       27885, 45831, 32935, 37396, 29890, 20207, 35819, 44496, 44792,
       44676, 45181, 18966, 39718, 36471, 22174, 11165, 10104,    83,
        2231, 44988, 26313, 11603, 21502, 39647,  3022, 11610, 20348,
       14391, 12716, 38172, 35652,  9235, 37795, 21233, 22647, 44098,
       45147, 47308, 10384, 22626, 39767,  8187, 23988,  3279, 35623,
        2201, 42955, 34900, 32343,  1896, 38890,  5295, 36303, 14503,
        2397, 20099, 45596, 35005,  3272, 21100, 39717, 40026, 22115,
       34733, 26562, 33585, 26702, 35698, 42078, 36270, 14400, 11278,
       28152, 18823, 20565, 42126,  1931, 11697, 31871, 34010, 38252,
       18258, 13409, 29886,  1582,  3261,  9560, 10905, 24237, 40912,
       40744,   492,  6271,  7874,  9719, 46117, 17929, 34412, 41390,
       37800,  6808, 17503,  4396, 49048, 20903, 21244, 29733, 38780,
       19391,  5047,  2140, 29727, 30617,  4480, 36608, 19064, 27161,
       28884, 43360,   113, 48837, 18869, 37601,  2008, 20998, 25617,
       28553, 44076, 40426,  1149, 45945, 32487, 16355,  4264, 18914,
        2813, 34291,   739, 29622, 32399, 27381, 17804, 19119, 48823,
       48741,  1891, 26358, 32815,  5241, 47923,  5564, 24853, 10735,
         191, 45561, 49310,  6821, 10496, 45058, 47246, 15543, 49912,
       25968,  5126,  9184, 14837, 45540,  4736, 47864, 10994, 22198,
        8115, 47346,  9443, 24439,  8038, 46671, 12759, 16198, 38142,
        8897, 44839, 33652, 22595, 33605, 18219,  9689, 36936, 28735,
       23621,  7613,  4325, 35483, 19263, 21113, 12985, 19940, 31839,
       23873, 25804, 20738,  4898,  9486, 19956, 32579, 17643, 10459,
       30925, 47286,  6460, 22267, 47285, 44664, 42500, 29326, 20707,
       40692, 43066, 29778, 42996, 43544, 21410, 43844,  7611,  7577,
       22739,  3104, 40107, 33636, 34599, 22612, 17944, 16764, 42137,
        6680, 34339, 28240, 38597,  9958, 18524, 10778, 47323,  9082,
       44353, 21599, 46167, 40866, 28977, 28206,  3573, 45610, 27998,
       17398, 34868, 35284, 15889,  8277, 18916, 25811, 27682,  6164,
        7831,  1716, 28914, 33254, 37423, 43587, 17467,  4820, 41416,
       37467, 29137,   595,  2195,  5958, 37190, 48055, 35020, 41973,
       38962, 16881, 45874, 21101, 14356,  7437, 12646, 25513,   106,
       22132, 26623,  8235, 48125,  5185, 19706, 25906,  9975, 34983,
       14011,  5546, 48156, 27612, 43048, 49144,  6176, 48462, 26112,
        1255, 45917, 42917, 30664, 17162, 31903, 18397, 39653, 39317,
       40019, 33190, 32897, 27643, 38607, 12172,  8455, 17519, 36766,
        5365,  9206, 35210, 42265,  8830, 32663, 14624,  3094,  3244,
       24127, 31018,  2483, 27252, 25258, 38904, 43198,  6880, 21257,
       46926, 30941, 19283, 14149, 17799, 11244, 17813, 44357, 31692,
       18171, 26098, 26172, 27068,  9023, 14045, 46102,   901, 49681,
       24740, 19748,  4303, 25566, 24444, 13893, 13623, 18076, 31725,
       14826, 32448, 39462, 28599,  1970, 35150, 20089, 18950, 36484,
       17022, 29556, 44490,  3614, 35586, 28829, 26182, 27618, 37420,
       45784, 35169, 28929, 41732, 40147,   503, 25088, 11977, 48597,
        4823, 15429,  8180,  2954, 48737, 37534, 46395, 25987,  6598,
       44577, 19028, 25728, 32560, 47784, 18312,  7678, 42367, 12490,
       30123, 32765, 10003,  3121, 10408, 16430, 47646, 30444, 27061,
       44569,  8940, 38689, 15135,  1821, 17933, 26948, 29014, 39015,
       40415,   943, 23827, 36733, 11001,  9159, 13355, 47814,  9567,
       30788, 23731,  3250, 46477, 12422, 33571, 24855, 41413, 21943,
       33661, 17120, 39210,  1782, 16330, 42770, 24588, 22177, 25744,
       28323, 35562, 13095, 33356,  4580,   627, 11461, 23890,  5987,
        4828, 28153,  9844,   609, 36678,  9951, 19342, 16729, 29479,
        2658, 44414, 49955, 25072, 47881, 24629, 38371, 12452,  1506,
       17630, 22010, 27861, 32578,  7334, 28103, 28066,  2721, 39454,
       46699,  2454,  2523, 32506, 35035, 22524, 38901, 40609, 13923,
       30252, 29690, 36253, 22584, 43807, 26703, 38969, 16847,  5283,
       38158, 30514, 20479, 15422, 46028, 25040, 20662, 21483, 48175,
       39142,  3541, 35617, 47793, 41163, 24347,  4029,  4742, 44382,
       44927, 14866, 46068, 27395, 49405, 44010, 18968, 27975, 37606,
       18467, 36309,  9787, 32249, 41920, 45735, 26506, 39993, 34730,
       11457, 28846, 17977, 20248,  7694, 48123, 25036, 40082, 39586,
       25507, 14278, 18166,  1940, 11547, 39746, 49158, 40923, 18030,
       45465, 39427, 23091, 34772,  6288, 40059, 20176, 29909, 31662,
       32273,    76, 27005, 45829, 36118, 18792, 34573, 27603, 24537,
       19563, 46280, 21954, 30343, 26650, 12653, 39841, 13717, 25988,
       44066, 36521, 49637, 43199,  8567, 41765, 29401, 42454, 45197,
       33739, 10306,   214, 45388, 49398, 32146,  3006, 21507, 47656,
        8402, 46024, 34251, 14564, 10246,  9261, 42452,  8788,  6708,
       30247, 11984, 17296, 15159, 47002, 27575, 17371, 31056, 44212,
       21592, 25994,  5071,  5352, 46243, 45241, 31815, 34175, 36475,
       25285, 12531, 49278, 49244,  1773, 49385, 40237, 26013, 40063,
        3345, 21273, 31637, 15420,  9513, 20441,  8565, 11275, 33656,
       38393, 21300, 22233, 25393, 31162,  8075, 48455, 28159, 46830,
        2757, 39781, 10448, 45405, 32115, 34753, 49300, 13125, 22915,
       23464, 39564, 49453, 34077, 45167, 31483, 21182, 36266,  2477,
        2218, 13531,  5191, 26970, 20069, 32040, 32940, 41786,   541,
       34957, 21010, 30178, 14610, 16293,  8896,  1026, 19248,   306,
       33316, 21542, 45577,  3597,  9683, 42556, 47592,  8821, 22815,
       28888, 29574, 29529, 43325, 18642,  5056, 49125, 35288, 25506,
       46788,   219, 35178, 17052, 27784, 25338,  9971, 49074, 15597,
        7703, 16051, 20013, 40222, 28656,  1602, 20600, 43882, 17336,
       43900,  8394,  1456, 26218, 44810, 23961,  3216, 16107, 42140,
       18394, 16728, 39546, 18252, 16631, 46302, 36818, 25907,  8012,
       11720,  4454, 31905,   883, 18989, 31205,  6149, 37751,  4826,
       41415, 15031, 35280, 42714,  3685, 16234, 17061, 29825,  3719,
       49284, 20250,  3526, 10658, 11311, 48766, 24027, 18846, 10187,
       43634]), [5, 7, 8, 9])]
Collaboration
DC 0, val_set_size=1000, COIs=[2, 3, 8, 9], M=tensor([2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.226, 0.04464438664913178)
DC 1, val_set_size=1000, COIs=[0, 1, 8, 9], M=tensor([0, 1, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04425914561748505)
DC 2, val_set_size=1000, COIs=[4, 6, 8, 9], M=tensor([4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.244, 0.04420206964015961)
DC 3, val_set_size=1000, COIs=[5, 7, 8, 9], M=tensor([5, 7, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04452222609519958)
D00: 1000 samples from classes {8, 9}
D01: 1000 samples from classes {8, 9}
D02: 1000 samples from classes {8, 9}
D03: 1000 samples from classes {8, 9}
D04: 1000 samples from classes {8, 9}
D05: 1000 samples from classes {8, 9}
D06: 1000 samples from classes {2, 3}
D07: 1000 samples from classes {2, 3}
D08: 1000 samples from classes {2, 3}
D09: 1000 samples from classes {2, 3}
D010: 1000 samples from classes {2, 3}
D011: 1000 samples from classes {2, 3}
D012: 1000 samples from classes {0, 1}
D013: 1000 samples from classes {0, 1}
D014: 1000 samples from classes {0, 1}
D015: 1000 samples from classes {0, 1}
D016: 1000 samples from classes {0, 1}
D017: 1000 samples from classes {0, 1}
D018: 1000 samples from classes {4, 6}
D019: 1000 samples from classes {4, 6}
D020: 1000 samples from classes {4, 6}
D021: 1000 samples from classes {4, 6}
D022: 1000 samples from classes {4, 6}
D023: 1000 samples from classes {4, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.0756532991528511) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.07162128928303718) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.346, 0.10060960775613785) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.262, 0.09178792214393616) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.08844312906265259) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.08061573424935341) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.407, 0.13195230773091315) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.319, 0.1186123097538948) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.373, 0.09776843130588532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.428, 0.0912200716137886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.15404716596007348) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.14389754155278206) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.382, 0.116108429312706) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.1028368262052536) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2008201377093792) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.19386132979393006) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.383, 0.11486499130725861) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.12005862887203693) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.2020772139430046) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.1812336931824684) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.396, 0.12786139059066773) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.12526907271891832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.411, 0.2141107092946768) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.188261329382658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.391, 0.12239557676017285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1390012750029564) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.363, 0.259074219936505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.19005238442867994) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.397, 0.1381242759525776) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.1493007049560547) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.21411252587661148) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.2096376870945096) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.393, 0.1398952288478613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.16985845598578453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.241391664955765) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.22949583377689123) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.15190104061365128) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.1669399950876832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2810998134780675) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.2314559705518186) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[8, 9], M=tensor([0, 1, 2, 3, 4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.7033333333333334, 0.018940463066101074)
DC Expert-0, val_set_size=500, COIs=[2, 3], M=tensor([2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.822, 0.014769307374954223)
DC Expert-1, val_set_size=500, COIs=[0, 1], M=tensor([0, 1, 8, 9], device='cuda:0'), Initial Performance: (0.922, 0.006159715548157692)
DC Expert-2, val_set_size=500, COIs=[4, 6], M=tensor([4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.874, 0.011269920859485865)
SUPER-DC 0, val_set_size=1000, COIs=[2, 3, 8, 9], M=tensor([2, 3, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[0, 1, 8, 9], M=tensor([0, 1, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[4, 6, 8, 9], M=tensor([4, 6, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7469d82e7cd0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7469d879ca60>, <fl_market.actors.data_consumer.DataConsumer object at 0x7469d810b4c0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7469d85c0940>, <fl_market.actors.data_consumer.DataConsumer object at 0x7469d86ea490>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO2', '(DO0']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.01445788037776947) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005215799026191234) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.010813547974452376) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.21295269902795552) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8526666666666667, 0.01522644450267156) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.44, 0.09065570414066315) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.518, 0.05886350527405739) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.532, 0.05884190040826798) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.822, 0.014285830408334732) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.005299063481390477) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.00976587244682014) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.19367254177480936) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8613333333333333, 0.01219196431338787) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.08516356864571571) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.52, 0.06767355227470398) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.527, 0.06419181653857231) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.015307797759771346) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.0055487329289317135) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.015162688562646508) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.18755929440259933) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.902, 0.012548698196808497) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.09235125199705363) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.492, 0.07955021223425865) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.503, 0.07213012963533401) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.818, 0.016424855917692185) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.00492060574516654) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.014898478833027183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.19021719025820494) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.872, 0.01508875930806001) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.0933035658672452) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.09102719366550445) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.48, 0.07125089302659035) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.015222140178084374) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.0060725552206858996) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.842, 0.018904029788449406) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.18578734716121106) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.868, 0.01754028875256578) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.10923472580313683) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.0740371531099081) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.08562864966690541) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO1', '(DO0']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.01647471594810486) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006122789079323411) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.906, 0.0088485938757658) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.15772271784767508) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9006666666666666, 0.008298041152457396) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.06672745917737484) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.538, 0.06520988719165326) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.606, 0.05088705503940582) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.01628545966744423) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006044520661234855) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.014926997119560838) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.1542983346581459) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8893333333333333, 0.011770999485005935) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.497, 0.07985042759776115) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.07345281203836203) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.536, 0.06861898541450501) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.016301411390304565) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.0069614253528416156) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.834, 0.020457370555959642) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.17858572719059884) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.892, 0.010142080453534921) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.492, 0.08685854810476303) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.55, 0.05896482157707214) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.482, 0.08614892202615738) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.015218684166669846) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.926, 0.007077121891546995) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.916, 0.010085758820176125) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.1683619604986161) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.010501100165769458) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.0793286216109991) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.519, 0.08103579342365265) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.557, 0.07063467359542847) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.848, 0.015581462532281876) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.007897038984578103) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.01230377563741058) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.1753113187327981) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.896, 0.012759264660067856) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.509, 0.0746499548703432) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.07354620133340359) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.575, 0.05314194640517235) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO4', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.01757722046971321) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006620996151119471) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.01234190334007144) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.17384082200005652) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.007985775039841731) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.514, 0.07597269898653031) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.549, 0.05991627837717533) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.09545189075171948) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.818, 0.016285877197980882) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.928, 0.007886818094179035) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.908, 0.010300548270344734) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.17893289494514467) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8973333333333333, 0.010238145041589935) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.499, 0.08815364708006382) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.09526338750869036) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.566, 0.05853416404128075) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.015477276831865311) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006050361011642962) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.009951291114091872) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.19317091500014066) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8913333333333333, 0.012222408751646678) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.08408043165504932) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.08852057386934757) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.538, 0.06171893343329429) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.01951301673054695) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006817099586129189) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.012340856954455375) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.1503869197424501) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9066666666666666, 0.010322435490787029) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.08614813435077667) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.08828209254145622) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.54, 0.07647749982774257) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.018089521169662476) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.0066207626685500145) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.9, 0.009616134379059076) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.16468119308166207) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8913333333333333, 0.014365110360085965) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.508, 0.0867549844533205) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.10239999255537986) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.508, 0.08280931860953569) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO3', '(DO5']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.018696845710277556) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.006404670159332454) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.011557533377781511) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.16493568912707268) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9173333333333333, 0.008310801284387707) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.544, 0.06277044877409935) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.534, 0.07819600398838521) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.605, 0.05384016966819763) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.018485023617744446) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.007262152922339738) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.01647677287273109) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.14948243882134557) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9166666666666666, 0.01028827831754461) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.07759452161192894) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.09704731894284487) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.561, 0.0656603424847126) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.01911019292473793) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.008173044146853499) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.904, 0.01087135074287653) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.15988409276865423) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9186666666666666, 0.010540908331555935) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.522, 0.07111541824042797) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.08469776330515742) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.596, 0.055098446168005466) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.021312727123498916) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.007149147383403033) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.010188396576792002) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.1693710927357897) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.017368419406033352) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.504, 0.08543869444727897) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.10860897288098931) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.533, 0.08045705756731332) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.018644760221242906) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.00647370872553438) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.904, 0.011289307996630669) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.17234602588042616) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.91, 0.012258700338502725) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.52, 0.08385305090993643) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.09882121078670025) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.542, 0.07658213359117508) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO0', '(DO4']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.82, 0.021633977875113487) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006189299354329705) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.9, 0.010841248914599418) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.16494573657214642) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.012274015419806043) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.0610831141769886) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.08561851293593645) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.55, 0.06747185990959406) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.01894030261039734) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.006641085559502244) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.01105309473350644) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.19016445506550372) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.922, 0.013241922524612164) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.538, 0.06880183739960194) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.518, 0.07486015256121754) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.571, 0.0576581045538187) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.021262720584869384) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.007094544844236225) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.01404156125150621) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.18177023543044923) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.912, 0.014500487727113069) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.566, 0.0631212518364191) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.504, 0.0984944821447134) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.57, 0.0590284585878253) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.02130261142551899) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006516481867060065) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.011057869736105204) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.1739214154407382) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.012722569568315521) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.524, 0.07495444869995117) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.519, 0.08767963515594601) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.07045598245412112) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.019120913624763487) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006772373139858246) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.014564190515317023) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.18391814065352083) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9113333333333333, 0.010673214407482494) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.06977217438817024) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.496, 0.09384086868539453) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.564, 0.06544211456179619) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO4', '(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.015905366078019144) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.926, 0.007255611412227154) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.9, 0.0118367641530931) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.22059592531388625) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.916, 0.008911963153940937) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.06895251470804215) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.533, 0.07819277842342853) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.587, 0.055494167000055314) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.01853388234972954) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.928, 0.007953267926350236) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.013247746664099395) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.1959420909686014) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.011994736212946009) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.07156913904845714) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.527, 0.07679987812042237) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.546, 0.07132971981540322) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.020485101968050003) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.007471787541173398) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.91, 0.011216869659721851) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.1774212521351874) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9253333333333333, 0.015920413290574895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.53, 0.07400226824730635) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.08430809417180717) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.526, 0.08333327609486878) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.01786679480969906) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.928, 0.009059121762868017) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.011553013723343612) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.434, 0.18342899552173914) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9213333333333333, 0.013208549580075972) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.546, 0.06958408690989018) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.534, 0.07590701751410961) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.554, 0.06858838450349868) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.017687623500823976) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.008090203937143088) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.906, 0.009051604487001897) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.1775266417134553) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9093333333333333, 0.010717374987434596) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.06428237552940845) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.07145361306518316) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.55, 0.07179580983519554) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO2', '(DO4']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.020441191375255584) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.007571199448080734) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.914, 0.009824085645377637) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.219153903439641) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9046666666666666, 0.011823906699816386) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.05983344481885433) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.53, 0.07180483414232731) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.579, 0.05232526844739914) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.01577623349428177) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.007240729104727507) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.908, 0.008983828730881214) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.432, 0.20828195328824223) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.896, 0.01709332208498381) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.5, 0.0859482376947999) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.548, 0.05616217777878046) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.517, 0.0690343581866473) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.018016097500920297) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006887944793328643) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.908, 0.009867772920057178) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.22052928413078188) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.914, 0.015997375016027944) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.556, 0.06671360167860985) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.08285149566829204) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.513, 0.07258615170884877) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.021974868923425674) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.007488363738171756) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.906, 0.011227817993611097) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.437, 0.22062871767766773) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.01746513661959519) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.57, 0.06561553189530969) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.502, 0.09439998876303435) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.565, 0.06271922735869885) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.01872861948609352) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.008816957137547434) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.008866506613790988) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.2397286328189075) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9133333333333333, 0.016974697756270567) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.07835034070909024) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.07549186962842941) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.512, 0.09044934190064668) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO0', '(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.01746873849630356) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.008061360475607217) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.01339073950611055) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.438, 0.24962055403599515) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9246666666666666, 0.00828350136401908) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.06405942055583) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.06696760781668126) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.596, 0.05787309277802706) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.826, 0.01906432358920574) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.007208043339196593) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.00861977717280388) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.1903327561840415) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9253333333333333, 0.010635705579460288) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.505, 0.08750724052637815) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.07999397804588079) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.505, 0.09274848172068596) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.816, 0.021525886625051498) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.008575298467185348) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.908, 0.011359835166484117) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.23261421816982328) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9193333333333333, 0.01560398662385584) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.536, 0.07847019296139479) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.542, 0.08110601382795721) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.521, 0.08291890720650553) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.83, 0.017401942014694215) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.010884864413907054) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.010477716587483882) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.18807860923558473) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9233333333333333, 0.01110760686507759) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.06858479478955269) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.577, 0.06004483040422201) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.06028210745751858) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.017225889801979064) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.008008517371257767) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.908, 0.01233197022229433) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2306639950722456) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.92, 0.012196320804068819) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.505, 0.08430196317844092) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.0811463918313384) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.53, 0.07173769508302212) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.226, 0.04464438664913178), (0.25, 0.0756532991528511), (0.338, 0.08844312906265259), (0.373, 0.09776843130588532), (0.382, 0.116108429312706), (0.383, 0.11486499130725861), (0.396, 0.12786139059066773), (0.391, 0.12239557676017285), (0.397, 0.1381242759525776), (0.393, 0.1398952288478613), (0.411, 0.15190104061365128), (0.44, 0.09065570414066315), (0.488, 0.08516356864571571), (0.488, 0.09235125199705363), (0.468, 0.0933035658672452), (0.452, 0.10923472580313683), (0.537, 0.06672745917737484), (0.497, 0.07985042759776115), (0.492, 0.08685854810476303), (0.527, 0.0793286216109991), (0.509, 0.0746499548703432), (0.514, 0.07597269898653031), (0.499, 0.08815364708006382), (0.518, 0.08408043165504932), (0.529, 0.08614813435077667), (0.508, 0.0867549844533205), (0.544, 0.06277044877409935), (0.518, 0.07759452161192894), (0.522, 0.07111541824042797), (0.504, 0.08543869444727897), (0.52, 0.08385305090993643), (0.558, 0.0610831141769886), (0.538, 0.06880183739960194), (0.566, 0.0631212518364191), (0.524, 0.07495444869995117), (0.537, 0.06977217438817024), (0.543, 0.06895251470804215), (0.527, 0.07156913904845714), (0.53, 0.07400226824730635), (0.546, 0.06958408690989018), (0.552, 0.06428237552940845), (0.549, 0.05983344481885433), (0.5, 0.0859482376947999), (0.556, 0.06671360167860985), (0.57, 0.06561553189530969), (0.531, 0.07835034070909024), (0.563, 0.06405942055583), (0.505, 0.08750724052637815), (0.536, 0.07847019296139479), (0.537, 0.06858479478955269), (0.505, 0.08430196317844092)]
TEST: 
[(0.236, 0.04354009944200516), (0.25025, 0.07285457700490952), (0.325, 0.08505733188986778), (0.3635, 0.09408129823207856), (0.36725, 0.11148307979106903), (0.381, 0.10974129223823548), (0.39125, 0.12281110650300979), (0.38925, 0.11663438200950622), (0.393, 0.1320787268280983), (0.38825, 0.133778289437294), (0.39625, 0.14531275349855424), (0.434, 0.08744479125738144), (0.4565, 0.08236774027347565), (0.4705, 0.09129599690437316), (0.45475, 0.09456987178325653), (0.43875, 0.10902708837389946), (0.524, 0.06775200146436691), (0.4895, 0.08246075221896172), (0.48275, 0.08484119349718094), (0.51075, 0.08103922924399376), (0.513, 0.07558903020620346), (0.508, 0.07758306628465653), (0.48175, 0.09250008210539817), (0.50325, 0.08790318948030472), (0.506, 0.08752951675653457), (0.483, 0.09064721277356148), (0.5515, 0.0643068041652441), (0.5055, 0.08121502935886384), (0.52975, 0.07233673159778119), (0.5, 0.08666911271214485), (0.5115, 0.0858145907819271), (0.542, 0.06645361071825027), (0.5195, 0.07489210060238838), (0.5305, 0.06693767760694026), (0.51625, 0.07525486040115356), (0.52925, 0.07109891547262669), (0.52375, 0.06978003266453743), (0.50025, 0.07473283216357231), (0.495, 0.07749154457449912), (0.522, 0.07523895308375358), (0.528, 0.06649359102547168), (0.5275, 0.062413141518831254), (0.4795, 0.0890390967130661), (0.53125, 0.0691456139087677), (0.52175, 0.0689204912185669), (0.50075, 0.08019203779101372), (0.52975, 0.06923486804962158), (0.48675, 0.08888361552357674), (0.50375, 0.0827128304541111), (0.51475, 0.07272546267509461), (0.49525, 0.08727152344584466)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.58      0.14      0.23      1000
           3       0.83      0.11      0.19      1000
           8       0.50      0.84      0.62      1000
           9       0.46      0.89      0.61      1000

    accuracy                           0.50      4000
   macro avg       0.59      0.50      0.41      4000
weighted avg       0.59      0.50      0.41      4000

Collaboration_DC_1
VAL: 
[(0.25, 0.04425914561748505), (0.265, 0.07162128928303718), (0.304, 0.08061573424935341), (0.428, 0.0912200716137886), (0.443, 0.1028368262052536), (0.447, 0.12005862887203693), (0.446, 0.12526907271891832), (0.459, 0.1390012750029564), (0.456, 0.1493007049560547), (0.464, 0.16985845598578453), (0.461, 0.1669399950876832), (0.518, 0.05886350527405739), (0.52, 0.06767355227470398), (0.492, 0.07955021223425865), (0.515, 0.09102719366550445), (0.511, 0.0740371531099081), (0.538, 0.06520988719165326), (0.524, 0.07345281203836203), (0.55, 0.05896482157707214), (0.519, 0.08103579342365265), (0.524, 0.07354620133340359), (0.549, 0.05991627837717533), (0.499, 0.09526338750869036), (0.505, 0.08852057386934757), (0.526, 0.08828209254145622), (0.498, 0.10239999255537986), (0.534, 0.07819600398838521), (0.507, 0.09704731894284487), (0.515, 0.08469776330515742), (0.483, 0.10860897288098931), (0.505, 0.09882121078670025), (0.523, 0.08561851293593645), (0.518, 0.07486015256121754), (0.504, 0.0984944821447134), (0.519, 0.08767963515594601), (0.496, 0.09384086868539453), (0.533, 0.07819277842342853), (0.527, 0.07679987812042237), (0.511, 0.08430809417180717), (0.534, 0.07590701751410961), (0.535, 0.07145361306518316), (0.53, 0.07180483414232731), (0.548, 0.05616217777878046), (0.516, 0.08285149566829204), (0.502, 0.09439998876303435), (0.515, 0.07549186962842941), (0.578, 0.06696760781668126), (0.532, 0.07999397804588079), (0.542, 0.08110601382795721), (0.577, 0.06004483040422201), (0.532, 0.0811463918313384)]
TEST: 
[(0.25, 0.04342002448439598), (0.266, 0.06890167579054833), (0.31225, 0.07728723338246346), (0.42825, 0.08714401468634606), (0.44875, 0.09824884018301963), (0.45125, 0.11498481222987175), (0.44725, 0.11927160546183586), (0.4605, 0.13327452754974364), (0.4625, 0.14331845676898955), (0.4655, 0.1620286877155304), (0.467, 0.16024554711580277), (0.51975, 0.056630822494626046), (0.51125, 0.0702774253487587), (0.492, 0.08097690400481224), (0.49375, 0.08969475921988487), (0.49175, 0.07409253802895546), (0.5265, 0.06795313385128975), (0.49725, 0.07494137161970138), (0.53775, 0.05999707154929638), (0.51525, 0.08360905927419662), (0.5165, 0.07255277019739151), (0.544, 0.058328911870718), (0.48525, 0.09531040224432945), (0.4965, 0.08993336504697799), (0.4955, 0.09081367379426956), (0.4715, 0.10604651594161987), (0.5375, 0.07997473150491714), (0.49775, 0.10010641610622406), (0.52475, 0.08745008689165115), (0.49375, 0.10735116481781005), (0.49375, 0.10252225181460381), (0.51475, 0.08626203471422196), (0.51425, 0.07665156573057175), (0.4935, 0.10125279235839844), (0.5085, 0.08821077945828437), (0.49925, 0.0941915782392025), (0.519, 0.08207921048998833), (0.512, 0.07728228831291199), (0.4995, 0.08581819450855255), (0.515, 0.07796697333455085), (0.522, 0.07652207216620445), (0.51075, 0.075532345443964), (0.5435, 0.05639849084615707), (0.50975, 0.08509700262546539), (0.49625, 0.09970147696137428), (0.51475, 0.07656763446331025), (0.55575, 0.06945866784453392), (0.51425, 0.08398903155326844), (0.519, 0.08440753546357155), (0.55725, 0.06329428990185261), (0.5165, 0.08521968302130699)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.70      0.28      0.40      1000
           1       0.84      0.08      0.15      1000
           8       0.54      0.83      0.65      1000
           9       0.45      0.88      0.59      1000

    accuracy                           0.52      4000
   macro avg       0.63      0.52      0.45      4000
weighted avg       0.63      0.52      0.45      4000

Collaboration_DC_2
VAL: 
[(0.244, 0.04420206964015961), (0.346, 0.10060960775613785), (0.407, 0.13195230773091315), (0.432, 0.15404716596007348), (0.437, 0.2008201377093792), (0.371, 0.2020772139430046), (0.411, 0.2141107092946768), (0.363, 0.259074219936505), (0.442, 0.21411252587661148), (0.431, 0.241391664955765), (0.437, 0.2810998134780675), (0.532, 0.05884190040826798), (0.527, 0.06419181653857231), (0.503, 0.07213012963533401), (0.48, 0.07125089302659035), (0.487, 0.08562864966690541), (0.606, 0.05088705503940582), (0.536, 0.06861898541450501), (0.482, 0.08614892202615738), (0.557, 0.07063467359542847), (0.575, 0.05314194640517235), (0.486, 0.09545189075171948), (0.566, 0.05853416404128075), (0.538, 0.06171893343329429), (0.54, 0.07647749982774257), (0.508, 0.08280931860953569), (0.605, 0.05384016966819763), (0.561, 0.0656603424847126), (0.596, 0.055098446168005466), (0.533, 0.08045705756731332), (0.542, 0.07658213359117508), (0.55, 0.06747185990959406), (0.571, 0.0576581045538187), (0.57, 0.0590284585878253), (0.552, 0.07045598245412112), (0.564, 0.06544211456179619), (0.587, 0.055494167000055314), (0.546, 0.07132971981540322), (0.526, 0.08333327609486878), (0.554, 0.06858838450349868), (0.55, 0.07179580983519554), (0.579, 0.05232526844739914), (0.517, 0.0690343581866473), (0.513, 0.07258615170884877), (0.565, 0.06271922735869885), (0.512, 0.09044934190064668), (0.596, 0.05787309277802706), (0.505, 0.09274848172068596), (0.521, 0.08291890720650553), (0.552, 0.06028210745751858), (0.53, 0.07173769508302212)]
TEST: 
[(0.2445, 0.04316330614686012), (0.35275, 0.09631808972358703), (0.40675, 0.12594314509630203), (0.4285, 0.14750396132469176), (0.43075, 0.19383546972274782), (0.37875, 0.19488197696208953), (0.4085, 0.20684573304653167), (0.3695, 0.2504873937368393), (0.435, 0.20733263194561005), (0.43075, 0.2325922714471817), (0.43575, 0.27226708149909973), (0.50175, 0.06153198391199112), (0.5, 0.06683704425394535), (0.48125, 0.0766266683936119), (0.4775, 0.07192145758867263), (0.464, 0.0911725473701954), (0.583, 0.053292827889323234), (0.521, 0.06791215527057648), (0.477, 0.09060272774100303), (0.542, 0.07105990439653397), (0.56575, 0.051864207908511165), (0.4795, 0.09556178751587868), (0.5735, 0.05618124958872795), (0.548, 0.06010780555009842), (0.514, 0.07953997507691384), (0.49875, 0.08516753432154656), (0.5805, 0.056795110195875166), (0.5385, 0.07026551382243633), (0.5785, 0.05826196274161339), (0.5075, 0.08365615251660347), (0.5475, 0.079518943592906), (0.5455, 0.07367760473489761), (0.5655, 0.059779321745038035), (0.541, 0.0617443223297596), (0.53375, 0.0707301438599825), (0.529, 0.07081228104233742), (0.55175, 0.05980093903839588), (0.52675, 0.07484660243988037), (0.5105, 0.08317729865014553), (0.545, 0.06773055656254291), (0.53375, 0.06760517659783363), (0.57825, 0.0525509397238493), (0.5275, 0.0646965194195509), (0.51925, 0.07010046268999577), (0.5615, 0.06332250510156155), (0.4985, 0.09226207286119462), (0.5765, 0.0576936958283186), (0.5085, 0.09659343898296356), (0.508, 0.08376962926983833), (0.54525, 0.061716256484389304), (0.52575, 0.07505012029409408)]
DETAILED: 
              precision    recall  f1-score   support

           4       0.66      0.29      0.40      1000
           6       0.90      0.12      0.20      1000
           8       0.58      0.84      0.69      1000
           9       0.43      0.85      0.57      1000

    accuracy                           0.53      4000
   macro avg       0.64      0.53      0.47      4000
weighted avg       0.64      0.53      0.47      4000

Collaboration_DC_3
VAL: 
[(0.25, 0.04452222609519958), (0.262, 0.09178792214393616), (0.319, 0.1186123097538948), (0.397, 0.14389754155278206), (0.436, 0.19386132979393006), (0.439, 0.1812336931824684), (0.441, 0.188261329382658), (0.443, 0.19005238442867994), (0.44, 0.2096376870945096), (0.444, 0.22949583377689123), (0.447, 0.2314559705518186), (0.44, 0.21295269902795552), (0.443, 0.19367254177480936), (0.443, 0.18755929440259933), (0.436, 0.19021719025820494), (0.445, 0.18578734716121106), (0.437, 0.15772271784767508), (0.43, 0.1542983346581459), (0.436, 0.17858572719059884), (0.443, 0.1683619604986161), (0.439, 0.1753113187327981), (0.441, 0.17384082200005652), (0.443, 0.17893289494514467), (0.437, 0.19317091500014066), (0.448, 0.1503869197424501), (0.444, 0.16468119308166207), (0.444, 0.16493568912707268), (0.43, 0.14948243882134557), (0.436, 0.15988409276865423), (0.444, 0.1693710927357897), (0.439, 0.17234602588042616), (0.451, 0.16494573657214642), (0.441, 0.19016445506550372), (0.439, 0.18177023543044923), (0.448, 0.1739214154407382), (0.44, 0.18391814065352083), (0.441, 0.22059592531388625), (0.442, 0.1959420909686014), (0.443, 0.1774212521351874), (0.434, 0.18342899552173914), (0.44, 0.1775266417134553), (0.445, 0.219153903439641), (0.432, 0.20828195328824223), (0.448, 0.22052928413078188), (0.437, 0.22062871767766773), (0.443, 0.2397286328189075), (0.438, 0.24962055403599515), (0.445, 0.1903327561840415), (0.446, 0.23261421816982328), (0.445, 0.18807860923558473), (0.446, 0.2306639950722456)]
TEST: 
[(0.25, 0.04352640879154206), (0.2605, 0.0880089095234871), (0.3135, 0.11392097294330597), (0.38675, 0.1381194160580635), (0.41975, 0.18566381484270095), (0.4255, 0.17218308329582213), (0.42525, 0.18001394498348236), (0.42625, 0.17942561841011048), (0.4315, 0.19832437139749526), (0.43375, 0.21836428129673005), (0.42825, 0.22052164554595946), (0.42525, 0.20143126249313353), (0.428, 0.18739588838815688), (0.428, 0.1766004251241684), (0.42575, 0.17691279488801956), (0.4305, 0.17856121629476548), (0.42825, 0.14876855581998824), (0.42625, 0.14515069216489793), (0.43225, 0.16815384006500245), (0.433, 0.1599065505862236), (0.43275, 0.16640988546609878), (0.4305, 0.16286404812335967), (0.435, 0.16815375375747682), (0.43, 0.1850123473405838), (0.4335, 0.1450786482691765), (0.432, 0.15409785747528076), (0.43425, 0.15757981705665589), (0.4285, 0.14201634752750397), (0.4335, 0.1507883837223053), (0.43575, 0.1612966793179512), (0.4265, 0.16055781185626983), (0.43625, 0.15455729126930237), (0.435, 0.17775583386421204), (0.43575, 0.1699419212937355), (0.43725, 0.1655229143500328), (0.431, 0.17476767593622208), (0.434, 0.20616042011976243), (0.43575, 0.18297780346870424), (0.43825, 0.16592522203922272), (0.43075, 0.17430331403017044), (0.43025, 0.1679065899848938), (0.43825, 0.210671042740345), (0.42025, 0.19613493418693542), (0.43575, 0.20963585066795348), (0.432, 0.20901898443698883), (0.43225, 0.22487596660852432), (0.42725, 0.23322025752067566), (0.43825, 0.18467101621627807), (0.43725, 0.21877672851085664), (0.4385, 0.17917134338617324), (0.436, 0.21742671984434128)]
DETAILED: 
              precision    recall  f1-score   support

           5       0.36      0.94      0.52      1000
           7       0.59      0.81      0.68      1000
           8       0.00      0.00      0.00      1000
           9       0.00      0.00      0.00      1000

    accuracy                           0.44      4000
   macro avg       0.24      0.44      0.30      4000
weighted avg       0.24      0.44      0.30      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [46]
name: alliance-3-dcs-46
score_metric: contrloss
aggregation: <function fed_avg at 0x720532e47c10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=46
Partitioning data
[[0, 5, 1, 6], [8, 3, 1, 6], [4, 7, 1, 6], [2, 9, 1, 6]]
[(array([41745, 36201, 46585,  2513, 33328, 10243, 10637, 36506, 39196,
        5426, 33363, 17115, 40873,  6858, 31004, 43504, 13419, 38618,
       35855, 20383, 35417,  1214, 20045, 49799, 26565, 31391, 28380,
       36491,  9171, 37841, 22680, 42044, 48869, 35529, 15439,  3295,
        6015, 39619, 39286, 17717, 47865, 49525,  1913,  8072, 44866,
        2707, 32661, 21633, 33950,  5888, 44968, 43828, 29444,  3184,
       44074, 11762, 18451,  9124,  1147, 32290, 33438, 22264, 46287,
         695, 12355, 26049,  9905, 21044, 32369, 12659, 45288, 12553,
       19082, 19106,  2355, 31654, 20862, 41606, 30749, 27434, 33647,
       40761, 39471,  1470, 24383,  5557, 39995, 44430,  3644, 42204,
       44849, 41068,   600, 32839, 29634, 42566, 47645, 10578, 31339,
       30709, 42517, 39793, 32233, 27301, 47215,  1950, 40018, 47969,
        5984,  5765, 12056, 14520, 14025, 34567, 29219, 24695, 25191,
       29286, 41751, 11356, 21143, 24507, 31048,  7972, 39691,  9430,
        1988, 35934, 12325,  3197, 12419,  1329,  4176, 19859, 24101,
       30811, 19634,  7511, 17166,  4165, 35626, 44116, 22320, 34040,
       28865, 30233, 23449, 19332, 12328,   189, 46763,  8062, 39589,
       14666, 14567, 33516, 26018, 32931, 44618, 35414, 16253, 44666,
       15546, 23377,  7251,  2413, 21248, 24351, 41200, 24931,  5371,
       10686,  7971, 29620, 43012, 30240, 18768, 22034, 17107, 20452,
       18572, 17316,  2885, 10146,   457, 43542, 27415, 34783, 14213,
       41238,  6399, 32662, 41821, 47617, 26723, 39329, 49717, 21272,
       35422, 14023, 48378,  7095, 28487,  1338, 18782, 45423,  3136,
       38478,  7581, 36545,  1044, 49507, 33502, 40649, 11913, 22286,
       36052, 46706, 16559, 13457, 19409, 49263, 29104,  5414, 14069,
       25160, 12492,   199, 12704, 49868, 14210, 13894, 35570, 43639,
       48671, 29977, 29518, 33934,  6090,  4524, 20349,  8136, 44476,
       28092,   866, 47379, 17959, 34381, 43378, 18203, 15713, 40957,
        2013, 16199,   984, 38507, 45181, 19437, 33634, 27472, 16949,
       24831, 30356, 30162,  4467,  2884, 46441,  2182, 48890, 39463,
       22941, 11725, 14992, 35543, 18016, 42494, 48135, 36139, 42218,
        1519, 11143, 46112, 48110, 43994, 21977, 12371, 36692, 35534,
       33295, 28283, 15229, 25603, 11861, 43273, 47038, 20732, 23185,
       34074, 13257,  4456, 22879, 48395,    81, 30965, 20043, 26138,
        3021, 37375, 23090, 18212, 44832, 16185, 22681, 32864, 20809,
        6794,  1224, 29037, 42491, 41915, 19024, 14929, 46318, 35194,
       28484, 22434,  7425,  8039, 38529, 23238, 31551, 22445, 27219,
       33477, 13073, 41942, 19933, 32255, 18280,  9383, 12300, 35166,
       40456, 31243,  5645, 11735, 22003, 32926, 10835, 37531,  5208,
       43535, 26589, 30308, 31109, 36235, 22731, 16431, 34884, 43654,
        3069, 36057, 25840, 41886, 19786,  7115,  1157,  3810, 16979,
       14841, 26277, 45142, 36912, 31631,  7729, 44033, 38792, 33067,
       40887, 30188, 34634,  7562, 11077, 40889, 37894,  6058,  1199,
       46525, 30189, 30063, 12682, 25699, 17708, 28999, 36335,  5850,
       13461, 33965, 23437, 16496, 18096, 23556, 29358, 46201, 42323,
       13680, 15378,  5448, 34112, 11274, 30847, 16835, 18189, 46606,
       11004, 25925, 27359, 39456, 24741, 45915, 12832, 44089, 49715,
       11491,  5369, 29804, 27397, 44561, 44687, 24867,  3660, 44740,
       14001, 28505, 38784, 28425, 30126, 21839, 12089,  4319,  3017,
        3005,   999, 30662, 36769,  1280, 43115, 15207,  3420, 48910,
       22671, 35394, 10541, 45581, 42538,  5233, 22483, 33927,  1942,
        2916, 23249, 23544, 30523, 33504, 35604, 35822, 49431, 15177,
       42613, 37341,  3022, 13252, 35390, 27345, 11441, 21075, 35602,
       34048, 22895,  4355,  4968, 40871, 26680,  3060, 17165, 11043,
       31901,  4694,  5698, 13604,   535, 46083, 35748, 18894, 36120,
        8902, 47773,  1998, 47720, 21586, 34493, 23895, 46674, 17384,
       27731, 39792, 12594,  8556, 10571, 37491, 20057, 20366,  5311,
       37799, 11482, 23714, 48021, 16197,  5683,  7683,  6656, 16412,
       43372,  7649, 14096, 39824,  5236,  5301, 35041,  1305, 30486,
       38257, 30912, 49435,  1380, 48496, 29252, 48612, 16250, 47040,
       31776, 35174, 10790,  8149, 35399, 47310, 14448,   547,  9459,
       40505, 11336,   257,  4250,  4533, 45923, 11367, 33686,  7005,
       17606,  7912, 43976, 15386, 22859,  1651, 11515, 15195, 22090,
       42709, 47256,  9766, 48263,  4528,  7358, 41378, 37847, 44269,
       38911, 29792, 38078, 26344,  5663, 28906, 38744,  5149,  6161,
        5099, 18583, 37007, 35803, 47030, 46140, 31534,   855, 11130,
        9232, 26740, 30752, 35847, 23423, 45500, 16077, 45377, 43089,
       31300, 38795, 18388, 47288,  8307,  1064, 16810, 20856, 15662,
       10964, 34906, 18902,   893, 40172, 41519, 31737, 28352, 20616,
        3346, 25799, 49469, 34595, 36400, 41607, 13763, 14643, 21149,
        3661, 43618, 48275, 28027, 45714, 20304, 49973,  1694, 25750,
       19363, 18754, 23142, 11716, 14201, 32913, 26614, 16598, 41579,
       24220, 11683, 22957, 47237,  5392, 26450, 25196, 32937, 30182,
        4945, 40060,  6420,  3808, 21152,  5197, 24566, 15306, 49509,
       14546, 15710, 14040,  8449,    96, 48791, 42143, 22943,   311,
       20155,  2756, 17701, 25582, 45769, 12766,  6244,  7378, 20376,
       25323, 42710,  3444, 43251,  9878, 47900, 24463, 31151, 28848,
        8848, 35322, 45488, 39800, 43416, 31619,  3886, 34809, 36879,
       19181, 11971, 25135, 36838, 25652, 34080,  6967, 45084, 43609,
         282, 19616, 23198, 28580, 28780, 23338, 12875,  7324,   879,
       20074, 43186, 40427, 23407, 37092,  9080, 37779, 28790,  9480,
       35884, 11767, 25208, 12169, 27769, 18932, 42666, 28784, 25874,
       26433,  1631, 46135, 37235, 46448, 42121,  7659, 49777, 28441,
       35757, 41502, 30088,  2475, 44011, 21216,  9953, 49129, 33738,
       42509, 33178, 38769, 13168, 35830,  9727, 26492,  2439, 45944,
       43912, 49279, 37714, 20527, 26232, 36089,  3686, 26618, 34286,
       33733,  4208, 22550, 12302,  4875, 49952, 18604, 36720, 30812,
       48361,  3942, 45889,  3986, 10649, 43166, 24671,  3521, 14443,
       47472, 38475,  3861, 18974, 49550, 37045, 10554,  7393,  2729,
       22066, 24431, 15664, 49046,  7277, 12367, 19508,  3098, 26333,
       47649, 15726, 29973, 39939, 35370, 21917, 29381, 35950, 15771,
       10939, 42762, 16670, 10176,  9360, 22356, 17776, 15291, 11459,
       10635, 25373, 41160,  3403, 29391,  7553, 34662, 34056,  9877,
        5148, 25680, 24143, 22525,   770, 20666, 32359, 21644, 33123,
       31639, 42404, 17317, 17417, 21717, 41297,  7976, 43250,  6599,
        8069, 49928, 10364, 13029, 12796,  4126,  3095,    19,  8956,
        3437, 34684, 48947, 22295, 47465, 40038, 30727, 39203, 19588,
       22631, 14024, 22428, 38230,   807, 30536, 33623,  7675, 22786,
       24918, 40436, 20377,   249, 29217, 42742, 44194, 21497, 24888,
       49516,  2411, 36564,  5680,  8293, 36290, 35571,  2777,  6267,
       25382, 36360, 37048, 18978, 19763, 24155, 25622, 43027, 30212,
       37261, 42049,  2684,  7650, 35170, 35458, 23366, 43157, 32125,
       45622, 13127, 22236, 13479,  4810, 24531, 35745,   235, 35312,
       15193, 46991, 48650, 11101, 28421, 29360, 34139, 20022,  2924,
       40313, 14916, 43298, 23005, 30953, 45019,  4279, 13988, 47331,
        2690, 15823, 14845, 16639, 24354, 32856, 23539, 18682, 37134,
        7918, 29627, 31279, 10374, 18029, 28492,  3341, 15612, 17723,
        4550, 36304, 31118, 32812, 17697,  3739, 38290, 13866, 49804,
       18338,  3683, 41448, 38662, 38397, 12066, 10600, 39205, 27464,
       25234, 18270, 19268, 16218, 31128, 38845,  8256, 40498, 33627,
        3401, 38336, 13596, 27928, 40613,  6832, 38026,  6517, 43414,
       37694]), [0, 5, 1, 6]), (array([10313,  1060, 22492, 47609, 49025, 35505,  9527, 40702, 34894,
       48611, 49501,  2942, 22714, 24062, 26623,  1325,  3258,  4313,
       47376, 39971, 47430, 14149, 29933, 12249, 31714, 27643, 29362,
       25412,  3123, 23399, 31917, 31183, 39299, 17010, 17170,  4283,
       18395, 14166, 20617, 41335, 12907, 43623, 39605, 20259, 49388,
       24260, 18950, 40048,  3240, 11378, 47181, 10369, 12866, 31370,
        5546,  1239, 36295, 39433, 32671, 30510, 42917, 11248, 30433,
       34296, 47793, 21451, 45095, 35766, 34369, 32714, 35730, 22776,
       49712,  6717, 45461, 33954,   723, 27692, 34892, 23497, 37002,
         580, 13929, 40578, 42162, 37036, 41247, 41413, 15164, 20909,
       18081, 42898, 23960,  9427, 29075, 39243, 47212, 25513, 10096,
       32619,  7610, 41078, 44659, 25163, 29954, 40470, 37959, 24328,
       46223, 16942, 41727, 23757,  5280, 15022, 13386, 11063, 21050,
        3319, 38315, 45398,  9922, 30997, 10810,  4204, 29437, 35416,
       46028, 11965, 49379, 25691, 48168, 42713, 40785, 16978, 39335,
       48124, 37722, 29868, 28230, 24526, 13504, 40881, 10856, 29895,
       12207, 13859, 30443, 11674,  1141, 11352, 46543, 47561,  3777,
       36299, 36444, 39577, 27846, 36498,  7693, 29559,  1398, 17168,
       14281, 10923, 23723, 46975, 10172, 35911, 42467, 13886,  5658,
       12772, 23567, 25126, 29490, 17482, 35074, 30509, 29782,  6166,
       29676, 13338, 48432, 37072, 27808, 49654, 47342, 28436,  5378,
       17820, 18492, 45104, 46101, 20749, 28768,  7129, 46309, 37642,
       32532,  1751,  7083, 47978, 49127, 20984, 19186, 40402, 10702,
       12087,  2819, 16630, 37307, 12699,  4001, 23672, 22920,  8006,
       30691, 25237, 19097, 16792, 34274, 29479, 38689, 20726, 12543,
       24045, 29371, 25072, 46580, 14693, 23872, 39198, 12615, 13388,
        9122,  5348, 26487, 30770, 49851, 27068, 30664,   170, 39653,
        6529, 26720, 48261, 23595, 35491, 36438, 43381, 43490, 15872,
       35459, 25934, 40670, 13189, 11743,   785, 44451, 47100, 38193,
       36080, 43606, 38894,  7171,  2406,   395, 46230, 27441, 39871,
        8710, 14623, 38445, 37415,  7432,  3486, 26657, 42604, 26097,
       13259, 45928,  6783, 46950, 25705,  4672, 40712, 13201, 40249,
       45282, 48592,  9973,  3795, 45914, 26862, 20210, 18780, 40954,
       13873, 16983, 47067,  2051, 38472,  3222, 23389, 23763, 38360,
        3320,  6284, 22979, 45362, 36425, 30339, 30452, 30687,  4350,
       12786, 31479, 45433, 20721,  9051, 10753, 47062,   869, 10468,
        3447, 44103, 28368, 42646,  4628, 15467, 27608, 32134,  3108,
       19414, 40651, 43762, 48762, 45117,   740, 15470,   922, 17658,
       46682, 33600, 49478, 33236, 15873, 35472, 21655,  5553, 19061,
       44120, 38338,  4876, 18324, 36140,  6332, 26964, 11103, 39420,
       22913,  6199, 16583,  4697,  1895,  4392, 19255, 28697, 17214,
       33345,  3218,  9667,  4428, 38999,   241,  6714, 10452, 26707,
       41827,  9760, 12798, 15388,  4405, 33238,  2132, 42953, 41421,
       48540, 34199, 20425, 44944, 23286, 43123, 12973,  4558, 47897,
       41853, 25899, 44943,  1841,  7411, 21703,  2082, 17062, 39604,
       17565, 24991, 23702, 10186, 37194, 16335, 22042, 46604, 46977,
        2447, 39865, 33653, 34517, 22861, 25218, 47586, 14107, 21274,
        7667, 26482,  8846, 33033,  6151,  4519, 41271,  8635,  7107,
        5838, 47950, 22377, 26849, 22919, 35790, 22244, 30192, 42122,
       49742, 46058, 17096, 35307, 15545, 32230,  7615, 43487,  5502,
       37421, 35697,  3242, 15860, 23584, 21835, 25627, 45949, 23409,
       48758, 17655, 43598, 34160, 30318,  9846,   792, 25545, 28657,
       45933,  7122, 49726,  8686, 44197, 47603,  5539, 12904, 22484,
       21898, 14986, 46605, 21085, 16526, 45667, 27796,  5260, 36930,
       38764,  2359,  9008, 33709, 35380, 18462,  4110, 49032, 48368,
       48085, 25509, 23992, 30947, 34170, 13999, 12164, 40936, 21796,
        5125, 25046,  8058, 41710, 24268, 36848, 27419,  9584, 34364,
        6383, 31599, 44889,  5282, 27174, 10107, 43520,  6779, 46099,
       35688, 15669, 39964, 49022, 15577, 14662, 37727, 17772, 40964,
       40713, 30696, 37830, 44415, 11134, 30039, 17964, 43274, 10795,
       32737, 35863,  6915, 41072, 23263,  4132, 22500,  8476,  5759,
        2433, 49225, 27953, 24544, 19886, 43806, 38064, 44051, 14967,
       14376, 31890, 35344, 33181, 41153, 43480, 41051, 15114, 31720,
       27260,  2204, 28671,  6411, 34802, 14384, 16613, 38658, 29710,
       16543, 39322, 24292, 20674, 22227, 41075, 33085, 38710, 27132,
        7148, 25331, 30019, 46904, 12977, 41189,  9335, 37455,  9998,
       36067, 30545,  7390,  4993, 14238, 48465, 31665,  2771, 40220,
        5616, 46547, 17169, 39340,  6226, 32462, 22788, 47670,  1790,
       42437, 43655, 42438, 12516,  9085, 34400, 37203, 41760, 38482,
       15315, 14367, 41445, 32584, 34258, 34905,  9646, 16736, 35576,
        2362, 10168, 12306,  4958,   250, 44204,  4648, 35910, 29874,
       25600, 23768,  4200, 47989, 22060,  7371, 38420, 12340, 28268,
       45289,  3025,  8127,  9668, 27405,  9317, 16855,  7875, 18908,
       31645, 44266, 30183, 43954, 48970, 18178, 30083, 25165, 44273,
       47190, 26368, 41439, 45821, 47423, 30999,  4354,  7284, 12629,
       39892, 45308,  7887, 30650, 40449,  1736,   593, 43608, 12451,
       45068, 40046, 12292, 34278, 29429, 10826, 13044,  4244, 47226,
       19571,  4099, 23439,  8189, 45972,  3827, 22790, 38223,  7428,
        6819,  6121, 47264, 17460, 34794, 40076, 24075, 15183, 48269,
       13311, 35465, 26350, 10564, 18825,  7124, 21706, 45014, 41946,
       49847, 33658, 40472, 17031, 47033, 49977, 49769, 17205,  3142,
       18907,   761, 44601,  3131,    60, 39204,  1413,   176, 14676,
       12247, 42163, 13523, 16211, 49959, 16286,  6851, 45317, 11799,
       33832, 21416, 33118, 43482, 24673, 46043, 15028, 24872, 41260,
       33566, 31748, 19877, 28042, 35535, 31943,  7091,  6025, 29332,
        1063,  4682, 34322, 16493,  3994,  1876, 10828, 21636,  3865,
       11239,  7843, 47984,  8557, 17675,  2152,  9233,  9428, 31232,
       35221, 45355, 49464, 27295, 15657,  3168, 47752,  3307, 16768,
       26727, 18407, 25851, 42619, 18533,  7025, 16696, 24275, 23412,
       18866, 47976, 38834, 36666,  3091, 45836, 33990, 26192, 12641,
       24397, 19366, 18024, 25709,  4430, 38677, 39678,  6768, 30864,
       31598,  4966, 39323, 13213,  9974, 47878, 37593, 12635, 25139,
       15592, 47790, 39159, 20482, 29846, 27367, 25913, 48103, 40438,
       44128, 35090,  9543, 38069, 35332, 33787,  1017, 20628, 44696,
       40274,  2937, 14084,  2479, 22746, 17966, 37426, 40879, 25612,
       41328,  9530,  1031, 11911, 39464, 33491, 17938, 23888, 10117,
       16789, 30302, 46835, 23771, 11147, 41188, 31928, 45078, 20663,
       16915, 29136, 36274, 47451, 31570, 19107, 24973, 36285, 17562,
       29752, 42510, 29436, 41656, 33364,  2810, 30130, 30481,  3588,
       20885,   931, 13600, 42261, 13216, 34804, 10510, 48065, 12879,
        4086, 32228,  9847, 33192, 40836,   473, 37922, 18542, 31142,
       43269, 44612, 33948, 29357, 20106, 31396,  8873, 36842, 46340,
       36645,  9131, 14951,  9103, 19239, 46612,  5682,   488, 12609,
       20027,  6672, 36704,  1248, 10001, 19927, 41534, 10026, 27765,
       19562,  7272,  4899, 33758,  3775,  1989, 30202, 41293, 17796,
       20003, 33704, 13331,   819, 21572, 45607, 16404, 46473, 10718,
       32010, 18261, 46066, 49754,  1175,  3018, 38396, 33759,  7669,
       10379, 12293, 24321, 14864,  9515, 49636, 18342, 13076, 23036,
       17715, 31181, 33812, 26383, 20113, 44917, 27610, 13956,  9808,
       21756, 14621, 14694, 13066, 14065, 11429, 23628, 36847, 29103,
       25828, 23680, 29516,  8091, 46721, 24701, 33256, 34922, 12048,
       11751]), [8, 3, 1, 6]), (array([ 1937, 21260, 10485, 22369, 36168,  1657, 33408,  4753, 15238,
       19471,  5742, 28170, 40846,  6270, 40739, 23468,  5570, 19360,
       32799, 39182, 11765, 41618, 47630,  2573, 35228,  8423,  1169,
       11810, 16889, 42096, 13012, 42967,   665, 35324, 12406, 33401,
        8765,  6206, 27469, 15587,  9852, 15342, 27604, 21304, 16702,
       19340,  8462,  8379, 48087,    86,    20, 16173, 28658,  5880,
       42716,   816, 33749, 21966, 16878, 30037, 26838, 43509,  3908,
        8487, 38492, 18115,  4199,  3897, 44397, 37288, 48172,  1158,
       33209,  7984, 29065, 10477, 18293, 39947, 45605, 48484, 47857,
       19702, 29445,  1826,  3560, 32593, 37850, 23869, 21197,  2734,
       18493, 46790, 25871, 28555,  5631, 10629, 38123, 46251, 30561,
       42314,  9234,  6652, 33016, 11499,  3189,  1018, 18099, 21364,
        2792, 21016, 42104, 28049, 43151, 48945, 11435, 39265, 46164,
       33574, 30869, 22148,  7382,  4750, 29723, 20782, 16936,   934,
       15718, 42863, 22990,   844, 27649, 46036, 15781, 18821, 30384,
       31932, 26434, 28755,  1746,  3635, 14134, 30358, 41777, 15508,
       43241, 43637,  8046, 30527, 18690,  7100, 33245,  2155, 34777,
       49097, 19441, 49488, 14532, 18766,  3973,  8523, 20978, 43640,
        2486, 26301, 36552,  6432, 28892, 27390, 26233, 23611, 43055,
       23255, 21693, 24464, 47239, 15253, 18418, 36517, 37089, 12691,
        5653, 26823, 48699, 33947, 33272, 12843, 15656, 37313, 27479,
       27923, 18052, 43486, 11254, 11957, 42769, 30225, 40650,  6844,
       12282, 31505, 39125, 39036, 42442, 44629, 16659, 41246,  1264,
       13855,  3717, 38821, 30242, 42303, 31349,  7625, 42653, 19284,
        4394, 15663, 15538,  1182, 27614, 10426,   381, 24884, 25813,
       19790,  2943, 13896, 40665, 43044,  4290,  4781, 19833, 38281,
       48787, 14440, 27017,   563, 48801, 21254, 14397,  6452, 15760,
        1866, 43925, 24067, 23634, 32069,  8890, 45611, 44845, 37302,
       49763, 29158, 23880, 43531, 16260, 41624, 42423, 28759,   178,
        2275, 36495,   948, 39810, 40541, 42500, 32546, 15824, 49648,
       10305,  4216, 15002, 11935, 22136,  2807, 44986, 44798, 48857,
       23860, 45476,  8595, 37262, 38196, 36037, 16088, 11323, 21660,
       34733, 24050, 44338, 44858, 46747,   824,  5842,  3914,  2015,
       48423, 44105, 47082, 21317, 30032,  3135, 47263,  7385,  8498,
       24547, 18938, 40447, 43316,  6966, 49470, 49162, 45758, 35316,
       11656, 35922, 25724,   954, 39465,  3104, 48104, 19062, 19259,
       37893, 11428, 24129, 37472, 43498, 41418, 18667, 26165, 25310,
        5035, 22541, 46691, 42126, 28812, 39297, 46300, 44447, 33681,
       19772, 20900, 33019, 23304, 36680, 19642, 40461, 20846, 32668,
       19461, 30671, 23784, 35814, 23356, 18258, 35277, 19321, 17463,
       11761, 10528, 49231, 38131, 39246,  2601, 30195, 33220, 48727,
         163, 39961, 48759, 29733,  9462,  5809, 30036, 34788, 10852,
        7758,  3606,  3147, 44295, 44378,  6588, 41956, 40637, 34807,
       35407, 38178,   825, 41381, 37292, 39762,   636, 38358, 25889,
       26133, 30993, 27930, 32779, 21927,   641, 10573, 18443, 45561,
        7611, 29648, 13540, 32383,  8461, 16171,  6010,  5854, 31212,
        2783, 38220, 29592, 13482, 37882, 25090, 23196,  8367, 23298,
        6032, 19909, 43748, 41438, 14375, 37763, 17110, 17886, 33804,
       13373,  6235, 44297, 12364, 21787, 44249, 40010,  4554,  3487,
        6759, 41717, 48486,    11, 48837, 39494, 17503, 31262,  8351,
       27145, 22291,  5269,   131, 17872, 27672, 25778, 27400, 32660,
       10998, 20960,  7016, 46353, 29196, 12466, 48436, 27157,  9369,
       18997, 35164, 36654, 28936, 38688, 13185, 23829, 13083, 23993,
       29355, 20369, 12864, 49529,  3534, 25156, 31982,  3592,  4714,
       13493, 17589, 40694, 16824, 41185, 41877, 47828, 10602,  7101,
       26669, 22626, 28727, 41626, 28152, 38330, 24194,  9565, 19351,
       13852,  6668, 17312, 44725, 44436,  8856,  5827, 13323, 37257,
       39692,  1494,  3238, 29325, 29522, 43061, 12920,  2455, 38908,
        9854, 33897,  6371, 21752, 20312,  6295, 47529, 28183, 33126,
        1660, 16848, 29675, 34723, 11444,  7147, 10576, 16474, 12424,
       27073, 45600, 33230, 35914, 25718, 28938, 20313, 46238, 12100,
       46376, 40207, 33532, 15903, 32847, 14425,  2037,  9048, 34052,
        8965, 11910,  5257, 37505,  2615, 27601, 16925, 11163, 18160,
       32827, 16953, 46051, 14978, 47377, 32941, 42490, 21933, 42927,
       41291,  4101, 26298, 27284,  8625, 44591, 26937, 27195,  5287,
        9263, 41262, 49554,  7640, 37687,  4230, 41984, 35850, 32262,
       10359, 44767, 41654,  5917, 24975, 15051, 19667, 27802,  1037,
         978, 30489, 38822,   137, 38262,  6130, 43195, 41948, 22175,
       16089, 46731, 24680,  5722, 33845, 39250, 44841, 21331,    45,
        1079, 28655, 38725,  3907, 40935, 11571, 23959, 48528, 10035,
       23058, 26315, 11804, 15770, 29494, 45296,  8600, 10495, 17534,
        7832, 47568, 26899, 28005, 29485, 29285, 44541, 25209, 46717,
        3483, 14182, 48901, 32418, 35933, 19522, 34571, 30104, 32652,
       24398,  5552,  7374, 26026,  6639,  3992,  4334,  6589, 37801,
         493,  1571, 15885, 25611, 28896, 14502, 37551, 23648, 46649,
       44225,  7738, 40594, 17984, 21218, 40422, 35454,  4326,  9494,
       28265, 44272, 47606, 17626, 16400,  5422, 47470, 40081,  3020,
       36388, 49060,  9745,  3034,  9017, 19581, 22055, 13264, 44625,
       47088, 38579, 39654, 30458, 49629, 39679, 39771, 10569,  3285,
       38211, 46396,  7338,  6931, 17492, 16365, 29963, 45968, 39668,
        4636,  2545, 41563, 24527, 13006,  7312, 25279, 10022, 16937,
       44589, 22833,  8174, 21651, 13667, 32458, 21992, 21142, 15248,
       13608, 12335, 47162, 49121, 37920,  2762, 36376,   991, 44679,
       47370, 30227, 37630, 16003, 16069,  2888,  1159, 48400,  5833,
        8171, 40322, 28258,  7064, 44915, 43021, 14178, 30146,    25,
       27660, 14457, 36147, 21072, 10356, 40767, 17143, 12369, 49171,
       18224, 16389, 36620, 44985,  6747,  6941, 18816, 48866,  5398,
       32191,  5380, 39190, 35435, 13120,   863, 20717, 17908, 34786,
       44417, 30922, 47792,  6127,  4881, 24660,  9828, 46306, 17132,
       11038, 37997,    95, 45793, 41399, 42671,  5169,   103, 47396,
       35600, 34245,  7252, 42334, 13573, 23244, 26245, 34092, 30685,
       32637, 28432, 25657, 21560, 25747, 18025, 48664, 31500, 38936,
       10738, 36625, 32397, 49624, 48325, 27504, 29976, 14889, 10444,
       12396,  9762, 20362, 24418, 42027, 44163,  9732, 31820, 25406,
       21704, 39986, 26524, 11904, 31112, 36493, 37865, 40551, 18021,
        2019, 43933, 12014, 29214, 18325, 27577,  2915, 14547, 20878,
       13838, 48043, 32880, 43284, 10063,  6395, 40804, 47302, 11823,
        9126,  3589, 16822,  2965, 16762, 32105,  7339, 45878, 32762,
        4663, 45009, 34157, 26419,  7591,  6777, 12433, 38462, 46856,
        5708, 38298, 26798, 18334, 10783, 32739, 43431, 39759,  8215,
       22027, 36638, 29009, 12019, 28672, 37918,  2872,  3716, 33367,
       16097, 48977,  1023, 45225,  3603, 31616, 40047, 19731, 44626,
       12168,  9246, 30222, 23915, 48176, 10579,  8815, 27486,  3041,
       48819, 26054, 10936, 48710, 24519, 17597,  5558, 23881, 28501,
       17688, 45247, 22610,  7232, 29327, 14824,  2602,  7881, 14973,
       15343, 45443, 31891, 42327,  4751, 34542, 23470, 46791, 36586,
       39052, 19128, 36808,  7172,  5402,  8545, 19823, 11212, 31135,
        9356, 47953, 31784,  6664, 13806, 43353, 13143, 21722, 32212,
       15517,  2874, 43995,  3439, 43390, 39343, 15232, 31575, 23697,
       29426, 30033, 42358,  6035, 43439,  1176, 15231,  8827, 45663,
       46952,  8832, 18621, 18884,  7638, 14884,  3192, 22968, 40323,
       34564]), [4, 7, 1, 6]), (array([  742,  9067, 29220, 49888, 47450,   910,  6227, 41198, 43122,
       20779, 10433, 40281, 15176, 13786, 12711, 44392,  4588, 48893,
        3037, 12669, 21819, 44075, 41681, 45864, 28497, 41619, 32274,
       47632, 25041, 13433,  8543,  1492, 36125, 49397, 40701,  9552,
       38745, 12504,  7166, 38096, 26321,  7948,  5979, 14192, 29678,
       46114, 26541, 40398, 12742, 35311, 26186,  8881, 49802, 31495,
       34088, 38594,  7303, 28802, 14172, 26807, 43465,  9033, 21942,
       12583, 27825,  9001, 32951, 27498, 48589, 39062, 42267,  9989,
       32278, 29912,  1768,  1788, 26893, 13081, 43478,  3848, 37742,
       20547,  8186, 39019,  9526,  9455, 48545, 10160, 18652, 37745,
       24170, 30454, 22667, 48833,  3629, 47956, 44658, 48365, 41542,
       20580, 25449, 19964,  1530, 36025, 17844, 25491, 41933, 45074,
       28705,   108, 43010,  7853, 37560, 14229, 19950, 40177,  6408,
       22297, 42803, 40372, 15522, 33051, 34396,   820,  1444,  3462,
       11443, 31308, 18863, 13648, 20572, 48531, 42302,  1122, 43118,
       17955, 12046,   975, 31333, 28994, 28164, 19914, 43469,  1693,
       15696, 42648,  1040, 42468, 39991,  3476, 39051, 31813, 46819,
        6484,  5727, 28708, 19932, 49104,  7807, 40087, 33519, 45926,
       46522, 14829, 47491, 23911, 29728, 18027,  3119, 29448, 23530,
       19137, 12148, 22602, 33032, 10018, 37321,  2372, 16919, 35681,
       36450, 40583, 43766,  8871, 39436, 31562, 25859, 12602,  8843,
       18092, 32470, 25830, 23692, 20660, 47016, 31026, 31519, 24636,
       31759,  8800, 13564,  1533, 33449, 32816, 35482, 40403,  3101,
       22228,  9437,  3889, 41362, 16266, 18101, 31040, 14270,  1606,
       24694, 39687, 44739,  6228, 29181, 23453, 46530, 24496, 40225,
       44277, 15182, 36339, 12156, 27580, 37278, 27557, 41255, 38058,
       37816, 10767, 39928,  2033, 32321, 21167, 43134, 16279,  4426,
        6448, 20526, 19695, 38704,  8418, 49297, 17735, 29776,   369,
       22921, 42739, 21786, 10208, 38363, 28543,  6066,  7127, 45231,
       25319, 48017,   886, 17716,  3513, 27548,  3820,  6937, 29992,
       26069, 13881, 38379, 39884, 40699,  5437, 14099, 11230,  5611,
        1390, 39583, 40068, 47073,  2281, 41560, 13155, 27190, 30175,
       43039, 17468, 25009,  5671, 27420,  7793, 49926, 40303,  9213,
       17905, 22016, 27523,  3477, 24718, 10494, 49637, 49785, 46685,
        7457,    31, 31566,  1706, 25631, 36050, 24086, 26955,  2214,
       43511, 25159, 48371, 42346, 11312, 30853,   883, 34943, 32810,
       25837, 15750, 32242, 41976, 31287, 17218, 44052, 19842, 45906,
       10138, 21203,  3628,  5453, 35746, 42627, 37074,  1115, 37678,
       33200, 28391, 20587, 14809, 26061, 37709, 16641, 12835, 39806,
       27983, 39816, 26053, 40117,  2477, 24391, 33896, 43591, 11104,
       26496, 44132, 10178,  9025, 33865, 17228, 27784, 38380, 33025,
        6579, 43803, 36807,  6480, 36859, 46777,  8501, 15838,  6237,
       39984, 19302, 43346, 20713, 24542, 10390,  8505, 34796,  2061,
       27957,  5161, 23075, 27799, 41183, 46346, 36529, 38961, 22614,
       16720, 14224,  3850, 45063, 10467, 26057, 16347, 48192, 13795,
        6539, 32129,  1572, 41226, 14661, 39688, 19670,  3838, 29784,
        3409,  7010, 44058, 22651, 13827, 44716,  1231,   186, 20995,
       12104,  4364, 29193,  3652,  5465, 17090, 40450, 49442, 11755,
       45008, 38789, 35542, 39418, 31162,  5300, 40833,  8650, 35183,
       46781, 13096, 46269, 11777, 27336, 15142, 47512, 39453, 44339,
       42140, 23225, 11373, 34867, 47765, 36684, 30615,  4473, 45198,
       32115, 33417, 49147, 21584, 44469,  2908, 10312, 45498, 17798,
        9807, 13421, 11555, 28879, 32041, 15384, 19367,  3846, 24408,
       17068, 49003,  1972, 31403, 27380, 42973, 43619, 23137, 31490,
       34872, 14220, 14462, 41228, 27196, 12086, 46645,  3537, 26446,
       16591, 18878, 10781, 46861, 49385, 43646, 30814,  6353,  2143,
       43669, 43267, 48995, 46507,  9549, 29299, 46966, 13509, 19810,
       29473, 43290, 44487, 30821,  4856, 30006, 14593, 49663, 27404,
       45946, 16180, 49283, 14048, 47213,  2851, 13766, 40554, 24294,
       35866, 10868, 27130, 43710, 23313, 18670,  8427, 28963, 49426,
       29049,  5337, 46634, 28685, 43984,   834, 49585, 38157, 18527,
       14716, 31935,   325, 44185, 32811, 47706, 16471, 24336, 44235,
       35827, 27510, 30316, 30049,  4172, 19575, 36850, 18744, 31193,
       17319, 28383, 38958, 14964,  8729,  2428, 23060, 19745, 49326,
       13327,  7774, 11418, 38735, 41307, 40808, 47349, 19565, 34519,
       13191, 33615, 29959,  8893, 35207, 24151, 19678, 12981, 42237,
       47824,  4360, 47390, 49989, 41063, 11920, 32189,    97, 31502,
       45565, 10377, 44966,  2849,  5356, 35948,  5816, 31407,   134,
       34238, 41739, 23275, 10771,  6325, 19341, 10832, 21877, 22552,
       12158, 14455, 42753, 48917, 14074, 27538, 40290, 20047, 37309,
       42085, 49401, 43132, 41532, 43918,  2038, 27002, 28335,   364,
        9066,   936, 38452, 44474, 27582,  6429, 21947,  2844, 38542,
       36991, 26578,  1731, 11109, 37440,  4654, 38902, 19202, 21003,
       14249, 27730, 31583, 26818, 20680,  6272,  6831, 35508,  2101,
       15797,  4423,  9583, 16962, 35058, 16252, 29908, 32486, 34128,
        2318, 24338, 35531,  7297, 29999,   323, 36853,    79, 21150,
       27970, 14381, 28541, 29555, 34618, 17860, 29469,  3734, 21181,
        8124, 18495, 40431,  7019, 24923, 22499, 14101, 26158,  9113,
       39256, 37620,  6347,  8389, 26115, 43205, 45332, 43547,  7375,
       26331, 18238, 28122,  4171, 30009, 14855, 13997, 49148, 22906,
       30119, 27509, 38858, 19481, 46588, 23789, 33097, 42802, 40655,
       16777, 49933, 35756, 16981, 10693, 31655, 25300, 46889,  5971,
       28552, 26310, 22930, 29149, 48639, 38028, 34426, 41392,  2149,
       18425, 24807, 38818,  7137, 13890, 11654, 15390, 40298, 42291,
       24510, 31257, 34422, 47546, 15498, 13018,  6145, 18871, 36164,
       14392, 45806, 42660,  2989, 33701, 37907, 37926, 12359, 44431,
       20934, 11748, 24279, 44342,  5022,  3707, 31448, 47047, 12508,
       44018, 23025, 11066, 40640, 25704, 29749,  7848, 20332, 41379,
        2939, 17283, 48448, 41799, 37603, 29889, 11699, 21956, 40455,
       11090, 17238, 49076, 48789, 28194, 33069, 31812, 34926, 16584,
        4590, 31753,  1842, 45863, 40627, 46126, 19967,  5745, 17245,
       37119, 26335, 12077,  1837, 39252,  1678, 32922,  6754, 19701,
       40617, 29310, 49704, 27379, 12852, 15360, 11369, 28395, 11672,
       12305, 33543, 34550, 30193, 44684, 26190,  9477, 29506,  3833,
       30910, 15116, 45266, 11827,   326, 29223, 32385, 36880,  1955,
       40568, 33663, 15534, 15994, 22686,  9406, 46009,  5019,  7036,
       41537, 32859, 45958,  8820, 11875, 45585, 40153, 41909, 33594,
       38844, 35480,  6545, 31508, 13550, 10220, 34033, 35349,  3873,
       13484, 39337, 29457, 17484, 15804,  9367, 46429, 18380, 12434,
        6473, 38508, 42156, 10128, 29434, 48067, 17766,  7041, 23747,
       36576, 29802, 16337, 22266, 13188, 47454, 40982, 21084, 39046,
       24928, 21362, 35831, 11346, 13617, 45169, 12897, 22362, 35506,
        3926, 16177,  7267, 26206, 46092,  4789, 44712, 34942, 48337,
       12657, 39010,  8009, 15110,  4144, 23411, 22065,  7491, 34829,
       18742, 25519, 46913, 34031,  2267, 15179, 21177, 35351, 29408,
       35817,  7950, 21800, 35136, 11303, 18992,  4786, 25168, 22752,
       15991, 33726, 26872, 12273, 32381,  5025, 23563, 12933, 21935,
        4234, 43308, 25396, 27376, 40874, 40927,  5374,  4415, 33044,
       28869, 30469, 23618,  3241,  7820,  3070, 30569, 41749, 23328,
       22528, 28188, 31270,  8212, 22101, 48888, 28956, 38125, 33791,
       27937, 28454, 46948, 17680,   960, 10774, 48532, 31808,  4377,
       26066]), [2, 9, 1, 6])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 5, 1, 6], M=tensor([0, 5, 1, 6], device='cuda:0'), Initial Performance: (0.259, 0.0446454519033432)
DC 1, val_set_size=1000, COIs=[8, 3, 1, 6], M=tensor([8, 3, 1, 6], device='cuda:0'), Initial Performance: (0.249, 0.04491214168071747)
DC 2, val_set_size=1000, COIs=[4, 7, 1, 6], M=tensor([4, 7, 1, 6], device='cuda:0'), Initial Performance: (0.246, 0.04495431101322174)
DC 3, val_set_size=1000, COIs=[2, 9, 1, 6], M=tensor([2, 9, 1, 6], device='cuda:0'), Initial Performance: (0.251, 0.04425755798816681)
D00: 1000 samples from classes {1, 6}
D01: 1000 samples from classes {1, 6}
D02: 1000 samples from classes {1, 6}
D03: 1000 samples from classes {1, 6}
D04: 1000 samples from classes {1, 6}
D05: 1000 samples from classes {1, 6}
D06: 1000 samples from classes {0, 5}
D07: 1000 samples from classes {0, 5}
D08: 1000 samples from classes {0, 5}
D09: 1000 samples from classes {0, 5}
D010: 1000 samples from classes {0, 5}
D011: 1000 samples from classes {0, 5}
D012: 1000 samples from classes {8, 3}
D013: 1000 samples from classes {8, 3}
D014: 1000 samples from classes {8, 3}
D015: 1000 samples from classes {8, 3}
D016: 1000 samples from classes {8, 3}
D017: 1000 samples from classes {8, 3}
D018: 1000 samples from classes {4, 7}
D019: 1000 samples from classes {4, 7}
D020: 1000 samples from classes {4, 7}
D021: 1000 samples from classes {4, 7}
D022: 1000 samples from classes {4, 7}
D023: 1000 samples from classes {4, 7}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.418, 0.06142205399274826) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.411, 0.06509904915094375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.0901500660777092) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.08961409723758698) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.06931790328025818) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.07178343132138253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.285, 0.12618956410884857) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.273, 0.1021047906279564) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.0813574980199337) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.08620012563467026) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.322, 0.1603555630147457) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.12693029174208642) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.09731255877017975) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.11036875423789025) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.1944475924372673) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.15093016832321882) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.12286224584281445) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.15348628028482197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.21103904109448193) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.1675949716344476) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.13750439224019648) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.15476095771044493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.21013511917740108) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.21405988819152116) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.14960554386116565) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.17682396379858256) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2106591824889183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.2431645128428936) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.15374461515340954) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.18366238740086555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.24523430716246367) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.27802466233447193) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.16089225522428752) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.19264739524573088) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.2547052926644683) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2662605528570712) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.17724084822554143) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.1974615651369095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.2696862921901047) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.29413447110913693) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[1, 6], M=tensor([0, 1, 3, 4, 5, 6, 7, 8], device='cuda:0'), Initial Performance: (0.898, 0.015296673794587453)
DC Expert-0, val_set_size=500, COIs=[0, 5], M=tensor([0, 5, 1, 6], device='cuda:0'), Initial Performance: (0.944, 0.0053529364764690395)
DC Expert-1, val_set_size=500, COIs=[8, 3], M=tensor([8, 3, 1, 6], device='cuda:0'), Initial Performance: (0.936, 0.005041519492864609)
DC Expert-2, val_set_size=500, COIs=[4, 7], M=tensor([4, 7, 1, 6], device='cuda:0'), Initial Performance: (0.832, 0.01533126512914896)
SUPER-DC 0, val_set_size=1000, COIs=[0, 5, 1, 6], M=tensor([0, 5, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[8, 3, 1, 6], M=tensor([8, 3, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[4, 7, 1, 6], M=tensor([4, 7, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x720518114d00>, <fl_market.actors.data_consumer.DataConsumer object at 0x720518079370>, <fl_market.actors.data_consumer.DataConsumer object at 0x72051813a790>, <fl_market.actors.data_consumer.DataConsumer object at 0x72051859a340>, <fl_market.actors.data_consumer.DataConsumer object at 0x72052f7d64c0>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO4', '(DO1']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.0058891724292188885) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.00618116875179112) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.83, 0.014495044246315956) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.2061546819806099) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9006666666666666, 0.010284346736657122) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.521, 0.06014062340371311) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.506, 0.0776810709387064) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.504, 0.048795853700488806) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.007671266246121377) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006567511214874685) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.848, 0.013718870796263218) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.19369617828726768) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9266666666666666, 0.007278573568910361) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.561, 0.05984361365810037) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.568, 0.05546376422047615) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.55, 0.05820426441729069) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.006233081279322505) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.005167070727795362) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.832, 0.015683261450380088) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.19499317000806332) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9106666666666666, 0.010291599246052404) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.501, 0.10450348014477641) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.53, 0.10043013563379645) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.476, 0.10714851859211921) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.006364313867408782) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005752670733258128) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.842, 0.014623584814369678) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.22191571476124228) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9353333333333333, 0.007660767948332553) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.08303606356680393) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.10982814152538777) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.511, 0.09081347946077585) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.006351152826566249) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005960255481302738) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.852, 0.01376453460007906) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.18239906696602703) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9406666666666667, 0.009024517510513154) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.517, 0.10083703692257405) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.501, 0.11168151582404971) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.513, 0.09581479030475021) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO1', '(DO4']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.0064482487097848205) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006283556150272489) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.013814624711871146) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.201513444788754) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9426666666666667, 0.0072186374525384355) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.564, 0.0646248514316976) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.55, 0.0680936273932457) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.522, 0.08780141389742493) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.005591467347927392) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005456519640982151) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.84, 0.015664142087101936) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.454, 0.16529539940692484) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9446666666666667, 0.009263812571424447) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.529, 0.07718049143999815) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.09771838096529245) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.515, 0.09430138425156474) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.004911051428411156) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.007040435131639242) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.856, 0.013853358708322048) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.1986637821570039) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9326666666666666, 0.012474487714469433) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.53, 0.09090295374765991) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.1193103846795857) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.12126916783303023) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.0075409069370944055) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006095674846321344) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.01387499089539051) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.442, 0.1859277235502377) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9313333333333333, 0.013974499984527938) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.568, 0.07888397205714136) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.566, 0.07306914419308305) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.521, 0.09518596160411835) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.007173294362844899) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005749372445046902) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.82, 0.01726793583482504) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.17880388958379625) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9406666666666667, 0.0090169248378661) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.564, 0.0705792576931417) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.10529863780736923) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.502, 0.0874641549512744) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO0', '(DO4']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.006451838641893118) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006507224921137094) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.848, 0.01646218714118004) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.15467458784207702) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.008780628658714705) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.527, 0.09145732359588146) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.514, 0.11863263969123364) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.495, 0.09617090847715735) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.004789341147756204) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.007088720329105854) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.017107663556933404) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.15574216996878384) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.01070062279740038) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.07511295048973989) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.11671627430617809) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.516, 0.09954610355198383) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.006286657046992332) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.006494833447039127) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.015336217626929284) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.1524343203380704) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.007663602749972294) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.584, 0.06479132705740631) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.10262645991146564) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.523, 0.09588097225129605) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.0057099053210113195) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.006277286469936371) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.01538555671274662) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.17138424534723162) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.008433506706186259) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.05371844924241304) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.06690632012486458) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.539, 0.0791014034897089) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.006915648961090483) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006052187196910381) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.016669579543173312) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.17905203221924604) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9426666666666667, 0.014362326710051396) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.546, 0.08513157625263557) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.495, 0.10451408407185227) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.496, 0.13033978368039242) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO4', '(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.006067370277363807) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006270414590835572) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.015085760042071343) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.17092295372858643) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9466666666666667, 0.006700648179355388) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.661, 0.03882639201544225) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.585, 0.059459498763084415) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.556, 0.06713995695114136) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.006868991453899071) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006223434157669544) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.016364526733756067) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.17522187575697898) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.008314787380492513) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.585, 0.061925490258261565) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.09177947479858994) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.522, 0.07590092350542545) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.006488067413913086) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.007597920507192612) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.013501232877373695) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.1453946851361543) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.008625557150769358) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.65, 0.05226876085228287) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.583, 0.06785191726684571) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.531, 0.09836479881964624) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.008577672969346167) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006792493611574173) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.014344231918454171) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.18050056927930563) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.007505619366420433) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.641, 0.04927403766289353) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.555, 0.08255955953150988) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.534, 0.07778636756911873) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006086873070162255) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.006577578745782376) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.017805000815540553) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.17791650306712836) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9286666666666666, 0.009986436178403285) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.661, 0.04233841887395829) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.549, 0.08564132168889045) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.581, 0.04941038126498461) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO5', '(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.008345886542403605) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.007327246114611626) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.014392547830939293) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.1851609045462683) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.0054086464860786995) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.694, 0.0370593940615654) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.631, 0.05289327864348888) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.574, 0.07671902224421501) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.00625169472687412) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.005617298766970634) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01463702792674303) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.18698557763546705) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.007152102204582965) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.623, 0.05789074526913464) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.10427259846404195) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.543, 0.08702041613683105) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007494961246935418) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006095023974776268) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01553791756927967) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.448, 0.19597565631754696) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.013128353583742864) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.599, 0.06193302712775767) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.10283234506100417) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.503, 0.10300986927794292) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006251397122978233) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006753776855766773) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.013003567278385163) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.184874643355608) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.009989625772252717) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.060026523496955635) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.528, 0.1095924586802721) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.491, 0.11147030731849372) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.00836652433872223) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006493727922439576) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01641282979398966) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.18322874953597784) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.00888640533985017) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.513, 0.09470887751691043) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.14607801216840743) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.498, 0.12691439437493682) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO4', '(DO0']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.006559285640250891) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.006269181489944458) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.015333268783986569) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.20104956233687699) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.0043979839901439844) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.587, 0.05728504432737827) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.0890819616690278) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.52, 0.09452648428827524) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005083301625913009) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005876467827707529) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.01836707091331482) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.1918140706960112) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9406666666666667, 0.010550372710917145) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.060593586795032024) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.492, 0.11860263880342245) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.512, 0.0999923131391406) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005349531961604953) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.005854002088308334) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.856, 0.016134704403579236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.2011821936368942) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.008816523580346257) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.619, 0.05215697745792568) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.09726765438169241) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.517, 0.09629212887585163) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.0063094135612482205) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006028322048485279) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.84, 0.015581665597856044) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.18849971390888096) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9613333333333334, 0.007200111525298174) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.617, 0.05122254563868046) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.534, 0.09283460861071945) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.52, 0.09697481282055379) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007221995767118642) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.0058755227103829384) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.01524098951369524) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.2022513088742271) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9453333333333334, 0.00689734097198622) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.677, 0.038616459012031555) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.579, 0.0629832868129015) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.555, 0.06740186532586813) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO5', '(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.0074378824368468485) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005623591303825379) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01640222755074501) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.21830086036212742) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9593333333333334, 0.005050658668701848) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.05157339714467526) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.09320000316202641) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.543, 0.08491760557889938) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007829102131829132) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.00499408845603466) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01472773402184248) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.20254474718123675) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9533333333333334, 0.008681854987156233) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.594, 0.06391330924071371) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.513, 0.10563865365553647) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.507, 0.11292076747003012) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007164867746236268) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005537459068000317) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.014480031833052635) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.2330693068630062) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.015804987094343224) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.559, 0.07807544131187023) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.13667096934840084) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.489, 0.13507501922361553) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.0065275231646373865) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005673990044742823) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.014004900544881821) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.20996595074236393) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.007495127846477165) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.589, 0.06052870621159673) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.536, 0.10220002565160394) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.495, 0.10495989784412087) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006876465912209823) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.0056623643264174465) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.013710759580135345) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.23896697833389044) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.013997007516033591) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.596, 0.07223203375190497) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.10326859186124057) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.493, 0.12151392592675984) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO1']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO4', '(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.006567311281105504) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005665714658796788) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.014134118780493737) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.451, 0.21986760744825007) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9346666666666666, 0.009599604102305117) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.613, 0.059474259368143975) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.539, 0.10119512594863772) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.519, 0.09553026939183473) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007745945423201193) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005500602185726166) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.015333226069808007) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.20951822929270567) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9646666666666667, 0.004632629671522105) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.719, 0.033658783994615075) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.57, 0.07576039692386985) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.575, 0.06464311767742038) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005342254354618489) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.005206321269273758) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.015326867207884788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.20271974491514266) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9533333333333334, 0.008017122292735924) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.04599970716889948) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.11116002036258578) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.542, 0.08924979417212307) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.00777096354868263) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.0056705125793814655) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.013835582628846169) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.454, 0.23180351432459428) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9546666666666667, 0.009323342934871713) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.553, 0.07674066343158484) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.512, 0.11674875917471945) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.518, 0.1149199928380549) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.008013677593349712) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005696084924042225) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.013633604124188424) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.2166316314593423) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9353333333333333, 0.011289301122965601) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.576, 0.06511943197622895) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.11555143764801323) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.513, 0.11005283547192812) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.259, 0.0446454519033432), (0.418, 0.06142205399274826), (0.46, 0.06931790328025818), (0.467, 0.0813574980199337), (0.461, 0.09731255877017975), (0.458, 0.12286224584281445), (0.462, 0.13750439224019648), (0.452, 0.14960554386116565), (0.453, 0.15374461515340954), (0.469, 0.16089225522428752), (0.472, 0.17724084822554143), (0.521, 0.06014062340371311), (0.561, 0.05984361365810037), (0.501, 0.10450348014477641), (0.531, 0.08303606356680393), (0.517, 0.10083703692257405), (0.564, 0.0646248514316976), (0.529, 0.07718049143999815), (0.53, 0.09090295374765991), (0.568, 0.07888397205714136), (0.564, 0.0705792576931417), (0.527, 0.09145732359588146), (0.547, 0.07511295048973989), (0.584, 0.06479132705740631), (0.644, 0.05371844924241304), (0.546, 0.08513157625263557), (0.661, 0.03882639201544225), (0.585, 0.061925490258261565), (0.65, 0.05226876085228287), (0.641, 0.04927403766289353), (0.661, 0.04233841887395829), (0.694, 0.0370593940615654), (0.623, 0.05789074526913464), (0.599, 0.06193302712775767), (0.562, 0.060026523496955635), (0.513, 0.09470887751691043), (0.587, 0.05728504432737827), (0.627, 0.060593586795032024), (0.619, 0.05215697745792568), (0.617, 0.05122254563868046), (0.677, 0.038616459012031555), (0.629, 0.05157339714467526), (0.594, 0.06391330924071371), (0.559, 0.07807544131187023), (0.589, 0.06052870621159673), (0.596, 0.07223203375190497), (0.613, 0.059474259368143975), (0.719, 0.033658783994615075), (0.67, 0.04599970716889948), (0.553, 0.07674066343158484), (0.576, 0.06511943197622895)]
TEST: 
[(0.259, 0.043567125290632246), (0.40925, 0.05918567603826523), (0.46025, 0.06647206515073777), (0.46125, 0.07821208986639977), (0.4535, 0.09296726769208909), (0.4575, 0.11661085388064385), (0.45925, 0.12995803904533387), (0.44975, 0.14063709473609926), (0.44975, 0.1442626576423645), (0.4685, 0.15252220165729521), (0.4685, 0.16789883202314376), (0.54075, 0.05688074491918087), (0.5665, 0.06076540644466877), (0.52875, 0.09972752580046654), (0.54825, 0.07936679470539093), (0.5275, 0.09639094263315201), (0.576, 0.06332157018780708), (0.5385, 0.07439143207669258), (0.53125, 0.09056099992990493), (0.57875, 0.08181394410133362), (0.58175, 0.06862925386428834), (0.551, 0.09245776975154876), (0.566, 0.0754566706120968), (0.59625, 0.0654042271077633), (0.65025, 0.050268778309226034), (0.55525, 0.08632613351941108), (0.6755, 0.03786715151369572), (0.60475, 0.05979998080432415), (0.65475, 0.052801090493798254), (0.6545, 0.0463609973192215), (0.67475, 0.039290953263640405), (0.706, 0.03588040199875832), (0.62225, 0.05587584376335144), (0.61325, 0.06109554404020309), (0.57375, 0.06002396243810654), (0.52575, 0.0949737286567688), (0.59975, 0.0562920318543911), (0.62875, 0.05959196925163269), (0.64325, 0.047859736919403074), (0.6325, 0.050076412990689274), (0.677, 0.03843769688159227), (0.645, 0.049776291638612746), (0.598, 0.060037822127342225), (0.571, 0.07845681339502335), (0.599, 0.06070400542020798), (0.59225, 0.0727106286585331), (0.61025, 0.05992078864574432), (0.7305, 0.0334435123950243), (0.665, 0.047664799317717554), (0.5565, 0.0741975944340229), (0.60425, 0.06278656965494156)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.97      0.28      0.44      1000
           1       0.60      0.92      0.73      1000
           5       0.68      0.34      0.46      1000
           6       0.52      0.87      0.65      1000

    accuracy                           0.60      4000
   macro avg       0.69      0.60      0.57      4000
weighted avg       0.69      0.60      0.57      4000

Collaboration_DC_1
VAL: 
[(0.249, 0.04491214168071747), (0.411, 0.06509904915094375), (0.459, 0.07178343132138253), (0.462, 0.08620012563467026), (0.462, 0.11036875423789025), (0.467, 0.15348628028482197), (0.47, 0.15476095771044493), (0.467, 0.17682396379858256), (0.467, 0.18366238740086555), (0.47, 0.19264739524573088), (0.468, 0.1974615651369095), (0.506, 0.0776810709387064), (0.568, 0.05546376422047615), (0.53, 0.10043013563379645), (0.522, 0.10982814152538777), (0.501, 0.11168151582404971), (0.55, 0.0680936273932457), (0.499, 0.09771838096529245), (0.486, 0.1193103846795857), (0.566, 0.07306914419308305), (0.515, 0.10529863780736923), (0.514, 0.11863263969123364), (0.507, 0.11671627430617809), (0.522, 0.10262645991146564), (0.562, 0.06690632012486458), (0.495, 0.10451408407185227), (0.585, 0.059459498763084415), (0.546, 0.09177947479858994), (0.583, 0.06785191726684571), (0.555, 0.08255955953150988), (0.549, 0.08564132168889045), (0.631, 0.05289327864348888), (0.505, 0.10427259846404195), (0.491, 0.10283234506100417), (0.528, 0.1095924586802721), (0.493, 0.14607801216840743), (0.546, 0.0890819616690278), (0.492, 0.11860263880342245), (0.511, 0.09726765438169241), (0.534, 0.09283460861071945), (0.579, 0.0629832868129015), (0.54, 0.09320000316202641), (0.513, 0.10563865365553647), (0.493, 0.13667096934840084), (0.536, 0.10220002565160394), (0.532, 0.10326859186124057), (0.539, 0.10119512594863772), (0.57, 0.07576039692386985), (0.525, 0.11116002036258578), (0.512, 0.11674875917471945), (0.488, 0.11555143764801323)]
TEST: 
[(0.25425, 0.04382993084192276), (0.417, 0.06269155278801918), (0.465, 0.06890970841050148), (0.4675, 0.0824232129752636), (0.466, 0.1052005781531334), (0.47175, 0.14798526340723037), (0.474, 0.14918128669261932), (0.47475, 0.1711675282716751), (0.474, 0.17799552154541015), (0.4715, 0.1876283633708954), (0.47575, 0.19261900579929353), (0.50775, 0.07528527274727821), (0.5925, 0.05373585258424282), (0.52725, 0.09866530221700669), (0.52025, 0.11161799174547195), (0.50425, 0.10308214530348778), (0.5665, 0.06549001221358776), (0.508, 0.0947596884071827), (0.50125, 0.11555008772015572), (0.56125, 0.07335699847340584), (0.52225, 0.10063534000515938), (0.5285, 0.11311826582252979), (0.51825, 0.11287089338898659), (0.53975, 0.10229460486769676), (0.565, 0.0654588977098465), (0.5075, 0.10544187691807747), (0.584, 0.05826679217815399), (0.54875, 0.09120925533771515), (0.5865, 0.06627035331726074), (0.55175, 0.08360979910194874), (0.5585, 0.08056837783753872), (0.62575, 0.05323460203409195), (0.51775, 0.10356948572397232), (0.517, 0.1021675417125225), (0.525, 0.10894385462999344), (0.49575, 0.14705826675891875), (0.551, 0.08755932460725308), (0.50425, 0.11826610434055328), (0.53, 0.09867605745792389), (0.53575, 0.09216848701238632), (0.5895, 0.06309948010742664), (0.5415, 0.09443112325668335), (0.523, 0.10822334477305412), (0.49075, 0.1441659428179264), (0.5435, 0.10367298740148544), (0.539, 0.10740364998579026), (0.55325, 0.10122656151652336), (0.563, 0.07554301199316979), (0.53125, 0.11184539318084717), (0.522, 0.11327442064881325), (0.49275, 0.11310377296805382)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.45      0.97      0.61      1000
           3       0.90      0.04      0.07      1000
           6       0.53      0.92      0.67      1000
           8       0.96      0.04      0.08      1000

    accuracy                           0.49      4000
   macro avg       0.71      0.49      0.36      4000
weighted avg       0.71      0.49      0.36      4000

Collaboration_DC_2
VAL: 
[(0.246, 0.04495431101322174), (0.25, 0.0901500660777092), (0.285, 0.12618956410884857), (0.322, 0.1603555630147457), (0.379, 0.1944475924372673), (0.396, 0.21103904109448193), (0.418, 0.21013511917740108), (0.404, 0.2106591824889183), (0.419, 0.24523430716246367), (0.417, 0.2547052926644683), (0.416, 0.2696862921901047), (0.504, 0.048795853700488806), (0.55, 0.05820426441729069), (0.476, 0.10714851859211921), (0.511, 0.09081347946077585), (0.513, 0.09581479030475021), (0.522, 0.08780141389742493), (0.515, 0.09430138425156474), (0.469, 0.12126916783303023), (0.521, 0.09518596160411835), (0.502, 0.0874641549512744), (0.495, 0.09617090847715735), (0.516, 0.09954610355198383), (0.523, 0.09588097225129605), (0.539, 0.0791014034897089), (0.496, 0.13033978368039242), (0.556, 0.06713995695114136), (0.522, 0.07590092350542545), (0.531, 0.09836479881964624), (0.534, 0.07778636756911873), (0.581, 0.04941038126498461), (0.574, 0.07671902224421501), (0.543, 0.08702041613683105), (0.503, 0.10300986927794292), (0.491, 0.11147030731849372), (0.498, 0.12691439437493682), (0.52, 0.09452648428827524), (0.512, 0.0999923131391406), (0.517, 0.09629212887585163), (0.52, 0.09697481282055379), (0.555, 0.06740186532586813), (0.543, 0.08491760557889938), (0.507, 0.11292076747003012), (0.489, 0.13507501922361553), (0.495, 0.10495989784412087), (0.493, 0.12151392592675984), (0.519, 0.09553026939183473), (0.575, 0.06464311767742038), (0.542, 0.08924979417212307), (0.518, 0.1149199928380549), (0.513, 0.11005283547192812)]
TEST: 
[(0.2375, 0.04377405685186386), (0.25, 0.08629248949885368), (0.2875, 0.1204801225066185), (0.33075, 0.15272677546739577), (0.383, 0.18404866141080856), (0.396, 0.1998128536939621), (0.4165, 0.19864237356185913), (0.41075, 0.20010809701681137), (0.4185, 0.232751971244812), (0.4215, 0.2435290548801422), (0.42125, 0.25688072097301484), (0.496, 0.04721171754598617), (0.55, 0.055978660389781), (0.4695, 0.11073385119438171), (0.5025, 0.09412525084614753), (0.52325, 0.09330307793617248), (0.52575, 0.08769501072168351), (0.51275, 0.09025674897432327), (0.48025, 0.11719547885656356), (0.52575, 0.09563859474658966), (0.509, 0.08337603759765624), (0.50225, 0.09606876826286316), (0.5115, 0.1008453234732151), (0.5315, 0.09297190997004509), (0.55225, 0.08142910996079444), (0.49475, 0.13323287779092788), (0.5585, 0.06918142588436603), (0.54325, 0.07719872668385505), (0.526, 0.10292523351311683), (0.54275, 0.07842450618743896), (0.59775, 0.050367718651890755), (0.5695, 0.08017594823241234), (0.5405, 0.09395176807045937), (0.5065, 0.11036368682980538), (0.49025, 0.11512712928652763), (0.495, 0.13315930435061454), (0.52075, 0.09802239409089089), (0.51825, 0.09965133741497993), (0.52325, 0.09597842466831208), (0.5175, 0.09826523432135582), (0.545, 0.07131495553255081), (0.53575, 0.08812126842141152), (0.5035, 0.11897828236222267), (0.4855, 0.1400056726336479), (0.50575, 0.1077096109688282), (0.4905, 0.12800066295266152), (0.52425, 0.09806918230652809), (0.5775, 0.0685606714040041), (0.55425, 0.09379815843701363), (0.50775, 0.12008947494626045), (0.5235, 0.1126213575899601)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.63      0.96      0.76      1000
           4       0.64      0.07      0.13      1000
           6       0.42      0.92      0.57      1000
           7       0.97      0.14      0.24      1000

    accuracy                           0.52      4000
   macro avg       0.66      0.52      0.43      4000
weighted avg       0.66      0.52      0.43      4000

Collaboration_DC_3
VAL: 
[(0.251, 0.04425755798816681), (0.25, 0.08961409723758698), (0.273, 0.1021047906279564), (0.392, 0.12693029174208642), (0.402, 0.15093016832321882), (0.411, 0.1675949716344476), (0.459, 0.21405988819152116), (0.462, 0.2431645128428936), (0.46, 0.27802466233447193), (0.463, 0.2662605528570712), (0.458, 0.29413447110913693), (0.439, 0.2061546819806099), (0.445, 0.19369617828726768), (0.46, 0.19499317000806332), (0.457, 0.22191571476124228), (0.448, 0.18239906696602703), (0.456, 0.201513444788754), (0.454, 0.16529539940692484), (0.452, 0.1986637821570039), (0.442, 0.1859277235502377), (0.45, 0.17880388958379625), (0.456, 0.15467458784207702), (0.458, 0.15574216996878384), (0.455, 0.1524343203380704), (0.453, 0.17138424534723162), (0.448, 0.17905203221924604), (0.449, 0.17092295372858643), (0.456, 0.17522187575697898), (0.456, 0.1453946851361543), (0.457, 0.18050056927930563), (0.453, 0.17791650306712836), (0.447, 0.1851609045462683), (0.455, 0.18698557763546705), (0.448, 0.19597565631754696), (0.457, 0.184874643355608), (0.453, 0.18322874953597784), (0.451, 0.20104956233687699), (0.45, 0.1918140706960112), (0.46, 0.2011821936368942), (0.447, 0.18849971390888096), (0.443, 0.2022513088742271), (0.447, 0.21830086036212742), (0.45, 0.20254474718123675), (0.45, 0.2330693068630062), (0.457, 0.20996595074236393), (0.452, 0.23896697833389044), (0.451, 0.21986760744825007), (0.45, 0.20951822929270567), (0.449, 0.20271974491514266), (0.454, 0.23180351432459428), (0.439, 0.2166316314593423)]
TEST: 
[(0.2505, 0.04323132234811783), (0.25, 0.08614569437503815), (0.2725, 0.09757168874144555), (0.40125, 0.1206749073266983), (0.39925, 0.14345088809728623), (0.415, 0.15829271918535232), (0.46625, 0.20534850442409516), (0.4725, 0.2301350491642952), (0.46875, 0.26313387537002564), (0.4705, 0.2506633816361427), (0.4695, 0.2793130035400391), (0.44375, 0.19414916622638703), (0.452, 0.18302400529384613), (0.4685, 0.1860063576698303), (0.467, 0.20877283298969268), (0.454, 0.17219379889965059), (0.4685, 0.1911788853406906), (0.46675, 0.15723993265628813), (0.4665, 0.1879729549884796), (0.45025, 0.17715849006175996), (0.46225, 0.170378216445446), (0.4675, 0.14652824681997298), (0.4685, 0.14820439404249192), (0.46475, 0.14243967854976655), (0.4665, 0.1622620726823807), (0.458, 0.17004283183813096), (0.45775, 0.16049197137355806), (0.4665, 0.1671369411945343), (0.46725, 0.14098524135351181), (0.46575, 0.17373243170976638), (0.46525, 0.17190552961826325), (0.46075, 0.17916690707206726), (0.4675, 0.17957943546772004), (0.461, 0.18905696564912797), (0.46575, 0.18020505946874618), (0.46625, 0.17537076437473298), (0.4635, 0.19413758778572082), (0.46075, 0.1841425809264183), (0.46875, 0.1931402601003647), (0.458, 0.1822837005853653), (0.45825, 0.19318264752626418), (0.4595, 0.20739590656757353), (0.46275, 0.1956064522266388), (0.46575, 0.22346492505073548), (0.46525, 0.2006075654029846), (0.46275, 0.22953703683614732), (0.4655, 0.21055513167381287), (0.461, 0.19958328205347062), (0.45675, 0.1946303347349167), (0.46425, 0.22192077910900115), (0.45025, 0.2062423215508461)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.39      0.99      0.56      1000
           6       0.00      0.00      0.00      1000
           9       0.55      0.81      0.66      1000

    accuracy                           0.45      4000
   macro avg       0.24      0.45      0.30      4000
weighted avg       0.24      0.45      0.30      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [12]
name: alliance-3-dcs-12
score_metric: contrloss
aggregation: <function fed_avg at 0x7911a6c80c10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=12
Partitioning data
[[6, 1, 7, 4], [5, 0, 7, 4], [8, 3, 7, 4], [9, 2, 7, 4]]
[(array([40565, 32339, 39457, 16008, 30469, 15600, 36274, 46340, 27651,
       15424,  4824,  4421,  6548, 25892,  7823, 36318, 14392, 32170,
       25039,  1579, 38310, 46334,  6486,  2424, 38993, 15349, 25294,
       15048, 20755,  9406, 18256, 28382, 47734, 42975, 20535,  3401,
       25877,  3412, 11070, 32198, 33824, 45836, 20800,  7137, 49129,
       49110, 36522,  4201, 49584,   234, 25526, 49754,  7481, 43933,
       46846,  8734, 10007, 32904, 32752, 25704, 28177,  4118, 45759,
       14701, 26235,  6101,  2978, 18816, 45863, 35591, 20025, 42881,
       21719, 19834, 35577,   248, 37835, 22889, 35615, 39258, 36625,
       15657,  3122, 34245, 30390, 33367, 28504, 42209, 16064, 36613,
       23105, 48229, 16960, 22004, 30031,  6581, 36348, 46550,   298,
       30759, 12254, 45140,  1159,  8344, 17554,  1222,  2596,  3018,
        3676, 10783, 22567,  8053, 18025, 37045, 25448, 25569, 46134,
       14959, 41142,  3223, 12635, 49966,  8820, 33990, 36062, 17173,
       17858, 20324, 22058, 24231,  3873,  1585, 22363, 27843, 38337,
       27708,  9130,  4105, 25612, 25155,  2583, 13390, 42380,  5206,
       48206, 29214,  7247, 20789, 35780, 26542, 23760, 41491, 13176,
       34602,  4731, 15517, 37748, 27454,  9303, 20278,  1917, 33812,
       27602, 16508, 10176, 44573,  8171, 47961,   620, 25641,  8015,
        2777, 34564,  2439,  5497, 21730, 37796, 34264, 45637,  3926,
       31099, 43624,  5679, 39469, 42960, 31651, 12221, 21673, 37715,
        2179, 49808, 39337, 18396, 47407,  8867, 29089, 37404,  2043,
       32637, 33178,   738, 38786, 25551, 14291, 42584, 30089,  8827,
       30249, 46418, 29357, 21965, 32497, 40824, 49094, 40929, 34552,
        7044, 35060,   132,  5964, 23927, 39133, 41275, 41045,  6046,
       38959, 10757,  3554, 18571, 30100, 26231, 40206, 31826, 22295,
       11917, 26502, 15338, 42015,  4805, 11233, 27087, 25035, 30292,
       39808,  8170,  5169, 11310,  8128, 20717, 16927, 27868, 35550,
       13763, 14074, 34287, 19144, 44273, 37301, 39990, 33228, 23514,
       32603, 41576, 33112, 41445, 39936, 21088, 28347, 30923, 11580,
         466, 32418,  8124, 28847, 19310,  5211, 30676, 46219,  7722,
       44541, 28193, 34258, 30269, 30166, 12384,  7326, 16925, 43600,
       32458, 10363,   873,  9946, 34830, 49993,  9138, 45672, 42655,
       12741, 23275, 29966, 34376, 39097, 41087, 14101, 44097, 45569,
       16233, 25363, 36046, 38812, 13755, 22532, 24450, 40178, 36614,
        3231, 37153,  5171,   841, 39338, 13839,  5685, 18117,  2584,
       44109, 12865, 16083, 23933,  9118, 42734,  7735, 36736, 38756,
        4360,  2582, 13899, 12064, 23923, 13488, 20312, 23648,   262,
       37235, 26994, 33452,  7475, 26452, 13056, 39654, 45704, 10112,
       48612, 31512, 13897, 44483, 44971, 21933, 34637, 20475, 24873,
       39848, 26578, 40987, 21706, 36595,  5810, 11421, 21073, 42709,
        1304,   536, 31113,  9754, 19574,  4358, 37620, 18333,   690,
       19810, 21796, 14005,  6895, 28137, 27002, 24297, 32959,  7841,
       41525, 15295, 16320, 31072, 28882, 45488, 10271,  2910, 34700,
       37461, 32979,  4132, 44217, 40972, 31747, 10236, 19930, 44185,
       48294, 34064, 49186,  7622, 24284, 12676, 31195, 44686,  6318,
       26080, 24219, 36107, 49416, 46519, 40505, 34238, 16673, 27538,
       18066,  1694, 45444, 22762, 49937, 23100, 21651, 24417,   364,
       44134, 39805, 43251, 47540, 44726, 46177, 29791, 15655, 44993,
       45988, 19500,  8573, 37666, 24121, 21547, 11782, 27132,    75,
       33114, 15883, 28122, 32684, 19301, 15557, 34690, 29314, 29894,
       16368, 16275, 26637, 46396,  1781, 12516,  3620, 10614,  6068,
       48465, 38858, 12923, 30049,   815, 40751, 31539, 48312, 24466,
       44524, 11646, 15756, 20376, 38047, 36813, 23714, 12054, 49225,
       18832,  7130, 19546,  7832, 45736, 27871, 38950, 33859, 44487,
       37979, 44311, 16800, 14507, 29309, 38286, 33712,  1445,  9587,
       38703, 14641, 19461, 24917, 35908, 23785, 31777, 36114, 10281,
       10573, 40637, 24850, 17181, 15079, 29272, 17737, 10873, 26991,
       18165,  6573,  8897,  4037, 48567, 48055, 48041, 16026, 16750,
       37150, 26046, 34707,  8859, 27006,  7813, 44922,  7223, 33825,
       38195, 20678, 37930, 39259, 14680, 35682, 49903, 16537, 49745,
       12995, 27288,   181, 32374, 26107, 38279, 49058, 24197, 17436,
        3135, 10278,  8129, 26420, 38243,   972,  9133, 48837,  3637,
        5326, 11088,  5774, 39378, 18447, 42485, 41758,  2135, 48104,
       15070, 15511, 14449, 12658, 42895, 17521, 39408, 21660, 48697,
        9110, 41526, 12889, 42514, 49226, 26166, 43625, 13569, 28328,
        3955,  3157, 10858, 28987, 46754, 43360, 27985, 45000, 39307,
       10602, 21107, 36608, 23186, 15245, 42268,   797, 40300,  1930,
       13703, 28366, 17463, 48204, 44353, 25079, 37842, 28498, 20780,
       13039, 26017, 48256, 29196, 46620, 32208, 40820, 35696,  7969,
       31944,  3050, 39257, 14775,  8106, 21482, 42593, 19844, 45999,
       22204,  7331,  1742,   289, 41805, 44493,  6329, 39711,  1218,
       36278,  8595, 11996, 17636, 29535, 20109, 38693,  1071, 12787,
        1042, 31357, 40912, 20565, 17910,  6677, 46412,  8038, 16504,
       49925, 18227, 18468, 49183, 11426,  9805, 29644, 46672, 36589,
       35241, 28197, 35290,  5363, 40692,  9374, 21335, 34876, 27775,
        5842, 48848, 31117, 26733, 36369,  5269, 14493, 14290, 26947,
       37262, 36936, 42463, 37896, 10818, 34584, 43060, 40356, 22569,
        7621,  1309, 12900, 26056, 19200,  2801, 27187, 49627,  2458,
       35483, 18579, 12961, 16171, 33485, 40893, 16914,  6449, 23571,
         771, 44610, 46988, 44798, 35164, 28639, 10235,  1908, 34044,
       15235, 30422, 21535, 38667, 44424,  2103, 18517,  1308, 23910,
       24901, 31520,   973, 39633, 19638, 12244, 20016, 16654, 35869,
       21269,  2301, 38308, 45767, 15196, 44147, 43595, 31464, 14670,
        1169, 48505, 10250,  6838, 44336, 40146,  1387, 38135,  7436,
       23545, 28250, 24883, 28167, 19528, 33171,  9724, 15168, 45170,
       35066, 23779, 10861,  6144,  9529, 36671, 27494, 14557, 43549,
       49687, 10865, 39005, 46883, 49390, 44450, 11092, 46753, 19742,
       27461, 24448, 28837, 20515, 13453, 32818, 30464, 25579, 45515,
        2123, 26819, 28576, 19643, 45315, 10633, 10392, 10539, 35774,
       44400, 15024, 17801, 40228, 47716, 28464, 13846, 45723, 23460,
         856, 25779, 26638,  3577, 42750, 30616,  2106,  1022,   665,
       44125,  1882, 40962, 15482, 37214, 19264, 37854,  3945, 19846,
        1784, 47144, 37089, 23634, 28561, 22944, 13987, 10519, 28666,
       23329, 36294, 22243, 18409,  3897,  3936, 37431, 32893, 48634,
       20888,  1813,  9349, 23337,  7365, 41237, 31689, 47287,  5229,
        4057, 31033, 15703, 41825, 12444, 45408, 13469, 40674, 18404,
       29291, 49504, 20412, 18340, 41518,  1863, 23817, 23468, 33666,
       18052, 37274,  6654, 26584, 25434, 18875, 25620, 46940, 27384,
       26118, 14911, 30300, 40259, 39470, 16073, 30230, 30715,  6344,
        1575,  6844, 12136, 33342, 16802, 22536,  5998, 42725,  9848,
       30077,   153, 44705, 26772,  8523, 23211, 48429,  1516, 24046,
        1069,  2839, 32928, 26911, 19689, 18490, 32415,  6234, 46789,
       41263, 48211, 40210, 47345,  2842, 45323,  2835, 43553,  5405,
        6270, 41523, 26854, 17623,  1814, 10925, 21753, 40531, 45820,
       16323, 33408, 22461,  2647, 36976, 18304, 20782, 49402, 49954,
       10536, 41101, 38350, 24308, 47798,   572, 15515,   660, 19475,
       14262, 33140, 49902, 34670, 27817,  9061, 27558, 35714, 48997,
        1832, 43926,  1238, 14726,   868, 46528,  5514, 12522, 14019,
         490, 17211, 46446, 36523, 36317,  7656, 11033, 38395, 20543,
        8541, 13292, 40033, 30020, 32413, 12505, 41852, 31256,   934,
        8473]), [6, 1, 7, 4]), (array([ 9035, 26874, 11513, 48114, 30657, 23488, 40417, 41232, 30871,
       32769, 18463,  3173, 13676,  2864,  8494, 43759, 36121, 42293,
       34833, 19274, 41046, 28982, 42866, 41094, 31646, 29268, 27653,
       44838,  4125, 15861,  4627, 33957, 14495, 46608, 37114, 32849,
       32530, 16949, 40069, 20801, 27991, 42194, 20348, 45992, 46428,
       49267, 44284, 46165, 37508, 15653, 31409,  6580, 13251,  6436,
       16626,  1091, 20207,  6982,  9871, 41139, 27947,  5367, 26194,
       23273,  3451,  3080, 21677, 11995, 40632,  9337,  4890, 45553,
       38990, 28779, 30155, 26079,  7090, 18966, 14004, 45215, 39817,
       44358, 25333, 22890, 37899, 48935, 48832, 46310,  6561, 47218,
       12177, 16580, 17009, 22059, 30723, 37880, 40434, 38632,  5993,
       46853, 31059,  2089, 40051, 43038, 13579, 47103, 49988, 14627,
        2148, 18399, 27348, 40904,  8252, 40763,  6220, 13283, 43651,
        6807, 25751, 18347, 32044,  1199, 14259, 41340, 24877,  1545,
       29450,  2182, 19169, 24656, 47602,  9772, 47919, 11740, 32559,
       45587, 39124, 32658, 28425,  4120, 15450, 21233, 46130, 14992,
       27265, 39783, 47920, 41499, 26072, 46022,  6689, 42613, 36202,
       49547,  2374,  7398, 38944, 41886, 47659,  6058, 29630, 15544,
       34112, 34728, 49675, 27223, 47647, 31120, 49316, 11980,  1355,
       28090,  7140,  9445, 29422,  4990, 27941, 42549, 32124,   424,
       38739,  1061, 21598, 11396, 40688, 20493, 48754, 38403, 15019,
       21046, 37708, 48083, 30231, 47230,  8274, 20809, 29804, 28889,
       22483, 29836,  2194, 20075,  7446, 30952,  6037, 38709, 30054,
       10258, 23392, 24648, 18852,  8511, 27397, 20421, 19652, 36123,
       18772, 13057, 27666, 34299, 30162, 24909, 25552, 20171, 28438,
       19933, 20277, 36235, 33079, 18986, 49810,  9929, 33039, 41817,
       47555,  6707, 22908, 15336, 11600, 30393, 24953, 40105, 39846,
       24650, 46483, 46690, 43599, 27305,  4632, 40765,  2895, 28767,
       40432, 48556, 31023, 38312, 43659, 47560, 21930,  8753, 34070,
       16375, 38716, 15959, 43253,  3609, 28138, 38307,  1329, 28919,
       16513, 19409, 46137, 23010, 35528, 42183, 39396,  7808,  5684,
       41164, 21573,  8102, 45390, 20295, 43731, 46192, 44731, 10578,
       28003, 22123, 31481, 13964, 49130, 15153, 22271, 19273, 44967,
       12469,  2490, 20902,  4295, 18694,  8437, 44592, 25489, 10804,
       43138,  8660,  2156, 47805, 11218, 27278, 25668, 34711, 29732,
       16316, 16065, 15115, 34748, 29189,  2863, 37068, 29039, 26546,
       46934, 17221,  6904, 39819,  4721, 25477,   782, 28063,  6688,
       32363, 13714, 12936, 19945, 19974, 44430, 37788, 10031, 39138,
        9171, 38321, 18614, 43077, 31600, 24111,  3909, 45357, 13060,
        7588, 10486,  4619,  7643, 35873, 36530, 43245, 17082, 38320,
       11431, 37664, 16752, 48148, 34341, 11056, 21322, 35546, 11276,
        3987, 19493, 37146, 23629, 47557,   650, 27211, 36323, 47994,
       30539,  4952, 37165, 11709, 35952, 27901, 32140,  7599, 11178,
       38586, 11114,  9435, 20396,   284, 35608, 29293, 49513,  9236,
        5738, 48069, 19106, 22026, 16146, 21472, 32683, 12028,  4746,
       36497, 45186, 48239, 49799, 41167, 14468, 41212, 29518, 27042,
       46041,  4861, 31015, 46303, 22993,  4524,  2617, 48096, 29544,
       18864, 33530, 28650, 14216,  6518,  1708,  6187,  3582, 49340,
       31955, 42190, 29620, 45438,  4523, 11304, 32056, 25905, 47931,
       47645, 23746, 12542, 33386, 26081, 42844, 28068, 20743, 34568,
       28467, 38190, 49375,  7985, 39311, 40012, 11721, 17994,  3585,
       17154, 41030,  4096, 33711,  6090, 35028, 14958, 33062, 38580,
         332, 30888, 18763, 47271, 36007, 31246, 48059, 49992,  5441,
       45182, 37712,  2959, 48378, 30467, 38334, 30909, 48677,  1187,
       23984, 10326, 41879,  9058, 27080, 33831, 24713, 49346,  5331,
       34705, 28515, 26342, 48554,  8242, 37467, 34900,  2716, 37787,
       14143, 24026, 38142, 18997,   470, 36806, 20884, 49277,   948,
       12751, 36829, 40942, 42909, 11323, 27374, 28184, 44101, 16074,
       24267, 39717, 38225, 43066,  8966, 17913, 49809, 16719, 42184,
        5114, 14818, 21109,  6850,  9778, 17095,  6283, 47092, 27565,
       25742, 41347, 33096, 19250, 32920, 40120, 23290, 11046,  9071,
         318,  1165, 35284, 26494, 11393, 29718, 10927, 36897, 20550,
        9795, 43617, 20436, 32568, 46671, 46719, 27242, 17951,  6821,
       43435, 40684, 47923, 10226, 17494,  2612, 19418, 24782, 10685,
        9560, 22730, 12980, 43229, 24025, 35986,  5307, 13147, 23577,
       22207, 37190, 26397, 49610, 21100, 24970, 42651, 37131, 47880,
       31800, 26821, 17499,  4396, 48343,  8664, 39262, 35815, 30036,
       49229, 34273, 47835, 23356,  6460, 34191, 24371, 39261, 47438,
       35972, 26749, 27015, 36597, 31262, 23034, 18874, 29481, 41780,
       15853, 22291, 14603,  2814, 41370,  9239, 12838, 44911,  7482,
       19883, 32514, 49454, 45929,  6695, 43906, 38512, 43528, 41584,
       10884, 13272, 14191,  8745, 39799, 35421, 32888, 33949, 38155,
       26811,  6072, 49037, 30278, 47856,   688,  4555, 11284, 44081,
       37637, 25197, 15472,   575, 44115, 45376,   329, 26067, 46691,
       13540, 39155, 35962, 47169, 32354, 33884, 27078, 39017, 39720,
        7895, 19997, 20369, 10598, 23490, 11502, 13136, 34113, 43830,
       33349, 33636, 46996,  4903, 33155, 10106, 24954, 30972, 44286,
         746, 36034, 11102, 21329, 15551, 32688, 23880,   362, 43305,
        8764,  7717, 43671, 18037, 48029, 43782,  8643, 24088,  9768,
       23835, 23842,  1857, 17872, 12844,  5798, 33045, 18881, 12848,
       47590, 43242, 34826,  6653, 46834,  1395,   294, 23331, 33908,
       46982, 11260, 20099, 25304,  2000, 36037, 30410, 23778, 42251,
       49010, 12402,  4513, 22723, 40585, 26702, 33556, 34528, 33139,
        1046, 37497,  6696, 21899, 31939, 20120, 30915, 19936, 24889,
       13412,  4231, 19033, 26717,  1855, 43756, 33221, 28795, 37124,
       37778, 41500, 42403, 38455, 20253,  6465, 49669, 13834, 48529,
       46398, 47300,  9907, 30794, 25371, 45598, 24513, 37465, 41219,
       23247,  5259,  6785,  3668, 34410, 27897, 16383, 49421, 25114,
       10641,   130, 43714, 11573, 26062, 29070, 47482, 19822,  3252,
       38794, 34840, 35432, 39734, 42072, 24548,  1388, 47796, 14244,
       47415,  4297, 23279, 13023, 46339, 32088, 10640, 15909, 22949,
       13882, 30917, 36466, 33352, 35868, 26830, 21788, 22727, 14858,
       11967, 10706, 39169, 41453, 23748, 39573,  5940, 27074, 40707,
       14943, 18553, 14036, 17083, 42809,  9749, 17199,  1639,  9506,
       30942, 10979, 30435, 30911, 20306, 28100,  7368, 43306, 32533,
       43203, 29098, 43499, 15625, 24171, 11994, 16294, 42040, 44563,
       16241,  3314, 32989, 22288, 23512, 27247,  3288,  7052,  9615,
       48179,  1018, 45136, 21980, 22740,  4695, 28913, 21676,  1968,
       16433, 14111, 31305,  3522, 15596, 17958, 40770, 17412, 46241,
       36646, 20108, 16865, 37438, 28489, 30094, 32069, 17671, 23394,
       46164, 11209, 38787, 43327, 31767, 20325, 20407,  5285, 47139,
       44851, 24040, 27489, 45801, 36420, 29849, 44572, 34331, 10851,
       21666, 39945, 39344, 48738, 34589, 14256,  3112,  3360, 21708,
       42640, 30970, 37442, 18043, 31476, 34450, 46891, 47855, 21176,
       15582, 41246, 19657, 34082, 29420, 35135, 32518, 40301, 38593,
       15377, 42695,  5173, 41337, 22886, 27276, 22241, 40665, 48616,
       27180, 44040, 17420, 31627, 14181, 27017,  4429,  3761,  5495,
        8631, 35563, 15339, 14827, 16659, 13269, 28793,  9888, 22509,
       13764,  6378, 26586, 25147, 20060, 40071, 34051, 40793,  3176,
       49243, 11757, 38070, 38953,  6562, 19426, 47975, 48716, 16891,
       15256, 18418,  5788, 12022, 33129, 17719, 29024, 46255, 21629,
       14051]), [5, 0, 7, 4]), (array([25450, 26156, 12543, 48247,  1236,  2881, 16730, 20375, 23858,
        2417, 35995, 12400, 19402, 18221, 37437, 34655, 14767, 25987,
       27370,  4245,  9117,  2979, 29747, 19696, 40389, 47021, 15161,
        1770, 17416, 31714, 35315, 18691, 45697, 30087, 48469,  6632,
       40784, 16042, 14427, 10098, 48133, 43807, 19123, 27281, 16381,
       33626, 31879, 46392, 21477, 43662, 15260, 19603, 38867, 32896,
       40368, 10606, 46736,  5648, 45138, 32447,  5985, 18623, 32463,
       20668, 21383, 39981, 42467, 30770, 16607, 30835, 45168,  9960,
       43423,  4670, 32728, 17799,  2929, 22696, 28450, 42497, 40691,
         418, 27398, 33372, 12418,  9076, 16512, 24456, 46926, 43899,
        7112,  7387, 24787, 47114, 46175,  6337, 43001, 43883,  1479,
       41070, 48445, 28006, 23818, 12725, 40671, 22440, 49372, 41585,
       37318, 40881, 48698,  2489, 48794, 22374, 46938,  3130, 28284,
       25096, 20812, 17617, 19374, 23294, 26437, 28205, 13266,  5278,
       12650,  2820, 20405, 31692,  8489, 39838, 43344, 25756, 21257,
       48552, 32327, 42300, 42203, 12258, 21249, 45025, 40789,  8270,
        9572, 43540,  9669,  4502, 16692, 45354, 26195, 17519, 19167,
       42840,  8961, 26328, 43119,  7561, 21312, 17348, 45309, 34171,
       35173, 19720, 41674, 14211, 16483, 36229, 32207, 41008, 44937,
       37557, 29056,  4602,  9972, 42826,   562, 35864,  8211, 29968,
       37858, 43755,  5941,  6139, 27640, 49166,  9599, 13694, 32246,
       39895, 17007, 13139, 39802,  9651, 42965,  4713, 40495, 25951,
       29476, 34212, 16727,  2483, 38689,  8022,   222, 22649, 13886,
        8831, 32465, 32138, 48475, 35588, 16861, 16084, 49253, 24629,
       26694, 23172, 36742, 34893, 23085, 29929, 26254, 46735, 43895,
       33287, 49625, 25902,  4723, 27056, 29877, 19314, 21565, 48125,
       22682, 36803, 39621, 49916, 49553, 38469, 45853, 38809, 42763,
       25629, 16049,   244, 41298, 10191, 31317, 28800, 24652, 20638,
       33525, 16289,  1104, 34160,  4542, 35769, 49408, 33917,  3679,
       19830, 30995, 33007, 24733, 19101,  3429, 15313, 19578, 31400,
       44621, 18414,  4954,  7906, 48296, 13541, 14047, 25633,  5190,
       24696, 24751, 31463,  1461, 43490, 32732,  9794, 36351,  9164,
       11409, 42504, 22891,  1070,  7651,  8710,  6482,  8101, 26688,
       30586,  1010, 22011, 29523,  9298, 26224, 12039, 38068, 10506,
       34199, 11564, 14466, 28330, 22239, 13530, 30839, 19327, 49975,
       25307, 15122, 39021, 14993, 37187, 14161, 32405,  2141,  4151,
       27608, 20301, 22850, 40249,  7006, 19922, 21299, 36230,  6849,
        8330,  7050, 32702,   266,  4402, 44987,  2165, 46094,  9006,
        8028, 33280, 36440, 41713, 33964, 41269, 14574,  9700, 11357,
       22905, 24099, 34661, 35937,  7680, 44173, 20639, 35715, 23094,
       32139, 11386, 19221, 16731, 35772, 22411, 38364,  9008,  3997,
       24174, 44943, 42738, 35229, 42736, 34889,  2412, 29442, 34710,
       46425, 24434, 16851, 33600, 20625, 14187, 40973, 11902, 43827,
       31414, 43784,  4221,  6883, 30774, 49319, 15640, 16495, 36772,
       30346, 44381,   639, 16561, 20326, 32085, 42953, 20577,   922,
       17920,  5592, 29654,  3355, 25769, 44576, 11268,  5945, 49140,
       29464,  8408, 29960, 10017,  3571, 38647, 46347, 43737,  1265,
        2127, 47062, 44226, 34999, 44768, 31542, 42837, 17828, 46023,
       45670, 15080,  8246, 18520, 41271, 10931, 20882, 48346, 14253,
        7200, 25471, 13862, 41910, 20330, 36212,   857, 10414, 16646,
       43440, 27274, 37965, 20685,  9343, 40232,  9897, 43744, 17074,
       45402, 34185, 16271, 13824, 26900, 47838,  8790, 24148, 23372,
       29050, 39778, 10634, 10869, 27906, 15224, 42496,  2892, 14239,
       10066,  8041, 48772,  6724, 49918, 16341, 37370, 39158, 12517,
       38215, 47654, 26907,  5744, 48376, 12147, 17155, 38006,   207,
       13235, 27475, 47717, 28880,  8105, 30265,  1135, 26931, 10384,
       46045,  5620, 42063,  5995, 49843, 38176, 31336,  7817, 39107,
       32068, 10988, 45103, 28520, 32487,  7023, 26810, 34260, 43436,
       22601, 40646,  7756, 34511, 27600, 10295, 16557, 23344, 33681,
       39395,  6326, 43791, 33773, 43144, 27573, 44935, 40694, 23889,
       38595, 12722, 43091, 27824,  3611, 46211, 14614, 13132, 39400,
       27423, 45353, 25741,  7940, 12229, 46383, 29824, 10333, 20879,
       39304, 32440, 38121, 29195, 44076, 31818, 31671, 13732, 36407,
       39345, 17040, 36805, 48548, 19542, 24311,  2139, 15579, 47082,
       44784, 18481, 37636,  2807, 37302, 26767, 27400,  9486,  7682,
        4820, 29719, 29441, 29588,  3434, 36239, 17464, 41185, 22501,
       16399,  6966, 36959, 27043, 45343, 40709, 49470,  9891,  2244,
        1226, 28553, 27189, 42421, 44292,  1758, 27568, 30199,  6481,
       36256, 27226, 44363, 24505,  2113, 14222,  2750, 31432,  2278,
       21601, 21367, 33191, 40497, 43579,  4886, 10823, 20900, 13430,
       10216, 22901, 42052,  5579, 26916, 34308, 13974, 43309, 25131,
       18519, 25343, 26895, 24439, 39797, 12466, 13224, 40916, 47003,
       40295,  5200, 32211, 15979, 35982, 39111, 47828, 43940,   739,
       37055, 34955, 41700,  3279, 38539, 35698, 18405,  3914, 27748,
       33722, 10121, 32700, 15317, 31389,  3996,  8646, 46154, 48714,
       16184, 12901, 31331, 48307,  7016,  3623, 12163,  2776, 46517,
       20614,  9741, 10916, 47572, 28951, 40266, 44356, 15881, 23794,
       20158, 39700, 39276, 10621, 35003, 43215, 38732, 30066,  8946,
       39767,  5368, 47890, 25876, 41205, 47575, 14837, 16348, 12565,
        5863, 26633, 36136, 44222, 10712, 25116, 44135,  3150, 20960,
       22312, 35511, 29320,   722, 40837,  3699, 37670, 18828, 19361,
       13790, 40897, 42353, 17001,  2119, 26983,  3213,  9726, 33176,
       26104,  3933, 11329, 36417,  9672, 35153, 27036, 35002, 23330,
       37405, 37001, 22403, 36740, 41382, 41958, 17077,  2995,  4707,
       49488,  7933,  6314, 26838, 27790, 10888, 14186, 30559, 39501,
       22617,  1903, 37928, 31902, 32384, 31143,   831, 35522, 47155,
       28881,  2515, 10055, 37731, 14618, 42982, 34604, 31860, 44974,
       39413, 49352,  2210, 40277, 12213, 20778, 44656, 14310,  6168,
       23826, 18977, 44465, 47147, 26048, 35915,  5909,  7283, 24866,
        1762, 10015, 10179, 17513, 44900, 12417, 16240,  9638, 40943,
       31799,  3182, 19585, 18142, 31227,  5225, 14560, 20409, 18523,
        5815, 43758, 45586, 48046, 42795, 40195, 25014,  1569, 14082,
       27135, 19569, 20192, 43983, 41878, 49718,  3556, 17350, 43139,
       32100, 45623, 37530, 13310, 30306, 35186, 33674, 45397, 28696,
        7100, 10059, 38242, 22318, 42544, 30206, 45279, 41623,  4106,
        3397, 48235, 32654, 37123, 37960, 28796, 19006, 40067,  6720,
       46076, 27519,  6266, 26434, 42285, 48876, 32169, 13636, 45812,
       16155,  8331, 32643, 25703, 47348, 31306, 36755,  3322, 30841,
        2488, 31446, 46036, 30674, 45121, 30228,  3952, 22820, 23393,
        8913, 15525, 22210,  3189, 45749, 33027, 27613, 27199,  9153,
       27029, 25576, 47908, 13984, 46778, 39036, 27853, 35151,  4725,
       46011, 45688, 30242, 42603, 27100, 30425, 39576,  1303,  8447,
       46947, 40219, 29377, 43950, 39459, 43196,  9791,  5488, 31290,
        7636, 26380,  2175, 10196, 14635, 33401, 40088, 18829, 22443,
       26307, 27624, 39146, 49252,  2293, 30061, 26844,  2088, 29830,
       27227, 13803, 31053, 43765, 15744, 45712, 20978, 14914, 28658,
       47605,  1937, 47408,  5264, 28191, 40085,  2703, 33057, 33947,
        7984, 10765,  6894, 16136, 33657, 19170, 32373, 36694, 19548,
       44360, 37075, 10570, 34298, 38660,  1889, 28603, 20303, 35743,
       29582, 25863, 15917,  8361, 19627, 33979, 36105, 13427, 22513,
       30131, 21033,  7207, 14415, 33334, 36142,  5193, 32018, 35443,
       48223]), [8, 3, 7, 4]), (array([46298, 15758,  3526, 26668,  4622, 30615,   147, 28019,  5228,
        1504, 30038, 26888, 45235, 30578, 14599, 22252,  9683,  6643,
       46611,  4675,  1384, 28353, 21159, 32563,  3895, 27837,  9229,
       30973, 28358, 24107,  3237, 10405, 29356, 27815, 41419, 48140,
       18642, 20944, 41244,   278,  6293,   768, 17163, 25308, 28337,
       38426, 36321,  2767, 47641, 21300, 49249, 26394, 34515, 18260,
       36280,   657,  1231, 23410, 49287, 24638, 29529, 17139, 40875,
       29697, 32163,  3199, 39699, 30764, 39806, 17600, 18963, 33671,
       13782, 32838, 41765, 47765, 36521, 43619,  6158, 39319,  8841,
       43406, 23305, 14071, 47340, 45405, 39917, 31870, 30223, 29258,
       24550, 14205,  1332, 29193, 33248, 26945,  2158, 42414, 39117,
       18344, 37017,  2303, 26136, 32040, 17896, 20765, 26021, 47187,
        2585, 18539, 28656, 30503, 32072, 43141,  8402, 32156, 34409,
       26229, 30934, 47660, 31077, 44052,  8303,  7652, 13780, 43673,
       32552, 34077, 16267, 19670, 43614,  6934,  1390, 20129,  7752,
       35545, 12902, 27957, 30903,  3086, 38357, 19620, 11096, 27668,
       43539, 19369, 26518,     1, 32192, 11552, 34502,  5394, 29753,
        9501, 18430, 27791, 36508, 24133, 26506, 45984,  4618, 42102,
       24391,  8323, 36549, 29221, 48839, 29585, 33620, 12262, 21907,
        2838, 36288, 47896, 33676, 42362, 45871, 12011, 36887, 45673,
        3755, 33483,   360, 26600,   541, 21696, 24880,  7943,  9434,
       47875, 45684, 35218, 23075, 27965, 36504, 48751,  9513, 33224,
       41976, 13662, 36668,  2129, 19055, 44716, 33672, 14813, 49035,
       25483, 49266, 14626, 15616,  5133, 18666, 12186, 32245, 13380,
       24407, 23359, 26530, 10144, 44969, 42556,  1045, 30180, 17903,
        3977, 31176,  3298, 17716, 40387, 35423, 23856, 35184, 10622,
       38402, 11814, 28879, 22652, 40412, 15725, 42420, 37538, 48308,
       43591, 40693, 46723,  5138, 41580,  5300, 44280, 21189,  7410,
       19806, 34959, 41556, 42542, 18163, 14180,  7633, 35575, 45953,
       39687, 15118, 19969,  6315,  3193, 24210, 42489, 47563,  3454,
       36710, 16439, 13081,  9093,  8881, 40189, 34600, 13794, 10457,
       34433, 16357, 17114, 32361, 12720,  3520, 31074, 38170,    47,
       37515, 24500, 48504,  4525, 15977, 27685, 25268, 32470, 14500,
       29625, 40487, 34555, 21316, 33282, 27204,  1921, 22012, 38565,
        5634,  8789,  7340, 46633, 42155, 33564,  3161, 25379, 40444,
        1207,  3847, 48870, 27738, 29650, 34609, 37990, 20570,  1812,
       44658,  9989, 34866, 24994, 34398, 11500, 28689, 26146, 39339,
       49800,   646,  5087, 28426,  6477, 17344, 16742, 26956, 20226,
       31307, 31486, 37353,  1884, 21031,  2948, 42536, 19343, 25222,
       46967, 45277, 18457,  2533, 47940, 20660, 25628, 18964, 29902,
       16372, 49159, 15697, 47172, 39928, 13757, 36972, 13985, 15088,
        8037,  6073, 38129,  3944, 12043, 26651, 44070, 15430, 19899,
       26213, 30513,  2372, 22279, 42120, 42990, 23843, 18573, 16593,
       31475, 46405, 47632, 31333, 28452, 26990, 18269, 48498, 31636,
       19964,  5836, 46800,  5576,  9144, 16683, 46014, 46179, 44879,
       14588, 41407, 46258,  9952,  7074, 24610,  4811, 37166, 28227,
       44008, 27046,  2833, 48092, 27542, 31308,  8092, 48146, 26893,
        9408, 37811, 42995, 39935,  8121, 33531, 14172, 43441, 34088,
       35486, 48973, 30333,  9558, 30746, 48480, 16569, 47500, 44739,
       45803, 32582, 48733, 49397, 35420, 30371, 47360, 24719, 21210,
       31285, 10512, 16462, 42889, 33503,  1024,  2678,  5127, 35723,
       25095,   630, 16757, 41401, 10725, 45218, 18720, 36152, 29126,
       42588, 26760, 38638, 17339, 21063, 32706, 45021,  2354, 33834,
       23222, 21039,   957, 35947, 33403, 45335, 43367, 10018,  7258,
        3164, 35063, 21090,  5189, 20051,  5618, 35834, 18888, 42427,
       10594, 17188, 19918, 14583, 44181, 21301, 26276, 11867, 40955,
       15567, 18712, 24596, 17648, 19058,    43, 45708, 36513, 10552,
       26451,  2541, 46301,  6752, 43536,  8238, 49737, 34228, 42117,
       39030, 47406, 22405,  6939, 45610, 30484, 26165, 46792, 28048,
       23435, 24812, 16618, 49350, 40563,  1927, 45747, 48453, 12925,
       30150, 28502, 31789, 36691, 42386,  8052, 34412, 26826, 49396,
       28759, 32414,  1276,  6839, 46117, 15916, 35922, 43768,  7672,
       11469,  2616,  7565, 17720,  7463, 43902,  8465, 34329, 36562,
        7613, 20423,  5846, 25917, 13566, 48727, 42959,  8927, 24370,
       10957, 33150, 23364, 10348, 43532, 28195, 49796, 18292, 19600,
       24568, 48099, 30996, 40730, 27255, 25724, 13622, 32296, 33299,
        4582, 20707, 22854, 28771,  5403, 38500, 40025,  3711,  5116,
       17177,  7400, 36987,  8924, 17840, 13547, 25617, 44958, 42598,
       25778, 16025, 11621, 45281, 49774, 21317, 15550,  6726,  7141,
       11742, 17549, 48586, 10779, 48578, 24013, 40541, 23547,  3151,
       29018, 24995,  6827,  8984, 34788, 41473,  6335, 30890, 42812,
        8751, 23784, 38109, 26208, 29592, 25182, 14202, 35008, 28453,
       18124,  4078, 42951, 17989, 37935, 28429, 47039, 47665, 32053,
        6291, 15413, 43150,  7564, 33450, 47246, 25592,  4401, 49592,
       44762,  7101, 19064, 43322, 23486, 23909, 23609,  5761, 20314,
        4241, 33660, 13597, 37319,  8604, 37902, 45945, 33775, 27969,
       10034,  6641, 45150, 41390, 25836, 24233, 30556, 31529, 19744,
       18339, 45336, 44560,  7836,  2370, 19599,  9369, 11581, 29975,
       11414, 46857,  6513, 28965, 31224, 34469, 47422, 38325, 32329,
       22624,  8580, 22806, 21324,  9279, 31217, 22309, 17678, 41846,
        8109, 12627, 10041,  6680,  3733, 26752, 38965, 46167,   647,
       18139,  6979,  6864, 17159,  5996, 14431, 24090, 25226,   994,
       18914, 26411, 49850, 36144, 38284, 43168, 22687, 28911, 12888,
       27915, 25269, 24715, 30822, 45689, 12580, 18912, 29756, 48399,
       47205,  1435, 22205, 22149, 21251, 23853, 20276, 34793, 46521,
       16908,  9777, 23532, 49237, 37719, 45441, 25898, 27193, 35276,
       41766, 35624, 12510, 43335, 15656,  1347, 37875, 41737, 30528,
       12179, 33209, 43473, 32947,  9074, 16173, 46245, 40162, 14022,
       43632, 38115, 23655,  1804, 43304, 34808, 18161, 29218, 17507,
       13231, 10350, 14440, 13903,  5826, 13978,  6298,  9776,  1407,
       33744, 45867, 38720, 11586, 41934, 34852, 14283, 14316, 37591,
       29787, 39067,  4058, 37850,  2976, 48478,  4173,  1349, 25403,
       33501, 27390, 10561, 45517,  3587, 35811, 37777, 33796,  6603,
       36327, 21382, 16636,  3664, 17550, 15777, 39377, 41688, 22133,
       20698, 21252, 42910,  9655,  3560, 32285,  3757, 41590, 44703,
        7855,  2990, 46531, 41005, 41249,   725,  2830, 22990, 15538,
       23739, 30857, 18403, 30967, 26706,  3879, 28260, 31160, 48087,
       48484,   528, 35648, 14087, 12261, 28076,  6274,  3519, 22153,
       25299,  8379, 48985, 48447,  1654, 48937, 42938, 41451, 36690,
        8423, 39866, 19180, 29161,  5029, 43529, 48807,  3913, 28596,
       27282, 38995, 17179,  9155, 44846, 44095,  9389, 47399,  3695,
        6594, 22202, 44586, 17707, 49475,  8263,  6636,  6569, 49564,
        3690, 26713, 17845,  4978,  6652,  8271, 41186, 20446, 42675,
       14744, 19857, 39698, 42313,  6535, 11081, 24421, 48367, 21229,
       25860,  4280, 13428,  4642, 33740, 39891, 35175, 35077,  9019,
        3377, 45605, 16640, 18628, 28903, 13038, 41302, 32757, 18185,
       20826, 25776, 30291, 19882, 48008, 35185,  3326,  7043, 38103,
       17728, 21659, 40842, 45311,  6770, 38810, 43637, 43694, 21453,
       45221, 18434, 41777, 25401,  5742, 45396, 42769, 34352,  1324,
       12324, 44517, 34816,  8208, 10904, 25292, 39772, 18054, 34882,
       19980, 35797, 37193, 25099,  5358, 12331,  5813, 36247, 46174,
       48521]), [9, 2, 7, 4])]
Collaboration
DC 0, val_set_size=1000, COIs=[6, 1, 7, 4], M=tensor([6, 1, 7, 4], device='cuda:0'), Initial Performance: (0.25, 0.04443984019756317)
DC 1, val_set_size=1000, COIs=[5, 0, 7, 4], M=tensor([5, 0, 7, 4], device='cuda:0'), Initial Performance: (0.262, 0.04436455535888672)
DC 2, val_set_size=1000, COIs=[8, 3, 7, 4], M=tensor([8, 3, 7, 4], device='cuda:0'), Initial Performance: (0.275, 0.04432411301136017)
DC 3, val_set_size=1000, COIs=[9, 2, 7, 4], M=tensor([9, 2, 7, 4], device='cuda:0'), Initial Performance: (0.225, 0.044532188177108765)
D00: 1000 samples from classes {4, 7}
D01: 1000 samples from classes {4, 7}
D02: 1000 samples from classes {4, 7}
D03: 1000 samples from classes {4, 7}
D04: 1000 samples from classes {4, 7}
D05: 1000 samples from classes {4, 7}
D06: 1000 samples from classes {1, 6}
D07: 1000 samples from classes {1, 6}
D08: 1000 samples from classes {1, 6}
D09: 1000 samples from classes {1, 6}
D010: 1000 samples from classes {1, 6}
D011: 1000 samples from classes {1, 6}
D012: 1000 samples from classes {0, 5}
D013: 1000 samples from classes {0, 5}
D014: 1000 samples from classes {0, 5}
D015: 1000 samples from classes {0, 5}
D016: 1000 samples from classes {0, 5}
D017: 1000 samples from classes {0, 5}
D018: 1000 samples from classes {8, 3}
D019: 1000 samples from classes {8, 3}
D020: 1000 samples from classes {8, 3}
D021: 1000 samples from classes {8, 3}
D022: 1000 samples from classes {8, 3}
D023: 1000 samples from classes {8, 3}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.07179966375231743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.062282080620527265) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.0866992349922657) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.10071668189764023) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.09029780164361) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.0706805988252163) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.09509955850243569) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.269, 0.12162477773427963) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.10274842362105846) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.0916736244559288) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.12655268445611) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.14859290887415408) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.404, 0.1201973113194108) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.11129003369808196) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.17445171085000039) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.18057729303836823) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.13035795422643423) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.451, 0.13108988019824028) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2068759422376752) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2312651706188917) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.1425810459293425) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.158301618501544) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2383120637461543) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.24315960404276848) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.1451124506518245) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.457, 0.15763706554472445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.2743811624888331) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.2798636357784271) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.1760368304941803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.17171229308471084) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.3013828208167106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.29754252534732223) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.1761757607832551) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.17199157116003336) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.3272228060346097) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.30223537323623895) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.19416647404991091) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.448, 0.17022600414045155) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.3739811360947788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.2888700136058033) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[4, 7], M=tensor([0, 1, 3, 4, 5, 6, 7, 8], device='cuda:0'), Initial Performance: (0.5006666666666667, 0.021332343816757204)
DC Expert-0, val_set_size=500, COIs=[1, 6], M=tensor([6, 1, 7, 4], device='cuda:0'), Initial Performance: (0.93, 0.006120357226580381)
DC Expert-1, val_set_size=500, COIs=[0, 5], M=tensor([5, 0, 7, 4], device='cuda:0'), Initial Performance: (0.896, 0.007846746120601893)
DC Expert-2, val_set_size=500, COIs=[8, 3], M=tensor([8, 3, 7, 4], device='cuda:0'), Initial Performance: (0.94, 0.005073083020746708)
SUPER-DC 0, val_set_size=1000, COIs=[6, 1, 7, 4], M=tensor([6, 1, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 0, 7, 4], M=tensor([5, 0, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[8, 3, 7, 4], M=tensor([8, 3, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x79118c2d5d30>, <fl_market.actors.data_consumer.DataConsumer object at 0x791198723670>, <fl_market.actors.data_consumer.DataConsumer object at 0x7911980ad400>, <fl_market.actors.data_consumer.DataConsumer object at 0x79118c2286d0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7911987b2490>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO3', '(DO4']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.005811518348753452) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.924, 0.006396491993218661) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.0059738272950053215) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.27610219695232807) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.728, 0.018347647959987324) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.503, 0.04694299155473709) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.04997269883751869) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.496, 0.08968950988352299) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.005527616254985332) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.926, 0.006471040848642588) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.006334978517144919) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.28457159326039255) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7866666666666666, 0.021436051309108733) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.488, 0.06290922045707703) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.05904430764913559) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.567, 0.04402137380838394) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00550898162368685) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.005358880959451199) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.0063697544652968644) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.453, 0.2741119239144027) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.782, 0.02363389603793621) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.506, 0.05769545283913612) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.452, 0.08120045006275177) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.492, 0.05262350597977638) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.00495232648216188) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.912, 0.008902362649329006) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.005337453596293926) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.28216832579672335) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7626666666666667, 0.03254398081700007) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.05278193652629852) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.07124325877428055) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.509, 0.05346456742286682) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.004816708607599139) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.004765938848257065) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.005636379277333617) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.2984778027734719) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7833333333333333, 0.032828378210465116) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.513, 0.06588439708948135) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.07353230926394462) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.498, 0.06265664422512054) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO5', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004462808011099696) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.0070291524399071935) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.00587836054712534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.33883501549623907) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7986666666666666, 0.018104371011257173) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.538, 0.052112605452537535) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.419, 0.08391874194145203) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.0661552906036377) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.005061810908839106) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005068954810500145) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.006380217503756285) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.456, 0.3043590642688796) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7993333333333333, 0.032926383475462596) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.0593006649017334) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.09293907952308655) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.471, 0.07649035808444023) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004390086814761162) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005448568055406213) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.006797631479799748) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.3110767403873615) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.806, 0.03582143106063207) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.543, 0.05531623417139053) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.45, 0.0818610200881958) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.07366991704702378) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004271849905140698) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.006005437845364213) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.006233014263212681) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.3201156357405707) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.784, 0.06234205045302709) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.482, 0.08593821577727795) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.0999598733484745) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.09309585824608803) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.003948396228253841) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.93, 0.006639353409409523) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.006847537182271481) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.31829073870554564) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.788, 0.033772158414125446) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.05688665395975113) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.454, 0.09755923673510551) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.08396532505750656) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO1', '(DO3']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005045178622938693) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.005393893882632256) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.005167230326682329) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.449, 0.3327400239589624) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7973333333333333, 0.0400824672182401) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.05832956326007843) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.08873237431049347) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.08103553274273873) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.00485106423124671) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.005838800117373467) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.004981502301990986) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.466, 0.2873366604782641) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.802, 0.048644904216130574) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.549, 0.06222600221633911) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.13096602270007132) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.11465777513384819) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004059214942157268) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.00579211064428091) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.0059066516011953354) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.464, 0.3391117548206821) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7966666666666666, 0.03833454551299413) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.537, 0.0680527966916561) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.09461046588420868) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.444, 0.09209097847342491) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0039571567960083485) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.00626361906901002) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.007241191336885095) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.27767694076150656) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7786666666666666, 0.05748953598737717) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.594, 0.05507417035102844) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.465, 0.09993416807055473) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.08705048859119416) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004506430566776544) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.0061999431736767295) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.0074348624609410765) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2935663059479557) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7873333333333333, 0.03458310285210609) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.06564645582437516) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.07619768875837327) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.45, 0.08058909356594085) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004696362790185958) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.932, 0.007304712559096515) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.006542028047144413) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.2746070552766323) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.808, 0.04369531975189845) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.56, 0.062199686765670774) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.07978941172361374) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.09174588012695313) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004207953712437302) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.006584331361576915) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.0056589127443730835) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.464, 0.3225184682765976) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.818, 0.041899547149737676) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.06030236515402794) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.472, 0.08006140917539596) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.411, 0.10005216586589813) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005598944854922593) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005433486148715019) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.006107433527708053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.454, 0.3619621836983133) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.814, 0.028264519691467285) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.567, 0.05387782818078995) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.07805218240618705) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.409, 0.09243500930070878) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004917522026225925) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.005862019535154105) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.005885974779725075) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.2858199120787904) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7926666666666666, 0.04687842694918315) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.551, 0.06244763779640198) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.0791971065402031) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.1146704735159874) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.974, 0.0042064783065579835) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.006526502726599574) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.008314906381070615) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.34487594159180296) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.818, 0.04668729211886724) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.06031467974185944) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.09350445237755775) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.11432057309150696) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO2']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO5', '(DO0']
DC Alliance --> ['(DO1']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.003985893566161394) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006275713667273522) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.007494655605405569) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.27813099809689446) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8186666666666667, 0.02267809205253919) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.56, 0.04468694886565208) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.05899743247032165) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.07993598532676696) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005372681740205735) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.005294701930135488) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.007231018989346922) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.31904681543260816) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8126666666666666, 0.030177842438220977) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.55, 0.05999713695049286) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.09015474331378937) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.447, 0.09320430308580399) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0044446563385427) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005941799227148294) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.007531250432133675) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.452, 0.3099228038759902) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.812, 0.049859656264384586) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.556, 0.05582500720024109) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.07462994918227196) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.09290787690877915) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0034521134681999685) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.0055025290679186585) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.00752374214772135) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.32496830622572453) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.822, 0.04739946603775024) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.573, 0.0565176482796669) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.465, 0.08460067874193192) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.1033034593462944) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0038627608977258206) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.006052612802013755) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.006573884237557649) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.3166520271357149) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7906666666666666, 0.029662563274304072) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.053633747696876524) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.06477908253669738) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.07339677184820176) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO3', '(DO4']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004030911706387997) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005281611058861017) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.005692820932716131) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.2728402336575091) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.804, 0.020932113776604335) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.538, 0.05809436494112015) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.059399743020534516) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.07338085150718689) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004990459541790187) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.0052488130573183295) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.006043492503464222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.314062346784398) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7693333333333333, 0.02326970045765241) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.55, 0.05114438158273697) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.0655152884721756) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.438, 0.07506216561794281) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.00565484324330464) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006600368481129408) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.0061611078279092905) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.3112301975558512) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8053333333333333, 0.033628934681415555) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.563, 0.0514193639755249) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.496, 0.06939033478498459) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.08797271126508713) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004137358536943793) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006244862798601389) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.005682441991288215) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.35764545140974224) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7866666666666666, 0.03301685132582982) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.572, 0.0468332456946373) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.495, 0.06404503113031387) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.07513441383838654) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004257244896143675) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.00790134836686775) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.005835879614576697) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.473, 0.332069503800245) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7806666666666666, 0.029392385005950927) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.049331383138895034) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.512, 0.05994537970423698) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.06489760091900826) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO0', '(DO1']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005048582526855171) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005594725374132394) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.005882950378581882) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.32487950806342997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8226666666666667, 0.01908985213935375) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.559, 0.0477487034201622) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.05519896796345711) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.513, 0.055733068406581876) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004656239362433553) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005878780394792556) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.006333011277019978) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.3085768931619823) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8173333333333334, 0.02683638696372509) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.569, 0.051616499423980715) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.476, 0.0759298104941845) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.07962010616064072) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004342314438894391) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006332877349108457) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.007198319125920534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.3066927602868527) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.826, 0.02789103870590528) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.558, 0.06105947801470756) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.09536276668310166) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.08339370757341386) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005285009423270821) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.0051683178171515465) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.005793434357270599) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.34331636591628195) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8173333333333334, 0.03659617666403452) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.534, 0.06490603905916215) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.436, 0.10557710042595864) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.10437290093302727) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005031425772234798) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.007044853815808892) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.948, 0.006771688431501389) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.3115347077352926) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8153333333333334, 0.0387984368801117) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.0566721932888031) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.452, 0.09693822813034057) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.433, 0.0948784556388855) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO4']
DC 3 --> ['(DO1', '(DO0']
DC Alliance --> ['(DO3']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005220170123851858) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.005308466577902436) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.007357442067819647) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3152116536018439) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8253333333333334, 0.033152062912782033) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.516, 0.06529178693890572) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.46, 0.09784316048026084) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.07883507519960403) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004150384730193764) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.005109575603157282) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.006737736903131008) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.2620193031951785) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.84, 0.03398123144110044) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.539, 0.06422501239180566) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.488, 0.08439188757538796) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.09787410029768943) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.004849868770688773) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005211393740028143) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.005856666393578053) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.3406300708753988) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8186666666666667, 0.03743685474991799) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.523, 0.059595950424671175) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.445, 0.09789821526408196) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.09285613280534745) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.005123840780928731) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.005295842319726944) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.0063149483966408295) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.29959044242929667) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8293333333333334, 0.030547996898492177) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.535, 0.07400919342041015) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.481, 0.09013665488362313) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.466, 0.07197023928165436) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004398699915036559) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.004706122366711497) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.007771359608042985) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.3363359412723221) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8086666666666666, 0.04143967745701472) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.485, 0.0797867743819952) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.11216407138109206) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.439, 0.08895479422807694) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04443984019756317), (0.25, 0.07179966375231743), (0.294, 0.09029780164361), (0.405, 0.10274842362105846), (0.404, 0.1201973113194108), (0.451, 0.13035795422643423), (0.451, 0.1425810459293425), (0.461, 0.1451124506518245), (0.459, 0.1760368304941803), (0.468, 0.1761757607832551), (0.465, 0.19416647404991091), (0.503, 0.04694299155473709), (0.488, 0.06290922045707703), (0.506, 0.05769545283913612), (0.539, 0.05278193652629852), (0.513, 0.06588439708948135), (0.538, 0.052112605452537535), (0.555, 0.0593006649017334), (0.543, 0.05531623417139053), (0.482, 0.08593821577727795), (0.563, 0.05688665395975113), (0.555, 0.05832956326007843), (0.549, 0.06222600221633911), (0.537, 0.0680527966916561), (0.594, 0.05507417035102844), (0.547, 0.06564645582437516), (0.56, 0.062199686765670774), (0.562, 0.06030236515402794), (0.567, 0.05387782818078995), (0.551, 0.06244763779640198), (0.586, 0.06031467974185944), (0.56, 0.04468694886565208), (0.55, 0.05999713695049286), (0.556, 0.05582500720024109), (0.573, 0.0565176482796669), (0.583, 0.053633747696876524), (0.538, 0.05809436494112015), (0.55, 0.05114438158273697), (0.563, 0.0514193639755249), (0.572, 0.0468332456946373), (0.577, 0.049331383138895034), (0.559, 0.0477487034201622), (0.569, 0.051616499423980715), (0.558, 0.06105947801470756), (0.534, 0.06490603905916215), (0.555, 0.0566721932888031), (0.516, 0.06529178693890572), (0.539, 0.06422501239180566), (0.523, 0.059595950424671175), (0.535, 0.07400919342041015), (0.485, 0.0797867743819952)]
TEST: 
[(0.26325, 0.043459017485380176), (0.25, 0.06910176756978036), (0.28625, 0.08651740956306457), (0.405, 0.09793887826800346), (0.40425, 0.11450791484117508), (0.45, 0.12330296683311462), (0.4495, 0.13521357721090316), (0.46975, 0.1373603984117508), (0.46, 0.1669597494006157), (0.47575, 0.16677658635377884), (0.46975, 0.18435369455814363), (0.513, 0.043281702905893324), (0.51275, 0.058044826924800876), (0.55225, 0.05204351057112217), (0.57375, 0.049758353680372236), (0.53825, 0.05953567422926426), (0.562, 0.04805945432186127), (0.5585, 0.05588784871995449), (0.541, 0.052157868102192875), (0.504, 0.07915754237771035), (0.5765, 0.054138013869524), (0.5605, 0.0525928700864315), (0.56225, 0.05561033739149571), (0.552, 0.060508376717567446), (0.598, 0.05136430615186691), (0.5685, 0.0572715862095356), (0.57725, 0.05792201782763004), (0.594, 0.055790613785386085), (0.582, 0.047800223007798195), (0.573, 0.05676365341246128), (0.58875, 0.05640993608534336), (0.587, 0.0402547974139452), (0.5715, 0.054371656730771066), (0.58375, 0.04885707917809486), (0.60625, 0.05098235523700714), (0.596, 0.04902171513438225), (0.56, 0.05298110254108906), (0.57275, 0.04420375695824623), (0.5825, 0.0452967831492424), (0.61125, 0.039162710845470426), (0.6075, 0.04149892158806324), (0.5745, 0.04281763856112957), (0.5945, 0.04716578640043736), (0.59325, 0.056509653866291046), (0.5415, 0.061584515184164045), (0.5685, 0.051121012285351757), (0.54825, 0.058864484563469884), (0.5705, 0.05921995629370212), (0.54225, 0.056317177906632425), (0.55025, 0.07072540037333966), (0.5065, 0.07540129414200783)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.99      0.39      0.56      1000
           4       0.38      0.79      0.51      1000
           6       0.37      0.13      0.19      1000
           7       0.61      0.71      0.66      1000

    accuracy                           0.51      4000
   macro avg       0.59      0.51      0.48      4000
weighted avg       0.59      0.51      0.48      4000

Collaboration_DC_1
VAL: 
[(0.262, 0.04436455535888672), (0.251, 0.062282080620527265), (0.373, 0.0706805988252163), (0.463, 0.0916736244559288), (0.459, 0.11129003369808196), (0.451, 0.13108988019824028), (0.443, 0.158301618501544), (0.457, 0.15763706554472445), (0.461, 0.17171229308471084), (0.446, 0.17199157116003336), (0.448, 0.17022600414045155), (0.488, 0.04997269883751869), (0.478, 0.05904430764913559), (0.452, 0.08120045006275177), (0.468, 0.07124325877428055), (0.473, 0.07353230926394462), (0.419, 0.08391874194145203), (0.443, 0.09293907952308655), (0.45, 0.0818610200881958), (0.434, 0.0999598733484745), (0.454, 0.09755923673510551), (0.466, 0.08873237431049347), (0.431, 0.13096602270007132), (0.453, 0.09461046588420868), (0.465, 0.09993416807055473), (0.463, 0.07619768875837327), (0.462, 0.07978941172361374), (0.472, 0.08006140917539596), (0.483, 0.07805218240618705), (0.468, 0.0791971065402031), (0.459, 0.09350445237755775), (0.498, 0.05899743247032165), (0.456, 0.09015474331378937), (0.498, 0.07462994918227196), (0.465, 0.08460067874193192), (0.487, 0.06477908253669738), (0.515, 0.059399743020534516), (0.474, 0.0655152884721756), (0.496, 0.06939033478498459), (0.495, 0.06404503113031387), (0.512, 0.05994537970423698), (0.516, 0.05519896796345711), (0.476, 0.0759298104941845), (0.461, 0.09536276668310166), (0.436, 0.10557710042595864), (0.452, 0.09693822813034057), (0.46, 0.09784316048026084), (0.488, 0.08439188757538796), (0.445, 0.09789821526408196), (0.481, 0.09013665488362313), (0.453, 0.11216407138109206)]
TEST: 
[(0.25325, 0.0434291261434555), (0.25125, 0.06006230038404465), (0.37275, 0.067833630412817), (0.46325, 0.08763139560818672), (0.46075, 0.1062627123594284), (0.45225, 0.12552101576328278), (0.4415, 0.1515355521440506), (0.457, 0.1510611606836319), (0.45775, 0.1647727227807045), (0.44675, 0.16469195330142974), (0.45125, 0.16312419641017914), (0.5295, 0.045014713644981386), (0.5045, 0.05522022999823093), (0.4465, 0.08003661316633225), (0.49275, 0.06739581479132176), (0.4815, 0.07010808354616165), (0.435, 0.08102250054478645), (0.47025, 0.08958326160907745), (0.46875, 0.0781244653761387), (0.4485, 0.09661877876520157), (0.4615, 0.09224389323592186), (0.46775, 0.08699274960160255), (0.43075, 0.1270232561826706), (0.45925, 0.09353061044216156), (0.47075, 0.09329267752170563), (0.4925, 0.071820536673069), (0.47575, 0.07575206425786019), (0.49, 0.07643972390890122), (0.48475, 0.07740189114212989), (0.488, 0.07747595259547234), (0.48, 0.08997483095526695), (0.516, 0.05792636449635029), (0.472, 0.09229278430342674), (0.49675, 0.07104431337118149), (0.47075, 0.08393360370397568), (0.51175, 0.06337400805950165), (0.537, 0.05781329146027565), (0.4985, 0.06354840998351574), (0.521, 0.06501399981975556), (0.50525, 0.05926313379406929), (0.50875, 0.05993857379257679), (0.51675, 0.054658988937735555), (0.487, 0.07401886659860611), (0.44775, 0.0946775823533535), (0.4535, 0.10419439083337784), (0.45975, 0.09245400869846344), (0.45725, 0.09802797746658325), (0.489, 0.08414704930782319), (0.45625, 0.09708277207612992), (0.47925, 0.08638932698965072), (0.44425, 0.11242070916295052)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.87      0.03      0.05      1000
           4       0.36      0.82      0.50      1000
           5       0.60      0.17      0.26      1000
           7       0.53      0.76      0.63      1000

    accuracy                           0.44      4000
   macro avg       0.59      0.44      0.36      4000
weighted avg       0.59      0.44      0.36      4000

Collaboration_DC_2
VAL: 
[(0.275, 0.04432411301136017), (0.387, 0.0866992349922657), (0.451, 0.09509955850243569), (0.448, 0.12655268445611), (0.451, 0.17445171085000039), (0.464, 0.2068759422376752), (0.464, 0.2383120637461543), (0.47, 0.2743811624888331), (0.468, 0.3013828208167106), (0.467, 0.3272228060346097), (0.47, 0.3739811360947788), (0.496, 0.08968950988352299), (0.567, 0.04402137380838394), (0.492, 0.05262350597977638), (0.509, 0.05346456742286682), (0.498, 0.06265664422512054), (0.468, 0.0661552906036377), (0.471, 0.07649035808444023), (0.433, 0.07366991704702378), (0.426, 0.09309585824608803), (0.43, 0.08396532505750656), (0.444, 0.08103553274273873), (0.422, 0.11465777513384819), (0.444, 0.09209097847342491), (0.432, 0.08705048859119416), (0.45, 0.08058909356594085), (0.431, 0.09174588012695313), (0.411, 0.10005216586589813), (0.409, 0.09243500930070878), (0.426, 0.1146704735159874), (0.395, 0.11432057309150696), (0.432, 0.07993598532676696), (0.447, 0.09320430308580399), (0.432, 0.09290787690877915), (0.437, 0.1033034593462944), (0.448, 0.07339677184820176), (0.474, 0.07338085150718689), (0.438, 0.07506216561794281), (0.426, 0.08797271126508713), (0.435, 0.07513441383838654), (0.47, 0.06489760091900826), (0.513, 0.055733068406581876), (0.435, 0.07962010616064072), (0.437, 0.08339370757341386), (0.421, 0.10437290093302727), (0.433, 0.0948784556388855), (0.456, 0.07883507519960403), (0.43, 0.09787410029768943), (0.436, 0.09285613280534745), (0.466, 0.07197023928165436), (0.439, 0.08895479422807694)]
TEST: 
[(0.27825, 0.043275490760803226), (0.38925, 0.08313762971758842), (0.45525, 0.09075938367843628), (0.461, 0.11947788047790528), (0.466, 0.16523100256919862), (0.4715, 0.1961016225218773), (0.4705, 0.22676054126024245), (0.47375, 0.25869140833616255), (0.4735, 0.28333983933925627), (0.47325, 0.3097177770137787), (0.47275, 0.3541496697664261), (0.51525, 0.08135656863451005), (0.606, 0.04045413614809513), (0.52825, 0.05058672681450844), (0.53725, 0.049555675223469736), (0.537, 0.05869223003089428), (0.48475, 0.06296157521009445), (0.4645, 0.078357567101717), (0.44325, 0.07107662048935891), (0.4515, 0.0863195829987526), (0.4405, 0.0830810396373272), (0.461, 0.07956541875004769), (0.41725, 0.1157712539434433), (0.4375, 0.09379667684435844), (0.45, 0.08316526776552201), (0.468, 0.07449913448095322), (0.42175, 0.09103516387939453), (0.423, 0.09689916157722474), (0.44025, 0.0898203222155571), (0.42025, 0.11432458546757698), (0.40275, 0.11187575194239617), (0.46625, 0.075183612793684), (0.45475, 0.09154467847943305), (0.45, 0.08956315463781357), (0.43475, 0.10004529228806495), (0.4785, 0.06869279304146766), (0.488, 0.07165661399066449), (0.4515, 0.07515922096371651), (0.4425, 0.08330368527770042), (0.4635, 0.07063973957300186), (0.495, 0.060300984501838684), (0.53075, 0.05107281033694744), (0.47575, 0.07748778682947159), (0.45625, 0.08121601182222367), (0.426, 0.10129606270790101), (0.4405, 0.0922820551097393), (0.469, 0.07773677855730057), (0.43875, 0.09207523289322853), (0.46175, 0.08543148270249366), (0.501, 0.06716726687550545), (0.461, 0.0837644607424736)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.47      0.28      0.35      1000
           4       0.36      0.76      0.48      1000
           7       0.60      0.69      0.64      1000
           8       0.98      0.12      0.21      1000

    accuracy                           0.46      4000
   macro avg       0.60      0.46      0.42      4000
weighted avg       0.60      0.46      0.42      4000

Collaboration_DC_3
VAL: 
[(0.225, 0.044532188177108765), (0.25, 0.10071668189764023), (0.269, 0.12162477773427963), (0.416, 0.14859290887415408), (0.441, 0.18057729303836823), (0.446, 0.2312651706188917), (0.463, 0.24315960404276848), (0.455, 0.2798636357784271), (0.467, 0.29754252534732223), (0.469, 0.30223537323623895), (0.468, 0.2888700136058033), (0.456, 0.27610219695232807), (0.472, 0.28457159326039255), (0.453, 0.2741119239144027), (0.46, 0.28216832579672335), (0.445, 0.2984778027734719), (0.44, 0.33883501549623907), (0.456, 0.3043590642688796), (0.457, 0.3110767403873615), (0.458, 0.3201156357405707), (0.447, 0.31829073870554564), (0.449, 0.3327400239589624), (0.466, 0.2873366604782641), (0.464, 0.3391117548206821), (0.465, 0.27767694076150656), (0.463, 0.2935663059479557), (0.467, 0.2746070552766323), (0.464, 0.3225184682765976), (0.454, 0.3619621836983133), (0.47, 0.2858199120787904), (0.469, 0.34487594159180296), (0.459, 0.27813099809689446), (0.467, 0.31904681543260816), (0.452, 0.3099228038759902), (0.468, 0.32496830622572453), (0.47, 0.3166520271357149), (0.472, 0.2728402336575091), (0.465, 0.314062346784398), (0.474, 0.3112301975558512), (0.468, 0.35764545140974224), (0.473, 0.332069503800245), (0.471, 0.32487950806342997), (0.47, 0.3085768931619823), (0.469, 0.3066927602868527), (0.471, 0.34331636591628195), (0.467, 0.3115347077352926), (0.471, 0.3152116536018439), (0.471, 0.2620193031951785), (0.471, 0.3406300708753988), (0.465, 0.29959044242929667), (0.462, 0.3363359412723221)]
TEST: 
[(0.23075, 0.04349978512525558), (0.25, 0.09680754977464676), (0.27325, 0.11629943352937698), (0.42225, 0.14138725328445434), (0.44, 0.17147793173789977), (0.4455, 0.21885153269767763), (0.4645, 0.2289725457429886), (0.4565, 0.2658118067979813), (0.46975, 0.28245870041847226), (0.472, 0.28643305253982543), (0.475, 0.2728337438106537), (0.463, 0.2620408856868744), (0.4685, 0.26923286700248716), (0.45725, 0.26045228374004364), (0.46325, 0.2690428065061569), (0.44925, 0.28419970750808715), (0.449, 0.3229838447570801), (0.4605, 0.28974362671375276), (0.45775, 0.29583727633953094), (0.46775, 0.30613681077957156), (0.4495, 0.30308424615859986), (0.45525, 0.3167091481685638), (0.46775, 0.2727919021844864), (0.466, 0.324041108250618), (0.46825, 0.2668137836456299), (0.46875, 0.28102641069889067), (0.46675, 0.26257235336303714), (0.4665, 0.30925244724750517), (0.45975, 0.3453839455842972), (0.46825, 0.27440577661991117), (0.47025, 0.3306275420188904), (0.464, 0.26514643120765685), (0.467, 0.30702472615242005), (0.452, 0.2972376353740692), (0.46725, 0.31140156435966493), (0.46925, 0.30397657823562624), (0.4715, 0.26143612498044966), (0.4695, 0.29980772733688354), (0.4695, 0.2960189507007599), (0.469, 0.34308142626285554), (0.4735, 0.3185326164960861), (0.47325, 0.3105178529024124), (0.4745, 0.2963372246026993), (0.47225, 0.29468345987796785), (0.4755, 0.3316510628461838), (0.4725, 0.30024090385437013), (0.4715, 0.30421663105487823), (0.47675, 0.24978666961193086), (0.475, 0.32653768110275266), (0.4735, 0.2893152629137039), (0.473, 0.32296175813674927)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.33      0.99      0.50      1000
           4       0.00      0.00      0.00      1000
           7       0.00      0.00      0.00      1000
           9       0.87      0.90      0.88      1000

    accuracy                           0.47      4000
   macro avg       0.30      0.47      0.35      4000
weighted avg       0.30      0.47      0.35      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [23]
name: alliance-3-dcs-23
score_metric: contrloss
aggregation: <function fed_avg at 0x72f2a128ac10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=23
Partitioning data
[[0, 6, 4, 1], [5, 7, 4, 1], [9, 8, 4, 1], [3, 2, 4, 1]]
[(array([32659,  7396,   871,  8190, 33489, 44742,  8515,  5082,   965,
       11208,   700, 39329, 40740, 22304, 44430,   974, 28487, 44470,
       46964, 24750, 26632, 28921, 43184,  6663, 28273, 30029, 46367,
       39397, 38683, 15767, 48108, 33229, 13937, 41040,   276,  9831,
       45344,   927, 35963,  8842, 30591, 38617, 45559, 43659, 27768,
        2617, 28750,  1234,  5549, 34259, 20709, 33584, 29856, 34421,
       39553, 49734, 33380, 49130, 11897,  7445,  5003, 11958, 18763,
       26018,   600, 46936, 26667, 12477, 25023, 32870, 43077, 35570,
        9077, 16939, 46873, 32990, 37041, 39271, 47483,    93,  9121,
       33662, 34724, 33478, 49672, 12807, 45297,  6328, 20985, 35633,
       46625, 12674, 31391, 16554,  3347,    77, 48059, 30820,  9945,
       18102, 22924, 12297, 48777, 17222,  5557, 21815, 36415, 42231,
       12944, 47726, 47068, 30888,  7256, 33268, 22951, 42165, 39628,
       29380,  3564, 21143, 37035,  2427, 28000,  2010,   757,  7203,
       22118, 21803, 41745,  5349, 28272, 47329,  7925, 47339,  1715,
       21458, 16196, 38711, 12492, 17316,  5414, 48001, 21964,  1831,
       28305, 11218, 24113,  5642,  7263, 47994, 25199, 43548, 40956,
       42456,  4567,  4726,  9614, 28716, 32233,  2622, 13656, 27202,
       11372, 38791, 22458, 26656, 43427, 17620,  3731, 45232,  5093,
       45594, 20452, 16553, 11122, 14666, 44552, 24983, 19094,  6952,
       13850, 14434, 32107, 43475, 26764, 38307,  1376, 22194,  3721,
        1887, 43253, 34989, 10141, 10334,  4239, 17200, 33573,   348,
       14732, 31450, 46419, 39196, 41880, 14325, 11002, 38748, 45652,
       29881, 16647, 13031, 11615,  3987, 32056, 23776, 21469, 40556,
       20574, 49349, 13348,  1757, 29593, 44770, 13229, 10265,  2459,
       31083, 41242, 46749, 19004, 42929, 11682, 16235, 33258,  7772,
       48317, 37646, 17237,  5640,   436, 37078, 42829, 16632, 36386,
       38217, 44806, 26918, 39774, 45166, 43045, 28014, 37774, 45607,
       29833,  8942, 35340, 16404, 26339,  2252, 39512,  6096,   745,
       36348, 13137, 46342,  7848,  6581,  5785, 23691,  5176, 40551,
       24184,  6941,  5708, 42510, 18816, 20104, 13825, 39091, 17072,
       41444, 14293, 31042, 24955, 39889, 17957,  3307, 34684, 27177,
       15726, 14678, 49462, 33216, 46404, 23050, 45862,  8950, 19228,
       40879, 35455, 35585,  6093, 47497, 26275, 30222, 23451, 17105,
       31894, 42415, 40675, 18924, 42671, 25551, 47821, 10327, 13188,
       34804, 20310,  4094, 15552, 41672, 11070, 23539, 44258, 20141,
       18334, 31820, 12930, 21831, 15220,  5497, 13838, 23683,  5263,
        3655, 36913, 36192, 39148,  6521, 20734,  5419,  9125, 42046,
       16611,  7824,  9862,  9237,  5055, 42712, 31808, 40568, 34986,
        4785, 21155, 23760, 42850,   914, 34056, 19230, 35435, 21192,
       22909, 28106, 35600, 49747, 14024, 40929, 42778, 23390, 42460,
       11303,  8466, 35891,  6179, 33278,  7825, 15664, 12411, 48526,
       44422, 25680, 29381,  7632, 15517, 31806,  7335, 10431, 29241,
        3403, 22157, 30800,  3823,  4959, 44314, 10176, 31780, 38747,
       44612, 45637, 38864, 22986,  3435, 38893, 47513,  3841,   819,
        7715,  7705, 35444, 45361, 48176,  1154, 36704, 48460, 39756,
        6750, 30212,  3976, 47352, 23412, 19949, 39464, 35977, 47962,
        1529, 14274, 46322, 47276, 35900, 27240, 20771,  6968,  4926,
       34394, 29391, 28860, 24394, 49088, 21572, 41311, 36110,  2179,
       32060, 39326, 23517,  7025, 15536,  9021, 35480, 14940, 33690,
       42679, 33298, 32340, 38853, 26085,  2898, 11204, 47858, 26355,
       14163, 25793, 25430, 12641, 26415, 28501, 26478, 19239, 36396,
       45140, 30588,   863, 21941, 48027, 22295,  1035, 21958, 44573,
       35131, 38771, 34447, 39010,  7137, 44579, 35577,  3528, 48664,
        2810, 39537, 10665, 45895, 42836, 34829, 22332, 24673, 34139,
       12407, 24577,  3554, 42311,  1053, 31775, 32608, 35185, 30911,
        8254, 44215, 31281, 36711, 43486, 42665, 47605, 45665,  1269,
       36457, 34752, 45749, 38906,  1310,  9937, 34450, 43714,  3112,
        3225, 43032, 48447, 40088, 15777,  8375, 45767, 30600,  5677,
       43172, 47570, 42700, 23603,  6022, 39626, 22438,  4666, 47348,
       37656, 40827,  7822,  7362, 43758, 13778, 30435, 40529, 39501,
       25879, 11565, 42979,  1315, 41148, 16505,  9013, 36182, 29891,
       21382, 22139,  5173, 48334,  6844,  3442, 46333, 30950, 19585,
       22902,   158, 35839, 33872, 21161, 21659, 18363, 31902, 26710,
       15963, 36631, 18507, 38640,  6603, 48367, 39932, 22259, 49605,
       48881,  6569, 34425, 32302, 47015,  8804, 28649,  1826,   520,
       29771, 11586, 43914, 15667, 44465, 43683, 31932,  5742, 30501,
       35413,  5726, 48889, 38708, 48159, 39803, 29083, 27787, 20511,
       33998,  3360, 32388, 12416, 23525, 21473, 33038, 21281, 25153,
       21702, 27644, 32026,  4497,  5786, 22183,  2415, 27946, 47571,
       16893, 25781,    82,   429, 46660,  1804,  3077, 38593, 39169,
       34819,  3999, 41897, 24529,  4741,  9873, 20439, 18699, 24874,
        7163,  1923, 17608, 13959, 14258, 10925, 37172, 38794, 18293,
        2573, 20828, 41058, 44717,  2629, 44760,  3577,  2832, 16383,
       14857, 10076, 35318, 25024, 13464, 25444, 45812, 32784, 45644,
       17420, 15009, 49055, 31431, 45609,  2486, 34881, 35324,  6770,
       37004, 17513, 42915, 39833, 44572, 46521, 47318,  6990, 43975,
       22949,  3322, 17161, 10477,  4857, 10766, 31239,  8792, 42675,
        1569, 47530,    89, 43055, 31514, 23958, 10890, 14075,  2078,
       34816, 48521, 15415, 35915, 26926, 37089, 44850, 41249, 32786,
       39036, 23591,  8405, 22246, 33066, 12136, 28823, 32797, 28074,
       43228, 13310,  5607, 27428, 44516, 35187, 46036, 26844, 24239,
       31751, 48997,  6879, 18875,   343, 17266, 39356, 14490, 13903,
        4561, 19358, 46234, 46649, 24428, 18698, 44549, 26293, 24979,
       45470, 41404, 29318, 34327, 23317, 39769, 49314, 18562, 22270,
       22335,  2800,   547, 45258, 14035, 22067, 44824, 43359,  4326,
        1242, 10236, 29473, 14005, 43600, 11558, 29876, 24625, 37652,
       48113, 40520, 31288, 27089,  5301, 27994, 44266, 45455, 14904,
        6889, 36251, 36107, 32502, 36067, 12743, 37849, 14117, 36929,
       11955, 35291, 15756, 15299,  7992,  8374, 37260,  4909,   212,
       49234, 22816, 35613, 22422,  8244,  6908, 16848, 17986,   606,
       25710, 21615,  4171, 16882,  5282, 42007, 36400, 32462, 44488,
        7312, 23685, 31072, 20595,  5616,  4648, 30999,  3678, 31951,
        6122, 11037, 43932,    45, 15506, 49787, 49075, 47836,  1660,
       42802, 12914, 34097, 10248, 36611, 20079, 25463, 24470,  2306,
       40758, 48610, 11134, 19522,  3085, 33832, 16673, 19866, 31100,
       34802, 41607, 47824, 48294,  1565, 10122, 37098,  5885,    64,
        9610, 11744, 17539, 17950,  5287, 32235,  9524, 15797, 13240,
       49768,  9717,  5144, 11403,  7408, 14336, 46078,  4244, 29616,
       11308, 19564, 30218, 30712,  8339, 20536, 16755,   119, 40751,
       26937, 31147,  8093, 17014,  3034, 30445, 21850, 40311, 45201,
       44311, 42658,  9781,  4500, 38735, 19382, 26065, 26652, 36614,
       40431, 19341, 11163, 30103, 43290, 12080,  7587, 37428, 24541,
       19481, 21181, 48215, 47362,  1421, 32295, 10450, 28798, 27576,
       22419, 41922, 19125, 41768, 11109, 31720, 15447, 35357, 17772,
       19581,  4533, 13884, 12275, 40352, 41908,   561, 10678, 30154,
       30553,  4040, 29205, 17460, 17128, 37743, 19454, 29143, 18276,
       42663, 15814, 24873, 34118, 42976, 17017, 26294, 11954, 23020,
       31297, 18994, 38031, 12348, 14512, 34721, 35134, 11972, 48749,
       47001,  7019, 36040, 25388, 38192, 25135, 40250, 22499, 32988,
       29044, 16077, 41807, 15114, 43893,  9088, 12946, 12169, 29911,
       27371]), [0, 6, 4, 1]), (array([33576,  1922,   839, 20020, 33494,  4878,  8287,  3388,  4639,
       26548, 44538, 47095,  9288, 24553, 17293, 25774, 18928, 25740,
       29970, 33454, 31240, 18574,   339, 49070,  7518, 32122,  8282,
       17670, 25356,  9284,  8261, 46436, 47209, 14026, 47757,  8020,
       48181, 32199,  9391, 47884, 47211, 45043,  7846, 48843, 49552,
       22099, 42348,  5755, 15186, 37632, 23641, 47463, 41115,  4319,
       30140, 31439, 42645,  1849,  9537, 25722, 26836, 11560,  4579,
        4990, 15980,  1198, 22059, 26969, 43455, 39929, 47386, 44532,
       19012,  1280, 35534,  6338, 38600, 15301, 27691, 28659, 45731,
       21186, 29256, 49380, 49327, 15371, 34755, 18083, 14437,  3810,
        4920, 25583, 45108, 35034, 16852, 48189, 23234, 31782, 30196,
       11201, 47388, 24344, 10803, 14932, 28730, 20669,  5439, 29608,
       48555,   173, 36227, 43399, 35762, 26747, 23874, 19756, 40620,
       13872, 12912, 22890, 45215, 38366, 43979,  6958, 42088, 34763,
       22779, 25057,  8252,  4312, 29961, 48011, 23515, 46916,    27,
       41062, 29713, 41789, 13121, 17740, 37749, 25515, 28660, 11770,
       45448,  9665,  6697, 49188, 36954, 27377, 13073, 44706, 31355,
       18131, 22932,   784, 42776, 48207, 19182,  1618, 18569, 26418,
       46708, 16470,  2686, 29857, 41358, 10768, 19334, 24744,  7981,
       32598,  8240, 30931, 35892, 26587, 23689, 22941, 22473, 48161,
       46264, 19757,  8198, 25903, 27094, 45636,  1860,  1993, 43654,
       37921,  5729, 30370, 14875,  5561,  8894, 25870, 40934, 37451,
       47661,  3229, 16510, 22590, 21233,  7140, 49867, 22021, 26143,
       11347, 14033, 31781,  4862, 39757,  5305, 32957,  2916, 32889,
        6140, 39662, 41406, 41759, 30042, 37434, 35640,  7082, 25517,
        5690, 28306, 43599, 30235, 33578, 49661, 29302, 32364, 27633,
       33534,  5411,   670, 12051, 28321, 30523, 33142,  4641, 19652,
       17343,  1928, 10597, 41442, 39742, 26501, 46030, 47046, 46816,
       32347,  1898, 15073, 19466, 21995, 22115, 10584, 27374, 33469,
       49231,  2319, 34592, 25090, 20094, 20618, 12994, 30434, 39408,
       37683,  5258, 23773, 42598, 41795,   968, 42338, 26249, 38142,
       17627, 32374, 23486,  1783,  5710, 30883, 37695, 12985, 45147,
        8238, 49630, 23000, 12163, 38678, 47777, 48773, 29359, 39064,
       36495, 42284, 26733,  4088,  8343, 28262, 26217, 13442,   329,
       14319,  8907, 49109, 10277, 13039, 45127, 21394, 21576,  8498,
        7969, 46709,  7922, 45523, 47056, 14219, 21820, 26166, 35980,
       16582, 22980, 16187, 18802, 25241, 25132, 40010, 21409, 17129,
       29036, 32995,    11, 37100,  3213, 23828, 21857, 15647,  3364,
        5989, 19931, 33775, 36205, 33712, 49310, 32660, 17113, 31350,
       25370, 33921, 44986, 44374,  5812, 29937, 49116, 33585, 31341,
       12690, 45561, 27529, 28152, 23794,  3138, 34900, 44378,  4782,
       19372,  8744, 20576, 24948, 35407,  1927, 28257,  8360, 22625,
       30030,  4513, 10772, 46368,  6164, 13237, 31876, 40570, 18590,
        2119, 39136, 16091, 11414,  3473, 33527, 13547, 43229, 27916,
       27039, 31276, 22457, 24729, 44091, 17913,  3146,  1544,  4554,
        2211, 49648, 30797,   114, 23903,  1900, 28639, 36174, 15850,
       29557, 27098, 27910, 45747, 49103, 41473, 28392, 18938, 34990,
       22901, 27574, 25594, 42651, 24311,   825, 38196, 17616,  3627,
        6235, 33299,  5116, 31924,  9347, 49925, 37980, 47321, 44228,
       43724,  1113, 26422,  2620, 28977, 48579, 34260,  5792, 18339,
       29872, 46228, 29369, 41180, 38194, 10927, 40998, 18431,   388,
        6734, 23627, 14947, 38667,  6398, 21096, 20170,  9906,  9472,
       22031, 16162, 13284, 10687, 38286, 38225, 22669, 11260, 20175,
       40478, 39246, 10259, 29859, 15072, 47275, 48960, 12432,  3054,
       20423, 12364, 23585, 38094, 29486,  9719, 11990, 16025, 42887,
       42238,  3151, 43322, 13950, 29499, 30917,   669, 30794, 29582,
       13815,  8630,  2293, 40156,  6249, 24884, 46319, 30228,  3494,
        4103, 34852, 44963, 42662, 16426, 33667,  7563, 31075, 14090,
       14474, 31227, 24080, 46132, 10593, 41294, 13636,  7536, 37577,
       22673, 40523, 13164, 12108,  3952, 45221, 12406, 16194,  1557,
       26745, 13764, 10415, 16135, 11649, 32186, 39047, 40377, 43926,
       36945,  6212, 32969,  9963, 14744, 13282,  3936, 44705, 15503,
       10404, 47244, 46276, 37775, 49154, 21016, 33458, 18669, 31003,
        4973, 41216, 11862,  5067,  9962, 46415,  4286, 32224, 15715,
       31701, 38995, 40597, 37345, 16689, 14789, 25785, 40798,  2673,
       33666,  9695,  7595, 18014,  5919, 14957, 33370, 41101,  9254,
       42849, 41665,  5051, 27479, 48801, 48488, 29237, 38263, 13231,
       41093, 15131, 37469,  9780, 32229, 34144, 47735, 42312,  7100,
       27461, 40240, 19178, 46476,  8639, 13363, 39473, 18314, 33482,
       49204, 38824,  7365, 46789, 21222, 21081, 47868, 49902, 48018,
        6234, 14113, 31755,  4058, 22946,  2131, 15663, 39505, 20303,
       25692, 17437, 20060, 28407, 49597, 17845, 21708, 25834, 18473,
        7383, 47622,  7984, 44761, 44002, 23231, 21694, 30799,  9776,
       41191, 13102, 31429,  3695, 29635, 16659, 17123, 20373,  9153,
       28387, 23179, 41202,  9544, 28076, 37639, 25941, 48191, 27801,
       16020,  9342, 34119, 44747, 14469, 18690, 48309, 22580, 29254,
       46968, 30156,  6505, 46940, 36372, 18434, 16254, 43567, 15036,
       41100, 23637, 45645, 29404,  1952, 16073, 20054, 15668, 36671,
       45205, 34671,  3507,  4156, 30593,  1238,   674, 32590,   435,
       33971, 42313, 38092, 23022, 49951, 18912, 23421, 35598, 32280,
       26047,  2787,  5178, 31033, 41931, 49355, 28785, 32284, 15196,
       19721, 25383, 13663, 44891, 40116, 39853, 21642, 12333, 28614,
        3560,  9915, 30915, 33862, 23200, 41246,  5940, 20904,   831,
       40757, 17634, 49421, 35605, 12412, 25117,  6851, 15275, 43050,
        9582, 27238,  5076,   375,  3512, 34982, 27272, 49509,  8450,
        4261,  7738,   250, 23504, 40509, 11776, 31079,  5224, 22013,
       24486, 33640, 18885, 25479, 37611, 16711,  1724,  4206, 31395,
       43583, 26452, 26878, 44309, 44499, 31345,   493, 49326, 17409,
       36686, 34830, 20057, 49483, 38596, 27402, 29146, 48279,  9938,
       19226, 44575, 34533, 40518,  5084,   565, 12800, 40024, 18594,
        9407,  7147, 17626,  5947, 20731,  5971, 25459,  9232, 28640,
       16813, 31634, 44876, 37248, 49491, 24075,  8432, 30943, 38610,
       14501, 34902, 27716, 23489, 13217, 15456, 46135, 23107, 14524,
        8371,  9946, 29894, 41869, 34778,  5515,  5841, 12920, 11799,
       43251, 45672,  3212,  5629, 40386, 25278, 45602, 21245, 26450,
        9584, 47980, 18057, 44647, 14851, 19202, 49221, 41602, 21661,
        5666, 21199, 36086, 34813,  8124, 24043, 27404, 44589,  7401,
       26292, 11532, 32679, 42669,   427, 27330, 34400, 21627,  7659,
        7605, 12516, 14182,  3020,  6074, 31348,  5559, 18345, 39820,
        5791,  1973, 34064, 28393, 33194, 43398, 47540, 13843, 16272,
       21020, 25162,  3523, 40713, 15610, 46408, 39132,  6928, 34385,
       48182, 45834, 24580, 34940, 13877, 13849,  8995, 26267, 28626,
       21154,  6589,   599, 10298,   761, 47423, 17726, 16937, 34701,
       25852, 31351, 36058, 18134,  8944, 39349,  2259, 35033,   874,
       10479, 37844, 16576, 38650,  9646, 46240,  5567, 39692, 45988,
       24923, 33261, 27607, 14409, 17995, 13615, 30792, 48799, 26115,
       49409, 32168,  8625, 33561, 38911, 35783,  5834, 19265, 30716,
       27792, 26405, 33198,  6040, 20353, 17334, 22358, 36941, 42335,
       41259,  6736, 13701, 27970, 43984, 29875,  9889, 40422,  4021,
        9442, 49588, 43608, 20616, 21933, 29999, 41948, 44217, 12964,
       17309, 46219, 43609, 45866, 29974, 42785,  8305, 17205, 21808,
       40076]), [5, 7, 4, 1]), (array([ 2818, 19498, 43780, 29418, 43121, 36659, 32407, 28871, 21289,
       37575, 23657, 40875,  4041, 10219,  5140, 42253, 41920, 27900,
       23111, 40015, 28713,  4675, 47967, 42420, 39043, 24627, 21008,
       41905, 22482, 19724, 15215, 13711, 41089, 26016, 31291, 44860,
        2637,  2985, 26222, 47315,  7571, 37413,  2853,  1612,  9434,
       10806, 16396, 31881, 37678, 43903, 39581, 41899, 16046, 17404,
       18796, 17142, 30823,  5776,   428, 15082, 21960, 31956, 22285,
       17176, 41840, 17158, 38406, 26828,  6671,  3977, 47262,   269,
       33568, 22240, 22538, 28010, 29059, 15936,  7708,  1638, 22464,
       13429,   915, 41128, 18878, 27025, 38793, 22127,   408, 32766,
       38017, 36713, 19805, 22182,  4489, 33784, 49360, 47073, 40638,
       31869, 20518, 26561,  1767, 16803, 39930,  7583, 18779, 22623,
        4121, 42473,  7302, 38378,   270, 33483,  2313, 49583, 17289,
       27311, 46280, 22199, 47512, 35218,   867,  5668, 37074, 41415,
       39371, 40976, 24076,   881, 18963, 28747, 41771,  2609, 15901,
       36549, 13407, 20380, 46346, 30220,   672, 49831, 29073, 37040,
       15652, 21614, 34767, 39264, 21887, 44404, 10698, 14970, 35197,
       25094,  4584, 41161, 43882,  8992,  1000, 33813, 26769, 18943,
       21745, 36250,  6903, 41363, 20172,  3846,  2308, 48816, 26101,
        2890, 42994, 27975, 47697,  1078, 40043, 34223, 21170, 26511,
       23908, 19086, 42058, 29000,  2281,  9229, 26622, 48606, 19876,
       37070, 46654, 38481, 37762, 27755,  2829, 28783, 22543, 39449,
       33538, 34390,  1043, 12568,  9880, 17201, 15252, 11139, 15495,
       49057, 10762, 40819, 15027,  4289, 26012, 49208,  9511, 25736,
        9028, 25433, 28390, 40166,  6258, 44605, 23840, 36031, 35019,
       42139, 27815, 14112,  6364,  1940,  2671,  5203, 47435, 36797,
       19824, 28514, 39567, 24027, 49783, 21401, 16078, 31233, 34402,
       15249,  9432, 15031, 13705,  6984, 26506, 10309,  4003, 13157,
       42617,  6238,   190, 44059, 39613, 26623, 47713, 40070, 31462,
       46453,  1479, 14054, 17167, 27352, 35896,  6667, 30087, 18644,
        4272,  1512, 28016, 30639, 44416, 39336, 20762, 43408, 22340,
        9068, 26440,  3305,  9960, 14728, 43705, 12544, 39702, 30601,
       16700, 20942, 10876, 10139, 13233,  7455, 49253, 26105, 17130,
        4222, 35671, 23669, 27410, 29600, 47378, 21028, 36803, 23883,
       36744, 39075, 43175, 39139, 17617, 28768, 45617, 34615, 21541,
       14710,  3482, 33124,  4828,  1914, 48374,  9648, 20873, 36892,
       25783, 15322, 28134,  5600, 15630, 27153,  9567, 22518, 46068,
       48959, 27618, 21998, 42371, 21710, 48805,  3430, 25632, 31037,
       33225, 48081, 37710, 34468, 26538, 10688, 42270, 23960,  5687,
       15951, 37335, 25047, 28707, 32465,  7287,  8229,  5855,  1357,
       18593, 40270,  2721, 35726, 36108, 41240, 23872, 48641,  1261,
       39802, 23148, 43699, 15699, 27027, 18295,  6459,  4916,  1721,
       13355, 10016, 21568, 36255,   987, 35778, 31938, 29362, 39916,
       33119, 47791, 37064, 45635, 22862, 36876,  1289,  3681, 22440,
       14049, 13684, 12128, 18307, 47391, 45608, 28985, 32207, 44362,
       41281, 22147,    62, 33888, 17854,  4287, 30483, 45115, 44570,
       35909, 12319, 33651,  4474, 22600, 38808,  1702, 48457,  3313,
       30488,  3770, 13836, 13883, 46392, 32974, 21716, 23707, 40881,
        4794,  8678,   901, 19748, 23845, 20962, 45784,  7443, 10004,
         259, 38339, 14866,  3765, 34825, 48874, 46386, 35828, 41647,
       34248,  7535, 20740, 37641, 29734, 21830, 23399, 19768,  9269,
       41403, 30210, 15914, 24270,  2663, 49567, 41941, 10763, 19336,
        8831, 39350,   430, 26108, 48943, 34527, 13720, 11993, 23011,
       28450, 17252, 33570, 38322,  5904,  8384, 32281, 35628, 23818,
       12286, 28295, 41187,  8968, 30660,   971, 37483, 10245, 22218,
       27436, 21164,  2640,  4936, 46617,  7218,  7751, 23279, 42434,
        3296, 18627, 44270, 43549, 22050, 41265,   336, 19180, 35151,
        7110, 35443, 16749, 23133, 49352, 26512, 16636, 29471, 15625,
       15155, 31053, 33408, 32300, 24067, 11045, 11254, 30967, 49981,
       31549, 15773, 30815, 47611, 14649, 41554, 28200,  2488, 31939,
       40707, 41878, 33480, 47144, 12921,  1624,  2842, 49685,  2691,
       36523, 39977, 28711, 17145, 24659, 43239,    28,  7634, 15958,
       46077, 44450, 14921, 22820, 47591, 43586,   979, 15643,  2703,
       37466, 43238, 38869, 20810, 48235, 37214, 37987, 37977, 11997,
       42967, 31547, 48505, 18512, 22749, 24664, 12997, 18550, 23817,
       37234, 28219, 13005, 41788, 15409, 46049,  5844, 26351, 15864,
       43460, 10489, 15737, 45255, 13772, 37154, 18841, 47126, 40285,
       22677,  5998, 47374, 48969, 32017, 37960, 40111, 22750, 30125,
       20253, 43529, 40981, 28084, 43682, 46884, 15140, 34255, 19426,
       36965, 37241, 22203, 26360,   526, 20173,  8279,  1595,  9550,
       47855, 20769, 39752, 17402,  8361,  8504, 46185, 35564,  3957,
       34193,  8541,  6415, 47344, 49292, 42629, 26953, 20343,  1813,
       13466, 27535,  7382, 45729, 29670, 17150, 30425, 28228, 29329,
        8551,  8233,  2155, 36160, 16897,  5957,  5187, 26114, 39067,
       23654, 40486, 39596, 23671,  4924, 47384, 27417, 11810, 45712,
        5790, 28260, 47664, 36428,  2442, 12124, 32519, 44942, 37875,
       44162,  8939, 10237, 28769, 29376, 20485,  2461, 13855, 39005,
       43746, 39467,   372, 31476,  2327, 14787, 10447, 34784, 25017,
       19307,  4527, 30869, 37805,  7119, 19990, 25147, 24658, 45764,
       29361, 42643, 48860, 28892, 22985,  3383, 37288, 44563, 27860,
       34744,  8263,  4989, 24241, 28744, 29278, 36976, 11957,  1526,
       28903,  5501, 39565, 12261,  5931, 30681, 42769, 32929, 19746,
       45226, 25429, 41822, 16058, 19452, 15256, 42938, 18502, 24799,
       49390,  1974, 33209, 29121, 46507, 25817, 47519,  2339, 37512,
       48000,   617, 14659, 34832, 27506, 42113, 34862, 36594, 15779,
       21585, 21333, 17515,   261, 14871, 28784, 31899,  7706, 14859,
       23338, 33063, 16286, 32161, 29330,  7077, 24566, 44412, 15512,
       35105, 10100,   227,  1052, 49381, 45977, 31890, 29633,  4609,
       39256, 11890, 22450, 47968,  3485, 36938, 42292, 28958,  8221,
        1631, 42065,  4101, 15740, 32736,   168,  8475, 36388, 40046,
       23598, 11580, 22882, 16250, 35145, 29314, 25608, 16850, 49616,
       20777, 14713, 25700, 26059, 15891, 26737, 19616, 38278, 41289,
       11664,  2769, 38420,  6295, 48617, 40537,  8175,  4230, 19710,
       33114,   699,   396, 42281, 16275, 19399, 36672, 11683,  1571,
       36751, 36033,  2390,  8812, 19867, 20197, 48995, 15205,  6429,
        6191, 48636, 37269, 24284, 29311, 42918, 30014, 32603, 20214,
       43447, 34145, 40576, 28347,   311, 22165,  1410, 46731, 18926,
       38974, 20510, 26578, 39109, 44541, 45704, 31512, 38725, 41279,
       19628, 49894, 16855, 48970, 21437, 14900, 41794, 17856, 29325,
       25652, 39204, 49705, 20783,  5959, 10491, 11791, 18063, 43217,
       38892,  6272, 20988, 37995,  6120, 10393, 48740, 20914, 11427,
       17755,  8294,  3386, 24398,  2039, 28906, 43641, 29188, 49494,
       13526, 34068, 38571, 26637, 45580,  2268,  1694, 45859, 42211,
       26794, 15832,  2280,  9549,  7827, 23662, 21153,  4423, 12923,
       25150, 30245,  5759, 41322, 38549, 29285, 32410, 42734, 34426,
       43847,  6371, 14722, 22529,  6701,  2173, 22586, 18495, 10748,
       39240,  3840, 31107,   947, 11579,  6899, 37203, 14067, 46259,
       46359, 28539, 43132, 30486, 22163, 19500, 35454,  8875, 15496,
       21073, 12774, 33327, 23311, 41356, 19609, 11698, 39974, 12622,
       20874, 48845, 10953, 23578, 49121, 36085, 45011, 49436, 18206,
        2511, 15704, 43605,  9765,  8571, 12373,  3414,  2227, 36872,
        3059]), [9, 8, 4, 1]), (array([ 6304, 37804, 27012, 40216, 43318, 23884,  9846,  6309, 25738,
        3864, 20721, 17440,  3813, 15453, 26707,  4197, 47851,  2127,
       29685, 20447, 10869, 41892, 32250,   846,  8210,  6607, 40165,
       41826, 44173, 15421, 19820, 16619,  9293, 11998, 11477,  6014,
       40246, 37973, 43462, 37662, 11387, 16366, 46825,  7753,  7086,
        1608, 46058,  6405, 34099, 41285,   998, 13700, 16301, 15603,
        9034, 11564, 22420, 38304, 12428, 28953, 25490, 15383,  4558,
       27233, 35124,  1510,  8319, 30868, 33133, 29810, 42752, 16547,
       22030, 49970, 24726,  1958, 43922,  4674, 18814, 37770,  6619,
       12111,  8356, 13001, 30817, 38992,  1057,  7432, 35092, 22620,
        9024, 18127, 29012, 14368, 31178, 18998, 28864, 21686, 43843,
       46121, 21053, 22913, 14108, 26849, 10193, 24749, 47100, 10746,
       41412, 35128, 25882, 32451,  1539, 40584, 12174, 42930, 39108,
       27407, 15974, 10575, 47901, 23376, 11231, 13349, 20252, 13013,
       11120, 22406, 41016,  8781,  6913, 30266,  4402, 49079,  1556,
       34984, 44263, 27909, 20329,  5321, 47554, 38679, 21085, 19126,
       32297, 46542, 31179, 32846, 22919, 14118, 28253, 25281, 14070,
       32541, 33092,  2447,  2011, 20820,  1895, 23498, 22793, 49741,
       46182, 41229, 10958, 29215, 43411,  7906, 39294, 15545,  5455,
        2353, 45502, 11047, 39885, 11714, 27275, 48907, 29705, 20770,
       46215,  3571, 26357,  5823, 36211, 29520, 33267,  3905, 22294,
       36422, 32936,  8645,  9830, 11105, 43328,  7671,  6649, 46157,
       14089, 29839, 48040, 14171, 14703, 40232, 26044, 24060, 38364,
       17615,  1961, 17243, 45646, 16070,  3194, 24081, 48999, 11724,
       37906, 15053, 16295, 23145, 41917, 10942, 17047, 33463,  9720,
        4025,  5520, 44550, 22662, 27114,  5218, 22411,  1573, 38781,
       35323, 15247, 38932, 46417, 46605,  6540, 47722, 17018, 26559,
       45382, 38634,  9183, 23594, 18570, 21510, 42744, 14472, 27738,
        7151, 37255,  2784, 32899, 11137, 20558, 15977, 37686, 18143,
       25200,  9940, 44644, 32377, 33474, 40372, 33083, 30632, 11286,
       16833, 48733, 14595, 32807,  6626, 16739, 26425, 21099, 14811,
        7466, 12583, 17981, 45951, 39499, 20651, 31621, 36423, 38349,
       31040, 38038, 37856, 20673, 43826,  8055,  6616,  2957, 11454,
       16215, 20038, 30940, 10926, 40924, 28664,  2789, 14619, 10440,
        4616, 16462,  1258, 16581,   802, 11270, 34307, 43990, 34488,
       25101, 38592, 15314, 33519, 14785,  4038,  1107, 14157, 33054,
       35653, 30572, 24773, 38801, 44535, 20073,  3963, 14383, 38216,
        3569, 29625, 19280, 14285, 10671,   912, 14601,  4168, 15398,
       15789,   990, 28667, 26386, 47171, 18163,  3484, 37373, 41024,
       13934, 33183,  7807, 47936,  1475, 13487,   885,  2514, 17693,
       27239, 42227,  2004, 30437, 11003, 24927, 29779, 33175, 33350,
        6028, 30786, 26999, 39368,  7521, 20009, 30631, 27186, 17302,
       43244, 31884, 23673, 40398, 27546, 25440, 40403, 31454, 41884,
       11843, 42894, 34203, 13290,  7330, 27948, 42428, 47710,  4511,
       48779, 30665, 17834, 36502, 26976, 24064, 39847, 15443, 14106,
       40348, 33498, 40217,  9803, 46564, 43572, 21262, 29180, 34222,
       48169, 22511, 26588, 46819, 11884, 16446, 18204, 14229, 27498,
        6334, 26176, 43823, 33531, 32409,    90, 25982, 49914, 13814,
       11779, 10412,   522, 23062, 15369, 32763, 32480, 15589, 44634,
         804, 38459, 11089, 13273, 34873,  8706, 11587, 32057, 34885,
       21500, 13081,  2191, 27865, 47368, 27685, 33842, 36382,  5647,
       44879, 48414, 25030, 39062,  1798, 28834, 46575,   283,  3558,
       38182, 48092, 48608, 11443, 46214, 27594, 25398, 23453,  4237,
       26308,  6177, 43287, 10378,  5219, 31361,  4656, 17955,  8219,
        4634, 17273, 24623, 45533, 15929, 16609,  5384, 43692, 21063,
       26630, 19615, 33776, 47584, 12060,  2450, 32094, 22619,  5704,
        6943, 18099, 23634, 28174, 47908, 32478, 22743, 16908, 45133,
        8688,  7903, 14134, 40530, 40794, 28885, 33383, 45598,  5706,
       45786, 41625, 38354, 47795,  5298, 24221, 48195, 40098, 27017,
        5676,   621, 46153, 26497, 29070, 36142, 26329, 19059, 15126,
        9018,  1284, 27372, 14786, 30522, 34969, 20280,  8913,  8631,
        1704,  9750, 21052, 10604, 45022,  7043, 45110, 26943, 49687,
        6652, 12717,   477,  1348, 14853, 35378,  4137, 49586, 39945,
       36552,  1657, 30725, 21693, 33099,  9592,  4642, 41770, 43420,
       16105, 16880, 23943, 21794, 17450, 10299, 32523, 48179, 31306,
        8701, 11845, 17707, 23076, 48360, 30614, 48605, 41608, 39209,
       40301,  1984, 43897, 19704, 26882, 39878, 29852,  9364, 47576,
       46145, 27531, 41077, 39293,  9500,  1396, 45396, 21582, 45410,
        2190,  5225, 33221, 45756, 21188, 44994,  9507, 35395,  3556,
       14726,   959, 25779, 41103, 31954, 17613, 26886, 33215,  1937,
       45293,  9041, 27259, 18409,  9285, 14798, 36345, 24265, 43494,
        1339, 29128, 15734,  5829, 15454, 20341,  3283,  4846,  8987,
       40036, 22274,  7670, 49475, 37528,  9273, 19528, 17260, 43222,
       42695, 34776, 41097, 34727, 20629, 24925, 32494,  8137, 22385,
       20967, 22369,  3046, 38728, 16672,  5227, 37677, 47232, 48796,
       21649, 33080, 10500, 36071, 36731,  8114, 30077,  9779,  4885,
       11794, 47147, 44750, 47120, 32201, 20435, 18197, 44061,  2528,
       32253, 16263, 25798, 24572, 26975, 42907,  6344,  7394, 25142,
        3509, 40219, 17039,  5880, 48429, 46591, 47796,  2105,  7957,
       35457,  8066, 37741, 31898, 32029, 12975, 39946, 15238, 14635,
        9250,  4199, 15909, 16101,  9749, 18185, 15730, 27303, 44283,
       25643,  2297, 18264, 26233, 23739, 39541,  1406,  1640,   381,
       33688, 27762,  3913, 40077, 30243, 16525, 35050,  6049, 44525,
       17552, 48945, 31588, 37588,  5769, 38929, 27584, 25300, 35694,
       33390,   524,    99,  8085, 28089, 36331, 19575, 26424,  3836,
        3103, 45856, 18458,   301, 31102, 13108, 31445,  1240, 48669,
       23533, 28559,  7300,  8556, 34530, 31251, 30039, 15662, 39277,
       12741, 49769,  8001, 20840,  5897, 14593,  6375, 10403, 16023,
       36236, 33625, 36703, 43145, 19063, 33650, 49820, 17030,  4764,
       44818, 42707, 20605, 14519, 11007, 20442, 14598, 33126, 33624,
       44415, 16834, 42606, 12894,  2958,  4299,  2656, 48375, 41984,
       17621, 41888, 25596, 33050, 16964, 34263, 38078, 26741, 16588,
        8690, 30471, 17732, 47178, 29029, 14643,  9085, 22697, 16962,
       31523, 26746,  6779, 24268,  4942, 31329, 45001, 31830, 13002,
       17860, 22745,  6241,  8893, 47060, 17268, 22303, 41579, 31245,
       49230,  7540,  7375, 24620, 44725, 27636,  1494, 29394, 31587,
       43195, 33399, 31711,  3009,   226,  4578, 42031, 49447,   126,
       22545, 28394, 11817, 13037, 42121,  4464, 25377, 46323, 11242,
       14662, 45105, 11829,  4661, 23546, 13470, 42341,  6988, 12893,
       30182, 18927, 33615, 19301, 45946,  7214, 25910,  1251, 27194,
       37239,  5635, 28245, 46679, 32348, 16745, 32737, 19983, 24068,
        7503, 25961, 44971,  3233, 14964, 26471,  6495,  8779, 45715,
       42991, 20273, 23142, 40749, 32311, 38902, 30062, 13444, 45968,
       21888, 46730, 45097, 13311, 33598, 19880, 12560, 23088, 28957,
        8090, 26230,  1548, 30244, 41563, 22689, 41736, 25827, 29230,
        9012, 37755, 37320, 33237,  6456, 47453, 21893, 24561, 38682,
        5975, 49875, 35878, 46928, 22145,  4607, 41862, 26074, 21530,
       34791, 37253,  2067, 27228, 34618, 19484, 33569,  2597, 32042,
       13899, 24157,  8201,  2320,  4352, 30165,  9318, 33646,  9459,
       18223, 19242, 21230, 17274, 13647, 49392, 41739, 41026, 38838,
       17457, 28344, 27773, 41701, 47208, 30086, 33196, 39824, 41378,
       40762]), [3, 2, 4, 1])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.255, 0.04439314603805542)
DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.245, 0.04447525298595428)
DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.244, 0.045177888870239255)
DC 3, val_set_size=1000, COIs=[3, 2, 4, 1], M=tensor([3, 2, 4, 1], device='cuda:0'), Initial Performance: (0.209, 0.04494925320148468)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {0, 6}
D07: 1000 samples from classes {0, 6}
D08: 1000 samples from classes {0, 6}
D09: 1000 samples from classes {0, 6}
D010: 1000 samples from classes {0, 6}
D011: 1000 samples from classes {0, 6}
D012: 1000 samples from classes {5, 7}
D013: 1000 samples from classes {5, 7}
D014: 1000 samples from classes {5, 7}
D015: 1000 samples from classes {5, 7}
D016: 1000 samples from classes {5, 7}
D017: 1000 samples from classes {5, 7}
D018: 1000 samples from classes {8, 9}
D019: 1000 samples from classes {8, 9}
D020: 1000 samples from classes {8, 9}
D021: 1000 samples from classes {8, 9}
D022: 1000 samples from classes {8, 9}
D023: 1000 samples from classes {8, 9}
D024: 1000 samples from classes {2, 3}
D025: 1000 samples from classes {2, 3}
D026: 1000 samples from classes {2, 3}
D027: 1000 samples from classes {2, 3}
D028: 1000 samples from classes {2, 3}
D029: 1000 samples from classes {2, 3}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.06242937207221985) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06935057532787323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.10054781334102154) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09110854172706603) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.44, 0.07027108192443848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.07565721029043197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.10466695122420788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.323, 0.11809220957756042) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.08399075710773468) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.405, 0.08414981603622436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.10775397792458534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.355, 0.1430300070643425) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.09190552899241447) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.10456353434920311) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.15356273053586483) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.356, 0.16536943846940994) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.11256972487270832) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.11904050388932227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.17955271257087588) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.354, 0.1911297039538622) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.12555305766686797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.392, 0.12016960312426091) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.20252362125553192) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.376, 0.1672702697366476) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.13640131926350296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.12200692869722843) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1993113804385066) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.384, 0.1897293770611286) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.1368831439372152) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.1383049213439226) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.22230591307021677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.387, 0.205445771753788) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.15904230438452213) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.13590771255642176) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.251293695660308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.379, 0.1797317993193865) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.15739924981538206) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.422, 0.15744038719683887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.24946794794872404) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.2138650175333023) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[1, 4], M=tensor([0, 1, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.7866666666666666, 0.020253663380940756)
DC Expert-0, val_set_size=500, COIs=[0, 6], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.938, 0.005567504147067666)
DC Expert-1, val_set_size=500, COIs=[5, 7], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.844, 0.013536606922745704)
DC Expert-2, val_set_size=500, COIs=[8, 9], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.912, 0.008059985995292663)
SUPER-DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x72f20c7cdd00>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f288ef9f40>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f20c4f50d0>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f288d99c10>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f28020b220>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.003847681143786758) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.852, 0.012479427978396416) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.008226505281403661) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.16791463382542132) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.007022676706314087) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.623, 0.042906414300203324) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.08409164267778396) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.636, 0.04014549806714058) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004041791361523792) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.012117560110986234) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.914, 0.007320915885269642) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.18187071216106415) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.908, 0.007502878208955129) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.61, 0.05464022673666477) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.543, 0.06410713455080987) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.059894803375005724) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0041356230232631784) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.848, 0.01427882405370474) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.0073795332349836824) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.384, 0.17796269308030604) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9366666666666666, 0.007771758066543649) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.06523145939037203) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.09504102100431919) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.526, 0.08711566522717476) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.003926958607509732) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.013775676608085633) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.009341772452928125) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.39, 0.18586286914348601) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9273333333333333, 0.008398363390006125) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.08070279396697878) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.506, 0.11658919796347618) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.511, 0.08665287590026856) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004250137723516673) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.011458096139132977) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.00790807881206274) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.19636187960207463) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.00967427969009926) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.568, 0.08521790418960154) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.12569867165014148) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.553, 0.06772018916718661) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO5', '(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0042697460019262504) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.013089602798223495) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.006684655111283064) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.1889429318755865) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.00789181557825456) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.06950444707274436) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.10604448545724153) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.05213676621764898) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0042236464965390045) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.01225137310475111) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.006926236109808088) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.19248264494538309) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.00726333688168476) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.06831433653831481) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.13212380550056696) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.589, 0.05848917078971863) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004280983859673142) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.013157947883009911) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.007591322207823396) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.18859956175088882) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.005599909785358856) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.06900576259195805) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.12079699754714966) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.548, 0.10910616103187204) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004842857890587766) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.012756120033562184) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.008765105465892702) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.18596020790934562) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9393333333333334, 0.005701066291580598) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.614, 0.07002055843919516) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.497, 0.10172639000415802) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.615, 0.051779365867376324) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004326252298924373) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.013767411757260562) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007891500450670719) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.1995002319663763) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9306666666666666, 0.011398860960500315) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.06504194146394729) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.13381945761293174) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.519, 0.08455879624560475) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO0', '(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0039905990513507275) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.014773073993623256) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.0074698929209262134) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.19374949726462365) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9453333333333334, 0.005852358254293601) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.06478904560953379) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.10002275658398867) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.562, 0.08661769491434097) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005680075647484045) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.012428722254931926) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.00868667169753462) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.19630836768448354) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9366666666666666, 0.01006233569731315) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.07970046450197697) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.14232851550914347) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.1035879994854331) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0049370772036490965) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.86, 0.013269611306488514) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.010705246904864907) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.395, 0.1943335898220539) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.007594833188652047) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.06956687518209219) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.1237685352191329) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.493, 0.12622132801171393) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005626140659136581) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.014474653474986552) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.00880426334310323) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.1942524548768997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9593333333333334, 0.00864979760896919) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.578, 0.09140741817094386) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.504, 0.140980338960886) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.528, 0.09632812710106373) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004389177928038407) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.014474489171057939) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.916, 0.009176336150616407) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.20628052288293838) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.0058763757951674055) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.648, 0.05883897069841623) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.51, 0.10143321166932583) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.0809211077541113) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO0', '(DO1']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004150633720331825) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.01682855251431465) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.008425471927039326) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.1897297247350216) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.0055960046265584725) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.656, 0.053430247336626054) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.544, 0.08687428224086761) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.569, 0.08783422752469779) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004881495492416434) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.014530079446732997) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007089244501665234) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.1952621847689152) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9586666666666667, 0.006371227651179652) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.06234472940862179) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.09999874034523965) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.572, 0.0864538369551301) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004790908636030509) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.85, 0.016454683870077133) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.008253489972092212) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.391, 0.18681876353919505) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.007730355891673146) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.06559072494693101) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.13851818800903856) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.559, 0.07681870872154832) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004811840591908549) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.015209410946816207) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.0070821693670004605) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.17956819808483124) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.00568689592089504) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.701, 0.04570048639178276) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.592, 0.07109744427353144) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.549, 0.08435361970961094) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005368487735788221) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.01426409960910678) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.007798607435077429) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.1869236236438155) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9346666666666666, 0.009555411100508839) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.0562776116579771) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.496, 0.12590935208648443) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.07687915919721126) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005664593163084646) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.014858750207349659) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.008595803450793029) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.17318007181584835) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.009101755365915778) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.658, 0.05284174171276391) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.10740105171129108) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.522, 0.09611810853332281) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004726397097532754) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.01229113805294037) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.009723104499746114) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.19075863187760114) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9533333333333334, 0.0068951478819944895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.716, 0.03909710502624512) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.09868203625082969) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.54, 0.08818919777683913) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0039824969950132075) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.014801184624433517) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.904, 0.010360986157786101) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.1849568217024207) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.007922950500855222) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.669, 0.045903138995170595) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.099526559965685) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.53, 0.07902128844847903) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.00446139996905913) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.014880801651626824) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.008102527680806815) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.17181476548314095) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.007238551962480415) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.655, 0.05114454486966133) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.09253654158487916) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.598, 0.055659809451550246) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005324877392737107) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.013606543742120267) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.009285052169580012) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.395, 0.20635307467728853) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.01905477111624676) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.09107754266640404) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.15689046653028343) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.568, 0.07062129493197426) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO5', '(DO2']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003948806057800539) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.01334868810325861) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.902, 0.009741062085144222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.18422666417062283) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.007654955913603771) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.669, 0.060545794412493706) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.09429680480062962) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.546, 0.06820809826627373) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004974864718533354) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.01340842953324318) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.008500526686199009) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.1962214486449957) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.962, 0.008935708173446376) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.06943033240735531) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.1065296574011445) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.553, 0.06653892423585057) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004951728701620596) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.01220189156383276) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.009090961908455939) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.2049194903448224) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9386666666666666, 0.017208950664149597) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.636, 0.060447539824992416) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.13554383882135151) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.543, 0.06442530663823709) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006019399155542488) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.013455868624150753) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007789981831330806) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.19913368368148804) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9546666666666667, 0.01033494707199073) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.666, 0.050224379800260066) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.506, 0.10718843222036958) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.57, 0.062257656461093575) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004241532897169236) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.011296955034136772) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007851703158812597) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.20247165989875793) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.009751124230164956) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.08191006337851287) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.15408779985271395) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.584, 0.052411697644740345) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO3', '(DO0']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004276312539564969) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.012506277613341808) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.009215789132053032) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.19538852375745774) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9386666666666666, 0.01367969154844559) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.647, 0.05454343465715647) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.538, 0.09352891023829579) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.532, 0.0791387938298285) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0048826510676299225) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.01437948778271675) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.009430659875040874) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.2096841803714633) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.012127650675955616) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.651, 0.05634785621985793) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.1001481172516942) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.06578860289324075) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004491323423280846) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.015518425548449158) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.008907439115457237) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.21147712268680335) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.014730451426779225) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.06738089134916664) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.10747866490948946) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.549, 0.07886201689951122) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004125485762371681) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.013475437887012958) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.00850625207531266) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.20042242304980754) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.013650480463620624) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.638, 0.06281609321385621) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.0989569765329361) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.544, 0.09243208476901055) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0036637941297667565) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.014918070893734694) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.916, 0.008527837830595672) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.20109627268463373) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9473333333333334, 0.011832897842702474) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.679, 0.04101477404683828) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.061988353848457335) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.54, 0.07787463167868555) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0047786698519485075) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.014610475029796361) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.00893207938829437) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.23353527803719043) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9633333333333334, 0.00671921444390955) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.722, 0.030611154422163965) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.06761846463754773) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.55, 0.07285837307851761) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005739542003535462) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.011729036897420882) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.007777987674577162) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2195804446041584) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9573333333333334, 0.006024744763451357) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.712, 0.035734874680638316) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.08507925514131784) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.588, 0.056572203047573565) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.00509744052530732) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.012848024472594262) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.007658283735625446) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.21116627154499293) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9473333333333334, 0.00966971345615093) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.667, 0.05213667917996645) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.1035650593265891) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.06804015063401311) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0048576203279808395) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.013105181239545346) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.009565371081465854) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.2279284894913435) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9606666666666667, 0.009228147383350005) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.045762444876134394) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.08478810976445675) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.536, 0.08338946250360459) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004941964177975024) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.015373565837740898) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.007930327791487798) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.2200767252892256) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.0109446215951466) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.669, 0.04457183756679296) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.0798373880982399) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.565, 0.0707836461160332) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.255, 0.04439314603805542), (0.276, 0.06242937207221985), (0.44, 0.07027108192443848), (0.455, 0.08399075710773468), (0.465, 0.09190552899241447), (0.459, 0.11256972487270832), (0.464, 0.12555305766686797), (0.461, 0.13640131926350296), (0.475, 0.1368831439372152), (0.461, 0.15904230438452213), (0.469, 0.15739924981538206), (0.623, 0.042906414300203324), (0.61, 0.05464022673666477), (0.552, 0.06523145939037203), (0.542, 0.08070279396697878), (0.568, 0.08521790418960154), (0.592, 0.06950444707274436), (0.632, 0.06831433653831481), (0.627, 0.06900576259195805), (0.614, 0.07002055843919516), (0.609, 0.06504194146394729), (0.62, 0.06478904560953379), (0.592, 0.07970046450197697), (0.604, 0.06956687518209219), (0.578, 0.09140741817094386), (0.648, 0.05883897069841623), (0.656, 0.053430247336626054), (0.644, 0.06234472940862179), (0.629, 0.06559072494693101), (0.701, 0.04570048639178276), (0.632, 0.0562776116579771), (0.658, 0.05284174171276391), (0.716, 0.03909710502624512), (0.669, 0.045903138995170595), (0.655, 0.05114454486966133), (0.592, 0.09107754266640404), (0.669, 0.060545794412493706), (0.628, 0.06943033240735531), (0.636, 0.060447539824992416), (0.666, 0.050224379800260066), (0.628, 0.08191006337851287), (0.647, 0.05454343465715647), (0.651, 0.05634785621985793), (0.639, 0.06738089134916664), (0.638, 0.06281609321385621), (0.679, 0.04101477404683828), (0.722, 0.030611154422163965), (0.712, 0.035734874680638316), (0.667, 0.05213667917996645), (0.67, 0.045762444876134394), (0.669, 0.04457183756679296)]
TEST: 
[(0.2515, 0.043328432649374006), (0.2885, 0.05995646780729294), (0.4465, 0.06739776784181595), (0.4545, 0.0804382722377777), (0.46875, 0.08793868029117584), (0.46075, 0.10819000673294067), (0.46475, 0.12106116461753845), (0.46375, 0.1310386021733284), (0.475, 0.13236147272586823), (0.4615, 0.15334183567762374), (0.46875, 0.1515594100356102), (0.63575, 0.03986751249432564), (0.629, 0.0505862296372652), (0.57075, 0.06331710393726826), (0.53975, 0.0801052705347538), (0.5605, 0.08482692727446556), (0.58025, 0.0686833410859108), (0.63125, 0.06881481187045574), (0.61525, 0.06806477129459382), (0.61875, 0.06677181316912174), (0.6165, 0.06513042996823788), (0.624, 0.06286453288793564), (0.59, 0.08180458058416844), (0.601, 0.06676085004210472), (0.57725, 0.08906769478321075), (0.658, 0.05656105794012547), (0.64625, 0.05447123797237873), (0.64425, 0.06103437225520611), (0.60275, 0.06884567487239837), (0.6865, 0.045380160622298715), (0.6185, 0.05805411994457245), (0.66225, 0.052809613518416884), (0.705, 0.03925985363125801), (0.67825, 0.0413408300280571), (0.639, 0.052459643319249155), (0.57225, 0.09376976510882377), (0.65325, 0.06322079709172249), (0.618, 0.06864179876446724), (0.632, 0.061915184572339056), (0.65675, 0.05183396147191525), (0.6115, 0.08582036778330802), (0.6555, 0.05269519842416048), (0.636, 0.058147335022687915), (0.6275, 0.06715012829005718), (0.6305, 0.061253903687000275), (0.68975, 0.03693329315632582), (0.71525, 0.02944208785891533), (0.7095, 0.03358900763094425), (0.6565, 0.051325455129146574), (0.66675, 0.043947473861277106), (0.6745, 0.042094875320792195)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.88      0.57      0.69      1000
           1       0.82      0.90      0.86      1000
           4       0.49      0.89      0.63      1000
           6       0.79      0.34      0.48      1000

    accuracy                           0.67      4000
   macro avg       0.74      0.67      0.66      4000
weighted avg       0.74      0.67      0.66      4000

Collaboration_DC_1
VAL: 
[(0.245, 0.04447525298595428), (0.25, 0.06935057532787323), (0.352, 0.07565721029043197), (0.405, 0.08414981603622436), (0.389, 0.10456353434920311), (0.376, 0.11904050388932227), (0.392, 0.12016960312426091), (0.382, 0.12200692869722843), (0.384, 0.1383049213439226), (0.416, 0.13590771255642176), (0.422, 0.15744038719683887), (0.471, 0.08409164267778396), (0.543, 0.06410713455080987), (0.493, 0.09504102100431919), (0.506, 0.11658919796347618), (0.469, 0.12569867165014148), (0.479, 0.10604448545724153), (0.491, 0.13212380550056696), (0.484, 0.12079699754714966), (0.497, 0.10172639000415802), (0.475, 0.13381945761293174), (0.507, 0.10002275658398867), (0.475, 0.14232851550914347), (0.489, 0.1237685352191329), (0.504, 0.140980338960886), (0.51, 0.10143321166932583), (0.544, 0.08687428224086761), (0.508, 0.09999874034523965), (0.489, 0.13851818800903856), (0.592, 0.07109744427353144), (0.496, 0.12590935208648443), (0.522, 0.10740105171129108), (0.524, 0.09868203625082969), (0.524, 0.099526559965685), (0.532, 0.09253654158487916), (0.484, 0.15689046653028343), (0.535, 0.09429680480062962), (0.515, 0.1065296574011445), (0.508, 0.13554383882135151), (0.506, 0.10718843222036958), (0.499, 0.15408779985271395), (0.538, 0.09352891023829579), (0.505, 0.1001481172516942), (0.529, 0.10747866490948946), (0.508, 0.0989569765329361), (0.576, 0.061988353848457335), (0.562, 0.06761846463754773), (0.545, 0.08507925514131784), (0.524, 0.1035650593265891), (0.565, 0.08478810976445675), (0.54, 0.0798373880982399)]
TEST: 
[(0.23225, 0.043421665340662004), (0.25, 0.06652063864469528), (0.345, 0.07260621449351311), (0.4005, 0.08074362868070603), (0.388, 0.10054817044734955), (0.381, 0.11419055730104447), (0.39725, 0.11531978017091751), (0.3775, 0.1179304256439209), (0.38325, 0.13432818323373794), (0.41475, 0.1329578117132187), (0.41625, 0.15371146613359452), (0.483, 0.08090510621666908), (0.56625, 0.062273224353790284), (0.50825, 0.08969521242380142), (0.49775, 0.11457214257121086), (0.4905, 0.11983082807064056), (0.505, 0.09818166160583496), (0.507, 0.12367879992723466), (0.50025, 0.11422673326730728), (0.508, 0.09533783295750618), (0.494, 0.12653835120797158), (0.53125, 0.09434657847881317), (0.494, 0.13624453377723694), (0.50675, 0.11794721776247025), (0.51425, 0.1324572864472866), (0.53275, 0.09543985390663147), (0.56325, 0.08165759453177453), (0.53975, 0.09172377470135688), (0.48775, 0.1324475308060646), (0.6025, 0.06468038983643055), (0.5035, 0.1209629979133606), (0.531, 0.10095204275846481), (0.52975, 0.09396445623040199), (0.539, 0.0937991020977497), (0.545, 0.08525315141677857), (0.4865, 0.14701615571975707), (0.5465, 0.08797494146227837), (0.535, 0.09806547108292579), (0.5185, 0.12613015654683113), (0.51125, 0.10002485129237175), (0.4955, 0.14336590415239334), (0.561, 0.08228727009892464), (0.5215, 0.09332403054833412), (0.5375, 0.09550020790100097), (0.52325, 0.09317310911417008), (0.61025, 0.05467196168005466), (0.57275, 0.06129073974490166), (0.55925, 0.0768723088502884), (0.53325, 0.0930695975124836), (0.5905, 0.07513257056474686), (0.5545, 0.07242815500497818)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.82      0.92      0.87      1000
           4       0.39      0.93      0.55      1000
           5       0.70      0.19      0.30      1000
           7       0.84      0.18      0.30      1000

    accuracy                           0.55      4000
   macro avg       0.69      0.55      0.50      4000
weighted avg       0.69      0.55      0.50      4000

Collaboration_DC_2
VAL: 
[(0.244, 0.045177888870239255), (0.25, 0.10054781334102154), (0.252, 0.10466695122420788), (0.38, 0.10775397792458534), (0.413, 0.15356273053586483), (0.395, 0.17955271257087588), (0.436, 0.20252362125553192), (0.441, 0.1993113804385066), (0.441, 0.22230591307021677), (0.464, 0.251293695660308), (0.456, 0.24946794794872404), (0.636, 0.04014549806714058), (0.576, 0.059894803375005724), (0.526, 0.08711566522717476), (0.511, 0.08665287590026856), (0.553, 0.06772018916718661), (0.576, 0.05213676621764898), (0.589, 0.05848917078971863), (0.548, 0.10910616103187204), (0.615, 0.051779365867376324), (0.519, 0.08455879624560475), (0.562, 0.08661769491434097), (0.552, 0.1035879994854331), (0.493, 0.12622132801171393), (0.528, 0.09632812710106373), (0.558, 0.0809211077541113), (0.569, 0.08783422752469779), (0.572, 0.0864538369551301), (0.559, 0.07681870872154832), (0.549, 0.08435361970961094), (0.576, 0.07687915919721126), (0.522, 0.09611810853332281), (0.54, 0.08818919777683913), (0.53, 0.07902128844847903), (0.598, 0.055659809451550246), (0.568, 0.07062129493197426), (0.546, 0.06820809826627373), (0.553, 0.06653892423585057), (0.543, 0.06442530663823709), (0.57, 0.062257656461093575), (0.584, 0.052411697644740345), (0.532, 0.0791387938298285), (0.576, 0.06578860289324075), (0.549, 0.07886201689951122), (0.544, 0.09243208476901055), (0.54, 0.07787463167868555), (0.55, 0.07285837307851761), (0.588, 0.056572203047573565), (0.552, 0.06804015063401311), (0.536, 0.08338946250360459), (0.565, 0.0707836461160332)]
TEST: 
[(0.22875, 0.04402987053990364), (0.25, 0.09631045669317245), (0.25525, 0.0999217810332775), (0.39225, 0.10327717873454094), (0.41875, 0.14785195577144622), (0.405, 0.17185581016540527), (0.43225, 0.1947092747092247), (0.43925, 0.19275807696580888), (0.451, 0.21640128707885742), (0.4615, 0.24251160019636153), (0.452, 0.24052831733226776), (0.642, 0.0379025292545557), (0.57275, 0.058382247775793074), (0.551, 0.08149481834471226), (0.53725, 0.0800063688904047), (0.5585, 0.06402329462766647), (0.59175, 0.050436770126223565), (0.60575, 0.05462519839406013), (0.55725, 0.10437260511517525), (0.64825, 0.04537404415011406), (0.5395, 0.0752860870063305), (0.5715, 0.07693130770325661), (0.548, 0.09965679550170899), (0.51725, 0.12011030000448226), (0.54625, 0.08943267813324929), (0.57625, 0.075312015324831), (0.5835, 0.0792298995256424), (0.58175, 0.08089255473017692), (0.57525, 0.06995634254813195), (0.5545, 0.07625449199974536), (0.574, 0.07255110889673233), (0.5385, 0.09174531525373458), (0.55175, 0.08402313774824142), (0.54425, 0.0735305822789669), (0.6135, 0.052360476359724996), (0.5925, 0.06437702511250973), (0.57225, 0.06305759838223457), (0.57525, 0.06259017577767371), (0.572, 0.0598868450075388), (0.584, 0.05844637496769428), (0.5975, 0.04981890988349914), (0.5615, 0.07176271742582321), (0.584, 0.059801294177770616), (0.564, 0.0750150457918644), (0.5435, 0.085905727237463), (0.55825, 0.07199125148355962), (0.56675, 0.06775154605507851), (0.6075, 0.05249910348653793), (0.56975, 0.06385772623121738), (0.55975, 0.0761958609521389), (0.57425, 0.06442551705241203)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.42      0.91      0.58      1000
           4       0.72      0.94      0.82      1000
           8       0.89      0.25      0.39      1000
           9       0.77      0.20      0.32      1000

    accuracy                           0.57      4000
   macro avg       0.70      0.57      0.52      4000
weighted avg       0.70      0.57      0.52      4000

Collaboration_DC_3
VAL: 
[(0.209, 0.04494925320148468), (0.257, 0.09110854172706603), (0.323, 0.11809220957756042), (0.355, 0.1430300070643425), (0.356, 0.16536943846940994), (0.354, 0.1911297039538622), (0.376, 0.1672702697366476), (0.384, 0.1897293770611286), (0.387, 0.205445771753788), (0.379, 0.1797317993193865), (0.393, 0.2138650175333023), (0.388, 0.16791463382542132), (0.388, 0.18187071216106415), (0.384, 0.17796269308030604), (0.39, 0.18586286914348601), (0.402, 0.19636187960207463), (0.41, 0.1889429318755865), (0.401, 0.19248264494538309), (0.4, 0.18859956175088882), (0.394, 0.18596020790934562), (0.399, 0.1995002319663763), (0.398, 0.19374949726462365), (0.405, 0.19630836768448354), (0.395, 0.1943335898220539), (0.398, 0.1942524548768997), (0.399, 0.20628052288293838), (0.404, 0.1897297247350216), (0.398, 0.1952621847689152), (0.391, 0.18681876353919505), (0.403, 0.17956819808483124), (0.398, 0.1869236236438155), (0.399, 0.17318007181584835), (0.4, 0.19075863187760114), (0.394, 0.1849568217024207), (0.408, 0.17181476548314095), (0.395, 0.20635307467728853), (0.392, 0.18422666417062283), (0.401, 0.1962214486449957), (0.397, 0.2049194903448224), (0.405, 0.19913368368148804), (0.405, 0.20247165989875793), (0.41, 0.19538852375745774), (0.407, 0.2096841803714633), (0.397, 0.21147712268680335), (0.406, 0.20042242304980754), (0.401, 0.20109627268463373), (0.4, 0.23353527803719043), (0.408, 0.2195804446041584), (0.402, 0.21116627154499293), (0.4, 0.2279284894913435), (0.406, 0.2200767252892256)]
TEST: 
[(0.20325, 0.043842192202806475), (0.2555, 0.08742595201730728), (0.3265, 0.11303198945522308), (0.359, 0.13696498495340348), (0.36825, 0.15801430696249008), (0.3605, 0.18254355472326278), (0.382, 0.1589944772720337), (0.3905, 0.17838659977912902), (0.39525, 0.1921792615056038), (0.3875, 0.17070769482851028), (0.395, 0.20351121479272843), (0.39275, 0.160585995554924), (0.388, 0.17052445495128632), (0.396, 0.16684454333782195), (0.39925, 0.17414344781637192), (0.4025, 0.18204098910093308), (0.40475, 0.1763985412120819), (0.40675, 0.18053361362218856), (0.4025, 0.17893121004104615), (0.39825, 0.17515379637479783), (0.4035, 0.18655056494474412), (0.4025, 0.1819062532186508), (0.4035, 0.1865363311767578), (0.40325, 0.18289278525114058), (0.406, 0.18248910284042358), (0.40275, 0.19173169374465943), (0.39925, 0.17799725311994552), (0.40075, 0.18453045654296876), (0.40525, 0.16998803395032883), (0.40025, 0.16981042444705963), (0.399, 0.17475907063484192), (0.4025, 0.16345417004823684), (0.3975, 0.18047020602226257), (0.3995, 0.17469566005468368), (0.4055, 0.163934590280056), (0.39625, 0.19334499001502992), (0.39425, 0.17440282809734345), (0.3975, 0.1844991637468338), (0.3985, 0.18986140394210815), (0.41125, 0.18340982580184936), (0.408, 0.18872788935899734), (0.40625, 0.18454559314250946), (0.4005, 0.20214249861240388), (0.39275, 0.2014634467959404), (0.4015, 0.1882249174118042), (0.39775, 0.1875940009355545), (0.39725, 0.21734309726953507), (0.40275, 0.2050137251019478), (0.40075, 0.1980220187306404), (0.396, 0.21312646800279617), (0.40025, 0.20705248111486435)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.34      0.91      0.49      1000
           3       0.53      0.69      0.60      1000
           4       0.00      0.00      0.00      1000

    accuracy                           0.40      4000
   macro avg       0.22      0.40      0.27      4000
weighted avg       0.22      0.40      0.27      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [23]
name: alliance-3-dcs-23
score_metric: contrloss
aggregation: <function fed_avg at 0x72f2a128ac10>
alliance_size: 3
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=23
Partitioning data
[[0, 6, 4, 1], [5, 7, 4, 1], [9, 8, 4, 1], [3, 2, 4, 1]]
[(array([32659,  7396,   871,  8190, 33489, 44742,  8515,  5082,   965,
       11208,   700, 39329, 40740, 22304, 44430,   974, 28487, 44470,
       46964, 24750, 26632, 28921, 43184,  6663, 28273, 30029, 46367,
       39397, 38683, 15767, 48108, 33229, 13937, 41040,   276,  9831,
       45344,   927, 35963,  8842, 30591, 38617, 45559, 43659, 27768,
        2617, 28750,  1234,  5549, 34259, 20709, 33584, 29856, 34421,
       39553, 49734, 33380, 49130, 11897,  7445,  5003, 11958, 18763,
       26018,   600, 46936, 26667, 12477, 25023, 32870, 43077, 35570,
        9077, 16939, 46873, 32990, 37041, 39271, 47483,    93,  9121,
       33662, 34724, 33478, 49672, 12807, 45297,  6328, 20985, 35633,
       46625, 12674, 31391, 16554,  3347,    77, 48059, 30820,  9945,
       18102, 22924, 12297, 48777, 17222,  5557, 21815, 36415, 42231,
       12944, 47726, 47068, 30888,  7256, 33268, 22951, 42165, 39628,
       29380,  3564, 21143, 37035,  2427, 28000,  2010,   757,  7203,
       22118, 21803, 41745,  5349, 28272, 47329,  7925, 47339,  1715,
       21458, 16196, 38711, 12492, 17316,  5414, 48001, 21964,  1831,
       28305, 11218, 24113,  5642,  7263, 47994, 25199, 43548, 40956,
       42456,  4567,  4726,  9614, 28716, 32233,  2622, 13656, 27202,
       11372, 38791, 22458, 26656, 43427, 17620,  3731, 45232,  5093,
       45594, 20452, 16553, 11122, 14666, 44552, 24983, 19094,  6952,
       13850, 14434, 32107, 43475, 26764, 38307,  1376, 22194,  3721,
        1887, 43253, 34989, 10141, 10334,  4239, 17200, 33573,   348,
       14732, 31450, 46419, 39196, 41880, 14325, 11002, 38748, 45652,
       29881, 16647, 13031, 11615,  3987, 32056, 23776, 21469, 40556,
       20574, 49349, 13348,  1757, 29593, 44770, 13229, 10265,  2459,
       31083, 41242, 46749, 19004, 42929, 11682, 16235, 33258,  7772,
       48317, 37646, 17237,  5640,   436, 37078, 42829, 16632, 36386,
       38217, 44806, 26918, 39774, 45166, 43045, 28014, 37774, 45607,
       29833,  8942, 35340, 16404, 26339,  2252, 39512,  6096,   745,
       36348, 13137, 46342,  7848,  6581,  5785, 23691,  5176, 40551,
       24184,  6941,  5708, 42510, 18816, 20104, 13825, 39091, 17072,
       41444, 14293, 31042, 24955, 39889, 17957,  3307, 34684, 27177,
       15726, 14678, 49462, 33216, 46404, 23050, 45862,  8950, 19228,
       40879, 35455, 35585,  6093, 47497, 26275, 30222, 23451, 17105,
       31894, 42415, 40675, 18924, 42671, 25551, 47821, 10327, 13188,
       34804, 20310,  4094, 15552, 41672, 11070, 23539, 44258, 20141,
       18334, 31820, 12930, 21831, 15220,  5497, 13838, 23683,  5263,
        3655, 36913, 36192, 39148,  6521, 20734,  5419,  9125, 42046,
       16611,  7824,  9862,  9237,  5055, 42712, 31808, 40568, 34986,
        4785, 21155, 23760, 42850,   914, 34056, 19230, 35435, 21192,
       22909, 28106, 35600, 49747, 14024, 40929, 42778, 23390, 42460,
       11303,  8466, 35891,  6179, 33278,  7825, 15664, 12411, 48526,
       44422, 25680, 29381,  7632, 15517, 31806,  7335, 10431, 29241,
        3403, 22157, 30800,  3823,  4959, 44314, 10176, 31780, 38747,
       44612, 45637, 38864, 22986,  3435, 38893, 47513,  3841,   819,
        7715,  7705, 35444, 45361, 48176,  1154, 36704, 48460, 39756,
        6750, 30212,  3976, 47352, 23412, 19949, 39464, 35977, 47962,
        1529, 14274, 46322, 47276, 35900, 27240, 20771,  6968,  4926,
       34394, 29391, 28860, 24394, 49088, 21572, 41311, 36110,  2179,
       32060, 39326, 23517,  7025, 15536,  9021, 35480, 14940, 33690,
       42679, 33298, 32340, 38853, 26085,  2898, 11204, 47858, 26355,
       14163, 25793, 25430, 12641, 26415, 28501, 26478, 19239, 36396,
       45140, 30588,   863, 21941, 48027, 22295,  1035, 21958, 44573,
       35131, 38771, 34447, 39010,  7137, 44579, 35577,  3528, 48664,
        2810, 39537, 10665, 45895, 42836, 34829, 22332, 24673, 34139,
       12407, 24577,  3554, 42311,  1053, 31775, 32608, 35185, 30911,
        8254, 44215, 31281, 36711, 43486, 42665, 47605, 45665,  1269,
       36457, 34752, 45749, 38906,  1310,  9937, 34450, 43714,  3112,
        3225, 43032, 48447, 40088, 15777,  8375, 45767, 30600,  5677,
       43172, 47570, 42700, 23603,  6022, 39626, 22438,  4666, 47348,
       37656, 40827,  7822,  7362, 43758, 13778, 30435, 40529, 39501,
       25879, 11565, 42979,  1315, 41148, 16505,  9013, 36182, 29891,
       21382, 22139,  5173, 48334,  6844,  3442, 46333, 30950, 19585,
       22902,   158, 35839, 33872, 21161, 21659, 18363, 31902, 26710,
       15963, 36631, 18507, 38640,  6603, 48367, 39932, 22259, 49605,
       48881,  6569, 34425, 32302, 47015,  8804, 28649,  1826,   520,
       29771, 11586, 43914, 15667, 44465, 43683, 31932,  5742, 30501,
       35413,  5726, 48889, 38708, 48159, 39803, 29083, 27787, 20511,
       33998,  3360, 32388, 12416, 23525, 21473, 33038, 21281, 25153,
       21702, 27644, 32026,  4497,  5786, 22183,  2415, 27946, 47571,
       16893, 25781,    82,   429, 46660,  1804,  3077, 38593, 39169,
       34819,  3999, 41897, 24529,  4741,  9873, 20439, 18699, 24874,
        7163,  1923, 17608, 13959, 14258, 10925, 37172, 38794, 18293,
        2573, 20828, 41058, 44717,  2629, 44760,  3577,  2832, 16383,
       14857, 10076, 35318, 25024, 13464, 25444, 45812, 32784, 45644,
       17420, 15009, 49055, 31431, 45609,  2486, 34881, 35324,  6770,
       37004, 17513, 42915, 39833, 44572, 46521, 47318,  6990, 43975,
       22949,  3322, 17161, 10477,  4857, 10766, 31239,  8792, 42675,
        1569, 47530,    89, 43055, 31514, 23958, 10890, 14075,  2078,
       34816, 48521, 15415, 35915, 26926, 37089, 44850, 41249, 32786,
       39036, 23591,  8405, 22246, 33066, 12136, 28823, 32797, 28074,
       43228, 13310,  5607, 27428, 44516, 35187, 46036, 26844, 24239,
       31751, 48997,  6879, 18875,   343, 17266, 39356, 14490, 13903,
        4561, 19358, 46234, 46649, 24428, 18698, 44549, 26293, 24979,
       45470, 41404, 29318, 34327, 23317, 39769, 49314, 18562, 22270,
       22335,  2800,   547, 45258, 14035, 22067, 44824, 43359,  4326,
        1242, 10236, 29473, 14005, 43600, 11558, 29876, 24625, 37652,
       48113, 40520, 31288, 27089,  5301, 27994, 44266, 45455, 14904,
        6889, 36251, 36107, 32502, 36067, 12743, 37849, 14117, 36929,
       11955, 35291, 15756, 15299,  7992,  8374, 37260,  4909,   212,
       49234, 22816, 35613, 22422,  8244,  6908, 16848, 17986,   606,
       25710, 21615,  4171, 16882,  5282, 42007, 36400, 32462, 44488,
        7312, 23685, 31072, 20595,  5616,  4648, 30999,  3678, 31951,
        6122, 11037, 43932,    45, 15506, 49787, 49075, 47836,  1660,
       42802, 12914, 34097, 10248, 36611, 20079, 25463, 24470,  2306,
       40758, 48610, 11134, 19522,  3085, 33832, 16673, 19866, 31100,
       34802, 41607, 47824, 48294,  1565, 10122, 37098,  5885,    64,
        9610, 11744, 17539, 17950,  5287, 32235,  9524, 15797, 13240,
       49768,  9717,  5144, 11403,  7408, 14336, 46078,  4244, 29616,
       11308, 19564, 30218, 30712,  8339, 20536, 16755,   119, 40751,
       26937, 31147,  8093, 17014,  3034, 30445, 21850, 40311, 45201,
       44311, 42658,  9781,  4500, 38735, 19382, 26065, 26652, 36614,
       40431, 19341, 11163, 30103, 43290, 12080,  7587, 37428, 24541,
       19481, 21181, 48215, 47362,  1421, 32295, 10450, 28798, 27576,
       22419, 41922, 19125, 41768, 11109, 31720, 15447, 35357, 17772,
       19581,  4533, 13884, 12275, 40352, 41908,   561, 10678, 30154,
       30553,  4040, 29205, 17460, 17128, 37743, 19454, 29143, 18276,
       42663, 15814, 24873, 34118, 42976, 17017, 26294, 11954, 23020,
       31297, 18994, 38031, 12348, 14512, 34721, 35134, 11972, 48749,
       47001,  7019, 36040, 25388, 38192, 25135, 40250, 22499, 32988,
       29044, 16077, 41807, 15114, 43893,  9088, 12946, 12169, 29911,
       27371]), [0, 6, 4, 1]), (array([33576,  1922,   839, 20020, 33494,  4878,  8287,  3388,  4639,
       26548, 44538, 47095,  9288, 24553, 17293, 25774, 18928, 25740,
       29970, 33454, 31240, 18574,   339, 49070,  7518, 32122,  8282,
       17670, 25356,  9284,  8261, 46436, 47209, 14026, 47757,  8020,
       48181, 32199,  9391, 47884, 47211, 45043,  7846, 48843, 49552,
       22099, 42348,  5755, 15186, 37632, 23641, 47463, 41115,  4319,
       30140, 31439, 42645,  1849,  9537, 25722, 26836, 11560,  4579,
        4990, 15980,  1198, 22059, 26969, 43455, 39929, 47386, 44532,
       19012,  1280, 35534,  6338, 38600, 15301, 27691, 28659, 45731,
       21186, 29256, 49380, 49327, 15371, 34755, 18083, 14437,  3810,
        4920, 25583, 45108, 35034, 16852, 48189, 23234, 31782, 30196,
       11201, 47388, 24344, 10803, 14932, 28730, 20669,  5439, 29608,
       48555,   173, 36227, 43399, 35762, 26747, 23874, 19756, 40620,
       13872, 12912, 22890, 45215, 38366, 43979,  6958, 42088, 34763,
       22779, 25057,  8252,  4312, 29961, 48011, 23515, 46916,    27,
       41062, 29713, 41789, 13121, 17740, 37749, 25515, 28660, 11770,
       45448,  9665,  6697, 49188, 36954, 27377, 13073, 44706, 31355,
       18131, 22932,   784, 42776, 48207, 19182,  1618, 18569, 26418,
       46708, 16470,  2686, 29857, 41358, 10768, 19334, 24744,  7981,
       32598,  8240, 30931, 35892, 26587, 23689, 22941, 22473, 48161,
       46264, 19757,  8198, 25903, 27094, 45636,  1860,  1993, 43654,
       37921,  5729, 30370, 14875,  5561,  8894, 25870, 40934, 37451,
       47661,  3229, 16510, 22590, 21233,  7140, 49867, 22021, 26143,
       11347, 14033, 31781,  4862, 39757,  5305, 32957,  2916, 32889,
        6140, 39662, 41406, 41759, 30042, 37434, 35640,  7082, 25517,
        5690, 28306, 43599, 30235, 33578, 49661, 29302, 32364, 27633,
       33534,  5411,   670, 12051, 28321, 30523, 33142,  4641, 19652,
       17343,  1928, 10597, 41442, 39742, 26501, 46030, 47046, 46816,
       32347,  1898, 15073, 19466, 21995, 22115, 10584, 27374, 33469,
       49231,  2319, 34592, 25090, 20094, 20618, 12994, 30434, 39408,
       37683,  5258, 23773, 42598, 41795,   968, 42338, 26249, 38142,
       17627, 32374, 23486,  1783,  5710, 30883, 37695, 12985, 45147,
        8238, 49630, 23000, 12163, 38678, 47777, 48773, 29359, 39064,
       36495, 42284, 26733,  4088,  8343, 28262, 26217, 13442,   329,
       14319,  8907, 49109, 10277, 13039, 45127, 21394, 21576,  8498,
        7969, 46709,  7922, 45523, 47056, 14219, 21820, 26166, 35980,
       16582, 22980, 16187, 18802, 25241, 25132, 40010, 21409, 17129,
       29036, 32995,    11, 37100,  3213, 23828, 21857, 15647,  3364,
        5989, 19931, 33775, 36205, 33712, 49310, 32660, 17113, 31350,
       25370, 33921, 44986, 44374,  5812, 29937, 49116, 33585, 31341,
       12690, 45561, 27529, 28152, 23794,  3138, 34900, 44378,  4782,
       19372,  8744, 20576, 24948, 35407,  1927, 28257,  8360, 22625,
       30030,  4513, 10772, 46368,  6164, 13237, 31876, 40570, 18590,
        2119, 39136, 16091, 11414,  3473, 33527, 13547, 43229, 27916,
       27039, 31276, 22457, 24729, 44091, 17913,  3146,  1544,  4554,
        2211, 49648, 30797,   114, 23903,  1900, 28639, 36174, 15850,
       29557, 27098, 27910, 45747, 49103, 41473, 28392, 18938, 34990,
       22901, 27574, 25594, 42651, 24311,   825, 38196, 17616,  3627,
        6235, 33299,  5116, 31924,  9347, 49925, 37980, 47321, 44228,
       43724,  1113, 26422,  2620, 28977, 48579, 34260,  5792, 18339,
       29872, 46228, 29369, 41180, 38194, 10927, 40998, 18431,   388,
        6734, 23627, 14947, 38667,  6398, 21096, 20170,  9906,  9472,
       22031, 16162, 13284, 10687, 38286, 38225, 22669, 11260, 20175,
       40478, 39246, 10259, 29859, 15072, 47275, 48960, 12432,  3054,
       20423, 12364, 23585, 38094, 29486,  9719, 11990, 16025, 42887,
       42238,  3151, 43322, 13950, 29499, 30917,   669, 30794, 29582,
       13815,  8630,  2293, 40156,  6249, 24884, 46319, 30228,  3494,
        4103, 34852, 44963, 42662, 16426, 33667,  7563, 31075, 14090,
       14474, 31227, 24080, 46132, 10593, 41294, 13636,  7536, 37577,
       22673, 40523, 13164, 12108,  3952, 45221, 12406, 16194,  1557,
       26745, 13764, 10415, 16135, 11649, 32186, 39047, 40377, 43926,
       36945,  6212, 32969,  9963, 14744, 13282,  3936, 44705, 15503,
       10404, 47244, 46276, 37775, 49154, 21016, 33458, 18669, 31003,
        4973, 41216, 11862,  5067,  9962, 46415,  4286, 32224, 15715,
       31701, 38995, 40597, 37345, 16689, 14789, 25785, 40798,  2673,
       33666,  9695,  7595, 18014,  5919, 14957, 33370, 41101,  9254,
       42849, 41665,  5051, 27479, 48801, 48488, 29237, 38263, 13231,
       41093, 15131, 37469,  9780, 32229, 34144, 47735, 42312,  7100,
       27461, 40240, 19178, 46476,  8639, 13363, 39473, 18314, 33482,
       49204, 38824,  7365, 46789, 21222, 21081, 47868, 49902, 48018,
        6234, 14113, 31755,  4058, 22946,  2131, 15663, 39505, 20303,
       25692, 17437, 20060, 28407, 49597, 17845, 21708, 25834, 18473,
        7383, 47622,  7984, 44761, 44002, 23231, 21694, 30799,  9776,
       41191, 13102, 31429,  3695, 29635, 16659, 17123, 20373,  9153,
       28387, 23179, 41202,  9544, 28076, 37639, 25941, 48191, 27801,
       16020,  9342, 34119, 44747, 14469, 18690, 48309, 22580, 29254,
       46968, 30156,  6505, 46940, 36372, 18434, 16254, 43567, 15036,
       41100, 23637, 45645, 29404,  1952, 16073, 20054, 15668, 36671,
       45205, 34671,  3507,  4156, 30593,  1238,   674, 32590,   435,
       33971, 42313, 38092, 23022, 49951, 18912, 23421, 35598, 32280,
       26047,  2787,  5178, 31033, 41931, 49355, 28785, 32284, 15196,
       19721, 25383, 13663, 44891, 40116, 39853, 21642, 12333, 28614,
        3560,  9915, 30915, 33862, 23200, 41246,  5940, 20904,   831,
       40757, 17634, 49421, 35605, 12412, 25117,  6851, 15275, 43050,
        9582, 27238,  5076,   375,  3512, 34982, 27272, 49509,  8450,
        4261,  7738,   250, 23504, 40509, 11776, 31079,  5224, 22013,
       24486, 33640, 18885, 25479, 37611, 16711,  1724,  4206, 31395,
       43583, 26452, 26878, 44309, 44499, 31345,   493, 49326, 17409,
       36686, 34830, 20057, 49483, 38596, 27402, 29146, 48279,  9938,
       19226, 44575, 34533, 40518,  5084,   565, 12800, 40024, 18594,
        9407,  7147, 17626,  5947, 20731,  5971, 25459,  9232, 28640,
       16813, 31634, 44876, 37248, 49491, 24075,  8432, 30943, 38610,
       14501, 34902, 27716, 23489, 13217, 15456, 46135, 23107, 14524,
        8371,  9946, 29894, 41869, 34778,  5515,  5841, 12920, 11799,
       43251, 45672,  3212,  5629, 40386, 25278, 45602, 21245, 26450,
        9584, 47980, 18057, 44647, 14851, 19202, 49221, 41602, 21661,
        5666, 21199, 36086, 34813,  8124, 24043, 27404, 44589,  7401,
       26292, 11532, 32679, 42669,   427, 27330, 34400, 21627,  7659,
        7605, 12516, 14182,  3020,  6074, 31348,  5559, 18345, 39820,
        5791,  1973, 34064, 28393, 33194, 43398, 47540, 13843, 16272,
       21020, 25162,  3523, 40713, 15610, 46408, 39132,  6928, 34385,
       48182, 45834, 24580, 34940, 13877, 13849,  8995, 26267, 28626,
       21154,  6589,   599, 10298,   761, 47423, 17726, 16937, 34701,
       25852, 31351, 36058, 18134,  8944, 39349,  2259, 35033,   874,
       10479, 37844, 16576, 38650,  9646, 46240,  5567, 39692, 45988,
       24923, 33261, 27607, 14409, 17995, 13615, 30792, 48799, 26115,
       49409, 32168,  8625, 33561, 38911, 35783,  5834, 19265, 30716,
       27792, 26405, 33198,  6040, 20353, 17334, 22358, 36941, 42335,
       41259,  6736, 13701, 27970, 43984, 29875,  9889, 40422,  4021,
        9442, 49588, 43608, 20616, 21933, 29999, 41948, 44217, 12964,
       17309, 46219, 43609, 45866, 29974, 42785,  8305, 17205, 21808,
       40076]), [5, 7, 4, 1]), (array([ 2818, 19498, 43780, 29418, 43121, 36659, 32407, 28871, 21289,
       37575, 23657, 40875,  4041, 10219,  5140, 42253, 41920, 27900,
       23111, 40015, 28713,  4675, 47967, 42420, 39043, 24627, 21008,
       41905, 22482, 19724, 15215, 13711, 41089, 26016, 31291, 44860,
        2637,  2985, 26222, 47315,  7571, 37413,  2853,  1612,  9434,
       10806, 16396, 31881, 37678, 43903, 39581, 41899, 16046, 17404,
       18796, 17142, 30823,  5776,   428, 15082, 21960, 31956, 22285,
       17176, 41840, 17158, 38406, 26828,  6671,  3977, 47262,   269,
       33568, 22240, 22538, 28010, 29059, 15936,  7708,  1638, 22464,
       13429,   915, 41128, 18878, 27025, 38793, 22127,   408, 32766,
       38017, 36713, 19805, 22182,  4489, 33784, 49360, 47073, 40638,
       31869, 20518, 26561,  1767, 16803, 39930,  7583, 18779, 22623,
        4121, 42473,  7302, 38378,   270, 33483,  2313, 49583, 17289,
       27311, 46280, 22199, 47512, 35218,   867,  5668, 37074, 41415,
       39371, 40976, 24076,   881, 18963, 28747, 41771,  2609, 15901,
       36549, 13407, 20380, 46346, 30220,   672, 49831, 29073, 37040,
       15652, 21614, 34767, 39264, 21887, 44404, 10698, 14970, 35197,
       25094,  4584, 41161, 43882,  8992,  1000, 33813, 26769, 18943,
       21745, 36250,  6903, 41363, 20172,  3846,  2308, 48816, 26101,
        2890, 42994, 27975, 47697,  1078, 40043, 34223, 21170, 26511,
       23908, 19086, 42058, 29000,  2281,  9229, 26622, 48606, 19876,
       37070, 46654, 38481, 37762, 27755,  2829, 28783, 22543, 39449,
       33538, 34390,  1043, 12568,  9880, 17201, 15252, 11139, 15495,
       49057, 10762, 40819, 15027,  4289, 26012, 49208,  9511, 25736,
        9028, 25433, 28390, 40166,  6258, 44605, 23840, 36031, 35019,
       42139, 27815, 14112,  6364,  1940,  2671,  5203, 47435, 36797,
       19824, 28514, 39567, 24027, 49783, 21401, 16078, 31233, 34402,
       15249,  9432, 15031, 13705,  6984, 26506, 10309,  4003, 13157,
       42617,  6238,   190, 44059, 39613, 26623, 47713, 40070, 31462,
       46453,  1479, 14054, 17167, 27352, 35896,  6667, 30087, 18644,
        4272,  1512, 28016, 30639, 44416, 39336, 20762, 43408, 22340,
        9068, 26440,  3305,  9960, 14728, 43705, 12544, 39702, 30601,
       16700, 20942, 10876, 10139, 13233,  7455, 49253, 26105, 17130,
        4222, 35671, 23669, 27410, 29600, 47378, 21028, 36803, 23883,
       36744, 39075, 43175, 39139, 17617, 28768, 45617, 34615, 21541,
       14710,  3482, 33124,  4828,  1914, 48374,  9648, 20873, 36892,
       25783, 15322, 28134,  5600, 15630, 27153,  9567, 22518, 46068,
       48959, 27618, 21998, 42371, 21710, 48805,  3430, 25632, 31037,
       33225, 48081, 37710, 34468, 26538, 10688, 42270, 23960,  5687,
       15951, 37335, 25047, 28707, 32465,  7287,  8229,  5855,  1357,
       18593, 40270,  2721, 35726, 36108, 41240, 23872, 48641,  1261,
       39802, 23148, 43699, 15699, 27027, 18295,  6459,  4916,  1721,
       13355, 10016, 21568, 36255,   987, 35778, 31938, 29362, 39916,
       33119, 47791, 37064, 45635, 22862, 36876,  1289,  3681, 22440,
       14049, 13684, 12128, 18307, 47391, 45608, 28985, 32207, 44362,
       41281, 22147,    62, 33888, 17854,  4287, 30483, 45115, 44570,
       35909, 12319, 33651,  4474, 22600, 38808,  1702, 48457,  3313,
       30488,  3770, 13836, 13883, 46392, 32974, 21716, 23707, 40881,
        4794,  8678,   901, 19748, 23845, 20962, 45784,  7443, 10004,
         259, 38339, 14866,  3765, 34825, 48874, 46386, 35828, 41647,
       34248,  7535, 20740, 37641, 29734, 21830, 23399, 19768,  9269,
       41403, 30210, 15914, 24270,  2663, 49567, 41941, 10763, 19336,
        8831, 39350,   430, 26108, 48943, 34527, 13720, 11993, 23011,
       28450, 17252, 33570, 38322,  5904,  8384, 32281, 35628, 23818,
       12286, 28295, 41187,  8968, 30660,   971, 37483, 10245, 22218,
       27436, 21164,  2640,  4936, 46617,  7218,  7751, 23279, 42434,
        3296, 18627, 44270, 43549, 22050, 41265,   336, 19180, 35151,
        7110, 35443, 16749, 23133, 49352, 26512, 16636, 29471, 15625,
       15155, 31053, 33408, 32300, 24067, 11045, 11254, 30967, 49981,
       31549, 15773, 30815, 47611, 14649, 41554, 28200,  2488, 31939,
       40707, 41878, 33480, 47144, 12921,  1624,  2842, 49685,  2691,
       36523, 39977, 28711, 17145, 24659, 43239,    28,  7634, 15958,
       46077, 44450, 14921, 22820, 47591, 43586,   979, 15643,  2703,
       37466, 43238, 38869, 20810, 48235, 37214, 37987, 37977, 11997,
       42967, 31547, 48505, 18512, 22749, 24664, 12997, 18550, 23817,
       37234, 28219, 13005, 41788, 15409, 46049,  5844, 26351, 15864,
       43460, 10489, 15737, 45255, 13772, 37154, 18841, 47126, 40285,
       22677,  5998, 47374, 48969, 32017, 37960, 40111, 22750, 30125,
       20253, 43529, 40981, 28084, 43682, 46884, 15140, 34255, 19426,
       36965, 37241, 22203, 26360,   526, 20173,  8279,  1595,  9550,
       47855, 20769, 39752, 17402,  8361,  8504, 46185, 35564,  3957,
       34193,  8541,  6415, 47344, 49292, 42629, 26953, 20343,  1813,
       13466, 27535,  7382, 45729, 29670, 17150, 30425, 28228, 29329,
        8551,  8233,  2155, 36160, 16897,  5957,  5187, 26114, 39067,
       23654, 40486, 39596, 23671,  4924, 47384, 27417, 11810, 45712,
        5790, 28260, 47664, 36428,  2442, 12124, 32519, 44942, 37875,
       44162,  8939, 10237, 28769, 29376, 20485,  2461, 13855, 39005,
       43746, 39467,   372, 31476,  2327, 14787, 10447, 34784, 25017,
       19307,  4527, 30869, 37805,  7119, 19990, 25147, 24658, 45764,
       29361, 42643, 48860, 28892, 22985,  3383, 37288, 44563, 27860,
       34744,  8263,  4989, 24241, 28744, 29278, 36976, 11957,  1526,
       28903,  5501, 39565, 12261,  5931, 30681, 42769, 32929, 19746,
       45226, 25429, 41822, 16058, 19452, 15256, 42938, 18502, 24799,
       49390,  1974, 33209, 29121, 46507, 25817, 47519,  2339, 37512,
       48000,   617, 14659, 34832, 27506, 42113, 34862, 36594, 15779,
       21585, 21333, 17515,   261, 14871, 28784, 31899,  7706, 14859,
       23338, 33063, 16286, 32161, 29330,  7077, 24566, 44412, 15512,
       35105, 10100,   227,  1052, 49381, 45977, 31890, 29633,  4609,
       39256, 11890, 22450, 47968,  3485, 36938, 42292, 28958,  8221,
        1631, 42065,  4101, 15740, 32736,   168,  8475, 36388, 40046,
       23598, 11580, 22882, 16250, 35145, 29314, 25608, 16850, 49616,
       20777, 14713, 25700, 26059, 15891, 26737, 19616, 38278, 41289,
       11664,  2769, 38420,  6295, 48617, 40537,  8175,  4230, 19710,
       33114,   699,   396, 42281, 16275, 19399, 36672, 11683,  1571,
       36751, 36033,  2390,  8812, 19867, 20197, 48995, 15205,  6429,
        6191, 48636, 37269, 24284, 29311, 42918, 30014, 32603, 20214,
       43447, 34145, 40576, 28347,   311, 22165,  1410, 46731, 18926,
       38974, 20510, 26578, 39109, 44541, 45704, 31512, 38725, 41279,
       19628, 49894, 16855, 48970, 21437, 14900, 41794, 17856, 29325,
       25652, 39204, 49705, 20783,  5959, 10491, 11791, 18063, 43217,
       38892,  6272, 20988, 37995,  6120, 10393, 48740, 20914, 11427,
       17755,  8294,  3386, 24398,  2039, 28906, 43641, 29188, 49494,
       13526, 34068, 38571, 26637, 45580,  2268,  1694, 45859, 42211,
       26794, 15832,  2280,  9549,  7827, 23662, 21153,  4423, 12923,
       25150, 30245,  5759, 41322, 38549, 29285, 32410, 42734, 34426,
       43847,  6371, 14722, 22529,  6701,  2173, 22586, 18495, 10748,
       39240,  3840, 31107,   947, 11579,  6899, 37203, 14067, 46259,
       46359, 28539, 43132, 30486, 22163, 19500, 35454,  8875, 15496,
       21073, 12774, 33327, 23311, 41356, 19609, 11698, 39974, 12622,
       20874, 48845, 10953, 23578, 49121, 36085, 45011, 49436, 18206,
        2511, 15704, 43605,  9765,  8571, 12373,  3414,  2227, 36872,
        3059]), [9, 8, 4, 1]), (array([ 6304, 37804, 27012, 40216, 43318, 23884,  9846,  6309, 25738,
        3864, 20721, 17440,  3813, 15453, 26707,  4197, 47851,  2127,
       29685, 20447, 10869, 41892, 32250,   846,  8210,  6607, 40165,
       41826, 44173, 15421, 19820, 16619,  9293, 11998, 11477,  6014,
       40246, 37973, 43462, 37662, 11387, 16366, 46825,  7753,  7086,
        1608, 46058,  6405, 34099, 41285,   998, 13700, 16301, 15603,
        9034, 11564, 22420, 38304, 12428, 28953, 25490, 15383,  4558,
       27233, 35124,  1510,  8319, 30868, 33133, 29810, 42752, 16547,
       22030, 49970, 24726,  1958, 43922,  4674, 18814, 37770,  6619,
       12111,  8356, 13001, 30817, 38992,  1057,  7432, 35092, 22620,
        9024, 18127, 29012, 14368, 31178, 18998, 28864, 21686, 43843,
       46121, 21053, 22913, 14108, 26849, 10193, 24749, 47100, 10746,
       41412, 35128, 25882, 32451,  1539, 40584, 12174, 42930, 39108,
       27407, 15974, 10575, 47901, 23376, 11231, 13349, 20252, 13013,
       11120, 22406, 41016,  8781,  6913, 30266,  4402, 49079,  1556,
       34984, 44263, 27909, 20329,  5321, 47554, 38679, 21085, 19126,
       32297, 46542, 31179, 32846, 22919, 14118, 28253, 25281, 14070,
       32541, 33092,  2447,  2011, 20820,  1895, 23498, 22793, 49741,
       46182, 41229, 10958, 29215, 43411,  7906, 39294, 15545,  5455,
        2353, 45502, 11047, 39885, 11714, 27275, 48907, 29705, 20770,
       46215,  3571, 26357,  5823, 36211, 29520, 33267,  3905, 22294,
       36422, 32936,  8645,  9830, 11105, 43328,  7671,  6649, 46157,
       14089, 29839, 48040, 14171, 14703, 40232, 26044, 24060, 38364,
       17615,  1961, 17243, 45646, 16070,  3194, 24081, 48999, 11724,
       37906, 15053, 16295, 23145, 41917, 10942, 17047, 33463,  9720,
        4025,  5520, 44550, 22662, 27114,  5218, 22411,  1573, 38781,
       35323, 15247, 38932, 46417, 46605,  6540, 47722, 17018, 26559,
       45382, 38634,  9183, 23594, 18570, 21510, 42744, 14472, 27738,
        7151, 37255,  2784, 32899, 11137, 20558, 15977, 37686, 18143,
       25200,  9940, 44644, 32377, 33474, 40372, 33083, 30632, 11286,
       16833, 48733, 14595, 32807,  6626, 16739, 26425, 21099, 14811,
        7466, 12583, 17981, 45951, 39499, 20651, 31621, 36423, 38349,
       31040, 38038, 37856, 20673, 43826,  8055,  6616,  2957, 11454,
       16215, 20038, 30940, 10926, 40924, 28664,  2789, 14619, 10440,
        4616, 16462,  1258, 16581,   802, 11270, 34307, 43990, 34488,
       25101, 38592, 15314, 33519, 14785,  4038,  1107, 14157, 33054,
       35653, 30572, 24773, 38801, 44535, 20073,  3963, 14383, 38216,
        3569, 29625, 19280, 14285, 10671,   912, 14601,  4168, 15398,
       15789,   990, 28667, 26386, 47171, 18163,  3484, 37373, 41024,
       13934, 33183,  7807, 47936,  1475, 13487,   885,  2514, 17693,
       27239, 42227,  2004, 30437, 11003, 24927, 29779, 33175, 33350,
        6028, 30786, 26999, 39368,  7521, 20009, 30631, 27186, 17302,
       43244, 31884, 23673, 40398, 27546, 25440, 40403, 31454, 41884,
       11843, 42894, 34203, 13290,  7330, 27948, 42428, 47710,  4511,
       48779, 30665, 17834, 36502, 26976, 24064, 39847, 15443, 14106,
       40348, 33498, 40217,  9803, 46564, 43572, 21262, 29180, 34222,
       48169, 22511, 26588, 46819, 11884, 16446, 18204, 14229, 27498,
        6334, 26176, 43823, 33531, 32409,    90, 25982, 49914, 13814,
       11779, 10412,   522, 23062, 15369, 32763, 32480, 15589, 44634,
         804, 38459, 11089, 13273, 34873,  8706, 11587, 32057, 34885,
       21500, 13081,  2191, 27865, 47368, 27685, 33842, 36382,  5647,
       44879, 48414, 25030, 39062,  1798, 28834, 46575,   283,  3558,
       38182, 48092, 48608, 11443, 46214, 27594, 25398, 23453,  4237,
       26308,  6177, 43287, 10378,  5219, 31361,  4656, 17955,  8219,
        4634, 17273, 24623, 45533, 15929, 16609,  5384, 43692, 21063,
       26630, 19615, 33776, 47584, 12060,  2450, 32094, 22619,  5704,
        6943, 18099, 23634, 28174, 47908, 32478, 22743, 16908, 45133,
        8688,  7903, 14134, 40530, 40794, 28885, 33383, 45598,  5706,
       45786, 41625, 38354, 47795,  5298, 24221, 48195, 40098, 27017,
        5676,   621, 46153, 26497, 29070, 36142, 26329, 19059, 15126,
        9018,  1284, 27372, 14786, 30522, 34969, 20280,  8913,  8631,
        1704,  9750, 21052, 10604, 45022,  7043, 45110, 26943, 49687,
        6652, 12717,   477,  1348, 14853, 35378,  4137, 49586, 39945,
       36552,  1657, 30725, 21693, 33099,  9592,  4642, 41770, 43420,
       16105, 16880, 23943, 21794, 17450, 10299, 32523, 48179, 31306,
        8701, 11845, 17707, 23076, 48360, 30614, 48605, 41608, 39209,
       40301,  1984, 43897, 19704, 26882, 39878, 29852,  9364, 47576,
       46145, 27531, 41077, 39293,  9500,  1396, 45396, 21582, 45410,
        2190,  5225, 33221, 45756, 21188, 44994,  9507, 35395,  3556,
       14726,   959, 25779, 41103, 31954, 17613, 26886, 33215,  1937,
       45293,  9041, 27259, 18409,  9285, 14798, 36345, 24265, 43494,
        1339, 29128, 15734,  5829, 15454, 20341,  3283,  4846,  8987,
       40036, 22274,  7670, 49475, 37528,  9273, 19528, 17260, 43222,
       42695, 34776, 41097, 34727, 20629, 24925, 32494,  8137, 22385,
       20967, 22369,  3046, 38728, 16672,  5227, 37677, 47232, 48796,
       21649, 33080, 10500, 36071, 36731,  8114, 30077,  9779,  4885,
       11794, 47147, 44750, 47120, 32201, 20435, 18197, 44061,  2528,
       32253, 16263, 25798, 24572, 26975, 42907,  6344,  7394, 25142,
        3509, 40219, 17039,  5880, 48429, 46591, 47796,  2105,  7957,
       35457,  8066, 37741, 31898, 32029, 12975, 39946, 15238, 14635,
        9250,  4199, 15909, 16101,  9749, 18185, 15730, 27303, 44283,
       25643,  2297, 18264, 26233, 23739, 39541,  1406,  1640,   381,
       33688, 27762,  3913, 40077, 30243, 16525, 35050,  6049, 44525,
       17552, 48945, 31588, 37588,  5769, 38929, 27584, 25300, 35694,
       33390,   524,    99,  8085, 28089, 36331, 19575, 26424,  3836,
        3103, 45856, 18458,   301, 31102, 13108, 31445,  1240, 48669,
       23533, 28559,  7300,  8556, 34530, 31251, 30039, 15662, 39277,
       12741, 49769,  8001, 20840,  5897, 14593,  6375, 10403, 16023,
       36236, 33625, 36703, 43145, 19063, 33650, 49820, 17030,  4764,
       44818, 42707, 20605, 14519, 11007, 20442, 14598, 33126, 33624,
       44415, 16834, 42606, 12894,  2958,  4299,  2656, 48375, 41984,
       17621, 41888, 25596, 33050, 16964, 34263, 38078, 26741, 16588,
        8690, 30471, 17732, 47178, 29029, 14643,  9085, 22697, 16962,
       31523, 26746,  6779, 24268,  4942, 31329, 45001, 31830, 13002,
       17860, 22745,  6241,  8893, 47060, 17268, 22303, 41579, 31245,
       49230,  7540,  7375, 24620, 44725, 27636,  1494, 29394, 31587,
       43195, 33399, 31711,  3009,   226,  4578, 42031, 49447,   126,
       22545, 28394, 11817, 13037, 42121,  4464, 25377, 46323, 11242,
       14662, 45105, 11829,  4661, 23546, 13470, 42341,  6988, 12893,
       30182, 18927, 33615, 19301, 45946,  7214, 25910,  1251, 27194,
       37239,  5635, 28245, 46679, 32348, 16745, 32737, 19983, 24068,
        7503, 25961, 44971,  3233, 14964, 26471,  6495,  8779, 45715,
       42991, 20273, 23142, 40749, 32311, 38902, 30062, 13444, 45968,
       21888, 46730, 45097, 13311, 33598, 19880, 12560, 23088, 28957,
        8090, 26230,  1548, 30244, 41563, 22689, 41736, 25827, 29230,
        9012, 37755, 37320, 33237,  6456, 47453, 21893, 24561, 38682,
        5975, 49875, 35878, 46928, 22145,  4607, 41862, 26074, 21530,
       34791, 37253,  2067, 27228, 34618, 19484, 33569,  2597, 32042,
       13899, 24157,  8201,  2320,  4352, 30165,  9318, 33646,  9459,
       18223, 19242, 21230, 17274, 13647, 49392, 41739, 41026, 38838,
       17457, 28344, 27773, 41701, 47208, 30086, 33196, 39824, 41378,
       40762]), [3, 2, 4, 1])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.255, 0.04439314603805542)
DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.245, 0.04447525298595428)
DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.244, 0.045177888870239255)
DC 3, val_set_size=1000, COIs=[3, 2, 4, 1], M=tensor([3, 2, 4, 1], device='cuda:0'), Initial Performance: (0.209, 0.04494925320148468)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {0, 6}
D07: 1000 samples from classes {0, 6}
D08: 1000 samples from classes {0, 6}
D09: 1000 samples from classes {0, 6}
D010: 1000 samples from classes {0, 6}
D011: 1000 samples from classes {0, 6}
D012: 1000 samples from classes {5, 7}
D013: 1000 samples from classes {5, 7}
D014: 1000 samples from classes {5, 7}
D015: 1000 samples from classes {5, 7}
D016: 1000 samples from classes {5, 7}
D017: 1000 samples from classes {5, 7}
D018: 1000 samples from classes {8, 9}
D019: 1000 samples from classes {8, 9}
D020: 1000 samples from classes {8, 9}
D021: 1000 samples from classes {8, 9}
D022: 1000 samples from classes {8, 9}
D023: 1000 samples from classes {8, 9}
D024: 1000 samples from classes {2, 3}
D025: 1000 samples from classes {2, 3}
D026: 1000 samples from classes {2, 3}
D027: 1000 samples from classes {2, 3}
D028: 1000 samples from classes {2, 3}
D029: 1000 samples from classes {2, 3}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.06242937207221985) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06935057532787323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.10054781334102154) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09110854172706603) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.44, 0.07027108192443848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.07565721029043197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.10466695122420788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.323, 0.11809220957756042) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.08399075710773468) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.405, 0.08414981603622436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.10775397792458534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.355, 0.1430300070643425) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.09190552899241447) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.10456353434920311) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.15356273053586483) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.356, 0.16536943846940994) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.11256972487270832) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.11904050388932227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.17955271257087588) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.354, 0.1911297039538622) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.12555305766686797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.392, 0.12016960312426091) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.20252362125553192) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.376, 0.1672702697366476) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.13640131926350296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.12200692869722843) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1993113804385066) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.384, 0.1897293770611286) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.1368831439372152) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.1383049213439226) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.22230591307021677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.387, 0.205445771753788) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.15904230438452213) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.13590771255642176) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.251293695660308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.379, 0.1797317993193865) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.15739924981538206) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.422, 0.15744038719683887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.24946794794872404) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.2138650175333023) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=1500, COIs=[1, 4], M=tensor([0, 1, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.7866666666666666, 0.020253663380940756)
DC Expert-0, val_set_size=500, COIs=[0, 6], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.938, 0.005567504147067666)
DC Expert-1, val_set_size=500, COIs=[5, 7], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.844, 0.013536606922745704)
DC Expert-2, val_set_size=500, COIs=[8, 9], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.912, 0.008059985995292663)
SUPER-DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x72f20c7cdd00>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f288ef9f40>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f20c4f50d0>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f288d99c10>, <fl_market.actors.data_consumer.DataConsumer object at 0x72f28020b220>]
FL ROUND 11
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.003847681143786758) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.852, 0.012479427978396416) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.008226505281403661) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.16791463382542132) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9153333333333333, 0.007022676706314087) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.623, 0.042906414300203324) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.08409164267778396) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.636, 0.04014549806714058) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004041791361523792) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.012117560110986234) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.914, 0.007320915885269642) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.18187071216106415) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.908, 0.007502878208955129) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.61, 0.05464022673666477) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.543, 0.06410713455080987) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.059894803375005724) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0041356230232631784) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.848, 0.01427882405370474) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.0073795332349836824) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.384, 0.17796269308030604) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9366666666666666, 0.007771758066543649) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.552, 0.06523145939037203) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.09504102100431919) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.526, 0.08711566522717476) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.003926958607509732) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.013775676608085633) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.009341772452928125) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.39, 0.18586286914348601) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9273333333333333, 0.008398363390006125) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.542, 0.08070279396697878) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.506, 0.11658919796347618) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.511, 0.08665287590026856) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004250137723516673) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.011458096139132977) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.00790807881206274) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.19636187960207463) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.00967427969009926) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.568, 0.08521790418960154) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.12569867165014148) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.553, 0.06772018916718661) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO5', '(DO3']
DC Alliance --> ['(DO2']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0042697460019262504) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.013089602798223495) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.006684655111283064) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.1889429318755865) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.00789181557825456) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.06950444707274436) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.10604448545724153) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.05213676621764898) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0042236464965390045) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.01225137310475111) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.006926236109808088) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.19248264494538309) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.00726333688168476) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.06831433653831481) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.13212380550056696) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.589, 0.05848917078971863) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004280983859673142) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.013157947883009911) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.007591322207823396) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.18859956175088882) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.005599909785358856) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.06900576259195805) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.12079699754714966) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.548, 0.10910616103187204) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004842857890587766) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.012756120033562184) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.008765105465892702) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.18596020790934562) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9393333333333334, 0.005701066291580598) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.614, 0.07002055843919516) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.497, 0.10172639000415802) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.615, 0.051779365867376324) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004326252298924373) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.013767411757260562) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007891500450670719) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.1995002319663763) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9306666666666666, 0.011398860960500315) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.06504194146394729) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.13381945761293174) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.519, 0.08455879624560475) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Expert-0 --> ['(DO2']
DC Expert-1 --> ['(DO4']
DC Expert-2 --> ['(DO1']
DC 3 --> ['(DO0', '(DO3']
DC Alliance --> ['(DO5']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0039905990513507275) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.014773073993623256) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.0074698929209262134) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.19374949726462365) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9453333333333334, 0.005852358254293601) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.06478904560953379) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.10002275658398867) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.562, 0.08661769491434097) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005680075647484045) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.012428722254931926) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.00868667169753462) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.19630836768448354) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9366666666666666, 0.01006233569731315) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.07970046450197697) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.475, 0.14232851550914347) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.1035879994854331) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0049370772036490965) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.86, 0.013269611306488514) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.010705246904864907) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.395, 0.1943335898220539) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.007594833188652047) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.06956687518209219) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.1237685352191329) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.493, 0.12622132801171393) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005626140659136581) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.014474653474986552) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.00880426334310323) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.1942524548768997) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9593333333333334, 0.00864979760896919) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.578, 0.09140741817094386) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.504, 0.140980338960886) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.528, 0.09632812710106373) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004389177928038407) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.014474489171057939) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.916, 0.009176336150616407) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.20628052288293838) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.96, 0.0058763757951674055) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.648, 0.05883897069841623) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.51, 0.10143321166932583) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.0809211077541113) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Expert-0 --> ['(DO3']
DC Expert-1 --> ['(DO5']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO0', '(DO1']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004150633720331825) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.01682855251431465) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.008425471927039326) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.1897297247350216) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.0055960046265584725) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.656, 0.053430247336626054) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.544, 0.08687428224086761) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.569, 0.08783422752469779) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004881495492416434) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.014530079446732997) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007089244501665234) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.1952621847689152) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9586666666666667, 0.006371227651179652) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.06234472940862179) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.09999874034523965) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.572, 0.0864538369551301) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004790908636030509) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.85, 0.016454683870077133) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.008253489972092212) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.391, 0.18681876353919505) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.007730355891673146) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.06559072494693101) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.13851818800903856) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.559, 0.07681870872154832) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004811840591908549) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.015209410946816207) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.0070821693670004605) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.403, 0.17956819808483124) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.00568689592089504) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.701, 0.04570048639178276) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.592, 0.07109744427353144) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.549, 0.08435361970961094) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005368487735788221) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.01426409960910678) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.007798607435077429) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.1869236236438155) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9346666666666666, 0.009555411100508839) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.0562776116579771) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.496, 0.12590935208648443) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.07687915919721126) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO0']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005664593163084646) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.014858750207349659) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.008595803450793029) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.399, 0.17318007181584835) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.009101755365915778) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.658, 0.05284174171276391) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.522, 0.10740105171129108) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.522, 0.09611810853332281) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004726397097532754) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.01229113805294037) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.009723104499746114) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.19075863187760114) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9533333333333334, 0.0068951478819944895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.716, 0.03909710502624512) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.09868203625082969) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.54, 0.08818919777683913) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0039824969950132075) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.014801184624433517) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.904, 0.010360986157786101) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.1849568217024207) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.007922950500855222) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.669, 0.045903138995170595) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.099526559965685) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.53, 0.07902128844847903) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.00446139996905913) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.014880801651626824) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.008102527680806815) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.17181476548314095) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9506666666666667, 0.007238551962480415) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.655, 0.05114454486966133) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.09253654158487916) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.598, 0.055659809451550246) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005324877392737107) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.013606543742120267) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.009285052169580012) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.395, 0.20635307467728853) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.01905477111624676) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.09107754266640404) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.484, 0.15689046653028343) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.568, 0.07062129493197426) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Expert-0 --> ['(DO0']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO3']
DC 3 --> ['(DO5', '(DO2']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003948806057800539) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.01334868810325861) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.902, 0.009741062085144222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.18422666417062283) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.007654955913603771) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.669, 0.060545794412493706) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.09429680480062962) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.546, 0.06820809826627373) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004974864718533354) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.01340842953324318) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.008500526686199009) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.1962214486449957) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.962, 0.008935708173446376) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.06943033240735531) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.515, 0.1065296574011445) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.553, 0.06653892423585057) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004951728701620596) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.01220189156383276) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.009090961908455939) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.2049194903448224) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9386666666666666, 0.017208950664149597) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.636, 0.060447539824992416) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.13554383882135151) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.543, 0.06442530663823709) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006019399155542488) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.013455868624150753) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007789981831330806) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.19913368368148804) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9546666666666667, 0.01033494707199073) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.666, 0.050224379800260066) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.506, 0.10718843222036958) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.57, 0.062257656461093575) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004241532897169236) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.011296955034136772) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.007851703158812597) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.20247165989875793) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.009751124230164956) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.08191006337851287) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.499, 0.15408779985271395) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.584, 0.052411697644740345) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Expert-0 --> ['(DO5']
DC Expert-1 --> ['(DO1']
DC Expert-2 --> ['(DO2']
DC 3 --> ['(DO3', '(DO0']
DC Alliance --> ['(DO4']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004276312539564969) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.012506277613341808) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.009215789132053032) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.19538852375745774) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9386666666666666, 0.01367969154844559) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.647, 0.05454343465715647) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.538, 0.09352891023829579) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.532, 0.0791387938298285) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0048826510676299225) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.01437948778271675) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.009430659875040874) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.2096841803714633) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.012127650675955616) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.651, 0.05634785621985793) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.505, 0.1001481172516942) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.06578860289324075) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004491323423280846) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.015518425548449158) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.008907439115457237) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.21147712268680335) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9493333333333334, 0.014730451426779225) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.06738089134916664) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.10747866490948946) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.549, 0.07886201689951122) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004125485762371681) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.013475437887012958) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.00850625207531266) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.20042242304980754) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9486666666666667, 0.013650480463620624) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.638, 0.06281609321385621) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.0989569765329361) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.544, 0.09243208476901055) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0036637941297667565) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.014918070893734694) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.916, 0.008527837830595672) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.401, 0.20109627268463373) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9473333333333334, 0.011832897842702474) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.679, 0.04101477404683828) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.061988353848457335) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.54, 0.07787463167868555) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Expert-0 --> ['(DO4']
DC Expert-1 --> ['(DO3']
DC Expert-2 --> ['(DO5']
DC 3 --> ['(DO2', '(DO1']
DC Alliance --> ['(DO0']
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0047786698519485075) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.014610475029796361) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.00893207938829437) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.23353527803719043) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9633333333333334, 0.00671921444390955) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.722, 0.030611154422163965) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.06761846463754773) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.55, 0.07285837307851761) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005739542003535462) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.011729036897420882) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.007777987674577162) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2195804446041584) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9573333333333334, 0.006024744763451357) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.712, 0.035734874680638316) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.08507925514131784) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.588, 0.056572203047573565) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.00509744052530732) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.012848024472594262) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.007658283735625446) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.21116627154499293) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9473333333333334, 0.00966971345615093) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.667, 0.05213667917996645) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.1035650593265891) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.06804015063401311) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0048576203279808395) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.013105181239545346) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.009565371081465854) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.4, 0.2279284894913435) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9606666666666667, 0.009228147383350005) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.045762444876134394) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.565, 0.08478810976445675) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.536, 0.08338946250360459) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004941964177975024) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.015373565837740898) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.007930327791487798) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.2200767252892256) --> New model used: True
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9526666666666667, 0.0109446215951466) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.669, 0.04457183756679296) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.0798373880982399) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.565, 0.0707836461160332) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.255, 0.04439314603805542), (0.276, 0.06242937207221985), (0.44, 0.07027108192443848), (0.455, 0.08399075710773468), (0.465, 0.09190552899241447), (0.459, 0.11256972487270832), (0.464, 0.12555305766686797), (0.461, 0.13640131926350296), (0.475, 0.1368831439372152), (0.461, 0.15904230438452213), (0.469, 0.15739924981538206), (0.623, 0.042906414300203324), (0.61, 0.05464022673666477), (0.552, 0.06523145939037203), (0.542, 0.08070279396697878), (0.568, 0.08521790418960154), (0.592, 0.06950444707274436), (0.632, 0.06831433653831481), (0.627, 0.06900576259195805), (0.614, 0.07002055843919516), (0.609, 0.06504194146394729), (0.62, 0.06478904560953379), (0.592, 0.07970046450197697), (0.604, 0.06956687518209219), (0.578, 0.09140741817094386), (0.648, 0.05883897069841623), (0.656, 0.053430247336626054), (0.644, 0.06234472940862179), (0.629, 0.06559072494693101), (0.701, 0.04570048639178276), (0.632, 0.0562776116579771), (0.658, 0.05284174171276391), (0.716, 0.03909710502624512), (0.669, 0.045903138995170595), (0.655, 0.05114454486966133), (0.592, 0.09107754266640404), (0.669, 0.060545794412493706), (0.628, 0.06943033240735531), (0.636, 0.060447539824992416), (0.666, 0.050224379800260066), (0.628, 0.08191006337851287), (0.647, 0.05454343465715647), (0.651, 0.05634785621985793), (0.639, 0.06738089134916664), (0.638, 0.06281609321385621), (0.679, 0.04101477404683828), (0.722, 0.030611154422163965), (0.712, 0.035734874680638316), (0.667, 0.05213667917996645), (0.67, 0.045762444876134394), (0.669, 0.04457183756679296)]
TEST: 
[(0.2515, 0.043328432649374006), (0.2885, 0.05995646780729294), (0.4465, 0.06739776784181595), (0.4545, 0.0804382722377777), (0.46875, 0.08793868029117584), (0.46075, 0.10819000673294067), (0.46475, 0.12106116461753845), (0.46375, 0.1310386021733284), (0.475, 0.13236147272586823), (0.4615, 0.15334183567762374), (0.46875, 0.1515594100356102), (0.63575, 0.03986751249432564), (0.629, 0.0505862296372652), (0.57075, 0.06331710393726826), (0.53975, 0.0801052705347538), (0.5605, 0.08482692727446556), (0.58025, 0.0686833410859108), (0.63125, 0.06881481187045574), (0.61525, 0.06806477129459382), (0.61875, 0.06677181316912174), (0.6165, 0.06513042996823788), (0.624, 0.06286453288793564), (0.59, 0.08180458058416844), (0.601, 0.06676085004210472), (0.57725, 0.08906769478321075), (0.658, 0.05656105794012547), (0.64625, 0.05447123797237873), (0.64425, 0.06103437225520611), (0.60275, 0.06884567487239837), (0.6865, 0.045380160622298715), (0.6185, 0.05805411994457245), (0.66225, 0.052809613518416884), (0.705, 0.03925985363125801), (0.67825, 0.0413408300280571), (0.639, 0.052459643319249155), (0.57225, 0.09376976510882377), (0.65325, 0.06322079709172249), (0.618, 0.06864179876446724), (0.632, 0.061915184572339056), (0.65675, 0.05183396147191525), (0.6115, 0.08582036778330802), (0.6555, 0.05269519842416048), (0.636, 0.058147335022687915), (0.6275, 0.06715012829005718), (0.6305, 0.061253903687000275), (0.68975, 0.03693329315632582), (0.71525, 0.02944208785891533), (0.7095, 0.03358900763094425), (0.6565, 0.051325455129146574), (0.66675, 0.043947473861277106), (0.6745, 0.042094875320792195)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.88      0.57      0.69      1000
           1       0.82      0.90      0.86      1000
           4       0.49      0.89      0.63      1000
           6       0.79      0.34      0.48      1000

    accuracy                           0.67      4000
   macro avg       0.74      0.67      0.66      4000
weighted avg       0.74      0.67      0.66      4000

Collaboration_DC_1
VAL: 
[(0.245, 0.04447525298595428), (0.25, 0.06935057532787323), (0.352, 0.07565721029043197), (0.405, 0.08414981603622436), (0.389, 0.10456353434920311), (0.376, 0.11904050388932227), (0.392, 0.12016960312426091), (0.382, 0.12200692869722843), (0.384, 0.1383049213439226), (0.416, 0.13590771255642176), (0.422, 0.15744038719683887), (0.471, 0.08409164267778396), (0.543, 0.06410713455080987), (0.493, 0.09504102100431919), (0.506, 0.11658919796347618), (0.469, 0.12569867165014148), (0.479, 0.10604448545724153), (0.491, 0.13212380550056696), (0.484, 0.12079699754714966), (0.497, 0.10172639000415802), (0.475, 0.13381945761293174), (0.507, 0.10002275658398867), (0.475, 0.14232851550914347), (0.489, 0.1237685352191329), (0.504, 0.140980338960886), (0.51, 0.10143321166932583), (0.544, 0.08687428224086761), (0.508, 0.09999874034523965), (0.489, 0.13851818800903856), (0.592, 0.07109744427353144), (0.496, 0.12590935208648443), (0.522, 0.10740105171129108), (0.524, 0.09868203625082969), (0.524, 0.099526559965685), (0.532, 0.09253654158487916), (0.484, 0.15689046653028343), (0.535, 0.09429680480062962), (0.515, 0.1065296574011445), (0.508, 0.13554383882135151), (0.506, 0.10718843222036958), (0.499, 0.15408779985271395), (0.538, 0.09352891023829579), (0.505, 0.1001481172516942), (0.529, 0.10747866490948946), (0.508, 0.0989569765329361), (0.576, 0.061988353848457335), (0.562, 0.06761846463754773), (0.545, 0.08507925514131784), (0.524, 0.1035650593265891), (0.565, 0.08478810976445675), (0.54, 0.0798373880982399)]
TEST: 
[(0.23225, 0.043421665340662004), (0.25, 0.06652063864469528), (0.345, 0.07260621449351311), (0.4005, 0.08074362868070603), (0.388, 0.10054817044734955), (0.381, 0.11419055730104447), (0.39725, 0.11531978017091751), (0.3775, 0.1179304256439209), (0.38325, 0.13432818323373794), (0.41475, 0.1329578117132187), (0.41625, 0.15371146613359452), (0.483, 0.08090510621666908), (0.56625, 0.062273224353790284), (0.50825, 0.08969521242380142), (0.49775, 0.11457214257121086), (0.4905, 0.11983082807064056), (0.505, 0.09818166160583496), (0.507, 0.12367879992723466), (0.50025, 0.11422673326730728), (0.508, 0.09533783295750618), (0.494, 0.12653835120797158), (0.53125, 0.09434657847881317), (0.494, 0.13624453377723694), (0.50675, 0.11794721776247025), (0.51425, 0.1324572864472866), (0.53275, 0.09543985390663147), (0.56325, 0.08165759453177453), (0.53975, 0.09172377470135688), (0.48775, 0.1324475308060646), (0.6025, 0.06468038983643055), (0.5035, 0.1209629979133606), (0.531, 0.10095204275846481), (0.52975, 0.09396445623040199), (0.539, 0.0937991020977497), (0.545, 0.08525315141677857), (0.4865, 0.14701615571975707), (0.5465, 0.08797494146227837), (0.535, 0.09806547108292579), (0.5185, 0.12613015654683113), (0.51125, 0.10002485129237175), (0.4955, 0.14336590415239334), (0.561, 0.08228727009892464), (0.5215, 0.09332403054833412), (0.5375, 0.09550020790100097), (0.52325, 0.09317310911417008), (0.61025, 0.05467196168005466), (0.57275, 0.06129073974490166), (0.55925, 0.0768723088502884), (0.53325, 0.0930695975124836), (0.5905, 0.07513257056474686), (0.5545, 0.07242815500497818)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.82      0.92      0.87      1000
           4       0.39      0.93      0.55      1000
           5       0.70      0.19      0.30      1000
           7       0.84      0.18      0.30      1000

    accuracy                           0.55      4000
   macro avg       0.69      0.55      0.50      4000
weighted avg       0.69      0.55      0.50      4000

Collaboration_DC_2
VAL: 
[(0.244, 0.045177888870239255), (0.25, 0.10054781334102154), (0.252, 0.10466695122420788), (0.38, 0.10775397792458534), (0.413, 0.15356273053586483), (0.395, 0.17955271257087588), (0.436, 0.20252362125553192), (0.441, 0.1993113804385066), (0.441, 0.22230591307021677), (0.464, 0.251293695660308), (0.456, 0.24946794794872404), (0.636, 0.04014549806714058), (0.576, 0.059894803375005724), (0.526, 0.08711566522717476), (0.511, 0.08665287590026856), (0.553, 0.06772018916718661), (0.576, 0.05213676621764898), (0.589, 0.05848917078971863), (0.548, 0.10910616103187204), (0.615, 0.051779365867376324), (0.519, 0.08455879624560475), (0.562, 0.08661769491434097), (0.552, 0.1035879994854331), (0.493, 0.12622132801171393), (0.528, 0.09632812710106373), (0.558, 0.0809211077541113), (0.569, 0.08783422752469779), (0.572, 0.0864538369551301), (0.559, 0.07681870872154832), (0.549, 0.08435361970961094), (0.576, 0.07687915919721126), (0.522, 0.09611810853332281), (0.54, 0.08818919777683913), (0.53, 0.07902128844847903), (0.598, 0.055659809451550246), (0.568, 0.07062129493197426), (0.546, 0.06820809826627373), (0.553, 0.06653892423585057), (0.543, 0.06442530663823709), (0.57, 0.062257656461093575), (0.584, 0.052411697644740345), (0.532, 0.0791387938298285), (0.576, 0.06578860289324075), (0.549, 0.07886201689951122), (0.544, 0.09243208476901055), (0.54, 0.07787463167868555), (0.55, 0.07285837307851761), (0.588, 0.056572203047573565), (0.552, 0.06804015063401311), (0.536, 0.08338946250360459), (0.565, 0.0707836461160332)]
TEST: 
[(0.22875, 0.04402987053990364), (0.25, 0.09631045669317245), (0.25525, 0.0999217810332775), (0.39225, 0.10327717873454094), (0.41875, 0.14785195577144622), (0.405, 0.17185581016540527), (0.43225, 0.1947092747092247), (0.43925, 0.19275807696580888), (0.451, 0.21640128707885742), (0.4615, 0.24251160019636153), (0.452, 0.24052831733226776), (0.642, 0.0379025292545557), (0.57275, 0.058382247775793074), (0.551, 0.08149481834471226), (0.53725, 0.0800063688904047), (0.5585, 0.06402329462766647), (0.59175, 0.050436770126223565), (0.60575, 0.05462519839406013), (0.55725, 0.10437260511517525), (0.64825, 0.04537404415011406), (0.5395, 0.0752860870063305), (0.5715, 0.07693130770325661), (0.548, 0.09965679550170899), (0.51725, 0.12011030000448226), (0.54625, 0.08943267813324929), (0.57625, 0.075312015324831), (0.5835, 0.0792298995256424), (0.58175, 0.08089255473017692), (0.57525, 0.06995634254813195), (0.5545, 0.07625449199974536), (0.574, 0.07255110889673233), (0.5385, 0.09174531525373458), (0.55175, 0.08402313774824142), (0.54425, 0.0735305822789669), (0.6135, 0.052360476359724996), (0.5925, 0.06437702511250973), (0.57225, 0.06305759838223457), (0.57525, 0.06259017577767371), (0.572, 0.0598868450075388), (0.584, 0.05844637496769428), (0.5975, 0.04981890988349914), (0.5615, 0.07176271742582321), (0.584, 0.059801294177770616), (0.564, 0.0750150457918644), (0.5435, 0.085905727237463), (0.55825, 0.07199125148355962), (0.56675, 0.06775154605507851), (0.6075, 0.05249910348653793), (0.56975, 0.06385772623121738), (0.55975, 0.0761958609521389), (0.57425, 0.06442551705241203)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.42      0.91      0.58      1000
           4       0.72      0.94      0.82      1000
           8       0.89      0.25      0.39      1000
           9       0.77      0.20      0.32      1000

    accuracy                           0.57      4000
   macro avg       0.70      0.57      0.52      4000
weighted avg       0.70      0.57      0.52      4000

Collaboration_DC_3
VAL: 
[(0.209, 0.04494925320148468), (0.257, 0.09110854172706603), (0.323, 0.11809220957756042), (0.355, 0.1430300070643425), (0.356, 0.16536943846940994), (0.354, 0.1911297039538622), (0.376, 0.1672702697366476), (0.384, 0.1897293770611286), (0.387, 0.205445771753788), (0.379, 0.1797317993193865), (0.393, 0.2138650175333023), (0.388, 0.16791463382542132), (0.388, 0.18187071216106415), (0.384, 0.17796269308030604), (0.39, 0.18586286914348601), (0.402, 0.19636187960207463), (0.41, 0.1889429318755865), (0.401, 0.19248264494538309), (0.4, 0.18859956175088882), (0.394, 0.18596020790934562), (0.399, 0.1995002319663763), (0.398, 0.19374949726462365), (0.405, 0.19630836768448354), (0.395, 0.1943335898220539), (0.398, 0.1942524548768997), (0.399, 0.20628052288293838), (0.404, 0.1897297247350216), (0.398, 0.1952621847689152), (0.391, 0.18681876353919505), (0.403, 0.17956819808483124), (0.398, 0.1869236236438155), (0.399, 0.17318007181584835), (0.4, 0.19075863187760114), (0.394, 0.1849568217024207), (0.408, 0.17181476548314095), (0.395, 0.20635307467728853), (0.392, 0.18422666417062283), (0.401, 0.1962214486449957), (0.397, 0.2049194903448224), (0.405, 0.19913368368148804), (0.405, 0.20247165989875793), (0.41, 0.19538852375745774), (0.407, 0.2096841803714633), (0.397, 0.21147712268680335), (0.406, 0.20042242304980754), (0.401, 0.20109627268463373), (0.4, 0.23353527803719043), (0.408, 0.2195804446041584), (0.402, 0.21116627154499293), (0.4, 0.2279284894913435), (0.406, 0.2200767252892256)]
TEST: 
[(0.20325, 0.043842192202806475), (0.2555, 0.08742595201730728), (0.3265, 0.11303198945522308), (0.359, 0.13696498495340348), (0.36825, 0.15801430696249008), (0.3605, 0.18254355472326278), (0.382, 0.1589944772720337), (0.3905, 0.17838659977912902), (0.39525, 0.1921792615056038), (0.3875, 0.17070769482851028), (0.395, 0.20351121479272843), (0.39275, 0.160585995554924), (0.388, 0.17052445495128632), (0.396, 0.16684454333782195), (0.39925, 0.17414344781637192), (0.4025, 0.18204098910093308), (0.40475, 0.1763985412120819), (0.40675, 0.18053361362218856), (0.4025, 0.17893121004104615), (0.39825, 0.17515379637479783), (0.4035, 0.18655056494474412), (0.4025, 0.1819062532186508), (0.4035, 0.1865363311767578), (0.40325, 0.18289278525114058), (0.406, 0.18248910284042358), (0.40275, 0.19173169374465943), (0.39925, 0.17799725311994552), (0.40075, 0.18453045654296876), (0.40525, 0.16998803395032883), (0.40025, 0.16981042444705963), (0.399, 0.17475907063484192), (0.4025, 0.16345417004823684), (0.3975, 0.18047020602226257), (0.3995, 0.17469566005468368), (0.4055, 0.163934590280056), (0.39625, 0.19334499001502992), (0.39425, 0.17440282809734345), (0.3975, 0.1844991637468338), (0.3985, 0.18986140394210815), (0.41125, 0.18340982580184936), (0.408, 0.18872788935899734), (0.40625, 0.18454559314250946), (0.4005, 0.20214249861240388), (0.39275, 0.2014634467959404), (0.4015, 0.1882249174118042), (0.39775, 0.1875940009355545), (0.39725, 0.21734309726953507), (0.40275, 0.2050137251019478), (0.40075, 0.1980220187306404), (0.396, 0.21312646800279617), (0.40025, 0.20705248111486435)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.00      0.00      0.00      1000
           2       0.34      0.91      0.49      1000
           3       0.53      0.69      0.60      1000
           4       0.00      0.00      0.00      1000

    accuracy                           0.40      4000
   macro avg       0.22      0.40      0.27      4000
weighted avg       0.22      0.40      0.27      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [145]
name: alliance-4-dcs-145
score_metric: contrloss
aggregation: <function fed_avg at 0x793db7b5bc10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=145
Partitioning data
[[0, 3, 6, 8], [1, 4, 6, 8], [9, 7, 6, 8], [5, 2, 6, 8]]
[(array([33451, 31000,  3114, 10896,  4060, 19644, 44165, 35381, 42784,
       22062, 10061, 13549, 40945, 48788,  6015,  4859, 39301,  7347,
       13833, 47754, 38289,  3570, 44806, 46113, 42829, 15677,  1338,
       12492, 41880,  4273, 13712, 17232, 13627, 44568, 19945, 26018,
       25242, 36106, 19444, 10954, 21869, 26049,  3713, 35996, 38185,
       30267, 36163, 49050, 33058, 48847, 19778, 26060, 27080, 18572,
       24534, 32538, 42931, 12461, 28068, 31207, 15870, 32062, 42630,
       18211, 22129, 34856, 34017, 32376, 16380, 45186, 16763, 12659,
        4498, 24700, 20267, 44970, 24887, 26724,  6512, 15309, 37328,
       47473, 38618, 37022, 45344, 40635, 32078, 41055, 35269, 22674,
        3204, 26846,  6687,  4079, 46872, 34168, 31837, 47243, 30517,
       32885, 20015, 27415, 39236, 27280, 11812, 38222,   115, 36007,
        4940, 44668, 11734, 38433,  2720, 28080,  4619, 16596, 30029,
       16316, 19015, 17486, 48359, 34434, 11768,  9868, 48986, 16553,
       32479, 13401,  7504,  2365, 25027, 32233, 10326, 32967,  8249,
       38796, 13861, 44443, 18953, 42992, 30913, 14341, 44875, 45347,
       15735, 12911, 14482, 45102,  8858, 22844, 43777, 29295, 12178,
       24349, 33419,  9755, 37729, 25082, 17357, 17866, 26632, 11263,
        5994,  7655, 12149,  1147,  5194, 38545,  7320, 21881, 25362,
       43992, 12499, 34944, 14116,  7970, 12257, 24111, 35973, 38566,
       13714, 29860, 20217, 43757, 40211, 44580, 27768,  7299, 46367,
       33603,   564,  3441, 34917, 38466, 24837, 30895,   695, 17093,
       17994, 23581, 49717, 36052, 20398,  9257,  1522, 36737, 24735,
       30233, 20924, 22684, 25995, 47520,  3136, 14732, 41490, 48147,
        5589,  5828,  9671, 43951, 36149, 30963,  3582, 13368, 25178,
       20813, 16305, 41164, 41851, 21144,  7925, 47324,   405, 39329,
       48924,  4869,  1493, 31284,   392,  9096, 43542, 23746,  9287,
       48958, 15102, 40022, 20454,  5858, 46987,   527,  7603, 39211,
       37554, 16438,  3351, 22761, 37429, 41601, 27304, 17024,  4266,
       30346, 20784, 35960, 10085, 32844, 30318, 30305, 38093,  5507,
       41853,  5741, 29074, 38304, 39761,  1109, 25215, 23465, 12391,
       24733, 23974,  2738,  3365, 15247,  4025,  9034, 39289,  5586,
       33717, 37236, 19808, 47654, 21147, 42233, 13593, 33098, 48742,
       15421, 36249, 20252,  9830,  1951, 37194, 39543, 46349, 33582,
       20017,  7469,  3818,  5006, 18366, 14171, 35007, 45948, 43836,
       38982, 28237, 33633, 41053, 19577,  4558, 28996, 20496, 18188,
        8943,  8105, 22523, 18348, 41159, 46423,  6872, 13064, 21913,
       13581, 30839, 43110,  6205,  8101, 48088, 28513,  7049, 17530,
       38761, 20648, 16301,  5086, 11476, 39045, 14018, 34984, 24276,
        3756, 46977, 10463, 48019, 45117, 23695, 26485,  9394,  5578,
       48062, 25997, 38127, 17567, 47788, 27585, 15383, 47830, 31242,
       10565, 25062, 21373, 26556, 42633, 20625, 17920, 16714,  4472,
        1573,   878,  6716, 14536,    80,  9052, 46529, 27906,  1919,
       41521, 23700, 13688, 49386,  3749, 49233, 14924, 21873, 34647,
        3514, 36990, 21652, 37513, 32145, 35130, 27796, 12155, 38699,
       22008, 20338,  9817, 47997, 48402, 36511, 47603, 28835, 10439,
       43065,  6089, 15298, 36828, 41910, 19871, 49582, 37514, 34581,
       11434, 49574, 30159,  3524, 31632, 41578, 35923, 34822,   287,
        6724,  9345,  3110, 26064, 12904, 32264, 37105, 29109, 29096,
       44093, 48132, 17478, 26482, 34741, 31415, 46855, 35759, 41285,
       23278,  8067, 21832,  7224, 39235, 26036, 18442, 15614, 41845,
       11738, 39921, 13053, 10948, 23541, 11894, 38100, 17251, 33874,
       29865, 28050, 48849, 27673,  7343, 45217, 35323,  2353, 36269,
       42738,  3125, 42445,  3222,  8381, 37912, 23295,  2425, 43013,
       11294, 40651, 45590, 48346, 44456, 36946, 28725, 20965, 24384,
       26160, 14706,  3758, 46341, 42189,  8069, 40039,  1327, 45135,
        5398,  5964, 32918, 15338, 45169, 27346,  3780, 37378, 11392,
       13753, 40261, 21580, 14476, 17680, 46073, 22550, 17788, 31601,
       36903, 14906, 47161, 11651, 17566, 38268, 24354,  3122, 44654,
        9941, 23582, 48400, 26618,  2571, 43264,  6262, 41870,  9928,
       14342,  6506, 34552, 18103, 21497, 16387, 19275, 37013,  4217,
       12396, 35332,  3724, 14212, 14131,  2288, 11675, 20788, 42791,
       20972, 23708,  8991,  2112, 32694,  3892, 12168, 22926, 37481,
        8891, 22085, 35340, 23780,  1192,  3593, 10938, 12508,  8436,
       28690, 13120, 29767, 30323, 46307, 11917, 48977, 14608, 23135,
       46320, 20586, 35831, 25622, 48888, 23008, 15534, 31353, 24478,
        6938,  6949,  7825, 19139, 40202, 27648, 26228, 46473, 23208,
       36310, 21097, 15497, 48619, 39343, 41491,  3111, 25469,   899,
       41668, 45730, 49771, 15351, 26211, 10026,  2684, 36712, 40514,
       18736, 48532, 43392, 33696, 49406, 34965,   143, 19239, 34035,
       28277, 15650, 44012, 41603,  4391, 41309,  1563, 32869, 22889,
       22640, 42447, 31112, 36943, 24728, 49265, 15185,   248, 23086,
       42108, 45862,  5000,  5019, 15402, 17936, 37579, 40153, 19959,
       24000, 29560, 13683, 17172,   368,  5474,  9575, 26566, 30626,
       40357,  8612, 39913, 46322,  4082, 27450, 23516, 38751, 38508,
        2503, 43298, 32793, 25841,  8206,   819, 20934, 16539, 17857,
       49952,  9924, 31447, 18282, 21037, 22044, 12003, 30982, 21588,
        4421, 21192, 13635, 15424, 41549, 43074, 35900, 18992, 20141,
        8309, 11044, 48446,   117, 42099, 40264, 31616, 30625, 25442,
       15651, 25291, 39148, 46612, 10600, 15971, 39068, 35123, 34264,
       18126, 37956,  9681, 23596, 49829, 47115, 34804, 34643, 16822,
       17204, 33069, 46550, 22082, 15231, 37748, 13077, 36625, 24401,
        5095, 16927, 37822,  5921,  7064,  9762, 25255, 44428, 12632,
       10733, 21970, 49962,  5644,  8926, 10234, 31594, 13633, 39336,
       16430,  6250, 10442, 32464,  1914, 41732,  4092, 17442, 23845,
       36146, 46888, 31386, 42050, 46821, 15328,  4233, 16512, 16473,
       45318,  2923, 29746, 12418, 24811, 40109,  3885, 35013, 48943,
        2070, 26109, 25002, 29477, 12452, 40983, 21427, 28153, 33240,
       30299, 20954, 30438,  2462, 11591, 37042,  2586,  5675,  4397,
       28239, 46142, 14693, 13425, 36526, 12481, 17007, 30273, 27271,
        8006, 39502,  8123,  5716, 12457,   627, 21414, 11993,  8512,
       16435,   485, 47708, 10682, 11948, 13109, 31919, 49782, 48213,
       38453,  1250, 30406, 32915, 12605, 11244, 46714, 13729, 37147,
       14132, 11005, 33942, 35687, 22776, 28807, 41041, 18397, 13241,
       41689,  1383, 31526,  5487, 15635,  3614, 27820, 28125,  6989,
       14642, 18645, 16156, 40008,  1141, 39578, 23440,  8854, 43109,
        8574, 49291,  1967, 29340, 21579,  7443, 10680,   888,  8483,
       25902, 44458,  3215,  6219, 37994, 29147, 49182, 39069, 47770,
        6176, 26055,  8668,  2285, 14753, 35300, 30357, 10956,   546,
       32987,  5078,   909, 37308,  6679, 49778, 19502, 31832, 43872,
       23893, 49011, 23219, 40939,  2754, 21246,  9282, 42699, 27661,
       32619,  3319, 27854, 33356, 38008, 14934, 17277, 34910,  5910,
       44213, 47877, 11577,  2732,  8406, 22888,  1439, 16049, 41335,
       12194, 39577, 29493, 25111, 33407, 18141, 34892, 25047, 38105,
       43725,  5999,  7496, 45372,  9708, 37653, 27561, 32281, 31697,
       40429, 37587, 12801, 30807,  1080, 31778, 35661, 32567, 29027,
       15746, 37002,  6627,  6862, 28687, 21565, 16125, 34577, 25557,
       35268,  3800, 37642, 15795, 36940, 40641, 12442, 29175, 49827,
        2065, 31121, 19132, 24451, 33888, 48253, 12861, 37318, 28599,
       16856, 40483, 17210, 39032,  3169,  4723, 33869, 45442,  7013,
       29850, 20913,  6192, 47960, 48838,  8358, 18349, 45375, 28103,
        9960]), [0, 3, 6, 8]), (array([28634, 35692,  6709, 46330, 11418, 23607, 40152, 35799, 48975,
       35004, 29536, 28526, 13032, 37570, 20336, 45210,  4591,  9809,
       49561, 38257, 24191, 23213, 34263, 16736, 35174, 48631, 10153,
       47314, 29828, 36158, 11308, 42294, 18223,  3366, 30902, 49503,
        3822, 41061, 10271, 36033, 25071, 30676, 19310, 39405, 26905,
       18932, 21154,  1251, 49491, 25799, 24807, 49871, 33650, 41248,
       29121, 41607, 29963,  8350, 43711, 28615, 42281, 42637, 17539,
       17397, 47616, 13317, 24484,   644, 49376, 49269,  2455, 25105,
       22900, 23357,  4696, 40145, 29120, 27073, 21540, 22349, 22013,
       33870, 35357, 41654, 15299,  8825,  9251, 47190,  4379, 25827,
       18066, 19565, 32235, 28183,  3735,   119, 14717, 41739, 46421,
       12676,  2727, 11242, 45979, 30208,  2390,  7709, 42287, 22659,
       40178,  4624,  4909, 36101, 32736, 38047,  9095,  4557, 32099,
         493, 30804, 19574,  2180,  7352, 43145,   617, 39800, 32603,
       10714, 42684, 26241, 43935, 43207, 14101, 36991, 38157, 10248,
       45810, 17534, 21398, 49314, 30492, 46649, 36376, 41257, 18063,
       41576, 29791, 23601, 49416,  5356, 19500, 48806, 36872, 13402,
       39352, 37989, 39192, 49354, 12169, 25975, 20494, 40013, 47839,
       19667, 28127, 12049, 46359, 23058, 42410, 31147, 30716,  6922,
       28018, 34349, 34519, 29049, 48528, 30712, 44472, 21528, 10895,
       44965, 13763, 44359, 24263, 43006, 15710, 37401,  6623, 47606,
       38682, 16128, 31985, 17111, 13774, 14040, 39892, 24446,  9480,
       33392, 47151, 30019, 36679, 18134, 44097, 35805, 36334, 17772,
       32472, 28105, 48675, 30049, 14851, 16638, 47026,  4840, 27404,
       44309, 24280, 17312, 39240, 28896, 46966, 49795, 22867, 12628,
       46547, 42318, 28326, 35041, 40220, 10868, 26180, 37702, 10522,
       28248, 47836, 19046, 49933, 12412, 27814, 21877,  9565, 25479,
       10420, 33808, 44411, 12424, 17474, 26362,  9442, 39177, 43897,
       43934, 11794, 30474,   145, 41766, 18834, 11567, 27044,  5256,
       49645, 36345, 49740, 48787, 19197, 40228, 38247,  1953,  3217,
       24548, 30950, 45881, 15170, 33357, 22886, 20147, 33272, 27303,
       40375, 34574, 10766, 20730, 32533, 48429,  1471, 14036, 38386,
       41302, 22247, 33610,  2951,  2363, 15917, 44752, 18434, 32754,
       46947, 25414, 14816, 37775, 38660, 32930, 44978, 32882,  8068,
       14555, 42863,    89, 37996, 15723, 48591, 19045,  4413, 43758,
       24067, 41625, 20406, 25781, 33037, 27028, 40530, 36896, 33073,
       35839, 25474, 28658,  1385, 41270, 21048, 37757, 47573,  2216,
       11765, 46884, 38805, 34752, 48513, 22981, 22480, 14535,  8792,
       14974, 47149, 18455, 11565, 49352, 33222, 21949,  2673,  6378,
       38701, 28084, 22385, 14303,  6387, 43420, 49719, 37853, 47141,
       30794, 17083,  3695, 25292, 44002,  9010, 37563, 13828, 23994,
       21260, 37903, 27214, 47399, 49981, 31588, 43304, 47155,  6612,
        3908,   660, 43923, 36420,  5212, 25672, 39451,  2515, 18114,
        9141, 21607, 10015, 19704, 43928, 19170, 30044, 22606, 42303,
         581, 43202,  2442,  6940, 33002, 48689, 17770, 10629, 40835,
       45022, 22098, 43502, 15024, 24524, 44900, 13134, 17634, 48478,
       23187, 19980, 39932, 28160, 38296,  3688, 32066, 13391, 27342,
       31735, 28797, 20828, 11190,  5462, 40642, 27014, 22202,  2691,
       29353, 16669, 26118, 28561, 31699, 22173, 22284, 22902,  9550,
       27964, 37886, 40798, 46526, 46452, 48016, 27074, 49727, 30153,
       47922, 45555,  6331, 13995, 44994, 49027, 32119,  8405, 48228,
       18912, 14893, 29922,  9776, 10913, 32450, 17800,  8289, 30156,
        4400, 28803,  8404, 10888, 36665, 46077, 33743, 41777, 14382,
       11033, 48985, 16640, 15530,  4317, 16481, 49355, 33862, 37639,
       24498, 29172, 20142, 29265,  4642, 10536, 17260, 22360,  1659,
       38395,  1663, 18821,  3866,  7984, 32063, 19092,  6391, 40823,
       33623, 26383, 39120, 27660, 25000, 16508, 30724, 11875, 20310,
       28188, 26569, 16177, 20802, 32709, 49404, 11719, 49279, 10108,
       36525, 12666, 42261, 36060, 10327, 13403, 45140, 31128, 26748,
       30496, 15994, 15479, 10985, 11657,  3712,  4440, 48771, 17751,
       37654,  8617, 30302, 39994, 32338, 28943, 22113, 28644, 31405,
       45900, 49655, 30953,  6095, 22947,   234,  9278, 17925, 37351,
        4201, 38723, 43624, 13600, 38572, 29339, 46263, 18330, 30222,
       28194, 36984, 41652, 29408, 36816, 28481, 10400, 26872, 19707,
        1584, 41466, 45978,  6745, 13719, 40627,  1628, 21956, 38959,
        4415,  4810, 22347, 44959, 17766, 22157,  5785, 45266,  3942,
       44492, 31315, 17776, 13101,  6804, 29680, 32359, 46342, 18610,
        3683, 16429,  9855, 34942, 10345, 43482, 33528, 37625, 45944,
       11394,  3991, 47874, 15986,  7720, 48325,  6722, 32739, 32759,
       45420, 30193,  2631, 29360, 24424, 39889,  6454, 42013, 44258,
       12356, 21846, 33244,  1956,  8002, 24552,   807, 19834, 33420,
       38310, 28171, 47567, 40364, 40031,  8008, 16287, 23066,   862,
       33953, 39057,  3912,  9045, 11468, 29506, 12221, 34754, 45823,
       20800,  8466, 26771, 36666, 33405, 39823, 43611, 23412, 31270,
       15179,  2625, 39007, 21560, 33543,  1761, 28325, 42778, 30130,
       34644, 49853, 49516,  9877, 34031, 39680, 47240, 20268,    19,
        7232, 15064, 27803, 35672, 14973, 24510, 15810, 11310, 18494,
       33366, 29889, 15612, 17663,  4911, 48947, 26921, 14660,  9406,
       43210, 23686, 33411, 32768, 44675, 34823, 42888,  6749, 16442,
        9850,   680, 12879, 28722, 14951,  9785, 15039, 29973, 41707,
       32904, 44597, 28506, 26727, 31629, 48463, 16687,   770, 36296,
        7143,  4118, 18338, 22910,  3742, 31118,  4637, 43166, 13162,
        1176, 39678, 44602,  6046, 39639, 34694, 45355, 39637, 44393,
       30741, 26240, 45208,  8027, 47214, 11808,  2441, 40988, 37384,
       16246, 13591, 40137,  6508, 36949, 26603, 13958, 25386, 43755,
       46431, 38724, 44570, 36739, 16902, 41954, 41980, 33752, 44981,
       38133,  5906,  5378, 36151, 29479, 11148, 47206, 24878, 33410,
        4446, 27842, 16395, 13404, 18132, 31530, 19706, 27339, 10876,
        1862, 39707, 18996,  6775, 38301, 47894, 14158, 27857, 45095,
       33029, 41256, 12600, 10102, 30844, 47758, 40691, 26220, 37064,
       31372,  6474, 49330, 44057, 49257, 40491,  9387,  3268, 31695,
       25258, 45017, 22661, 47119, 46008,  6403, 24692, 40276,  8879,
       37420, 48602, 25238, 29919, 37798, 49025, 41678, 22712, 22649,
        1138,  2212, 34359, 22214, 47723, 22255, 12762,  7118,  4652,
        5297,  4313, 20239,  4848,  2803,  8222, 26463, 26898, 19238,
       49220, 46313, 15727, 38443, 11593, 49712, 45486, 10848,  5245,
       39334, 43705, 42032, 29782, 27982, 20000, 46015, 10763, 31628,
       20650, 34208, 42325, 38120, 14684, 43334, 15323, 23149, 29877,
       10346, 27976, 39383, 49446, 40136, 41207, 49960, 49144, 13695,
       17922, 27955,  2749, 34135,  6825,  8214, 45161, 15405, 13868,
       35540, 48805,   291,  9786,  4191,  9490, 15759, 16381, 11965,
       47342, 15429, 49042,  5463,  2663, 27536, 43409, 43161, 23342,
        8250,  3330, 31313, 32259, 42265, 40019, 35619, 36267, 18548,
       40805, 42749, 32611, 38237, 18822,  1617, 36579, 19622, 23617,
       38362, 40493, 32077, 29734, 18112, 36966, 48286,  4611, 43855,
       36295, 12207, 30388, 37832, 45839, 39106, 41957, 27243, 26703,
       48757, 42004,  5630, 22862, 41753, 17482,  1151,  4794, 29674,
        2666, 35177,  2109,  3740, 36933,  6139, 16792, 47000, 41543,
       23723, 43678, 14123, 28295, 35912, 45892, 15046, 29078, 43662,
       23538,  1702, 26156,  7507,  7108, 30123, 23438, 45853, 42024,
       48541, 22635, 41676,  1357, 17628,  4602, 22455,  6964,  2454,
       41555]), [1, 4, 6, 8]), (array([36549,  8103, 41592, 46390, 14059, 48503, 20804, 21775, 41996,
       19199, 15690, 30525,  6571,  9911, 39672,  4364, 11549, 22475,
       15337,  3097, 26585, 46221, 23358, 27589, 12315, 25476,  3006,
       47437, 35558, 39434, 46775, 24343, 48084, 40168, 28949, 36683,
         615, 20693, 49193, 20825, 13711,  7657,  7583, 28379, 46599,
        3755, 26150, 43566, 35749, 12483, 31324, 23819, 27569, 14545,
       31983,   438, 38651,  1893, 46848, 17905,  6111, 38328,  1504,
        4665, 34049,  6480, 37201, 37536, 18019, 20683, 16592,  7854,
       12008, 14467, 40257, 38414, 39830,  1049, 21470, 46296, 39440,
       21159,   672, 25094,  9382, 11173,  5591,  6129, 49880, 39884,
       11332,  1868, 24712, 41994, 19670, 49123,  2526,  3941, 43560,
       47175, 37058, 27675, 47641, 14326, 15616,  8295, 47086,  4768,
        2585, 41477, 14077, 24149, 12588, 20851, 19111, 40540, 38226,
       44678, 29186, 48292, 23383, 21520, 19076, 19432, 24987,  8728,
       35203, 32724, 10806,  3677, 22663, 44182, 39174, 26347, 16205,
       22497, 32273, 19976,  1263, 47036, 17747,  5650, 15159, 30785,
        3014, 18937, 29580, 24211, 35807, 41357, 35197,  5009, 33729,
       49670, 45146, 41329, 32133, 37780, 33856, 25648,   881, 22233,
       27287, 49370,  8323, 28709, 38789, 48654, 43882,  8561, 23141,
       46788, 43008, 46059, 18749, 12279, 41044,  2818, 40052, 40237,
       15993, 21289, 43901, 14083, 17163, 33547, 44871, 45844, 21010,
       34774, 15637,  6275, 27959,  4416, 13138,  2985, 20187, 17108,
       30161, 24494, 37040, 36059, 42101, 27058, 41316,  5532, 20613,
       14205, 37126, 11504, 24718, 39418, 43218, 15252, 19687, 46346,
       17109, 17709,  1706,  9715, 43696, 31905, 17435, 42154, 49087,
       40387,  7494, 42385, 29651, 15221, 28871,  1437, 38277, 39115,
       41181, 40229, 46744, 16488, 43141, 38280, 29349, 44901, 28141,
       23633, 43558, 48353, 21516, 45008, 15976,  2961, 49310, 14254,
       23310, 49162,   163, 20063, 15881,  9462, 33605, 21113, 43833,
       15539,  1927, 42951, 27824, 23773, 30066, 20971, 24090,  9581,
       23356, 47321, 46694, 47027, 32660,  7604, 20618, 36303,  4883,
       42386, 15999, 46283, 37679, 40426, 37159, 45630, 39354, 12365,
       11877, 25868, 36256, 30348,  9454, 49640,  4241, 10368, 28296,
       40837, 16175,  6748, 19089, 33374, 15324, 19691, 32073, 37884,
       36452, 21086, 30845, 24536, 25572, 10879, 13975, 47387, 18144,
        6119, 18443, 36936, 33936,   289, 23331, 29904, 29618, 23506,
       23330, 18975, 20879, 44278, 31073, 49058, 27913, 18620, 39246,
       34357, 32382, 16600, 17377, 26861,  9805,  7546, 20773, 28249,
       39168, 46455, 38616, 34440,  6600, 37474, 37660, 14556,  2370,
       45493, 31082, 22647, 45725, 10162, 14044, 38611,  3544, 35341,
       22965, 36977, 19037, 37573, 44115, 47856, 41755, 38196, 17499,
       18041, 26788, 25164, 46025, 17462, 22198, 31653, 35589,  6695,
        7385, 12925, 15290,  9294,  9891, 11781,  9329, 41397, 42366,
       17119,  3844, 31777, 29644, 35284, 14897, 26699, 45459, 32354,
       35290, 21268,  7544, 20010, 28812, 45939, 39307, 13482,  6301,
         916, 34518, 32973, 18022, 37019,   237,  6864, 23501,  4871,
       14641, 17436,  1908, 31461, 18997, 18350, 15047, 49695, 30163,
       16251, 26692, 13347, 32148, 22625, 33678, 34228, 12527, 13124,
       31597, 16633, 26086, 40579, 42895, 37211, 14983, 35812,  8481,
       25896, 18079,   152,  6051, 22612, 20166, 29669, 21493,  6792,
       49183, 36815,  2563,  6278, 47607,  4485, 31718, 38920,  5771,
       32545, 13514, 17649, 44471, 27672,  1197, 33341,  4928,  3251,
       44353, 49037,  5168,  5542, 19306, 23484, 25227, 21660, 28156,
       49903, 25957,  7482, 18313, 35056,  7314, 42368, 23456, 37100,
       13703, 12547, 36205,  7611, 22130, 49809, 31746, 20738, 22934,
       19600, 42485,  4386, 30670,  3157, 14236, 13796,  5796, 24897,
       20098,  7638, 27087,  3841,  2369, 21941, 11363, 36176, 32170,
         921, 39808, 40313,  2972, 46306,  6179, 29931, 19421,  3341,
       22266, 28061, 36716, 27610,  5723,  1453, 24989, 46850, 13441,
       24156, 10920, 20628, 38769, 39759,  4430, 44077, 18265,  7968,
       24215,  6521,  9245,  8983, 15631,  8171,  1819, 12550, 22290,
       38401,  2596, 49119,  1760, 32240, 17272, 11233,  1393, 19984,
       27872, 45836, 44244, 31794,  9482, 31421, 16675, 36061, 48079,
       31984, 38409, 46385, 13365, 12004, 12739,  8950, 44314, 40016,
       31142, 46193, 10256, 45863,  8253, 19588, 49776,  2179, 38397,
       21030,  3563,  7481, 12996,  5679, 42079, 33309,  6395, 37356,
       46273, 37682, 13261, 40927, 27611, 26340, 46492, 36913, 44068,
       45474,  1259, 16440,  2831, 23054, 38088, 21091,  2638, 23745,
       40436, 39969, 34304, 20986, 28042, 29456, 33551, 22525, 27367,
       13634, 27473, 11638, 14828,  2100, 34157, 47545, 29538,  5599,
        1550, 29958,  9468, 31133, 42527,  1246,  4605,  5177, 16606,
       39922, 46885,  4077, 14163, 49854, 19482,  6107, 42391, 15026,
       26701,  6962,  4786, 27577, 25612, 42358,  4606, 27503, 25935,
        5878, 46915, 42614, 25340, 36348, 25569, 41684, 47135, 37796,
       41399, 13619, 15048, 41350, 28859,  8633, 26122, 38391,  9974,
       26247, 18033, 22670, 40675, 34953,  7092, 24950, 40438, 26619,
        7315, 40047,  7001,  1741, 32856, 15458,   210, 19189, 26413,
          23, 16242, 31312, 31181, 33018,   721, 24213, 44011,   242,
       20254, 18050, 21146,  9339,  7267, 48526, 40568, 24895, 29935,
       14395, 39000,  9219, 43661, 29279, 12850, 43155, 30644, 12302,
       28776, 32339, 21438,  7348, 16639, 22068, 23030,  3403, 45385,
       32385, 38845,  2517, 20008, 38721, 45582,  1766, 25723,  4946,
       38137, 38227, 35098, 49459, 16059, 49804,  1837,  6768, 43538,
       32812, 20561, 35342, 30916, 46670, 22600, 36229, 49043, 44444,
       13582,  9818, 44927, 49551, 35468, 37258, 19124,  3881, 21755,
       26195, 16388, 42963,  5648, 30660,  2987, 30071, 43400, 17894,
       15698, 27515, 15268, 37329, 45536, 36985,  7834, 45743,  7131,
       15803, 44504, 36911,  4272, 11616,  4300, 39462, 48273, 43014,
       36650, 29030, 11091, 17531,  7366, 47094, 26643, 41165,  7996,
       30262,  3860, 47791, 22504, 30948, 18312, 16700, 46413, 19557,
       48311, 44528, 17933, 39367, 31981,  7440,   602, 20508, 47646,
       25749, 42897, 28115,  6238, 48158, 14821,  4955, 20081, 21298,
        3829,  6995,  5253, 29934, 20530, 13565, 13015, 40919, 40881,
       21671, 24716, 23031, 35419, 43159, 25669, 39419, 34893, 40292,
       11906, 48965, 46152,  5251, 44304, 35388, 48475, 23294,  8007,
        7886,  4761, 45024, 38881, 23281, 43540, 15022, 24286, 11207,
       32861, 27978,  2426, 49631, 39838, 26639, 35169, 14369,  8273,
       28855, 23045,  8952, 47713,  3165, 10245, 43381,  9378, 31625,
       19713, 23148, 21931, 49378, 38255, 13606, 48320, 38045, 20946,
        6337, 46012, 27858, 34212, 13504, 41264, 36443, 27699, 34171,
        5891, 27783,  9192, 36013, 13674, 42870, 21664, 48107,  7716,
       12585, 11149, 39089, 31917, 22518, 47596,   566, 41835, 40409,
       31795, 22462, 35496,  1964, 40470, 18461, 17405,  8852, 37643,
        9975, 26967,  9883, 47827, 35321, 33418, 35181, 47245, 25508,
       46028, 30018, 10120, 36097, 25042,  4283, 35295,  8672,  3384,
       48737, 15491, 20290, 32183, 15241,  2640, 46938, 16090, 22696,
        8468, 45309, 32783, 18444, 44681, 28928, 42933, 18892, 49542,
       46892, 19725, 32897, 44691,  6182, 48120, 24578, 45138, 45994,
       13131, 21596, 27507, 49372, 23728, 16942, 18795, 12951,    92,
       14464, 11461, 33104, 40575,  2176,  8211, 38382, 27252, 22616,
       47609, 24039, 26579, 20100, 20089, 38158, 19472, 34636, 49677,
       38214]), [9, 7, 6, 8]), (array([ 2260,   993, 34763, 37911, 39124, 49703, 43397, 20416,   681,
       21586, 13411, 43886, 11735, 13679, 44223,  1486, 10033, 41400,
       33552, 47211, 20083, 13103, 41587, 15403,  9174, 18552, 43330,
       18732,  3810, 36540, 22470, 45331,  6094, 44268, 15987, 25722,
       25670, 26479, 32255, 45260, 24899, 33136,  7072, 45991,  2657,
       10175,  5049, 12832, 35748, 27188, 20076, 20809, 35438, 45903,
       36952, 42537, 49548, 31303, 42538,  3008, 21889, 49399, 24663,
       39694, 31243, 25065, 38272,  5755, 49327, 43466,  4960, 18685,
       46017, 10897, 14405, 33423, 24226, 37790, 43063, 44312, 10709,
       30684, 32137, 37537, 42924,  5431, 30543, 26738, 39869,  5372,
       37740, 36587,  4319, 15849, 12567, 11953,  2532, 30356, 11495,
       37046, 15686, 12205, 11153, 24440, 19965, 10484,  9295, 23162,
        9729, 20477, 39539,  1691, 22478, 35640, 12480, 48566,  9450,
       30567, 14221, 48139, 41210, 17811, 22742, 49826, 48754, 49288,
        7518, 12716,  7873, 49859,  7699, 14477, 20922, 23233, 19786,
       34252,  2305, 48991, 49431, 23207, 26313, 30723, 19900,  9987,
        4572, 22830, 38997, 49579, 27701, 19437, 24287, 27844, 11314,
       27671,  8223, 24532,  4362,  5208, 39920, 27225,  9815, 38975,
        4153, 28090,  2242,  5780, 34634, 19735, 42686,  5905,  6982,
       13551, 46621, 21465, 15591, 20394,  7545, 31949, 36629, 31155,
       37151,  8730, 34941, 48181,  5962, 30028, 21352,  9337,   624,
       18038, 40014, 40721, 34303, 26028,  1134, 13930, 36246, 13345,
       38437, 19319, 12532, 16610, 39647,  3660,  7773, 19515, 11396,
       31190, 15861, 44740, 43796, 33707, 26728, 46720,  3616, 12135,
        5960,  3966, 35879, 29321, 25658, 18856, 25127, 46246, 21286,
        3249, 17293,  6778, 35410,   534, 39973, 48645, 21598, 21726,
       37486, 24714, 34074, 33313, 27462, 30105, 31781, 39446, 24952,
       44899, 12728, 11347, 15985,  8851, 16934, 47621, 42648, 22966,
       20556, 40288,  8186, 42986, 38741, 38287, 33771,  1812, 46642,
       49956, 21711, 29637, 42345, 36510, 13433, 35126, 17826, 12536,
       13777, 27546, 26630,  7807, 36293, 33829, 37305, 28343, 19337,
       24739, 46516, 25587, 10433, 20695,  1129, 37353, 47133, 31026,
        6976, 27430, 41208, 31999, 18485, 17239, 43122, 22726, 41856,
       33030, 46895, 13655,  8333, 12589, 29243,  4431, 13415,  7878,
       39887, 32850, 19163, 35157, 46214,  4471, 33580,  9446, 31721,
       38007, 41970, 26279, 15326, 44736,  6542, 49549,  7853,  2550,
       45692,  2983, 42602, 48014, 14017,  5707, 16232,  9144, 46527,
       44344,  3569, 35880, 24789,  3118,  7194, 33621,  6744, 17682,
       33932, 22797, 11766, 41890, 24313, 13757,  6488, 14768,   538,
        8264, 18204,  8534,  2377, 43118, 49099,  1500, 38451, 38806,
       26572, 30524, 22734, 26837, 18756, 20880, 19110, 24097, 23347,
        9989, 21316, 12978, 14915, 12669, 35320, 26234,  2291, 47508,
       15940,  7480, 33513,  6477, 31813, 12703, 25939, 40917, 47255,
       14655, 25830, 37578, 39935, 16352,  4375,  1067, 43450,  2091,
       16202, 22667, 35022, 49536, 34149, 31307, 32711, 44186, 43127,
       24643, 36095, 49020,  9350, 37821, 16327,   963, 36184,  7397,
       25650, 41273, 25149,  3423, 24272, 28708, 47903,  6317, 18564,
       21399, 42094, 10440, 36496, 21650, 25195, 36137, 17363, 36355,
       35477,  5781, 37840, 26956, 49274,  7477, 40716, 30493, 21145,
       32693, 37754, 28095, 23209, 24919, 15584, 23181, 17267, 12533,
       15977, 41498,  2033, 42891,  9933, 30791,  2626, 26629, 27764,
       22045,  2467,  8065, 39928, 37361, 26265,  1800,    54, 41599,
       22275, 16102, 39976, 16176, 29942, 42993, 35360, 24399, 44622,
       42532, 33929, 16165, 36242,  6106, 12161, 44652, 39983, 35255,
        1288,  1139, 12607, 31275, 41538,  5012, 31035, 42746, 31020,
       19153,  7929, 43421, 48885,  3476, 24717,  4824, 29253, 37284,
        1402,  4678, 13410, 44405, 11412, 15343, 23517, 45149, 28692,
       49364,  8431, 31342, 23230, 41320,  7137,  1194, 23764, 24176,
       25829, 13389,   347, 19000, 42509, 16008, 29802, 10203, 45453,
       21311, 33704,  3098, 37574,  9360, 48167, 22986, 25638, 33848,
       33536, 32606, 43258, 30526, 24864, 36188, 24743, 21264, 25519,
       11733, 43353, 49061, 27332, 36561, 20930, 21763, 34861, 14830,
       49200, 39200,  3976, 37077, 49747, 18653, 40519, 42291, 12014,
        4875, 40640, 23413, 24282, 27385, 30976, 19456, 18826, 48542,
       31685, 11303, 11429, 16097, 31598, 20830, 27464, 38738, 38747,
       30001,  8009,  4126, 40544, 15211, 31639, 31413, 37970, 49834,
       18571,  7795, 17697, 26139,     0, 20921, 15812, 21747, 12906,
       44163, 15038, 14345,  7073, 39630, 26387, 31808, 41476, 43459,
       26841, 46921, 23762, 46317, 36023,  2915, 15833, 42436, 34242,
       48203, 41064, 34045,  7226, 41621, 25681, 19184,  3290, 17555,
       21200, 19608,  6582,  1085,   755, 33466, 25526, 40437, 44999,
       28088, 46404, 33785, 33402, 13806,   361,  3986, 14027, 18965,
       28588, 33060, 10053,  1131, 28956, 48658, 37759,  1447, 43958,
       12034, 12218, 32949, 48072, 41326, 48047, 12031, 27229,  8589,
       25139, 44772, 12374, 30212, 42484, 36638, 24423, 19420, 34139,
       43986, 23734, 44014, 48022, 45958, 29327, 25815, 18542,  1248,
       41140, 34539,  6252, 20641, 15107, 30666, 27928, 25985, 11358,
       23375, 44128, 47653, 25795, 46066, 17075,  8108, 33924, 14133,
       35506,  5508, 23323, 18794, 33436, 31577, 23927,  7422,  1917,
       49928, 25493,  5122, 16489,  1637,  7393,  7321, 13184, 34716,
        8699, 42562, 44274,  3804, 31798, 32035, 49698, 40850,  7070,
       28490, 33541, 23876, 15489,    22,  2810, 48373, 35600, 37992,
       37933,  7967, 13916, 32628, 24397, 33381, 23563,  6246, 45759,
       33759, 22282, 49361, 16108, 17962,  7281,  1763, 46540, 21699,
       24085, 49090, 15619, 35150, 22010,  5255, 14208, 13350,  8270,
       33398, 12189, 13967, 41647, 25721, 13810, 11866, 20228,  7112,
       17854, 47999, 36636, 24608, 37681, 20467, 46048, 18295, 48125,
       12125, 44807, 40415, 18274, 31565, 18045,  5916, 32038,  3121,
       20395, 26214, 16558, 10572,  9722, 49625, 38239, 11117, 23858,
       41251, 29537, 20749, 35678,  5452, 48920, 16342, 47112, 15617,
       48872, 13738,  2658, 18367, 31187,   793, 38458, 10068, 34404,
       49108, 34834, 26563, 36555, 25780, 36024, 32092, 32356, 21953,
       10354, 17824, 49216, 10228,  9545, 36708, 12258, 10708, 12866,
        5222, 42822, 10395,  4514, 32370, 31553,  3333,  7646, 43913,
       16676,  5667,  4380,  3324, 43191,  2974, 34419,  3269, 11042,
       24631,  5280, 15835, 29762, 35035,  9154, 43035, 33158, 20082,
       21434,  6788,  6631, 27669, 38524,  9913,  1853, 29371, 10688,
       46668, 27640,  1404, 23509,  8401, 31489, 18327, 31066, 13545,
       27747,  1014, 34310, 27902,  8717, 43175, 32234, 16042, 44438,
        9306, 46707, 16830, 19618, 47842, 48124, 18675, 22589, 46292,
       47236, 36744,  5977,  1325, 26927,  1775, 20118, 37808, 35489,
       32275, 25072, 43985, 14281, 34295, 21140, 11200, 14211, 11674,
        2344, 36804, 49180,  4807, 35621, 43101,  6667, 34021,  4863,
        2471, 37060, 18530,   736, 34527, 21884,  5687, 24688, 48794,
       44362,  7046,  4974, 40547, 31052,  6977, 42449, 11798, 30770,
       32501,  1673, 42842, 26239, 14804, 42847, 48374, 10817, 23707,
       13388, 26306, 35808, 20500, 45461,  1468, 18853,  2847, 15688,
       42763, 11141, 43807, 47992,  6632, 18728, 27864, 22000, 49746,
       38374, 41447,  5777, 39015, 27490, 33815, 39098,  3353, 12426,
       47814, 15334, 23838, 19131, 29530, 22471, 40139, 20052, 27386,
       47506,  3950, 46764, 14260, 46854, 18104, 17246, 18501, 31196,
       27988]), [5, 2, 6, 8])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 3, 6, 8], M=tensor([0, 3, 6, 8], device='cuda:0'), Initial Performance: (0.259, 0.04455009317398071)
DC 1, val_set_size=1000, COIs=[1, 4, 6, 8], M=tensor([1, 4, 6, 8], device='cuda:0'), Initial Performance: (0.222, 0.04445403122901916)
                                                                                                                                                                                                                                                                                                                                                                                                                       classes {8, 6}
D04: 1000 samples from classes {8, 6}
D05: 1000 samples from classes {8, 6}
D06: 1000 samples from classes {0, 3}
D07: 1000 samples from classes {0, 3}
D08: 1000 samples from classes {0, 3}
D09: 1000 samples from classes {0, 3}
D010: 1000 samples from classes {0, 3}
D011: 1000 samples from classes {0, 3}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {9, 7}
D019: 1000 samples from classes {9, 7}
D020: 1000 samples from classes {9, 7}
D021: 1000 samples from classes {9, 7}
D022: 1000 samples from classes {9, 7}
D023: 1000 samples from classes {9, 7}
D024: 1000 samples from classes {2, 5}
D025: 1000 samples from classes {2, 5}
D026: 1000 samples from classes {2, 5}
D027: 1000 samples from classes {2, 5}
D028: 1000 samples from classes {2, 5}
D029: 1000 samples from classes {2, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO1']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.269, 0.06674954384565353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06847400909662246) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.08236931937932968) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09157281869649887) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.445, 0.06579716289043426) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.256, 0.08552074305713177) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.08714920091629029) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.307, 0.12057992538809777) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.08181295055150986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.33, 0.10531247013807296) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.462, 0.10465574797987938) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.398, 0.16387051099538802) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.45, 0.11633282428979874) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.385, 0.123072186216712) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.1520442082285881) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.394, 0.16512881445884706) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.14339558928459883) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.415, 0.1408000374212861) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.1840640387907624) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.405, 0.20105758833885193) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO1']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO3']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.441, 0.16513536177948118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.414, 0.16343627533689142) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.2134882289879024) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.408, 0.2007472256422043) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.16899262822791933) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.442, 0.1821278923470527) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.2760615418329835) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.20005470670759679) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.19086242514848709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.17739180225878953) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.469, 0.30164285308122635) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.419, 0.2237542724609375) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.2003168370425701) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.471, 0.19552786941453815) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.479, 0.34175975008215753) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.413, 0.2044250428378582) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.454, 0.20181888235360385) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.18588235468231143) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.350616532756947) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.404, 0.258475079447031) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[8, 6], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.8115, 0.01819793413579464)
DC Expert-0, val_set_size=500, COIs=[0, 3], M=tensor([0, 3, 6, 8], device='cuda:0'), Initial Performance: (0.908, 0.0079437797665596)
DC Expert-1, val_set_size=500, COIs=[1, 4], M=tensor([1, 4, 6, 8], device='cuda:0'), Initial Performance: (0.926, 0.006831320371478796)
DC Expert-2, val_set_size=500, COIs=[9, 7], M=tensor([9, 7, 6, 8], device='cuda:0'), Initial Performance: (0.948, 0.003796410886570811)
DC Expert-3, val_set_size=500, COIs=[2, 5], M=tensor([5, 2, 6, 8], device='cuda:0'), Initial Performance: (0.808, 0.017484361112117767)
SUPER-DC 0, val_set_size=1000, COIs=[0, 3, 6, 8], M=tensor([0, 3, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[1, 4, 6, 8], M=tensor([1, 4, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[9, 7, 6, 8], M=tensor([9, 7, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[5, 2, 6, 8], M=tensor([5, 2, 6, 8], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x793da8233af0>, <fl_market.actors.data_consumer.DataConsumer object at 0x793d746f9790>, <fl_market.actors.data_consumer.DataConsumer object at 0x793da800df40>, <fl_market.actors.data_consumer.DataConsumer object at 0x793da8156b20>, <fl_market.actors.data_consumer.DataConsumer object at 0x793da8336490>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9495, 0.005593648672103882) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.00791936436202377) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0023270390261895956) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.004641558441799134) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.84, 0.016549698412418365) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.0698281257301569) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.583, 0.06055856736004352) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.541, 0.12890716332755983) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.494, 0.05493247649073601) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.003946631208062172) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.918, 0.007319067818112672) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0027022882637102157) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.004111317689530551) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.842, 0.016228398382663725) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.566, 0.046918814703822136) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.647, 0.048633758809417484) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.589, 0.06199480525404215) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.607, 0.03498676243424416) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9595, 0.0035015099970623852) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.006802237126976252) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0027581526113208384) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.004125146803446114) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.856, 0.018552257001399994) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.589, 0.04072967047989368) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.663, 0.04565933903306723) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.607, 0.05598834421485663) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.03709090834856033) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.003273690799018368) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.007409165073186159) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0030222019174252638) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.004811606240313267) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.838, 0.020051951348781585) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.655, 0.03426800137758255) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.711, 0.031120807744562627) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.686, 0.03507328714430332) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.601, 0.04256254507601261) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9645, 0.0033474976577563213) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.0067008698321878914) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.00342583613214083) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.004873703743563965) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.82, 0.0180963679254055) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.64, 0.03495013397932053) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.708, 0.03291724118590355) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.721, 0.030486501291394234) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.629, 0.041064742147922516) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9625, 0.0037833168599754573) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.006750973677262664) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002795611996669322) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.005830637066159397) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.836, 0.01918186444044113) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.648, 0.03583774280548096) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.733, 0.031999764442443845) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.711, 0.031193397223949433) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.621, 0.04480154183506966) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.003207271724124439) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.00879999628663063) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0035471010588807987) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.004638054172042758) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.838, 0.0187244148850441) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.575, 0.05780704985558987) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.694, 0.038291259042918684) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.743, 0.028592939525842666) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.655, 0.03597791500389576) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.967, 0.003929728183167754) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.009840885293437168) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0034110204321332275) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.00531945694074966) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.838, 0.01975915813446045) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.659, 0.03787663304805756) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.733, 0.028175017327070236) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.711, 0.03555831214785576) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.627, 0.055410683162510396) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.004275885586685036) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.0075500069670379165) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0028521908903785514) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.005447163481730968) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.822, 0.019539140343666078) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.607, 0.0498597018122673) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.765, 0.028406167715787887) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.714, 0.03432231825590134) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.595, 0.05522183706238866) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9745, 0.0031966807938879358) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.009623209914192557) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0031742713469284354) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.005055692439898849) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.828, 0.01944138765335083) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.617, 0.0476167500205338) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.719, 0.03205913147330284) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.689, 0.040285685677081344) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.595, 0.06266378254443407) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.003525000408211781) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.924, 0.010090241211466492) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0035515143260126934) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.0058440762124955655) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.818, 0.01743844074010849) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.661, 0.03927871778607368) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.736, 0.027691813498735427) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.682, 0.0429579950645566) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.612, 0.052253487244248394) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.975, 0.0035384699729765997) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.009983882617205382) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0028458230833057316) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.00645088530718931) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.824, 0.021099165439605713) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.637, 0.039198606193065644) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.748, 0.0293095181286335) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.722, 0.034620816320180894) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.618, 0.05614401699602604) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.0038755658067093465) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.010766466291621327) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003339489795966074) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.95, 0.005452272453636397) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.83, 0.021625576972961427) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.622, 0.0506163008287549) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.739, 0.03134155197441578) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.693, 0.0406509530544281) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.602, 0.05190512895584107) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.00355687097475311) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.009650923741981387) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.00376259245788242) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.006745415171928471) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.84, 0.023655398190021514) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.03718542882800102) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.733, 0.0339346533715725) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.721, 0.03229257049411535) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.616, 0.04430319232866168) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9725, 0.003043434728155262) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.010140439367853105) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0031729035157477482) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.962, 0.007017012357187923) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.838, 0.024347596108913423) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.04618618831038475) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.694, 0.03795106044411659) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.706, 0.036115921467542646) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.585, 0.05684063783288002) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.004107389904424053) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.009456968411803246) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003722689772621379) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.962, 0.00574981818703236) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.832, 0.02131963014602661) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.641, 0.04409709455817938) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.705, 0.03461411303281784) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.701, 0.03935633324459195) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.58, 0.06519702407345175) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.004267610038030398) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.92, 0.010307191230356694) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.004211321347374905) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.962, 0.006123077328884392) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.826, 0.024871440529823305) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.668, 0.03776515594124794) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.71, 0.039254542710259556) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.713, 0.03759092681109905) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.591, 0.052906028985977176) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9685, 0.004194670319928264) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.011750586085952818) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003319064350820554) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.005414197402191349) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.82, 0.02486641013622284) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.611, 0.0477023506462574) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.676, 0.04005611670017242) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.694, 0.037685464149340986) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.584, 0.05741024328768253) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.004384497143295448) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.008857115980237723) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0033098835399214296) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.006349130078218877) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.82, 0.022589395821094514) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.64, 0.04075494921207428) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.695, 0.03788740134239197) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.673, 0.04261293392255902) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.583, 0.05420702095702291) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.004924610511081482) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.011056464416906237) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003784927738175611) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.005285097497544484) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.82, 0.023270686149597167) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.051384979292750356) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.707, 0.042356041789054874) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.67, 0.04977442303858697) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.563, 0.059842202536761764) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.975, 0.003917448442007298) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.922, 0.010409689374268056) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.0030941604995605304) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.006031861424376984) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.826, 0.02510496896505356) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.637, 0.04535165247321129) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.709, 0.034048959240317346) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.686, 0.039609109278768304) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.609, 0.05414754231274128) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.976, 0.004116160825089537) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.011164762005209923) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0033623148319375106) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.006597681792871299) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.824, 0.02356074905395508) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.615, 0.04625537244230509) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.732, 0.0367559395879507) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.724, 0.03530671806633472) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.667, 0.03758132615685463) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.003687462780486385) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.00982426891475916) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.0026847483328165255) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.00615398063880275) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.83, 0.0249100626707077) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.05051016168296337) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.687, 0.04123191010206938) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.738, 0.033826772451400755) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.618, 0.04521675613522529) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9725, 0.0041990200187719895) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.009836856098845602) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.004578868813520785) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.006304982105270028) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.82, 0.025034817337989807) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.666, 0.03764867849647999) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.713, 0.03656096707284451) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.703, 0.039456031061708925) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.621, 0.0489564595669508) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.0045764474988991425) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.009356851857155561) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004656506618322964) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.006135603106944472) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.834, 0.02351862758398056) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.647, 0.04503199648857117) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.701, 0.039160569071769714) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.67, 0.047230768263339995) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.605, 0.054759526640176776) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.0049392364830937365) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.926, 0.010256016962230205) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0033435090519778896) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.954, 0.008164759182487614) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.838, 0.02456542372703552) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.64, 0.04196756935119629) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.727, 0.03768050350248814) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.671, 0.04459763710666448) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.618, 0.051514208242297174) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.004697202631153231) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.00883763051405549) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0032579500689516863) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.006172215354861691) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.824, 0.02375641158223152) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.61, 0.05038272244483232) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.653, 0.053255205288529395) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.678, 0.04492053767107427) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.565, 0.06574474184215069) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.973, 0.004213483079671278) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.00924580365791917) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003287724853151303) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.0056428440086547195) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.844, 0.026967359244823456) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.03901698705554008) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.656, 0.05195142669975757) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.679, 0.04302533840760589) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.551, 0.06068956707417965) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.0042387948673494975) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.009276211485266686) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003786034883955608) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.005470451518960544) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.832, 0.027404593229293825) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.04798541337251663) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.679, 0.0446113050468266) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.668, 0.04480255511216819) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.579, 0.04535483396053314) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.974, 0.003948136116494425) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.936, 0.009398145658895374) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0036973987749233856) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.007574836827465333) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.852, 0.025460141658782957) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.587, 0.0455999313890934) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.685, 0.044267073618713765) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.692, 0.038598415799438957) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.546, 0.0547190865278244) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9775, 0.0045169536016846906) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.01005154314264655) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.0038603888610981587) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.006384912786539644) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.856, 0.02265657067298889) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.589, 0.0451036089360714) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.665, 0.045878913074731825) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.687, 0.042939871776849034) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.583, 0.050699898235499856) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.00416908528280328) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.932, 0.011109206650406123) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.003190132206482758) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.0063958670026331675) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.832, 0.021763318538665772) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.07597721388936043) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.701, 0.04416760075092316) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.686, 0.04082563947886229) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.549, 0.05879224136471748) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9685, 0.005353755687473949) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.010223241571336984) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0029813902813693857) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.966, 0.006166218511993065) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.828, 0.025243927359580993) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.054448576360940935) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.687, 0.04486210772395134) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.68, 0.04597314426675439) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.583, 0.04957333517074585) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.004768819022937578) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.00934710156917572) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0033708033650067365) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.006447589950636029) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.85, 0.023637205839157103) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.048993078589439396) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.682, 0.04219815813004971) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.676, 0.0490542415427044) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.544, 0.05840929452329874) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.005459938134272533) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.93, 0.010699383482336998) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.004528397698021522) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.96, 0.008437460229608404) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.846, 0.02352317726612091) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.581, 0.06188001671433449) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.617, 0.0598983181938529) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.654, 0.04890959090553224) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.524, 0.0729389967136085) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.004963123810905699) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.01022067328542471) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.004193735386207635) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.008270490034046816) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.842, 0.022561890780925752) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.567, 0.0538697934448719) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.669, 0.04822270740568638) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.691, 0.04443773119151592) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.577, 0.05558476337790489) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.004996054899195997) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.009516425885260105) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.004317169921980508) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.010768185725791569) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.834, 0.023964180946350097) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.61, 0.05466134676337242) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.682, 0.04840702837705612) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.688, 0.04103065650165081) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.532, 0.05668481349945068) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.005077532665945682) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.008799813698977232) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.004696995365432507) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.958, 0.00952176085076644) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.83, 0.025372090995311736) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.05699423080682754) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.637, 0.060763814106583595) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.68, 0.04602287644892931) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.529, 0.06672019489109517) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.004109057470341213) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.009348343908786774) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0034918857343873098) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.95, 0.00877195291146635) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.818, 0.026793995022773742) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.056027939200401305) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.706, 0.046214649364352225) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.701, 0.04490002825856209) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.562, 0.05312435609102249) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.004863215099669105) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.928, 0.011368844342650846) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.003107374721792553) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.008512373670339001) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.83, 0.021676299273967744) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.603, 0.04711838736385107) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.671, 0.043038873851299286) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.669, 0.05300006572157145) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.556, 0.050154392808675764) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.259, 0.04455009317398071), (0.269, 0.06674954384565353), (0.445, 0.06579716289043426), (0.451, 0.08181295055150986), (0.45, 0.11633282428979874), (0.447, 0.14339558928459883), (0.441, 0.16513536177948118), (0.452, 0.16899262822791933), (0.453, 0.19086242514848709), (0.448, 0.2003168370425701), (0.454, 0.20181888235360385), (0.487, 0.0698281257301569), (0.566, 0.046918814703822136), (0.589, 0.04072967047989368), (0.655, 0.03426800137758255), (0.64, 0.03495013397932053), (0.648, 0.03583774280548096), (0.575, 0.05780704985558987), (0.659, 0.03787663304805756), (0.607, 0.0498597018122673), (0.617, 0.0476167500205338), (0.661, 0.03927871778607368), (0.637, 0.039198606193065644), (0.622, 0.0506163008287549), (0.67, 0.03718542882800102), (0.627, 0.04618618831038475), (0.641, 0.04409709455817938), (0.668, 0.03776515594124794), (0.611, 0.0477023506462574), (0.64, 0.04075494921207428), (0.612, 0.051384979292750356), (0.637, 0.04535165247321129), (0.615, 0.04625537244230509), (0.626, 0.05051016168296337), (0.666, 0.03764867849647999), (0.647, 0.04503199648857117), (0.64, 0.04196756935119629), (0.61, 0.05038272244483232), (0.644, 0.03901698705554008), (0.618, 0.04798541337251663), (0.587, 0.0455999313890934), (0.589, 0.0451036089360714), (0.586, 0.07597721388936043), (0.609, 0.054448576360940935), (0.577, 0.048993078589439396), (0.581, 0.06188001671433449), (0.567, 0.0538697934448719), (0.61, 0.05466134676337242), (0.604, 0.05699423080682754), (0.632, 0.056027939200401305), (0.603, 0.04711838736385107)]
TEST: 
[(0.26825, 0.04347268170118332), (0.26725, 0.06421797105669975), (0.4395, 0.06346339270472527), (0.4565, 0.07869957354664803), (0.4475, 0.11173120200634003), (0.457, 0.1374445474743843), (0.4525, 0.15964710181951522), (0.458, 0.1622095382809639), (0.46325, 0.18300087195634843), (0.46175, 0.19399894100427628), (0.45875, 0.1961120474934578), (0.49425, 0.06674755337834358), (0.5575, 0.049031969398260114), (0.5895, 0.04118860127031803), (0.65725, 0.03369296953082085), (0.65175, 0.03427609528601169), (0.6435, 0.03699775356054306), (0.57625, 0.05826044017076492), (0.6635, 0.03715638542920351), (0.63225, 0.04796854041516781), (0.63025, 0.043610835254192355), (0.6725, 0.03443413408100605), (0.66075, 0.03608594319224358), (0.62, 0.04851006641983986), (0.67425, 0.03534520095586777), (0.638, 0.04306622163951397), (0.64625, 0.04201229077577591), (0.65475, 0.03871521610766649), (0.60525, 0.047869509354233744), (0.63075, 0.04165958912670612), (0.6065, 0.05077777181565762), (0.6375, 0.0444601584225893), (0.606, 0.046536499619483945), (0.632, 0.047525456339120864), (0.65125, 0.039505716502666474), (0.6345, 0.04559923006594181), (0.632, 0.04513495402038097), (0.61375, 0.049377041473984716), (0.6425, 0.0417273418828845), (0.619, 0.049226169645786286), (0.59075, 0.047309699460864066), (0.5985, 0.04624738484621048), (0.58925, 0.0705476347208023), (0.6, 0.05349411827325821), (0.6015, 0.047968178495764735), (0.59425, 0.058468457609415055), (0.5785, 0.054627481415867804), (0.6115, 0.05219358888268471), (0.618, 0.054739570513367655), (0.6325, 0.05468505302071571), (0.596, 0.04841089622676373)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.61      0.72      0.66      1000
           3       0.54      0.76      0.63      1000
           6       0.61      0.45      0.52      1000
           8       0.69      0.45      0.55      1000

    accuracy                           0.60      4000
   macro avg       0.61      0.60      0.59      4000
weighted avg       0.61      0.60      0.59      4000

Collaboration_DC_1
VAL: 
[(0.222, 0.04445403122901916), (0.25, 0.06847400909662246), (0.256, 0.08552074305713177), (0.33, 0.10531247013807296), (0.385, 0.123072186216712), (0.415, 0.1408000374212861), (0.414, 0.16343627533689142), (0.442, 0.1821278923470527), (0.462, 0.17739180225878953), (0.471, 0.19552786941453815), (0.463, 0.18588235468231143), (0.583, 0.06055856736004352), (0.647, 0.048633758809417484), (0.663, 0.04565933903306723), (0.711, 0.031120807744562627), (0.708, 0.03291724118590355), (0.733, 0.031999764442443845), (0.694, 0.038291259042918684), (0.733, 0.028175017327070236), (0.765, 0.028406167715787887), (0.719, 0.03205913147330284), (0.736, 0.027691813498735427), (0.748, 0.0293095181286335), (0.739, 0.03134155197441578), (0.733, 0.0339346533715725), (0.694, 0.03795106044411659), (0.705, 0.03461411303281784), (0.71, 0.039254542710259556), (0.676, 0.04005611670017242), (0.695, 0.03788740134239197), (0.707, 0.042356041789054874), (0.709, 0.034048959240317346), (0.732, 0.0367559395879507), (0.687, 0.04123191010206938), (0.713, 0.03656096707284451), (0.701, 0.039160569071769714), (0.727, 0.03768050350248814), (0.653, 0.053255205288529395), (0.656, 0.05195142669975757), (0.679, 0.0446113050468266), (0.685, 0.044267073618713765), (0.665, 0.045878913074731825), (0.701, 0.04416760075092316), (0.687, 0.04486210772395134), (0.682, 0.04219815813004971), (0.617, 0.0598983181938529), (0.669, 0.04822270740568638), (0.682, 0.04840702837705612), (0.637, 0.060763814106583595), (0.706, 0.046214649364352225), (0.671, 0.043038873851299286)]
TEST: 
[(0.218, 0.04341068071126938), (0.25, 0.06590802815556526), (0.252, 0.08200803625583648), (0.3275, 0.10096185243129731), (0.392, 0.1177846822142601), (0.42225, 0.1349279899597168), (0.42825, 0.15713550812005997), (0.44525, 0.175697412610054), (0.46675, 0.1712297403216362), (0.47075, 0.19024631458520888), (0.4625, 0.18031305837631226), (0.5915, 0.0589816370755434), (0.64725, 0.046575801715254786), (0.66475, 0.0441335696130991), (0.7125, 0.029734585680067538), (0.7035, 0.03355724187940359), (0.73475, 0.030893090326339006), (0.69125, 0.037572838425636294), (0.752, 0.026122863300144673), (0.743, 0.0295483216047287), (0.69775, 0.032694380313158036), (0.73825, 0.027386752903461455), (0.7495, 0.02732591603323817), (0.72525, 0.028679107643663885), (0.7405, 0.029094233594834804), (0.6935, 0.03750181816518307), (0.714, 0.030659372039139272), (0.69, 0.037426592551171776), (0.66925, 0.039080425009131434), (0.70125, 0.034982711624354124), (0.71225, 0.03769412810727954), (0.7215, 0.03216777087748051), (0.7255, 0.03555335545912385), (0.7015, 0.036828843340277674), (0.72125, 0.0333449449762702), (0.709, 0.03627568747848273), (0.7285, 0.03460963153652847), (0.6455, 0.051813884139060976), (0.65225, 0.05149223557114601), (0.695, 0.04008620375394821), (0.68275, 0.04288405062258244), (0.65275, 0.0453505971506238), (0.708, 0.04100085461139679), (0.673, 0.04478281912207603), (0.68425, 0.03760996632277966), (0.64, 0.05786424665153027), (0.66675, 0.047329296320676804), (0.67425, 0.04589125031232834), (0.63775, 0.05702521413564682), (0.70575, 0.039589461117982866), (0.688, 0.03796598434448242)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.75      0.79      0.77      1000
           4       0.63      0.58      0.60      1000
           6       0.61      0.81      0.70      1000
           8       0.81      0.57      0.67      1000

    accuracy                           0.69      4000
   macro avg       0.70      0.69      0.69      4000
weighted avg       0.70      0.69      0.69      4000

Collaboration_DC_2
VAL: 
[(0.222, 0.04478240823745728), (0.442, 0.08236931937932968), (0.397, 0.08714920091629029), (0.462, 0.10465574797987938), (0.472, 0.1520442082285881), (0.475, 0.1840640387907624), (0.472, 0.2134882289879024), (0.468, 0.2760615418329835), (0.469, 0.30164285308122635), (0.479, 0.34175975008215753), (0.474, 0.350616532756947), (0.541, 0.12890716332755983), (0.589, 0.06199480525404215), (0.607, 0.05598834421485663), (0.686, 0.03507328714430332), (0.721, 0.030486501291394234), (0.711, 0.031193397223949433), (0.743, 0.028592939525842666), (0.711, 0.03555831214785576), (0.714, 0.03432231825590134), (0.689, 0.040285685677081344), (0.682, 0.0429579950645566), (0.722, 0.034620816320180894), (0.693, 0.0406509530544281), (0.721, 0.03229257049411535), (0.706, 0.036115921467542646), (0.701, 0.03935633324459195), (0.713, 0.03759092681109905), (0.694, 0.037685464149340986), (0.673, 0.04261293392255902), (0.67, 0.04977442303858697), (0.686, 0.039609109278768304), (0.724, 0.03530671806633472), (0.738, 0.033826772451400755), (0.703, 0.039456031061708925), (0.67, 0.047230768263339995), (0.671, 0.04459763710666448), (0.678, 0.04492053767107427), (0.679, 0.04302533840760589), (0.668, 0.04480255511216819), (0.692, 0.038598415799438957), (0.687, 0.042939871776849034), (0.686, 0.04082563947886229), (0.68, 0.04597314426675439), (0.676, 0.0490542415427044), (0.654, 0.04890959090553224), (0.691, 0.04443773119151592), (0.688, 0.04103065650165081), (0.68, 0.04602287644892931), (0.701, 0.04490002825856209), (0.669, 0.05300006572157145)]
TEST: 
[(0.23925, 0.04380939894914627), (0.4305, 0.07938512188196183), (0.4055, 0.0837293835580349), (0.463, 0.1001882000863552), (0.472, 0.1443197346329689), (0.477, 0.17273092937469484), (0.4735, 0.20166757559776305), (0.474, 0.25924735152721406), (0.47175, 0.28262863409519196), (0.47725, 0.32329213321208955), (0.47725, 0.3337722965478897), (0.5385, 0.1256511310338974), (0.602, 0.058051818147301676), (0.6135, 0.05282975926995277), (0.70075, 0.03249391248077154), (0.7305, 0.028208583630621432), (0.72875, 0.028520703449845315), (0.74975, 0.02640219558030367), (0.7345, 0.033126050017774106), (0.732, 0.0323543365933001), (0.71375, 0.03648079298436642), (0.6945, 0.040760566517710685), (0.74675, 0.03012984424456954), (0.71225, 0.03839439900964498), (0.74825, 0.029157550100237132), (0.725, 0.03242799837514758), (0.7275, 0.0351353408806026), (0.7155, 0.03622643774002791), (0.7105, 0.0366473757699132), (0.691, 0.040814912594854834), (0.676, 0.04809868001937866), (0.715, 0.037183359593153), (0.74125, 0.03394325722008944), (0.74475, 0.030975177250802517), (0.7295, 0.034626241415739056), (0.68475, 0.04286728154122829), (0.71175, 0.03906475765258074), (0.69425, 0.039560432501137256), (0.70325, 0.039841932654380796), (0.68275, 0.040617526710033415), (0.708, 0.034691213086247445), (0.70075, 0.038660290516912936), (0.71125, 0.034621266752481464), (0.70575, 0.03911400035023689), (0.68025, 0.04414313019812107), (0.6815, 0.04202897274494171), (0.707, 0.038341235306113955), (0.70675, 0.03620999154448509), (0.68975, 0.0404312564432621), (0.7085, 0.04120617550611496), (0.68075, 0.04678938999027014)]
DETAILED: 
              precision    recall  f1-score   support

           6       0.68      0.87      0.76      1000
           7       0.87      0.61      0.72      1000
           8       0.68      0.45      0.54      1000
           9       0.58      0.80      0.67      1000

    accuracy                           0.68      4000
   macro avg       0.70      0.68      0.67      4000
weighted avg       0.70      0.68      0.67      4000

Collaboration_DC_3
VAL: 
[(0.331, 0.04419129967689514), (0.257, 0.09157281869649887), (0.307, 0.12057992538809777), (0.398, 0.16387051099538802), (0.394, 0.16512881445884706), (0.405, 0.20105758833885193), (0.408, 0.2007472256422043), (0.413, 0.20005470670759679), (0.419, 0.2237542724609375), (0.413, 0.2044250428378582), (0.404, 0.258475079447031), (0.494, 0.05493247649073601), (0.607, 0.03498676243424416), (0.625, 0.03709090834856033), (0.601, 0.04256254507601261), (0.629, 0.041064742147922516), (0.621, 0.04480154183506966), (0.655, 0.03597791500389576), (0.627, 0.055410683162510396), (0.595, 0.05522183706238866), (0.595, 0.06266378254443407), (0.612, 0.052253487244248394), (0.618, 0.05614401699602604), (0.602, 0.05190512895584107), (0.616, 0.04430319232866168), (0.585, 0.05684063783288002), (0.58, 0.06519702407345175), (0.591, 0.052906028985977176), (0.584, 0.05741024328768253), (0.583, 0.05420702095702291), (0.563, 0.059842202536761764), (0.609, 0.05414754231274128), (0.667, 0.03758132615685463), (0.618, 0.04521675613522529), (0.621, 0.0489564595669508), (0.605, 0.054759526640176776), (0.618, 0.051514208242297174), (0.565, 0.06574474184215069), (0.551, 0.06068956707417965), (0.579, 0.04535483396053314), (0.546, 0.0547190865278244), (0.583, 0.050699898235499856), (0.549, 0.05879224136471748), (0.583, 0.04957333517074585), (0.544, 0.05840929452329874), (0.524, 0.0729389967136085), (0.577, 0.05558476337790489), (0.532, 0.05668481349945068), (0.529, 0.06672019489109517), (0.562, 0.05312435609102249), (0.556, 0.050154392808675764)]
TEST: 
[(0.3205, 0.04325020852684975), (0.2555, 0.08785385882854461), (0.29475, 0.11601749670505523), (0.39875, 0.15733955764770508), (0.3965, 0.1590762510895729), (0.403, 0.1939288192987442), (0.40575, 0.19325960141420365), (0.40525, 0.19291542088985444), (0.412, 0.21559283995628356), (0.412, 0.19505343741178513), (0.411, 0.25018797516822816), (0.49425, 0.053733363792300225), (0.59225, 0.03549486282467842), (0.61625, 0.03693841813504696), (0.613, 0.040376226566731926), (0.63575, 0.04243461546301842), (0.6205, 0.04390091142058373), (0.64475, 0.03692458624392748), (0.615, 0.055566569074988366), (0.59875, 0.05629007688164711), (0.5785, 0.06314995007216931), (0.6235, 0.0530026058703661), (0.605, 0.055417799711227414), (0.60725, 0.05250095199048519), (0.62125, 0.045859506964683536), (0.59525, 0.057597565427422524), (0.58475, 0.06299398344755172), (0.59125, 0.0526034294962883), (0.58525, 0.057118160367012026), (0.59475, 0.054963426724076274), (0.57125, 0.061852757185697556), (0.61125, 0.05550776624679565), (0.66025, 0.03928453539311886), (0.60475, 0.046473208963871), (0.63075, 0.04719227977842093), (0.6085, 0.0551403835862875), (0.6185, 0.05274659901857376), (0.536, 0.06655386397242546), (0.538, 0.06058777998387813), (0.5735, 0.04653523842990399), (0.545, 0.05532813277840614), (0.58025, 0.052986835941672326), (0.5305, 0.060688393712043764), (0.5725, 0.04959894160926342), (0.53825, 0.05825152140855789), (0.51675, 0.07159927332401275), (0.577, 0.05296620166301727), (0.53925, 0.056009415894746784), (0.5275, 0.06577947250008583), (0.56275, 0.05372100128233433), (0.55925, 0.051933956921100616)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.42      0.45      0.44      1000
           5       0.72      0.53      0.61      1000
           6       0.49      0.81      0.61      1000
           8       0.83      0.45      0.58      1000

    accuracy                           0.56      4000
   macro avg       0.62      0.56      0.56      4000
weighted avg       0.62      0.56      0.56      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [104]
name: alliance-4-dcs-104
score_metric: contrloss
aggregation: <function fed_avg at 0x7dc435bcac10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=104
Partitioning data
[[5, 9, 0, 3], [4, 1, 0, 3], [6, 2, 0, 3], [7, 8, 0, 3]]
[(array([29961, 11298,  5755, 34458, 42818,  8742, 20483, 38411, 20171,
         767, 11024, 14802, 11949, 32795, 30095, 15575, 40417, 30072,
       43979, 47688, 40721, 25662, 30618, 14004,  7435, 10429, 35189,
       38600,  2071, 35606,  6017, 37958, 20615, 33853, 13316, 35905,
       16623, 30098, 27844, 37451,  3495, 35602, 35712, 15606, 41318,
       16358, 43338, 22445, 24648, 18626, 23392, 11269, 35053, 38437,
        5853, 35652, 23623, 47167,  7926, 34183, 21561, 45581, 48303,
        8929, 37874, 26503, 46901, 23146,  9284, 17778, 18845, 38369,
       23385, 48701, 26747, 47024, 49638,  8851, 21610, 47555, 24819,
       11740, 21586,  9383,  7601,  7417, 41942,  1717, 43495, 11928,
       16282,  9286, 35534,  9594,  2410, 32468, 24981, 46606, 12239,
       15685,  3229,  7090, 25262,  5606, 40490,  3471, 45698, 35501,
       15849, 32919, 43387, 33138, 48454, 40561, 32926, 46684, 34008,
       44371, 27679, 43759, 30260, 26095, 43878, 34048, 43852, 47810,
       49472, 28460, 45728, 29896,  4002, 48730, 48139, 15130, 14677,
       45184,  1911, 25981, 33818, 39002, 30063, 28845,  3794, 37680,
       13624, 18913, 35510, 44435, 46074, 11405, 48750, 24196, 31145,
       19150, 28652, 30757, 21543, 27345, 16626, 41376, 40044, 12177,
       20122, 22679,  2877, 21910, 23234, 47988, 18167, 23724, 44904,
       36035, 46925,  3653, 35993, 46267, 22999, 37471, 47920, 44094,
       48537, 49927, 41082,  5736, 31791, 45206, 44106, 30085, 45503,
       13960,  8243,   607, 47054, 37366, 18764, 18319, 20207, 10703,
       29110, 23614, 17332, 42745,   988, 41062, 25248, 36132, 38202,
       49920, 42323, 47258, 31031, 19735, 49848,  2361, 48632, 11725,
       42533, 36653, 24295, 28736, 40765, 22390, 34763, 27359, 49052,
       22061, 34941, 27885, 20460, 26589,  5970, 30134, 31222, 40140,
        4494, 49428, 17773, 27956, 45583, 36952, 15775, 49431,  5288,
       19201, 46187, 44150, 47485, 36439,  1662, 18230, 33778, 15785,
       10375, 20172, 37050, 41580, 26822, 25010, 49779, 22366,  3032,
       18860,  2758, 32595, 23798,  8826, 13906, 38331,  8364, 32489,
       10622, 29458, 24537, 31605, 13078, 37547, 41169, 15142, 49369,
       22415, 42627,  6178, 18749, 23877, 29431, 21273, 40508, 13669,
       12793,  9425, 28907, 44714,  5801, 38378, 35057,  6150, 49360,
        2208, 38820, 49793, 38827, 41976, 18213,  5591, 37094, 19824,
       40040,  1893,  7441, 44009, 14008, 42591, 44088,   360,  5244,
       33432, 29115,   664, 27237, 33784, 19505, 12152, 39510, 34943,
       39025, 22959, 43997, 29551, 12680, 33224, 32266, 36031, 40931,
       24609, 42159,  3479, 19766, 28713, 24731, 40215,  6643,  5621,
        2302, 47511, 26289, 34502,  1078, 27837, 32635, 49112, 43695,
       13364,  7556, 23651,  4289, 45876, 17709, 30392, 43839, 40735,
       26393, 16533, 19934, 21537,  6357,  2477, 39041,  4749, 34049,
       48544, 13767, 28850, 39280, 39801, 14458, 24987, 13074, 26113,
       39731, 16818, 10231, 44410, 35203, 47426, 34147, 29672, 38314,
       23225,  3915, 28915,  9466, 16614, 46269, 23770, 32745, 17544,
       40722,  3061, 28899, 30178, 10751, 42704, 24683, 39557, 30175,
       29418, 47076,  7698, 15096, 21237, 15586, 29651, 24603, 36329,
       45735, 23129, 41054, 38561, 15901, 10523, 45463,  1572,  8716,
       24960, 34129, 44947,   512, 26447, 12116,   406, 39870, 32838,
       21688,  4675, 22815, 10161, 24042,   763, 18430, 46195, 47025,
       25965, 26880, 30503, 39572, 15566, 23359,  5881, 19474, 30771,
       32902, 47469,  2609, 23091, 41683, 31453, 13421, 47985,  5622,
        9947, 45791, 44996, 48751,  1115, 13491, 32701, 26852, 44332,
        7652, 27287, 40915, 34640, 26600,  1906, 47709, 40037, 36401,
       45048, 34485, 27899,  6201, 22346, 39894, 33928, 15252, 40693,
       35746,  5234,  6268, 16708, 29529, 19380, 21506, 32575, 48345,
       30758, 37242,  2880,  9990, 28931, 24581, 25624, 35750, 17196,
       47916, 19390, 33960, 24404, 45666, 31726, 11913, 44771, 43324,
       30114,  5719, 10197,  2341, 28972, 19286, 16238, 25920, 10911,
       45297, 27487,   185, 37867, 12634, 18893, 12764, 34783, 16825,
       43989, 23002, 39170, 23796, 16219, 35796, 29307,  7788,  3842,
       10847,  8741,  3292, 46576,   527, 31728, 21878, 27047, 45067,
        9819,  7731, 17082, 37962, 49359, 24356, 43289, 18763, 34568,
       29578, 38041,  6954, 11325, 32471, 24257, 24587, 35731,  3329,
       38044, 29927, 48986, 28724, 41762, 21469, 28272, 42711, 44645,
       10650, 16391, 43657, 46523, 10130,  8857, 20686, 28467, 38043,
        3789,  3689, 28570, 41855, 40703, 22557, 37358, 41146, 17352,
        5414, 13419, 30378, 22299,  1243, 12916, 41222, 48324, 31471,
       27301, 13260, 13966, 15258, 12922, 28833,  6833, 28515, 47651,
       13660, 26644, 10499, 10159, 18768,  5770, 41435, 38963, 39329,
       10275,  6242, 32905, 11372, 47525, 40802, 34259, 47906, 22167,
         557,  3277,  8079,  8018, 34722, 20454, 25212,  8759, 21439,
       41258,  1306, 13368, 41797, 42992, 26704, 13174, 35369, 34234,
         695, 44864, 44624, 27460, 15827, 41274, 24395, 48852,  2053,
       31600, 25748, 15483, 48147,  9193, 29053, 16048, 15304, 10954,
        1319, 32631,  2145, 11122, 15848, 39392,  9697, 17432, 41038,
       21092,    77, 28092, 49633, 10545, 41040,   555, 41745, 18358,
       41907, 49219, 24072, 19492, 49837, 35089, 31412, 36464, 17257,
       19294, 21288, 36955, 40432, 24365, 12944, 37068, 16225, 10782,
       31863, 30968,  4721,  2451, 35658, 21143, 31391, 38478, 49308,
       31976, 22970, 15542, 22449, 47925, 37049, 16552, 27855, 38828,
       44296, 33128, 11768, 21275, 22628, 42309, 21087, 16196, 17947,
       19664, 14867, 11975, 35039, 12430, 39574, 39763, 13401, 36606,
       40851, 23528, 34109, 20287, 30271, 48671, 35690, 19974, 37102,
       23851, 24764,   940,  7389, 12145, 38040, 41021, 42382, 15210,
       15444, 38442, 40378, 34204, 10293,  6496, 39211, 11205,  4558,
       18371, 21373, 23715, 34536, 47995, 45949, 21687, 47284, 38048,
       40651,  5493,  9904, 43551, 19922, 13329, 45552, 40587,  8380,
       36948, 36930, 25380, 33049, 21728, 23389,  9183,  1963,  4166,
       38095,  8356, 47146,  1405, 45885,  7426, 26964, 46094, 43813,
       34011, 37966, 20139, 30347, 49322, 16680,  6482, 44399, 42122,
       45631, 34967, 34517, 20630, 48200, 40249, 11628, 42800, 32172,
       15595, 28020, 27796,  7997, 31148, 40127, 24077, 43598, 46817,
       27734,   416, 46286, 15383, 41330, 25019, 21366, 37863, 33841,
       32180,  2337, 44530,  5329, 31698, 43402,  5478, 44084,   377,
       20050, 48306, 42357, 10135, 20639, 25947,  3340, 32520, 41016,
       32219, 18769, 12500, 20204, 32379, 34155, 21783, 20770, 45908,
       11067,  1895,    91, 10653, 33894,  4294, 43784, 35832, 30461,
        6061, 43336,     9, 25882, 34742, 16714, 11366, 48039, 20271,
       41372, 22611, 28373, 16817, 39151, 37217, 16405, 20529, 37408,
       17930, 18127, 26716, 16209,  4435, 27424, 10869,  5209, 14032,
       35715, 34997, 41705, 24733, 23270, 41364, 29423, 32304, 32725,
       12988, 32917, 27683, 45250, 48629, 24434, 15127, 42233,  5007,
       49190, 36951, 12971, 24522,  3190, 32621, 23590, 41159, 45508,
        3763, 21835, 44868, 24364,  5423, 15417, 13205, 24335,  1150,
       44029, 15640, 32293, 28731, 34317, 49982, 33525, 14064, 19617,
       20410, 17451, 41057, 47478, 22302, 17644, 39341,  6837, 39543,
       35931, 22759, 42752, 37910, 42482, 16872, 40690, 39384, 39158,
       48097, 21672,  2197, 37415,  8876, 38012,  9120,  4481, 29273,
       34822, 26564, 49384, 43704, 30753, 25492,  6732, 31567, 46882,
       44093,  5260,  8105, 12973, 34061, 33582, 32914,  1265, 20784,
       25235, 30817, 46013, 42134, 35935, 34055, 34649,  7493, 20631,
       41853]), [5, 9, 0, 3]), (array([39470, 41148, 10032, 35811, 15140, 44629, 29163, 13803,  6022,
       23743,    89, 12313, 43632, 11116, 43746, 13453, 41246,  7634,
       34410, 41688,  6298, 28577, 40718, 30970, 29138, 41337, 48860,
       28097, 20906, 10904, 41852, 23134, 14600, 15152, 27105, 15790,
         310, 27579, 45274,  9502, 47951, 40529, 16967, 45441,  9685,
       12591, 28414, 21676, 18300, 49154,  3587, 23252, 22491, 33099,
       14060, 11125, 24933, 12106, 36533,  4527, 13636, 10925, 36646,
        6879, 35791, 29787, 25636, 40848,  3219, 48513, 25216, 18450,
        2418,  5902, 22902,  4750, 24290,   520, 15434, 12084, 46803,
        7459,  9780,  4328, 11860, 43049, 25474,  6605,  1889, 21419,
       20450,  1904, 22246, 30153, 13303, 11626,  2691, 42418, 45100,
       45645, 40277, 28151, 41788,  3533,  1303, 42907,  6415,  4725,
       32280, 32491, 31306, 43044,  7818, 47622, 32634, 34306, 47922,
        2155, 39125, 38755, 31061, 24804, 49981,  1866, 25423,  7078,
        1851, 44095,  7295, 21946, 47312,  2999, 32446, 46528, 43386,
        7382,  3577,  7099, 10890, 44974, 31860,  7811, 28904, 36225,
       27847, 24900, 37075, 48309, 23354, 49480, 45595,  5029, 11020,
       40299,  4227, 19471,  3664, 30942, 30354, 10982, 41836, 38730,
       24967, 33827, 12590, 14105, 35429, 49690, 39803, 16672,   856,
       16961, 15909, 36755, 13145, 26943, 43627, 29770, 31256, 13646,
       34418, 26512, 44846, 18818, 16294, 33296, 43682, 22468,  9872,
       43188, 40088, 35304, 22606, 35023, 38776, 11226, 12826,  3089,
       29086,  7154, 11573, 15482, 45764, 40862, 46220, 14474, 34671,
       47796, 41648, 37639, 29830, 36466,  1315,  1333, 17038, 39209,
       47305, 27247, 20243, 18476, 23445, 33735, 33290, 22053, 14576,
       16514,   904, 13134, 49390, 22825, 11769,  3776, 39576, 40067,
        9441, 11279, 18821, 15643,  3695,  3112, 19458,  7536, 11423,
       25099,  5119,  4381, 48338,  6110, 49950, 27914, 44726, 44936,
       42318, 38841,  7607, 28045, 36844, 41445, 20162, 48845, 17244,
       37264,  2394, 19382, 28127, 31854, 10644, 34064, 15710, 15748,
       21151, 17752, 24446, 48656, 17694, 21407, 36546, 35011, 44671,
       38278, 40967, 49151, 27024, 45535, 44138, 24110, 33063, 25544,
        3452, 12373, 46866,  8189, 26491, 35453, 48584, 18438,  7884,
       34911, 43854, 42221, 11532,  8001, 34128,  4591, 31352, 38610,
       20988, 34246, 11558, 35605, 10795, 34530,  6226, 12977, 22818,
       22829, 19079, 14358,  4492, 11075,  1380, 12494, 43217, 22675,
       18654, 41627, 33951, 35357, 41217, 32988, 26008, 20307, 11571,
       36473,  4101,  2428, 12210, 11920, 34063, 22697, 37551, 26292,
       43297, 19939, 24678, 22490, 48574, 39455, 25331, 48806,  4200,
       46135,   834, 26375, 29536, 42085, 12231, 10049, 40075,  9263,
       14740, 17856, 47035, 24288, 25280,  7349, 34913, 26065, 45001,
       44526, 17861, 35051, 18890, 45569, 41742, 25975, 18927,  3414,
       10110, 25558, 41519, 18206, 12741, 39638, 23895, 38919, 24762,
       21422,  6429,  9407, 44499, 27794,  1578, 49121, 48915, 19500,
       17031, 23500, 34637, 11407, 21848, 46173, 37480,  6786, 42121,
        9717, 18702, 19031, 47568,  2849, 35644, 11971, 28532, 35086,
       26789,  2067, 44824,  8869, 22379, 43206, 10035, 40423, 30245,
       33230, 13402,  4858, 40520, 26080, 27402, 37260,  7706, 34162,
       33188, 47540, 15740, 42443,  9411, 16471, 42060, 11403, 42684,
       38630, 41849, 21871, 21118, 42113, 44674, 24975,  4696, 35567,
       26267, 41209, 25974,   840, 48441, 23020, 44960, 41579, 40518,
       20746, 33808, 43893, 46177, 22636, 14201, 16745, 43865, 24979,
       33196, 41092, 49438, 26405, 19226, 46993,  5627, 23709, 31101,
       33271, 14659,  9534, 25278, 24240, 43668,  6411, 39353,  9524,
       35827, 13468, 26509, 48269,  2511, 24003, 48263, 15069, 45483,
       25488,  2752,  3840,  9400, 37136, 17838,  3574, 39995, 20105,
        4651, 30695, 43700, 19066, 14778, 31693, 14909, 20682,  8392,
       35326, 45800, 41220,  2662, 32884, 20831, 42073, 16010, 18298,
       44884, 41677,  8882, 26592, 45737, 35446, 35279,  4165,  5341,
       24793, 44468, 30895, 40343, 14848, 15439, 13031,  9714, 11080,
       48778, 20537, 19639, 32592, 16567, 46380, 49387, 48119, 34831,
       25489, 18979, 13860, 13181, 25605, 49375,  7490,  8849, 35758,
       29732, 17383, 41774, 41806, 39236, 18302, 36119, 48136, 13223,
       45143,  2598, 26003, 32661,  6328, 20976,  3548, 30971,  4524,
        2365,  2171,  6190, 46409, 29468,  9934,  6512,  3337, 18900,
       19216, 31947,  3113, 15949, 21458,  3066,  6085, 47326, 36106,
       16100,  7674, 25337,  2027, 43002, 36786, 48856, 49660, 48470,
        3024, 27986, 11424, 45089,  1338, 28231, 12420,  1142, 11224,
       18231, 31648, 21564, 28897, 28035, 17884, 15900, 32091, 15509,
       34280, 38547, 35403, 39179, 46480,  2066, 20275,  9292, 24367,
       39196, 16733, 34746, 17764, 23478, 46137, 39589,  9119, 26840,
        4311, 25422, 30730, 32247,  3900,  1711, 20790,  7660,  5566,
       45546,  5477, 11671, 32460, 29497, 37729,  4030,  6354, 35286,
        8249, 43998, 32131,  1759, 37177, 36386, 23056, 38566, 32058,
       47865, 14407, 31481, 17763, 32016,  9920, 31048, 35281, 13902,
       38345, 32670, 13861, 18659, 18849, 38205, 44067, 33336, 31654,
       10526, 46987,  5924, 43664, 49276,  5858,   405, 35079, 32398,
       12344,  4571, 34828, 26027, 14329,  1424, 43771,  3644, 32968,
       49863, 43368,   417, 11078, 23972, 39121,  4192, 36389, 21338,
       27609,  5549, 18315,  6904, 33514, 34123, 42621,  2459, 16423,
       41800, 38419, 42272, 30101, 26083, 35529,  6929,  6124, 20566,
       27147, 44742, 35059, 23924, 14138, 33864, 38656, 36038, 27278,
        7002, 18704, 26913,  3852, 30605, 11937, 38344, 16370,  1477,
        9895, 41651, 30152, 29561, 13189,  9300, 21736,  9343, 32702,
        1002, 25307, 16536, 12598, 21909, 20522, 26064, 37924, 36970,
       24348,  1271, 43487,  6162, 41725, 25008, 26613, 26372,  6309,
       38880, 38894, 19221, 13941, 46659, 33914, 43554,  3795, 18111,
       11564, 27652, 22517, 34835, 32732, 11131, 17322, 41734, 44313,
       28091, 32098, 19871, 18169, 23120, 49368, 49336, 40788,  7584,
       37546, 37998,  1363, 22933, 49713, 30838, 22406, 38423, 30022,
       12289, 10383,  4177, 38127, 25256, 47526, 12475, 18222, 49481,
       24135, 36366, 36054, 14225, 36448, 18180,  4799, 30960, 34159,
       36027,  3785, 26336, 35083, 24174, 20215,  5741, 32963, 13857,
       34581, 49015, 23935, 29810, 18662, 31198, 33511, 15706,  2042,
        8330, 35583, 20151, 21739, 28991, 12681,  7835, 17904, 47892,
       34116, 18862, 41815, 40668, 32449,  1057, 19780, 41134, 33355,
       42056, 25055, 10784, 35428, 28529, 15341, 16628, 33642, 22223,
       10770, 34170, 16583, 15397, 24871,  4437, 47819,  8781, 45921,
       13986, 38761,  1803, 40954, 27761, 46085, 33714, 12105,  9166,
       11477,  3447, 24100, 16335, 14108, 13925, 20593, 20121, 31632,
       34345, 10611, 38781, 15739, 30756, 27698, 15753, 42153, 41271,
       19060, 33168, 43685, 38976, 36809, 36979, 40399, 22453,  8722,
       24384, 45251, 37176, 12966, 21326, 21851,  9700, 38879, 39300,
       33601,  8122, 22209,  8760, 10418, 34016, 32325, 36743, 15877,
         479, 43730, 20159, 47553,  2383, 34461, 14373,  2594, 33236,
        6466,  6669, 10244,  4669, 38066, 18072, 24551,  5425,  3110,
       36800,  8686,  1196, 35629, 33955, 24022,   691, 32844, 28523,
       24751, 49298, 31141,  7467, 32021, 28149,  4672, 48045, 33031,
       36216, 15345, 45439,  3043, 28513, 37204,  5797,  4310, 23995,
        9914, 16231,  2223, 29950, 49430, 49944, 22826,  6914,  5106,
       41102, 31261,   774,  9371, 37765, 13811, 33972, 24598, 22161,
       21287]), [4, 1, 0, 3]), (array([20177, 36044,   854,  9901, 32063, 44492, 31408, 40153, 32074,
       45741, 19281, 35219, 30128, 44023,  3504, 18722, 41297, 27687,
        3991, 33948, 11638, 31660, 35136, 46791, 34020, 14476, 30938,
       31356,  5723, 46377, 15986, 37887,  3341, 13188,  3098, 34142,
       45925,  8256, 42584, 23821, 24556,  8633, 10013, 30813,  3716,
       38061, 49082, 25815, 10961,  1191, 49708, 16478, 42881, 16760,
       10738, 10111, 30001, 29976, 46951,  1176, 15823, 37917,  5728,
        9275, 30910, 23606, 45585, 45863, 26469, 41969, 17235,  9625,
       42196, 30685, 22337, 29506, 17427, 30610, 10786, 24879, 26685,
       29108, 13018, 39203, 21941, 23945, 45978, 21612, 49041, 38665,
       13495, 47396, 13804, 39457, 38738, 14043, 46979, 38461,  1959,
       33733, 26243,   937, 49343, 42525, 29141, 17574, 13120, 31342,
        7092, 49412, 23087,  7881, 47552, 16475, 35830, 28254, 15513,
       11204, 43977,  4167, 11181, 32904, 23193,   355, 45878, 35117,
       49200, 27022, 47649, 44259, 44405, 37992, 16455, 32345, 40565,
       34612, 48886, 39122, 23208, 44028, 16945, 22256, 43268, 13866,
       10889, 35747, 40142, 20527, 42167, 39161, 18909, 20766, 40206,
       30682, 27177, 37021, 47821,  6941, 14345, 23542, 34092, 44417,
       41350, 30976,   409,  6517, 37033, 47499, 43869,  6035, 23144,
       28986, 38993,  1837, 14838,  2831, 16539, 24928,  4095, 27229,
        7084, 10600,  5402,  9727, 40710, 43155, 14236,  5381, 48723,
       29233, 25999, 10108, 25829, 20501, 42836, 18655, 28533,  6935,
       34740, 37609, 18868, 49771,  3943, 38707, 43481, 42975, 47150,
       47361, 49212, 32454, 46602, 18085, 38372, 18050, 14679, 34058,
       11903, 18618, 43666,  3311,  7252, 17204, 42762, 28668, 29931,
        9237, 35435,  9681, 34542, 18254, 34563, 31465, 34686, 35370,
       18420,  8063, 45524, 20254, 37219,  7927,  8603, 26619, 17536,
       11110, 22631, 14163,  8069,  8212, 32493, 45155, 42694,  8750,
        2291, 28399, 34488, 39540, 13639, 29612, 38352,  9798, 28397,
       13985,  9463, 33300,  7850, 36441, 22602, 47450, 12584,   271,
        1614, 14918, 37838, 14559,  5773, 10391, 48427, 14423, 25366,
        9168, 17954, 38546,  5506, 29135,  9900,  5535, 28705,  2080,
       22974, 37374, 25263, 13477, 12940, 29912, 22672, 18184, 25312,
       12473, 45023,  5550,  3455, 30021, 36006,  1492, 44193,  8676,
        6073, 38991,  6230, 25757, 44910, 13321, 49020, 13036, 43106,
        6836, 46652, 42014, 18485, 31401, 46877, 49870,  2567, 21486,
        1139, 24617, 24500, 29658,  7714, 12720,  5746,  7136, 34373,
         281, 24789, 11471, 40221, 36598, 44256, 41619,  4679, 37560,
       29988, 30108,  1523, 13934,  8037, 22280,    47,  9217, 31802,
       19130, 17695, 37586,  4014,  4109, 22104, 45550, 13071, 16028,
        6901, 15310,   288,  5012,   108, 14314, 24980,  7890, 27764,
       17980,  1067, 31704,   673, 40860, 36994, 43503,  6927, 33449,
       15895,  4719, 44730, 48270,  5118, 21590,  2812, 18027, 27049,
       46365, 40087, 46405,  6816, 42450, 25276, 33350, 20671, 34149,
       22441, 36920, 31063,  8776, 19944, 28890, 48498, 40687, 24699,
       42271,  5150,  2744, 35126, 30102, 32387, 42193,   648, 20797,
       35656,  4418, 41894, 36268,  3416,  6041, 31884,  7576, 43484,
       38556, 25295, 11478, 45451, 34698, 10398, 20323, 14195, 22603,
       37781, 18328,   800, 39389, 14793, 42472, 46299, 15109, 10217,
       38072, 44652, 22331,   483, 47674,  6317, 10833, 10725, 17325,
       40908, 30694, 42098, 41362, 22843,  1777, 33977, 47360, 33412,
       42185,  3982, 38981,  4012, 22486, 29058, 26893, 45864,  1354,
       24927,  1677, 36385, 43823, 30003, 19899, 30224, 11961, 35360,
        3372, 38705, 41662,   335, 32730, 37208, 13770, 26110,  7688,
       12945, 42803,   864, 49357, 23343, 39426, 21076,    24, 43116,
       45021, 37515,  5281, 11221, 48833, 37178, 44668, 24721, 42399,
       40680,  7328, 39628, 26776, 49869,  4490,  2504, 28363,  2574,
        4941, 27444,   165, 32403, 39650, 24169, 22924, 44464, 11989,
       20834, 12534, 15893, 39281, 16704, 46324, 11642,  8813, 23805,
       26656, 28866, 47201, 19127,  9542,  6374, 19444,  8365, 20523,
       19662, 25976, 32483, 45085, 44141, 32455, 40649,  3332, 22648,
       25668, 30487, 39716, 42755,  1466, 25419, 20039,  1999,  5657,
       37558, 22418,  2460, 41900, 19541,   989, 22329, 24476, 40479,
       25616, 33328, 18591, 20097, 28625, 32025, 48145, 34460, 25930,
        9983, 42678,  7444,  6400, 18706, 11297, 23847, 15622, 17026,
       44794, 13030, 21023, 33148, 15707, 46763, 26842, 12178, 30820,
        1381, 23684, 19277, 48209, 45176,   905,  1185, 31435, 37355,
       38479, 30324, 35418, 36993, 38235, 29881, 26824,  8774, 42952,
       21308, 42945, 35782, 31201, 38551,  3214, 32873, 14615, 21248,
       31700,  8155,  6054,  2932, 32194, 33380, 21297, 28619, 34131,
        4229, 32316, 16596, 48318, 12660, 27440, 37276, 48891, 21881,
       45827, 16521, 41352, 29257, 12133, 11944, 41664, 29944,  1664,
       34758, 22408,  8558, 42573, 34711, 32617,  3609,  5346, 39904,
        1926, 42010, 45373, 47328,   189,  9533, 44568, 17355, 17520,
       25191,  5428, 23024, 24445, 35121,  6711,  3204, 35996, 37279,
        2675, 26045, 33087, 34601, 22858, 13011, 13483, 37712, 20539,
        8098, 32002, 37674, 40648,  8810, 34919, 10893,  8912, 29884,
       44476, 26258,  9801,  8118, 16375, 32444, 20169,  5028, 38847,
       25789, 22026, 27686, 31521, 36558, 10989, 33654, 33076, 25807,
       46625,  8190, 48643, 33926,   129, 39883, 20449, 48635, 21869,
       48024, 28650, 38580, 22967, 46935, 17154,  9616, 42630,  1755,
       31105, 49490, 10215, 14056, 21467, 25820, 47691,   199, 10365,
        6687, 28487, 28921, 37189, 35269, 11451, 33416, 46873, 29940,
        5494,  3515, 25318,  7290, 36781, 29324, 30571, 11271, 21898,
       43626, 23295, 17530, 49970,  9163, 16014, 37135, 26688, 35794,
       48722, 22866, 48062,  9489, 40246, 35549, 41285, 17441, 34545,
        3905, 20017, 34756, 25922, 46204, 25378, 39482,  3490,  5800,
       17051, 25062, 48331, 26711, 21882, 26689, 47413, 46561, 14089,
       44381, 47643, 30896,  3016, 31606, 22914, 16070, 30687, 38626,
       40226, 22011, 47075, 25627,   174, 31886, 43254, 22555, 39166,
       42757, 15247, 33560, 23145, 48326,  6602, 11458, 26609, 43716,
       34361, 32312, 46950, 26210,  7734, 37209, 24707, 20153,  9034,
       25825,  7909, 11199, 39420, 21655, 28067, 33801,  4720, 19895,
       39979, 36074, 45454, 22922, 33952, 41910, 37105, 23483, 48144,
       37180,  3178,  9052, 38552, 33301, 32358, 43034, 30305, 25771,
       41013, 49901,  5183, 11236, 49054, 39822, 18393,  1487, 45479,
       36063, 16339,  9357, 23963, 43922, 17698, 20917,  3910, 30862,
       44550, 48377,   801, 17195, 36124,  9307,  6658, 40369, 43084,
        7765, 12147,  2738, 43500,  2770, 45152,  5720, 24991, 44543,
       46595, 12558, 44234, 32607, 21703, 40326, 27433,    26,  3218,
       45670, 44127, 32006, 20131, 46596, 31295, 35245, 44267,  9345,
       25673, 24839, 26097,   494, 43233, 36518, 48999,  8101,  3807,
       47313, 32264,  6402, 37697, 43283, 34579, 11294, 40097, 44110,
        1127,  3371,  6199, 44376,  9332, 37967, 40584,  5221, 34984,
       37675, 23461, 14558,   776, 13584, 15937, 24611, 26260, 12783,
       32583, 30986, 46712, 17413, 23314, 10070, 22007, 49146, 17835,
        6590, 42647,  6945, 18497, 22994, 46157, 21351, 12647, 17661,
       27753,  5870, 27759, 36230, 19518, 26907, 23278,  5389,  6674,
       12707, 11864, 42021,  7205, 24949, 24749, 37038, 20691, 41540,
       46229, 33915, 36969, 45850, 27263, 46232, 29050, 19381, 36714,
       20338, 29960, 10949, 15226,  1109, 39609,  2505, 26862, 40459,
       14942]), [6, 2, 0, 3]), (array([37228, 15148, 28398, 26615,  5771, 18916, 23880, 46397, 40005,
       45365, 28815, 21340, 47665, 13445, 10874, 45459, 24820, 37482,
          85, 10073, 12844, 24159, 19244, 20094, 24892, 47575, 35767,
       39667, 30939,  3071, 25568,  6734, 22723, 38575, 49335,  3988,
       48066, 14742, 16265, 31212,  5403, 38358, 43617, 46797,  4248,
       42366, 22806, 33191, 16706,  6655, 29535, 31321, 44353, 43150,
        2405,  9590, 40374,  1386, 19997, 29886, 11581, 14531, 11391,
       39081, 13935,  5325,  5976, 39164, 44081,  1583, 37489, 16995,
       27657,  5220, 33821, 40977, 43929, 28015, 15364, 22126, 26056,
       29614, 17720, 12271, 20618, 34904, 12633,  5307, 35120, 17913,
       26916, 49377, 42959, 41239, 26702, 41437,  1902, 10754, 21751,
       39304,  3817, 38104, 27451, 17398, 45292, 16344,  2969, 46154,
        2397, 46465, 38195,  1489, 42555, 43848, 41805, 37293,  3835,
       25283,  3637, 35008, 19159,  1483, 39400, 13799, 35462, 15067,
       32640, 37971,   662, 44578, 46954, 29789, 12794, 33330, 47924,
       44892, 13026, 10156,  6157, 20429, 29859,  9939, 49456,   727,
       25865, 42018, 16096, 46167, 27929,  7141, 33388, 32490, 48307,
       38367, 43556, 45682, 46027, 41959, 46266, 34165, 21060, 48767,
        7185, 11321, 25791, 16879, 27985, 19206, 28206, 21314, 29910,
       46549, 36718, 47657, 15332, 44935, 12397,  7197, 10496, 41347,
       30484, 39351, 45657,   652, 41526, 48732,  4427, 49048, 14066,
        7420, 27217, 29962, 19779, 34733, 13566, 15400, 15842,  1549,
       46672, 15932, 47817, 24930, 43036, 48166, 34584,  6827, 16031,
       44123, 25724, 40243, 44366, 42268, 26314, 18417, 49116, 26692,
       11553,  6335, 44086, 18620, 10818, 39246, 36270, 16654, 16138,
       12941,  3824, 22782, 24371, 33935,  8887,  3782, 47673, 12535,
       16996, 43305, 31389, 42453, 37159, 31529, 20223,  7829, 31653,
        4736,  6713, 41517,  6307, 37948, 37862, 48055, 45327, 18104,
       30433, 15964, 49445, 22440, 26951, 47527, 40491, 32243,  8972,
       13054,  9972, 14010, 32574, 48076,   766, 29414, 19195, 42794,
       19147, 46248, 22462, 20400, 13131, 19175, 38469, 39621,  5348,
       37703, 21312,  5701, 20455, 26337,  4891, 36733, 15721, 21377,
       17116, 25823, 37207, 44199, 20405, 35864,  8241, 40702, 33093,
        7334, 49571, 19851, 33562, 40252, 21830, 22920, 25207, 37583,
       14123, 29868, 29199, 40493, 48959, 13674, 30018,  1770, 32543,
         193, 39802, 44877, 11993, 20652, 18593, 11652,  8401, 41616,
       27864, 43149, 23436, 21253,  1383, 47854,  7118, 44205,  4287,
       44382, 24841, 39298, 49393, 36799,  7455, 26603, 22262, 47206,
       10682,  1250, 12115, 48690, 34152, 31104, 36534, 47791, 19733,
       26246, 34910, 23973, 30510, 12543, 34248, 36013, 48311,  3497,
       19416, 44304, 20089,  1794,  6995, 28384,  9418, 10475,  6418,
       27932, 32915, 26977,  8073, 40939, 16479, 42617, 43899, 24444,
       21936, 46552, 23766, 31170, 23839, 47978, 38525, 35139, 38949,
         442, 12041, 22584, 44458,  3159, 29762, 26714, 48678, 23258,
       35115,  2160, 28435,  5185, 40429,  7104,  3670, 15198, 24386,
       37416, 11977, 40598, 20540, 18171, 33287, 45619, 40458, 19479,
       13672, 46151, 20237, 18853,  3144, 40408, 41988, 19472, 48871,
       30808, 26094, 30411, 29371, 42050, 42842, 18623, 30380, 31423,
       32567, 40778,  6397, 31966, 10095, 34419, 30328, 32463, 33335,
       17345, 28728, 31121, 13355, 19887, 19141,  8526, 35562, 20006,
       19768, 16084, 24350,   880,  1702,  1357, 38276, 30844, 26988,
       49905, 16118, 25258, 21172, 25794, 48602, 23153, 10606, 45890,
        3584, 23215, 13406,  3353,  6508, 42676, 42388, 43278, 12233,
       10272,  2923, 45240, 41570, 34560,  1967, 10445, 23583, 11842,
       11501, 11010, 38315,  8852, 13967,  2395, 17013, 37335, 28874,
       23282, 15043, 45776, 28461, 42182,  1144, 48212, 33000, 27934,
        2169, 16632, 29060, 18508, 37083, 26427, 41588, 29487, 22962,
       44749, 11721, 29295, 33933, 21983, 38433,  1270, 15466, 22190,
       32251,  8549,  3665, 24700, 45615, 15288, 38112,  9171, 41300,
       35594,  9221, 46458, 18525, 27940, 19370,  7747, 32505, 39149,
       11178, 25144,  1329,   497, 19004,  3180, 31459, 21105,  2718,
       17088, 38335, 40644, 48105, 49091, 17620, 26300, 41960, 36363,
       28033, 22881,  3263, 15436, 48280,  7360,  9296,  3466, 21777,
       13850, 10855, 40589,  5984, 30829, 26879, 23157, 43965, 11475,
        7826,  2780, 10266, 15678,  8200, 43750, 24639,  8478, 20434,
       22200, 30667, 17237, 47379, 34022, 32190, 13751, 48508, 14450,
       43659, 38198, 28449, 31674,   663, 29274, 23419, 41574, 34537,
        7808, 42934,  4368, 26488, 13238, 24939, 36149,  8437,  9512,
        2258, 42298, 10353,  6665, 14708, 12785,  1044, 49656, 37265,
       12818, 45113, 22799, 23182, 19527, 11431, 16460,  7504, 38003,
       43184, 27323, 10607, 45936, 48246, 12079, 21227,  8072, 30811,
       22973, 27004, 35991, 30652, 23610, 46831, 24179, 16316, 10513,
       46303, 16554, 12083, 16230, 19814, 48850,  8695,  4314, 48729,
        5484, 11919, 46807,  1816,  5010,  2659, 22276, 47220, 29941,
       25160, 14080, 30002, 36446, 17133, 43219, 41818, 33438,  9004,
        2714, 46655, 33242, 37165, 36623, 30830, 11682, 13106,  8355,
       24881, 30158, 16146,  9375, 29977, 49723, 37181, 13668, 31358,
       43814, 11691, 30078,  4079, 35685,  6629, 42597, 24642, 24291,
       39530, 12836, 19084,   436, 45621, 24325, 47143, 19812,  3441,
       27981, 33732, 49734, 18192, 23738, 19804, 16002,  7792, 39380,
       26723, 29336, 40504, 30304, 34964, 27562, 24600, 46964, 38326,
       18614, 21413, 18100, 10323,  8444, 14509, 36909, 12181, 11029,
       47317, 26761, 21892, 42722,  2345,  6640, 16015, 34125, 14591,
       25070,  7203, 33453, 32139,  2559, 36870, 11188, 49983, 42825,
       25934,  9760, 37163, 25547, 26160, 43735, 43773, 48058, 18798,
       49500, 25971, 31649, 31603, 24330, 42084,  1098, 37817,  8599,
       16869, 15057, 34437, 37453, 38364, 14574, 13061,  3514, 11976,
         949, 31254, 39761, 11257, 11293, 20034, 28738, 14527, 37700,
        6060,  8164, 25745,   550, 27298, 23122, 14861, 18387, 46653,
        2164,  8161, 28838, 28028, 19787,  9737,  1554, 20961, 43742,
       12050,    21, 11938, 16366, 23417, 22778,  5848, 44926,  4288,
       49819, 29692,  9667, 26883,  9982, 44456, 17414, 18865, 34623,
       36418, 29839, 22364, 28433, 33964, 30634, 46841, 23027, 31788,
       23239, 37973, 23465, 44802,  3756,  7107, 27156, 32686, 19838,
         101, 42789,  9421, 28808, 27742, 44142, 29460,  1685, 13064,
       21103, 40511, 39689, 48540, 30624,  9222,  3003,  3376, 17144,
       31094, 14340, 28819, 38171, 39533,  4402, 10011, 25077, 39608,
        8067, 30386, 36273, 43800, 20877, 42997, 38699, 49599, 14148,
       35449,  4392, 17430,  9399, 41511,  8246,  8635, 11231, 25443,
       26906, 13463, 18771,  8422, 25006, 48849, 47973, 32524, 28813,
       35595,  3818, 11578, 25986,  2518, 30779, 28983, 25688,    74,
       33211, 13914, 49499, 28029, 23998, 30573,  6186, 18422, 46962,
       13313,  1778,  8381, 20601, 27467,  6980,  6394, 26302, 41269,
       17909, 31825, 33925, 17098, 21962, 25219, 45091, 22664, 12821,
       32541, 10958,  4791, 32982, 33393, 28034, 44562, 45053, 46347,
       11743, 30839,  5062,  1461, 22638, 29979, 42411, 44409, 34952,
        5915, 31676, 20007,  2359,  3758, 25782, 12631, 32113,  1427,
        6883, 15856, 38932, 37156, 16814,  4083, 25706, 15083, 43105,
       31366, 18954, 39789, 23763, 11040, 34590, 28984,  6343,  8822,
        9943, 18389, 47539, 38951, 43533, 27212, 31161, 15080, 38377,
       21510, 32350, 37773, 20849, 38897, 11409, 37304,  6724, 46198,
       39171]), [7, 8, 0, 3])]
Collaboration
DC 0, val_set_size=1000, COIs=[5, 9, 0, 3], M=tensor([5, 9, 0, 3], device='cuda:0'), Initial Performance: (0.25, 0.045359280467033386)
DC 1, val_set_size=1000, COIs=[4, 1, 0, 3], M=tensor([4, 1, 0, 3], device='cuda:0'), Initial Performance: (0.274, 0.04425249779224396)
DC 2, val_set_size=1000, COIs=[6, 2, 0, 3], M=tensor([6, 2, 0, 3], device='cuda:0'), Initial Performance: (0.254, 0.04541688692569733)
DC 3, val_set_size=1000, COIs=[7, 8, 0, 3], M=tensor([7, 8, 0, 3], device='cuda:0'), Initial Performance: (0.255, 0.044730136394500734)
D00: 1000 samples from classes {0, 3}
D01: 1000 samples from classes {0, 3}
D02: 1000 samples from classes {0, 3}
D03: 1000 samples from classes {0, 3}
D04: 1000 samples from classes {0, 3}
D05: 1000 samples from classes {0, 3}
D06: 1000 samples from classes {9, 5}
D07: 1000 samples from classes {9, 5}
D08: 1000 samples from classes {9, 5}
D09: 1000 samples from classes {9, 5}
D010: 1000 samples from classes {9, 5}
D011: 1000 samples from classes {9, 5}
D012: 1000 samples from classes {1, 4}
D013: 1000 samples from classes {1, 4}
D014: 1000 samples from classes {1, 4}
D015: 1000 samples from classes {1, 4}
D016: 1000 samples from classes {1, 4}
D017: 1000 samples from classes {1, 4}
D018: 1000 samples from classes {2, 6}
D019: 1000 samples from classes {2, 6}
D020: 1000 samples from classes {2, 6}
D021: 1000 samples from classes {2, 6}
D022: 1000 samples from classes {2, 6}
D023: 1000 samples from classes {2, 6}
D024: 1000 samples from classes {8, 7}
D025: 1000 samples from classes {8, 7}
D026: 1000 samples from classes {8, 7}
D027: 1000 samples from classes {8, 7}
D028: 1000 samples from classes {8, 7}
D029: 1000 samples from classes {8, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO5']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.321, 0.059396352589130404) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.0762955961972475) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.317, 0.08767266011238098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.45, 0.0864397220313549) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.06267942577600479) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.09520264573395253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.333, 0.12743896949291228) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.09229815077781678) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.07932132732868194) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.301, 0.11468889297544957) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.39, 0.15861733889579774) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1226446467190981) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.10350485199689866) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13766499603539706) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.17524761104583741) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1584977867305279) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.425, 0.12845924731343986) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.39, 0.15613945835083723) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.18912637266516685) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.481, 0.19356412656605243) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO3']
DC 1 --> ['(DO5', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.434, 0.14465451043099165) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.431, 0.17646908596530556) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.19739552516490222) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.22081087283417583) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.1671894628610462) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1882399409338832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.415, 0.18544992347434164) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.23137943388335408) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.1650114922504872) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.1933495959304273) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.428, 0.22099166601337492) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.2418985731303692) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.1857669574674219) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.21793028167169542) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.268153380241245) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.479, 0.272056638228707) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.432, 0.21375495231803507) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.23726580816414208) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.27721165637299416) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.48, 0.26264312120061367) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[0, 3], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.7555, 0.019792987287044525)
DC Expert-0, val_set_size=500, COIs=[9, 5], M=tensor([5, 9, 0, 3], device='cuda:0'), Initial Performance: (0.864, 0.014043234599754215)
DC Expert-1, val_set_size=500, COIs=[1, 4], M=tensor([4, 1, 0, 3], device='cuda:0'), Initial Performance: (0.924, 0.007941799638792872)
DC Expert-2, val_set_size=500, COIs=[2, 6], M=tensor([6, 2, 0, 3], device='cuda:0'), Initial Performance: (0.862, 0.012280856512486935)
DC Expert-3, val_set_size=500, COIs=[8, 7], M=tensor([7, 8, 0, 3], device='cuda:0'), Initial Performance: (0.96, 0.0036122717913240196)
SUPER-DC 0, val_set_size=1000, COIs=[5, 9, 0, 3], M=tensor([5, 9, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[4, 1, 0, 3], M=tensor([4, 1, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[6, 2, 0, 3], M=tensor([6, 2, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[7, 8, 0, 3], M=tensor([7, 8, 0, 3], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7dc428241f70>, <fl_market.actors.data_consumer.DataConsumer object at 0x7dc428149d60>, <fl_market.actors.data_consumer.DataConsumer object at 0x7dc428218eb0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7dc4281ef9a0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7dc428665d90>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.874, 0.010991585068404674) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004953951858682558) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002294408769812435) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.01478220153786242) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.003910866072634235) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.531, 0.0645534948259592) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.551, 0.07493435553461313) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.498, 0.06050598081946373) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.546, 0.1071524434350431) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9055, 0.007412107583135367) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.00445416472107172) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002712814268190414) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.012724240496754646) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.968, 0.003797024172497913) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.585, 0.049364851012825964) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.611, 0.05959854617342353) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.573, 0.05624754682183266) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.603, 0.06104728062450886) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.918, 0.007034294562414289) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004192296213470399) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0022674373284680767) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.013308275047689676) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.0042067642228794284) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.64, 0.039147841796278955) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.633, 0.05598449590615928) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.583, 0.04536744970083237) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.615, 0.059201851420104507) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9185, 0.006930048193782568) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.004676172116305679) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.0025256215853150933) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.012813609544187783) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.003710119736439083) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.03985655578970909) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.623, 0.05218482429534197) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.603, 0.044257869005203246) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.624, 0.049247145429253576) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9185, 0.006986056613270193) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005794029464246705) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0022791606767568738) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.015615705661475658) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.00476812148996396) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.621, 0.047099037274718286) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.639, 0.05095427404344082) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.609, 0.04315116593241691) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.618, 0.05357923308759928) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9265, 0.007493582423543557) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005248632491100579) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002960134046152234) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.014419610671699046) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.0056493849784019405) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.633, 0.037347400307655335) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.664, 0.04122888460755348) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.627, 0.04468450033664703) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.642, 0.04564088509976864) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9265, 0.007507602335419506) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007410190857714042) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003183717945008539) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.014669812604784966) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.004324715013324749) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.602, 0.04380184018611908) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.688, 0.038588111877441406) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.645, 0.04154341906309128) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.637, 0.044390617221593855) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9245, 0.008149557902477681) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005837083352729678) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0028083140917588025) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.013339141042903065) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.003941904409148264) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.03606247943639755) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.671, 0.049539245001971724) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.619, 0.045319867610931396) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.629, 0.05081037163734436) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.935, 0.007149222643114626) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.007340088035736699) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0036403996906010433) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.01462482187896967) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.005414541853111587) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.03923467706143856) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.615, 0.05281341263279319) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.582, 0.05062305986881256) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.675, 0.05259070612490177) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9295, 0.008090911434264854) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006662116679304745) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0023724280612077565) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.016292224369943143) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.004931784387503285) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.037392334520816806) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.721, 0.03175914756953716) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.621, 0.045429308861494065) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.641, 0.051273112155497076) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.007315669646952301) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0066091656008502465) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.003241450410510879) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.01481475592404604) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.968, 0.004537409254349768) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.611, 0.053763357259333136) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.655, 0.053574360985308885) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.604, 0.048281490683555604) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.666, 0.0418431014418602) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.933, 0.009026103448239155) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0068299545294721615) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0033638220568536783) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.015597501397132873) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004292736024013721) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.649, 0.03391189932823181) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.711, 0.03438514178991318) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.595, 0.04906456309556961) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.671, 0.043836831778287885) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9345, 0.008456572287948803) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.0070417476514703595) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002563874817278702) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.01639667311683297) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.0057343682516075205) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.04505856922268867) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.704, 0.03439344997704029) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.553, 0.05759781211614609) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.676, 0.04124637323617935) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9305, 0.009458052248810418) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005300677409395576) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.0024910255428694655) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.016565964415669442) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004308939342794475) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.03689878782629967) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.7, 0.03438923051953316) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.55, 0.0550957618355751) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.677, 0.04422350770235062) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.008691805461538023) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.00596224059863016) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.00321321191318566) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.01696736992150545) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.004340415895348997) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.621, 0.038544788971543315) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.699, 0.034845386957749726) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.05781593102216721) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.684, 0.03616573172807693) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.009533378033433109) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.006072227090131491) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.986, 0.0023171697702491657) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.01896820143610239) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.968, 0.004311499694711529) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.651, 0.039196858525276185) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.716, 0.0331058924049139) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.05853276693820953) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.681, 0.03846486604213715) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9355, 0.008355461700819434) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.006721464743139222) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.003085737102403073) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.015561874486505985) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.968, 0.004484739670668205) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.579, 0.04719967560470104) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.685, 0.04170861675962806) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.05797230553627014) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.666, 0.04435255852341652) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.009470772450789809) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0061569230500608686) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0029903876619719084) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.014357601813971997) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.00419704074292531) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.619, 0.04324448388814926) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.696, 0.03704631482064724) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.548, 0.05697040432691574) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.672, 0.0407764059305191) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.929, 0.00923753994004801) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005995759667159291) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.002836122562206583) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.016210791543126105) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.004152682235624525) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.04695139651745558) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.7, 0.03703138330578804) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.521, 0.06520220947265624) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.672, 0.041197559028863906) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9375, 0.00813048788998276) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.006156714064971311) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.00294003162298759) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.01912911859434098) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.004763754523730313) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.625, 0.04488866962492466) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.719, 0.03486934889853001) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.555, 0.06651147067546845) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.666, 0.04245605356991291) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.939, 0.008500748205929994) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.007549991076695733) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0025885268239071594) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.018456716794520617) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004572913372510811) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.588, 0.05510284277796745) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.726, 0.03163460413366556) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.548, 0.0667890767455101) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.7, 0.040096805185079576) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.937, 0.008911476816865615) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.007077966440498131) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0030981662546982988) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.01785031565837562) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.004468298244784819) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.605, 0.04884500573575497) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.707, 0.034166932478547095) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.539, 0.068774255245924) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.667, 0.04513841795921326) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.01044025529839564) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007183666623197496) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.002678771191305714) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.01924158283032011) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.005968105813488364) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.597, 0.0488756782412529) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.698, 0.031806013941764834) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.565, 0.0669960092306137) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.668, 0.046996025562286374) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9355, 0.010403914350899868) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005870011409977451) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0027591567049594233) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.01863628692552447) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.0054925973676145075) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.04602028173208237) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.713, 0.03343240685760975) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.08538381117582321) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.693, 0.038484960734844205) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9325, 0.009488446164177732) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.007181488533038646) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0028375040106911912) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.01961085657775402) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.005281416549973074) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.05859528623521328) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.712, 0.03312940334528685) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.532, 0.06881767660379409) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.667, 0.046215984255075455) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9335, 0.010387689072696958) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.007758783173048869) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.004470026654506) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.01691789124161005) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.005883548103331123) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.045615816984325645) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.642, 0.0579862934499979) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.58, 0.05430969947576523) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.649, 0.05549259197711945) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.93, 0.010857571098371408) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.007875587239886045) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0038450467660914) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.01850691905617714) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.968, 0.0052102281451225285) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.601, 0.052101950481534) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.706, 0.03416846492886543) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.564, 0.06002319741249085) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.664, 0.04587805797159672) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9325, 0.010247828767518513) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006726047955860849) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.004147983646722423) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.021096212409436703) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.0050350087429651465) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.05035646061599255) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.682, 0.04090061032492667) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.506, 0.0779096896648407) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.684, 0.039540403723716734) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.009752907823421992) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.008162812809652678) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.004255528829242394) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.016869888349901885) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.005966824546371754) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.068796376273036) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.66, 0.04953942716494203) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.56, 0.06194470918178558) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.676, 0.04787994334101677) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.009382941354066133) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.007263827004950144) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.004028896455423819) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.017889226958155632) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.968, 0.004381886182105518) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.596, 0.051742311999201776) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.696, 0.03716631327569485) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.555, 0.06773818904161454) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.672, 0.04646346390247345) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.008666698733926751) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006909674679627642) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.0038329406206048587) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.018058913737535476) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.972, 0.004381757968456441) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.597, 0.06398559787124396) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.7, 0.04237676229700446) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.564, 0.06469947388768196) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.671, 0.04811920803785324) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.009557622866355813) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.008385893548780587) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0036474355687478237) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.01759545477014035) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.97, 0.005130437481851004) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.08693953461945057) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.693, 0.04441721059381962) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.538, 0.07316405853629113) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.658, 0.049222037866711615) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9345, 0.009518947056028992) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0071503135159728115) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.005127580977505772) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.016911276431754232) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.97, 0.00633820968493798) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.07218753892555833) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.646, 0.05923976104985922) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.582, 0.057442379266023635) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.668, 0.04950304961204529) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.009280718056717888) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006550958731691935) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003121747159093502) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.02029883836209774) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.006336130106452401) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.578, 0.06984587955474854) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.707, 0.03470559747517109) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.57, 0.06054171386361122) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.688, 0.04017108145356178) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9325, 0.010973429737205151) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.00842036698199081) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.004140771148177379) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.01732936314307153) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.006574349770671688) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.059516092985868456) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.66, 0.04868223492614925) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.557, 0.05985822921991348) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.685, 0.04282428452372551) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.927, 0.010730544100806583) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.007395902803225909) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.004560511103424688) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.019552397799678146) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.007260157819138826) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.621, 0.05109221513569355) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.704, 0.03952310121804476) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.561, 0.07010286509990692) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.645, 0.05363568024337292) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9315, 0.008932836398947984) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.008236547927837819) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0047360655964216675) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.017640257405117155) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.974, 0.006427722243359309) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.598, 0.05936514323949814) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.702, 0.0403834334500134) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.555, 0.07530310827493668) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.662, 0.05211802940070629) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.010139511470100842) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.007472934243967756) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0030482313272659667) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.01687534211575985) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.005815002478460883) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.581, 0.05150219419598579) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.683, 0.03963112077116966) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.533, 0.06783287328481674) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.67, 0.05017975597083569) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9315, 0.012880150605225935) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.008077478868537583) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.982, 0.0029659939382472656) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.017393900798633694) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.0080025972239673) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.579, 0.054605647414922714) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.702, 0.03527464708685875) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.544, 0.05964411532878876) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.655, 0.05859282848238945) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9285, 0.011566340170364129) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.008878429404750933) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0042588768779937706) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.02078294014465064) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.008582094874939685) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.04267834809422493) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.702, 0.03687106895353645) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.562, 0.06154885667562485) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.659, 0.05023457546532154) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.045359280467033386), (0.321, 0.059396352589130404), (0.405, 0.06267942577600479), (0.434, 0.07932132732868194), (0.459, 0.10350485199689866), (0.425, 0.12845924731343986), (0.434, 0.14465451043099165), (0.452, 0.1671894628610462), (0.465, 0.1650114922504872), (0.456, 0.1857669574674219), (0.432, 0.21375495231803507), (0.531, 0.0645534948259592), (0.585, 0.049364851012825964), (0.64, 0.039147841796278955), (0.628, 0.03985655578970909), (0.621, 0.047099037274718286), (0.633, 0.037347400307655335), (0.602, 0.04380184018611908), (0.627, 0.03606247943639755), (0.618, 0.03923467706143856), (0.626, 0.037392334520816806), (0.611, 0.053763357259333136), (0.649, 0.03391189932823181), (0.618, 0.04505856922268867), (0.634, 0.03689878782629967), (0.621, 0.038544788971543315), (0.651, 0.039196858525276185), (0.579, 0.04719967560470104), (0.619, 0.04324448388814926), (0.606, 0.04695139651745558), (0.625, 0.04488866962492466), (0.588, 0.05510284277796745), (0.605, 0.04884500573575497), (0.597, 0.0488756782412529), (0.609, 0.04602028173208237), (0.606, 0.05859528623521328), (0.612, 0.045615816984325645), (0.601, 0.052101950481534), (0.604, 0.05035646061599255), (0.583, 0.068796376273036), (0.596, 0.051742311999201776), (0.597, 0.06398559787124396), (0.586, 0.08693953461945057), (0.583, 0.07218753892555833), (0.578, 0.06984587955474854), (0.592, 0.059516092985868456), (0.621, 0.05109221513569355), (0.598, 0.05936514323949814), (0.581, 0.05150219419598579), (0.579, 0.054605647414922714), (0.629, 0.04267834809422493)]
TEST: 
[(0.25, 0.044425843864679335), (0.315, 0.05723254606127739), (0.4005, 0.06019690066576004), (0.43425, 0.07564766851067543), (0.455, 0.09838825124502182), (0.426, 0.12078447866439819), (0.4355, 0.13630374038219453), (0.44775, 0.15698001247644425), (0.46175, 0.15570431262254714), (0.45275, 0.1750297738313675), (0.43225, 0.19910654628276825), (0.527, 0.06227784091234207), (0.59475, 0.046981871470808984), (0.6415, 0.037548506639897825), (0.64975, 0.035944655060768126), (0.64075, 0.04353973954916), (0.64325, 0.035218308441340926), (0.62275, 0.03966344904899597), (0.64675, 0.03273937960714102), (0.63875, 0.03599010384827852), (0.6305, 0.034551365181803705), (0.6165, 0.04937778188288212), (0.652, 0.03226190445572138), (0.6245, 0.041446293458342554), (0.65075, 0.03403296792507172), (0.637, 0.03736185308545828), (0.653, 0.03724282870441675), (0.60275, 0.045010578289628025), (0.626, 0.039734860360622404), (0.606, 0.04559520058333874), (0.618, 0.04336678574234247), (0.61075, 0.052823788061738014), (0.6255, 0.04634587630629539), (0.616, 0.043660127952694895), (0.61975, 0.043366797491908075), (0.61175, 0.05393244254589081), (0.62825, 0.0446736890822649), (0.60975, 0.05000626319646835), (0.5925, 0.049351303681731226), (0.58125, 0.06553691805899144), (0.5975, 0.051574123471975326), (0.601, 0.059855750516057016), (0.59225, 0.08143747195601464), (0.582, 0.06916513547301292), (0.58225, 0.06699644875526428), (0.603, 0.05294524785876274), (0.626, 0.04625866949558258), (0.61625, 0.05349217234551907), (0.58175, 0.05092757496237755), (0.58175, 0.054559052020311354), (0.62975, 0.04189872045814991)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.87      0.58      0.70      1000
           3       0.50      0.46      0.48      1000
           5       0.50      0.69      0.58      1000
           9       0.76      0.79      0.77      1000

    accuracy                           0.63      4000
   macro avg       0.66      0.63      0.63      4000
weighted avg       0.66      0.63      0.63      4000

Collaboration_DC_1
VAL: 
[(0.274, 0.04425249779224396), (0.25, 0.0762955961972475), (0.251, 0.09520264573395253), (0.301, 0.11468889297544957), (0.395, 0.13766499603539706), (0.39, 0.15613945835083723), (0.431, 0.17646908596530556), (0.459, 0.1882399409338832), (0.462, 0.1933495959304273), (0.466, 0.21793028167169542), (0.462, 0.23726580816414208), (0.551, 0.07493435553461313), (0.611, 0.05959854617342353), (0.633, 0.05598449590615928), (0.623, 0.05218482429534197), (0.639, 0.05095427404344082), (0.664, 0.04122888460755348), (0.688, 0.038588111877441406), (0.671, 0.049539245001971724), (0.615, 0.05281341263279319), (0.721, 0.03175914756953716), (0.655, 0.053574360985308885), (0.711, 0.03438514178991318), (0.704, 0.03439344997704029), (0.7, 0.03438923051953316), (0.699, 0.034845386957749726), (0.716, 0.0331058924049139), (0.685, 0.04170861675962806), (0.696, 0.03704631482064724), (0.7, 0.03703138330578804), (0.719, 0.03486934889853001), (0.726, 0.03163460413366556), (0.707, 0.034166932478547095), (0.698, 0.031806013941764834), (0.713, 0.03343240685760975), (0.712, 0.03312940334528685), (0.642, 0.0579862934499979), (0.706, 0.03416846492886543), (0.682, 0.04090061032492667), (0.66, 0.04953942716494203), (0.696, 0.03716631327569485), (0.7, 0.04237676229700446), (0.693, 0.04441721059381962), (0.646, 0.05923976104985922), (0.707, 0.03470559747517109), (0.66, 0.04868223492614925), (0.704, 0.03952310121804476), (0.702, 0.0403834334500134), (0.683, 0.03963112077116966), (0.702, 0.03527464708685875), (0.702, 0.03687106895353645)]
TEST: 
[(0.27025, 0.043206459641456606), (0.25, 0.07365380853414535), (0.25025, 0.09170785903930664), (0.305, 0.110004026055336), (0.39225, 0.13228247344493865), (0.3865, 0.14950004374980927), (0.42625, 0.1695541403889656), (0.4605, 0.1806327423453331), (0.45975, 0.18581818532943725), (0.4595, 0.20804015803337098), (0.4505, 0.22729678165912628), (0.538, 0.07301934373378753), (0.6035, 0.055241242840886114), (0.617, 0.052568207442760466), (0.62625, 0.048117617532610894), (0.64575, 0.04524275673925877), (0.652, 0.04094806185364723), (0.701, 0.036317258149385453), (0.66825, 0.04298847271502018), (0.6305, 0.047316423088312146), (0.72825, 0.030105047561228275), (0.65175, 0.04955049154162407), (0.72125, 0.03175023113191128), (0.7165, 0.031088834144175053), (0.6995, 0.03230308086425066), (0.70775, 0.034647127740085125), (0.73325, 0.03120416797697544), (0.68975, 0.03943254986405373), (0.7005, 0.03458596151322126), (0.682, 0.03640865524113178), (0.71725, 0.03347309861332178), (0.72375, 0.03124683105200529), (0.719, 0.03324974419176579), (0.70675, 0.03093369921296835), (0.69925, 0.03366062715649605), (0.707, 0.0324459615200758), (0.62625, 0.056523373171687125), (0.705, 0.03350135641545057), (0.68175, 0.03973159112781286), (0.664, 0.04605113327503205), (0.69625, 0.03537722599506378), (0.695, 0.040852366045117376), (0.676, 0.04352386402338743), (0.6405, 0.054151259884238245), (0.715, 0.033971180059015754), (0.669, 0.04435645979642868), (0.71025, 0.0368765667155385), (0.70375, 0.03944055879116058), (0.68225, 0.0389645057246089), (0.705, 0.03485245931893587), (0.72175, 0.03359880774468184)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.85      0.69      0.77      1000
           1       0.84      0.90      0.87      1000
           3       0.62      0.69      0.65      1000
           4       0.59      0.61      0.60      1000

    accuracy                           0.72      4000
   macro avg       0.73      0.72      0.72      4000
weighted avg       0.73      0.72      0.72      4000

Collaboration_DC_2
VAL: 
[(0.254, 0.04541688692569733), (0.317, 0.08767266011238098), (0.333, 0.12743896949291228), (0.39, 0.15861733889579774), (0.423, 0.17524761104583741), (0.423, 0.18912637266516685), (0.413, 0.19739552516490222), (0.415, 0.18544992347434164), (0.428, 0.22099166601337492), (0.418, 0.268153380241245), (0.431, 0.27721165637299416), (0.498, 0.06050598081946373), (0.573, 0.05624754682183266), (0.583, 0.04536744970083237), (0.603, 0.044257869005203246), (0.609, 0.04315116593241691), (0.627, 0.04468450033664703), (0.645, 0.04154341906309128), (0.619, 0.045319867610931396), (0.582, 0.05062305986881256), (0.621, 0.045429308861494065), (0.604, 0.048281490683555604), (0.595, 0.04906456309556961), (0.553, 0.05759781211614609), (0.55, 0.0550957618355751), (0.558, 0.05781593102216721), (0.558, 0.05853276693820953), (0.558, 0.05797230553627014), (0.548, 0.05697040432691574), (0.521, 0.06520220947265624), (0.555, 0.06651147067546845), (0.548, 0.0667890767455101), (0.539, 0.068774255245924), (0.565, 0.0669960092306137), (0.475, 0.08538381117582321), (0.532, 0.06881767660379409), (0.58, 0.05430969947576523), (0.564, 0.06002319741249085), (0.506, 0.0779096896648407), (0.56, 0.06194470918178558), (0.555, 0.06773818904161454), (0.564, 0.06469947388768196), (0.538, 0.07316405853629113), (0.582, 0.057442379266023635), (0.57, 0.06054171386361122), (0.557, 0.05985822921991348), (0.561, 0.07010286509990692), (0.555, 0.07530310827493668), (0.533, 0.06783287328481674), (0.544, 0.05964411532878876), (0.562, 0.06154885667562485)]
TEST: 
[(0.25275, 0.04452108883857727), (0.3225, 0.08423771148920059), (0.348, 0.12218378227949142), (0.39725, 0.15200227892398835), (0.41825, 0.16702659863233565), (0.416, 0.18177365124225617), (0.3995, 0.19000737953186037), (0.40175, 0.17578768062591552), (0.42075, 0.21132051086425782), (0.41375, 0.25228228414058684), (0.42475, 0.2621416676044464), (0.49425, 0.06165938206017017), (0.5515, 0.056918161258101466), (0.5875, 0.04643604685366154), (0.57225, 0.04692082215845585), (0.594, 0.04535527186095715), (0.607, 0.04637687529623508), (0.61075, 0.04524887028336525), (0.5915, 0.04601358541846275), (0.575, 0.052119587168097496), (0.60975, 0.047080952540040015), (0.5995, 0.049296200409531594), (0.60075, 0.049560405015945434), (0.54725, 0.05790209180116653), (0.54925, 0.05890310600399971), (0.56125, 0.05992785415053368), (0.55525, 0.058417862713336946), (0.559, 0.05820524278283119), (0.55425, 0.058805901423096656), (0.534, 0.06547306361794472), (0.545, 0.06317285074293613), (0.5315, 0.06697955566644669), (0.5385, 0.07080373127758503), (0.54525, 0.07081514720618724), (0.481, 0.08574507746100425), (0.52725, 0.06899307024478912), (0.586, 0.054536001831293104), (0.54825, 0.06186560356616974), (0.511, 0.07539041039347649), (0.559, 0.06191449415683746), (0.53825, 0.07023690797388554), (0.562, 0.06323088283836842), (0.5335, 0.07236340782046317), (0.575, 0.05593725529313087), (0.55075, 0.06492831727862358), (0.55225, 0.061017062276601794), (0.54525, 0.06988062435388565), (0.545, 0.07468742138147354), (0.546, 0.06762785150110721), (0.547, 0.06096691831946373), (0.5535, 0.06205918993055821)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.82      0.31      0.45      1000
           2       0.42      0.70      0.52      1000
           3       0.52      0.46      0.48      1000
           6       0.71      0.74      0.72      1000

    accuracy                           0.55      4000
   macro avg       0.61      0.55      0.55      4000
weighted avg       0.61      0.55      0.55      4000

Collaboration_DC_3
VAL: 
[(0.255, 0.044730136394500734), (0.45, 0.0864397220313549), (0.465, 0.09229815077781678), (0.477, 0.1226446467190981), (0.477, 0.1584977867305279), (0.481, 0.19356412656605243), (0.477, 0.22081087283417583), (0.479, 0.23137943388335408), (0.48, 0.2418985731303692), (0.479, 0.272056638228707), (0.48, 0.26264312120061367), (0.546, 0.1071524434350431), (0.603, 0.06104728062450886), (0.615, 0.059201851420104507), (0.624, 0.049247145429253576), (0.618, 0.05357923308759928), (0.642, 0.04564088509976864), (0.637, 0.044390617221593855), (0.629, 0.05081037163734436), (0.675, 0.05259070612490177), (0.641, 0.051273112155497076), (0.666, 0.0418431014418602), (0.671, 0.043836831778287885), (0.676, 0.04124637323617935), (0.677, 0.04422350770235062), (0.684, 0.03616573172807693), (0.681, 0.03846486604213715), (0.666, 0.04435255852341652), (0.672, 0.0407764059305191), (0.672, 0.041197559028863906), (0.666, 0.04245605356991291), (0.7, 0.040096805185079576), (0.667, 0.04513841795921326), (0.668, 0.046996025562286374), (0.693, 0.038484960734844205), (0.667, 0.046215984255075455), (0.649, 0.05549259197711945), (0.664, 0.04587805797159672), (0.684, 0.039540403723716734), (0.676, 0.04787994334101677), (0.672, 0.04646346390247345), (0.671, 0.04811920803785324), (0.658, 0.049222037866711615), (0.668, 0.04950304961204529), (0.688, 0.04017108145356178), (0.685, 0.04282428452372551), (0.645, 0.05363568024337292), (0.662, 0.05211802940070629), (0.67, 0.05017975597083569), (0.655, 0.05859282848238945), (0.659, 0.05023457546532154)]
TEST: 
[(0.2535, 0.04368370050191879), (0.4385, 0.08277494287490844), (0.46125, 0.08807962861657143), (0.47575, 0.11717280966043472), (0.48025, 0.15088662540912628), (0.47625, 0.1845778265595436), (0.48075, 0.20741392266750336), (0.48275, 0.2172055230140686), (0.48175, 0.22952548241615295), (0.481, 0.25865569376945496), (0.47825, 0.2520137227773666), (0.5435, 0.09865977942943573), (0.5915, 0.05568980316817761), (0.61425, 0.054425792396068574), (0.64, 0.04343159680068493), (0.623, 0.050165301263332365), (0.64775, 0.041655938491225245), (0.66375, 0.03815385750681162), (0.635, 0.04451615239679813), (0.66175, 0.04407582311332226), (0.64275, 0.04570071744918823), (0.681, 0.03721855200827122), (0.67875, 0.038528959080576894), (0.6925, 0.03499324958771467), (0.704, 0.036400986544787885), (0.702, 0.0309039613828063), (0.69925, 0.03368876761943102), (0.69875, 0.03568062375485897), (0.708, 0.034200953878462316), (0.667, 0.038822884730994704), (0.6925, 0.03707000422477722), (0.695, 0.03495144104212523), (0.67275, 0.03825899800658226), (0.673, 0.040057926148176194), (0.70625, 0.033776558913290504), (0.693, 0.03695015784353018), (0.66075, 0.045308836698532105), (0.67275, 0.03956487499177456), (0.67625, 0.03633480125665665), (0.669, 0.04266849805414677), (0.68975, 0.03827255841344595), (0.69025, 0.04060555814951658), (0.67275, 0.03993375741690397), (0.66775, 0.04250105485320091), (0.7095, 0.03331894685328007), (0.7015, 0.036474418990314006), (0.65075, 0.04887132796645165), (0.659, 0.04755086341500282), (0.6825, 0.04167901350557804), (0.6755, 0.04577477765083313), (0.675, 0.04440015572309494)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.76      0.52      0.62      1000
           3       0.73      0.47      0.57      1000
           7       0.62      0.88      0.73      1000
           8       0.67      0.83      0.74      1000

    accuracy                           0.68      4000
   macro avg       0.69      0.68      0.66      4000
weighted avg       0.69      0.68      0.66      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [89]
name: alliance-4-dcs-89
score_metric: contrloss
aggregation: <function fed_avg at 0x73f4d9ca3c10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=89
Partitioning data
[[3, 9, 1, 4], [0, 2, 1, 4], [8, 6, 1, 4], [5, 7, 1, 4]]
[(array([13581, 11353, 30571, 14064, 46596, 26097, 45402,  7200, 17497,
        3134,  2665, 24099, 41137, 17655, 33371, 39773, 48088, 45634,
       31198, 16586, 19455,  8179, 40587, 39868,  3000, 45217,  1966,
       26036,  8408, 29979,  6556,   367, 25315, 33443, 44913, 10209,
       21106, 41513,  7075, 23885, 32075, 21687, 40651, 18814, 45216,
       41725, 43262, 47902, 11805, 33980,  1625,  6198, 39135, 35154,
       19496, 13371, 24847, 43254,  2947, 15753, 27722, 13062, 27735,
       37370, 15706,  5876, 24538, 43744, 42590, 43843, 20932,   806,
       19913, 20469, 39287, 48131, 20770, 26022, 14779, 39980, 13253,
       14091, 12076, 15467,   685, 24348,  6422, 39865,  9332,  8162,
        2565, 19726, 37924, 39187, 45502, 21712, 34689, 43017, 17897,
       39165, 42970, 34999, 13205, 39101, 38879, 20371, 44856, 39398,
       43237, 32541, 36113, 35071, 15080,  1164, 38897, 41951, 39158,
       38444, 43428,  5725, 32404, 36159, 47666,  8434,   995, 13686,
       37546, 10101, 17155, 12872, 30809, 44110, 11047, 40433, 14936,
       20488, 43034, 17333, 49430, 22244,  4197,  7869, 18570, 48592,
        3927, 25640, 23526,  7259,   639, 16190, 27683, 19617, 39461,
       40097, 37057, 21551,  7997, 32938, 26132,  6014, 16364, 18400,
       22505, 19422, 12370, 14580, 13302,  7665, 39289, 14993, 31861,
       24922, 45250,  6368, 42997, 38891, 41639,  4672, 49143, 19631,
       36219, 28237, 25259, 26707,  5493, 14938,  6343, 46155, 40809,
       36124, 26357, 30213,  9024, 10534, 24606, 14232, 46595,  5553,
       47196, 18414, 11370, 43726, 45914, 45456,  1696, 40399, 49078,
       20534, 38390, 39885, 24082, 20058, 11359, 47062, 28786, 28554,
       11623,   314,  3910,  8988, 42198,  9300, 27673,  9429, 31449,
       35449, 26558,  8105, 46423, 33422, 27658, 21731,  3875, 12007,
       45901, 25257, 25563, 42646, 41705,  5502,  8386, 35427, 22226,
       24704, 15335, 39689, 26665, 26900, 22523, 42682, 42407, 19380,
       44809, 39319, 38402,  2506, 11918, 12086, 36029, 49360, 13889,
       10533, 12541,  5009, 10138, 24173,  3477, 37536, 36967, 30127,
       14099, 39572, 37857, 44646, 48769, 23990,  7838, 31744, 26597,
       47066, 47687, 24436,  2632, 49031, 42224, 49662, 11901, 41631,
       15141, 33248, 35504, 48431, 10373, 34034, 29148, 41611, 19601,
       26832, 21138, 28932, 40267, 28376, 26908,  6002, 34666, 36050,
        7040, 39993, 42739, 48126,  3974, 15252, 34772, 31918, 38601,
       39699, 38663, 14571, 44606, 42278, 33025, 35064, 35749, 36475,
       45465,  9701,  6158, 34006, 28495,  5079, 23169, 18439, 14572,
       38328, 27807, 39870, 30525, 40030, 46243, 45101, 33053,  6984,
        3929, 34957, 29105, 31637, 35852, 25973, 13346, 12040,  1000,
       36705, 38380, 40254, 45284,  6360,  3479, 27168, 32164, 20013,
        2199,  1115, 40082,  3316,  9302, 32439,   205, 41904, 34480,
       15817, 31378, 27957, 21571, 49029, 49405, 41284, 32879, 40559,
       33281, 16163, 40895, 26531,  8532, 17099, 36590, 18648, 28949,
       32902, 34232, 46698, 35607,  7302, 49074, 19674, 46651, 27752,
       12011, 20799, 14880, 26252, 11111, 17452,   316,   188,  6480,
       33886, 34903, 20787,  4407, 38324, 39611, 30080, 12288, 43171,
        6457,  3700,  3633, 17875, 11517, 18183, 49403, 40693, 10523,
        3876, 43522,  3023, 29628, 10056, 36741, 41535, 23783, 37126,
       19199, 44009,   269, 39331, 40742, 34474, 23657,  4587, 28259,
       32841, 17171, 13128, 48451, 10252, 36109, 49634, 41508, 36859,
        8841, 28304, 41395, 13642, 42885, 24048, 43614, 36165, 24731,
       47303, 43104, 26945, 30580, 10717, 10118, 23341, 41683, 46844,
       15993, 10078,   613, 17607, 24246, 34843, 41746, 34549, 41541,
       19805, 44280, 30733, 33024, 37349, 30678, 34423, 33679, 24893,
       28915, 37968,  4256,  1767,  6610, 46227, 20675, 10859, 26013,
       26674, 15586, 27821, 42452, 24320, 47365, 32937, 37849,  6241,
       14507, 34810, 12929, 10714, 13002, 36868, 20667, 20155, 44725,
       25459, 42976,  4528, 25689, 17275, 33261, 43932, 15386, 40822,
       19354, 31890, 20619, 47470, 29429,  5568, 48128,  6733,  9318,
       46679, 40145, 45126, 31352,   997, 21938,  3379, 29986, 42793,
       29963, 30198,  2731, 18456, 22060, 25046, 20368, 35106, 28001,
       35850, 30782, 23662,  5344, 16986, 12586, 10035,  6383, 19526,
        7992,  3734, 11708,  2259, 16310, 47279, 11804,  9327, 42282,
       19773, 46396,  7861, 35863, 38078, 29318, 45077, 22437, 17279,
       35567, 44541, 29874, 24119, 27215, 11522, 15905, 41472, 45613,
       49933, 22479, 35692, 35021, 38223, 38892, 27868, 46461, 42347,
       41502, 47900, 47178,  9411, 11971, 47588, 44341, 39422,  7723,
       34420, 28861, 29019, 36611, 42081, 10122, 40000, 46162, 20307,
       18752, 14007, 49594, 44767, 16066, 31720, 32736, 48595, 34629,
        7430, 31747,  4047, 15286, 13166, 31348, 31583, 26950, 17534,
       14061, 38175, 33748, 41289, 21407, 18654, 33624, 19046, 16745,
       15738, 26470, 18671, 47377, 41262, 27792, 12169, 35516, 22472,
       41520, 19769, 41723,  7145, 12185, 42163,  7683, 23929, 19324,
       23020,  6608, 18069, 32410,  4553, 13366, 13922, 20463, 37422,
       31282,  4011, 15934, 10655, 18160,  6623, 38577, 13296, 25323,
       29875,  1907, 44472, 24161,  6703,  9380, 47680, 30457, 36331,
       11824, 10079, 16992, 15583,  2469, 42085, 20796, 39906,  8346,
       31203, 34906, 14964, 20937, 22349, 31607, 18526, 48086, 17032,
       45316, 17718, 26740,  6831,  9232, 13108, 33866, 18961, 11558,
       21796, 19485,  6798, 38725, 15897, 44869, 19164, 45212,  2023,
       42007, 40258, 41215, 29023, 13417, 21153, 23203, 13849, 33561,
        2390, 49643, 46357, 28476,  9668, 31523, 26465, 34993, 14878,
       43711, 11782,  4565, 42802, 45704, 48539,  7130, 46514, 36251,
       15114, 46177, 32235, 48807,  6485, 18912,  2363,  3948, 48613,
       25232, 46061, 19568, 13834, 17632,   345, 28372, 36047,  9061,
        3802,  2300, 48351, 19118, 42313, 34562, 21733, 12737, 35915,
       28577, 40757, 24996,  7317, 17878, 14635,  3464, 46526,   505,
         449, 30585, 28407,   887, 11138, 21912, 10447, 15170, 19643,
       23367, 38964, 28040, 23792, 47239, 40642, 40309, 21187, 23022,
       32518, 42700, 37389, 44032, 36383, 25977, 44397, 19475, 19340,
       43139, 10736, 48605, 16216, 41814, 38351, 42750,  3533,  2842,
        6924,  6891, 36817, 25954, 18060, 41162,   632, 19952, 36566,
       40794, 29419, 37923, 36896, 33095, 47837, 43595, 14956, 26860,
       49027, 10865, 36976, 23755, 26892, 23211,  7296, 14036, 24265,
       28319, 25355,  8987, 46133,  7787, 23057,  7851, 42544, 42729,
       12026, 41301, 33796, 48478, 10808, 48824, 38598, 22259, 39656,
        6877,  4043,  9615, 44517,  1695, 19543, 22246,  3027, 49608,
       32494, 27535, 29835,  4305, 30630, 14670, 29737, 13466, 36160,
       47257, 15718,  8396, 23954,  1315, 42910, 48997, 42640, 35877,
       36652, 43032, 28614, 14777, 35180,  4158, 43203, 18043, 15267,
       44200, 38454, 24630, 15036, 13427, 36090, 30353,   669, 35573,
       27259, 26838, 35714, 30900,  4687, 32643,  2905,  9149, 43650,
       10765, 45775, 34217, 41451, 13691, 45644,  1791, 43860, 28600,
       29070, 18289, 42229, 38776,  5940, 41691, 31003, 27247, 29129,
       21676, 27322,   563, 15152, 39127, 26929, 25346, 14879, 41530,
       29184, 22990, 18300, 29827, 46778,  1516,  8784, 44729,  7382,
       49213, 22798, 22698, 39348,  3236,  9771, 22509, 14474,  1350,
       11957,  2178, 47120, 16033, 16897,  6270,  7743, 10633,  3522,
       12301, 33013, 12075,  2703, 36777, 37089,  9507, 14255,  4527,
        9713, 10900, 31143, 30499,  8137, 28799, 37996, 27891, 45234,
       35954, 41186, 41669,  1934, 24778,  5495, 27596, 15508, 31840,
       22580]), [3, 9, 1, 4]), (array([44167, 49992, 38943, 14754, 19493, 22993, 23796, 41606,  9059,
       38238, 10847,  2365, 33117, 47981, 47328, 16460, 30321,  3304,
       41745, 45412, 30160, 49303, 11424, 35788, 43829, 43012, 27434,
       26607, 37178, 21087, 33943, 44143,  7573, 14945, 38039, 34017,
       28354, 18271, 10637, 35121, 30709, 40125, 37022, 13324, 26922,
       18073,    49, 47486, 19041, 32602, 35858, 11963,  8971, 49490,
       42931,  2617, 47834, 41300,  6743, 28794, 44925, 27911, 37160,
       38036, 46738,  7643,  6276, 42476, 26776, 32206, 48254, 32056,
       41324, 46725, 14116, 22973,  3731, 29225, 14698, 26391,  1381,
       36858, 16137, 10690, 27278, 39329, 24113,  3335, 11734,  2720,
         871,  1935, 45163, 41355, 38345, 11356, 24720, 43875,  2714,
       10205,  4721, 37517,  1524, 17677,  4155, 30383, 44967, 36464,
       42446, 29003, 42456, 28972, 42183,  2574,  6190, 40796,  1950,
       34944, 24079, 41907,  4081, 47468, 21974, 26060, 25942, 17917,
       43649, 17762, 40639, 42628, 10340, 41235, 36353, 33258, 48556,
        5864, 48926, 15304,   733,  3789, 36106, 25212, 45344, 10275,
       32369,  8553, 10990, 17717,  2633, 42332,  1473, 25288, 38613,
       16531, 35286, 25190,  1926, 49517,  2962, 24096, 30428,  2401,
       28665,  4854, 34018, 11372, 28456, 29412, 27137, 20891, 49557,
       10577,  8199, 10301, 36357,   115, 12083, 47105,  5457, 32670,
       16186, 48246, 15077, 34083,  3815, 28921, 23825, 49656,    93,
       15946, 38478,  4653, 48327, 38043,  7093, 44175, 26361, 32107,
       10148, 27042,  4353,  2107, 25236, 25616, 30078, 14153, 26296,
       11224, 46945, 39606, 12420, 46287, 24939, 11709, 18080, 43479,
       32934,  3906,  4477, 40485,   965, 24644,  4552, 30172, 38499,
       24750, 40679, 47771, 41600, 27487,  8062, 20813, 20602,  1187,
       26783, 38179,  1759,  3184, 19778, 47865,  8535, 17115, 34036,
       14413, 39650,  4952,  2413, 11612, 40652, 29364,  2393, 11380,
       39646, 11873, 40348, 37878, 26128,  1527, 43781, 10567, 47826,
        3901, 37713, 13848, 10651, 29742, 36481, 16402, 43898, 49457,
       23918,   933, 27484, 36025, 39882, 26386, 10628,  6097, 23537,
       14946,  9597, 19868, 18040,  2626, 47932, 32792,  6488, 36213,
       28933, 35792,  6369, 37298, 38032, 42889, 36377, 25148, 38745,
       30665, 35031, 25929, 21867, 30333, 30746, 22281,  3193, 20216,
       32697,   522, 38459, 17739, 38054, 25381, 11934,  4525,  9503,
       42588, 13179, 20038, 33396, 46564, 39237,  1798, 35486, 21746,
       40527, 44995, 44427,  2927,  2202,  9151, 32071, 46261, 38001,
       40463, 39847, 48090, 46088,  4549,   513, 40333, 46943, 42774,
       14811, 21452,   403,  7761, 36759, 37645, 13270, 33644, 26176,
       13814, 32908, 39407, 13195, 42600,  8032, 17702, 10833, 34196,
       44658,  6843, 13130, 49730, 20700, 24706, 45460, 42928, 46326,
       40282, 35096,  4518, 20547,  9712, 45131, 19056,  6757,  7662,
       20656, 44335, 19932,    24, 31705, 33361, 11730, 17834, 20183,
       17086,  9249,   218, 31501, 37581,    47, 19815, 26795, 45533,
       46509, 14314, 16288, 49093, 12361, 42990, 12589, 26864, 25989,
        5871, 11348,  4370,  5889, 35616,  3259, 32406, 11962, 25695,
       12067, 13901, 21135, 36170,   196, 36726, 13770, 10742, 29625,
       38129, 27097, 10461, 37446, 49784, 15139, 29379,  7664, 49693,
       41925, 44008, 26999, 43523, 48101, 24409,  6946,   724,  7658,
       43747, 32419,  1139, 32067, 26572, 41775, 40087, 48735,  2126,
       18027,  8081, 10756, 42450, 34925,  9011,  9940,  3349, 42581,
       34734, 15065, 37315,  9790, 12148, 27980,  7477, 31007, 10204,
       31707, 42564,  3944, 27877, 11548,  4930, 35104, 41721, 21145,
       26442, 48951, 14088, 29907,  2136, 43041,  6772, 46957, 46584,
        3321, 42158, 19899, 10940, 36994, 14646, 27356, 13934, 10968,
       29637, 13819, 36581,  1789, 27746, 47939, 43853, 12276, 19202,
       34336, 32564,  9549, 32472, 35344,  1574, 43214, 29252, 22298,
        2727, 18048, 39652, 38756, 33207,  2173, 15363, 24385, 14502,
       42921, 26077, 23313, 18890, 27942, 39338, 29777, 17268, 21150,
       31019, 41529, 48820, 16471, 23332, 45769, 29188,  7198, 18078,
       30407, 18844, 27465, 10229, 45675,  1037, 10795, 35965, 20047,
       37702, 34364,  8547, 34850,  9898, 44841, 15885,  5171,  9476,
       37826, 16385, 41710, 45834,  2587, 21152, 45157, 12773,  8529,
       14379,  6814, 32258, 44524, 16077,  4492, 28643,  9717, 44109,
       10039,  7288, 45475, 31253, 12923, 16638, 30313, 21593, 10446,
        3346, 38086, 20168,  9878, 29044, 38420, 42475, 10953, 41248,
       22089,  6074, 42734, 23260, 14376,  7338, 46827, 26550, 19939,
       46544, 49381, 46859,  4100, 22553, 20725,  4132, 49045, 26930,
       28655, 13852, 34163,  8965, 16947, 16594, 48709, 40971,  5811,
       42448, 46900, 33951, 33126, 47529, 24446,  2038, 42239, 29146,
       27447, 33611, 39195, 49348, 47088,  9857, 46993, 40518,  7326,
       49629, 29226, 41706, 26787, 48667,    79,  5422,   840, 46866,
         261, 11631, 44204, 34128, 17112, 29901, 14859, 47310, 31957,
        5157, 28626, 48234, 24284,  6226, 40449, 28127, 21309, 34118,
        7261, 30804, 19185, 45972, 27089, 31959,  8944, 48681, 24228,
       40152, 25993, 23127, 39996, 37666, 10074, 33742,  9400, 12080,
        2072, 45011, 24151, 35635,  4265, 16777,  6349,  7349, 14549,
       15756,  8450, 29828,  1901, 37743, 29512,  4496, 43274,  3414,
       31079, 44591, 11696, 25050,  7605, 12100, 31288, 32462, 28298,
        4993, 42710, 43480, 36400, 12620, 23275, 36663, 10479, 12025,
       28420,  2200, 23213,  8475, 38908, 29029, 23210, 44423, 26295,
       48639, 33314,  6813, 19989, 18527, 36813, 42413,  8127, 24484,
       41963, 24008,  1464, 19571, 30062, 26870, 44537, 15984, 41209,
       27262, 19181, 32743, 20206, 27282,  8913,   272, 47975, 20340,
       42072, 30307, 14305, 38033,  1268, 11582, 17197, 41219, 31446,
       10661, 45293, 20515, 12313, 36317, 46076, 29852, 28823, 15012,
       34350, 28289,  9650,  8405, 16155,  9816, 26844, 14388, 10951,
        2695,  7059, 31733, 40188, 21781, 23013,   399, 37577, 29111,
       18404, 12997, 28084, 28903, 32969, 39413, 25579, 46947, 43228,
       35806, 31953, 29698,  5491, 46531, 31804, 38668,  6879, 31998,
       12417, 27029, 48884, 17953, 48447, 27220, 27390, 19331, 11860,
         660,  8049, 30881, 47139, 49669, 34477, 39772, 25098,  9141,
       41766, 19345, 12489, 40116, 37172, 34114, 48801, 42603, 16880,
       41270,  8184,  7811, 24810,  2647, 18365, 38603,  9568, 29525,
       31549, 48202, 35185, 17039,  1804, 39377, 21629, 16672, 32803,
        7110, 43758,  7269, 14584, 40027, 16749, 32818, 36420, 13510,
       10282, 10966,  3684, 18560, 27946, 23739, 49685,  4987, 43810,
       48902,  6911, 35432,  1904, 28830, 43231, 19264, 40949,  4642,
        3326, 41337, 41056, 22443, 22617, 13764, 16772, 25403, 24594,
       27489, 10154, 24846, 21659,  8930, 36442, 20343,  2155, 33279,
       44760, 42979, 46998, 28809, 15744, 31277, 38995, 11872, 31709,
       25931, 46650,  1952, 14810,  6339, 33482,  1992, 49114, 27630,
       36514, 41103, 28755, 28751, 22949, 36731,  3784, 42907, 25871,
       14284,  2673, 20011, 19943, 19704, 10264,  4297, 14051,  4447,
       22018, 20409, 42982, 18813, 37853, 44614, 31932, 32533, 33226,
        5067, 27455, 10134, 45199,  1007, 23200, 49318, 11021,  1425,
       27221, 36740, 42418, 30449, 17725, 24084, 32413, 24785, 34956,
       21194, 20342, 34328,  6605,  6128, 47079,  7383, 27649, 14492,
       44007, 16890, 39177, 24804,  2225, 20608, 43584, 10904, 33266,
       34182, 21597, 12846, 24776, 22144,  2672, 45114, 39467, 17483,
       11249,  8114, 45400, 15624, 42636, 26631, 19407,  9260, 20521,
       10131]), [0, 2, 1, 4]), (array([48247, 20762, 49551, 29868, 44236, 29075, 15260, 16466, 14891,
       30357, 24692, 25521, 34712, 33441, 16422, 38587, 39802, 23438,
        6651, 34491, 38686,  4283,  9599, 13227, 46892, 31423, 27507,
       39094, 13425, 20455, 49985,  9265, 45024, 28396, 23408, 15743,
       45062, 38416,  1162, 46488, 34473, 10856,  9675, 48757, 19979,
       42333,   716,  4298,  4823, 13582,  1512, 39184, 26055,  8222,
       24554, 46033,  1763, 39254, 49681, 20491,  1751, 34441, 41761,
       49861, 19851, 36221, 26463, 10923,  4955, 34369, 49389, 20650,
       29782, 12426, 19748, 28599, 44384,  8329, 16132,  8495, 39507,
         460,  9722, 42469, 32562,  4204,  1370, 48541,  4780, 49899,
       15250, 42693, 18565, 19509, 17588, 21157, 37318, 27864, 43913,
       40391,  8138, 25501, 31489, 26131, 49388, 16727, 23205, 15046,
       39336, 19887, 45779, 31456, 20822, 39715, 17894,  9192, 22268,
       11327, 42713, 30026, 48984, 13874,  8384, 44180, 21036, 41629,
       33398, 18444,  6153, 37643,  2434, 24435, 47597, 39702, 42254,
       35593, 40345, 29371, 47342, 11180, 13702,  4758, 19666,  8830,
        8248, 12513, 10443, 26147, 38460, 14872, 25855, 38221, 38925,
       20617, 47395, 43492, 37633, 47506, 38065, 31778,  6825, 19605,
       16724, 45104, 20479, 13992,  6773, 27458,  9023, 26784,  5283,
       45912, 13788, 20288, 48905, 15811, 29056, 19991, 45620,  3215,
       33815,  3518, 13386, 28491, 24876, 43109,  2979, 13576, 10836,
       34210, 45447, 19836, 35224, 14642, 10822, 25869, 39317, 45240,
       40702, 30955, 16086, 34656, 27715, 35367, 45868, 15049,  3832,
       12319, 10956, 30844,  5730, 10234, 42933, 23312, 31725, 36390,
       27615, 14693,  6846, 46938, 19398, 46888, 13004,  6789, 31841,
       43183, 16178, 12861, 25618, 17394, 17220, 40062,  8526,  4652,
       26004, 16044, 17335, 19720, 38694, 49851, 27842, 21499,  9346,
       12646, 32213, 27988, 43982,  6912, 11674,  2918,   164, 41870,
       14162, 17035,  7041, 21424,  2267,  2939,  2019, 28678, 30130,
        6918, 47347, 43070, 22534, 45970,  5318, 31382, 43426, 23520,
       19128, 10113, 16507, 24418, 13123, 37574, 48201, 38197, 26151,
        7392, 13796, 32637, 36360, 31616, 27927, 49289, 18801, 46134,
       38061, 26243, 35099, 40653, 39874, 30176, 35114, 44078, 18816,
        3976, 43452, 34652, 34432, 37992, 22654,  7137, 20324, 19511,
       48298, 38865,  4208, 20098, 44612, 17248, 24184, 36192, 41184,
       47734, 26324,  2494, 35499, 25985, 31558, 14039, 41004, 12214,
       47071, 48400,  7215, 36586, 17680,  1031, 26066, 15497, 28378,
        9653, 40274, 36808, 15651, 43488, 44194, 39007, 30427, 16546,
       12321,  5745, 30367, 38438, 27450, 37796,  5679, 46124, 28139,
       12765, 30626,  6572, 11644, 35270, 45585, 10792, 26429, 34804,
       35900, 23821, 49804, 23025, 27134, 33425,  3655, 33012, 32793,
       34362, 25396, 48723,  6367,  6395,  6810, 11319, 34322, 38537,
       48460,   588, 14830, 19834, 37926, 21917, 25417,  5907, 28990,
       39252, 41448, 34505,  3018, 20767, 28986, 31465, 18851,  1766,
       17282, 48229, 12445, 31933,  2729, 20993, 25729, 43439, 16760,
       40811, 17000, 24117, 25085, 48710, 17075, 11150,  7143, 17952,
       30100, 16404,  9303, 40929,  3716, 47464,  5025, 44708, 40164,
       46743, 40357, 45019, 31099, 48932, 44566, 12122, 38993, 28956,
       15294, 45958, 28021, 44518, 49892, 43645, 20930, 47752, 29223,
       47728, 13797,  9468, 24198, 35703, 49305, 20293, 41909,  9808,
       49311,  3070, 41809, 46913, 22100, 26122, 49708, 18336, 45383,
       22345, 17788, 37481, 25067, 41074, 45823, 15220,  1374, 23881,
        7675, 43538, 24882, 49776, 37487, 25747, 36089, 44696,  2965,
       13683, 43298, 20561, 19924, 30741, 38606, 49210, 35208, 46180,
       29677, 43577, 48665, 18407, 22101, 23912,  7976, 31228, 16129,
       41593, 21200, 43481, 32534, 28374,  2851, 21104, 17970,  4858,
       45628, 31935, 43145, 12893, 49416,  3937, 40279, 37844, 48740,
        4099, 24466, 20856, 48182, 18744, 15748, 42438,  7005, 14204,
       27419, 45500, 47749,  3062, 22984, 40537,  1578, 49383, 13996,
       13505, 30136, 23837, 46173, 47616, 19609, 48728, 14048, 10387,
        7529, 10912, 22829, 37494,  9538, 22306, 32580, 16985, 47845,
       11850, 22884, 21675,  6899, 48917, 34442, 29693,  3452, 40815,
       38646, 20376, 28163, 24137, 47035,   206, 35048, 18263, 37512,
       12071, 36641, 42237, 17815, 44889, 34701,   942, 49658,  7777,
       33963, 35364, 17662,  2445, 38822, 13144, 14110, 22227, 33164,
       21218, 47987, 30783, 45535, 39737,  5917, 33762,  8693, 36158,
       36237, 30083, 15299, 24986, 26074,  4459, 34909, 14904,  5144,
       16588, 18425, 28417, 14005,  5197,  4557,  5816, 34426, 15183,
       49616, 45099, 45899, 18178, 18388, 49932, 28532, 25926, 41964,
       36298, 15609, 20931, 32438, 35081,  2656, 15988, 32474, 39811,
       42287, 32418,  9781, 12280, 33392, 49494, 44309,  4533, 22788,
       44308, 17106, 17856, 14068, 27716, 17334, 40453, 19898, 15855,
       47362, 18246,   364, 13234, 35953, 24774,  8576, 13858,  1551,
       42709,  6353, 41627, 38192, 29953, 35666,  2802, 10248, 19351,
       44411,  1021, 35366, 45013, 19490, 10453,  3781, 41307, 20444,
       47633,  8977, 38028, 44395, 39277, 26794, 15832,  2883,  1694,
       15496, 34376, 39745, 32681, 14512, 19265, 26115,   325, 16682,
       19313, 49080, 46359, 14209, 48669, 19598, 32011, 21181,  5635,
       37239, 30553, 13952,  2268, 40341, 43167, 10188, 20746, 28175,
       18518, 33838, 22037, 45210, 27178, 32941, 43684,  1605, 48954,
       14288, 39912, 37213, 42341,  1020, 25377,  4949, 16223,  9095,
       39109, 32083, 34791, 49716, 17454,  9012, 21180, 35508, 37533,
       34233, 16855,   250, 30949,  3444, 21870, 40034, 12800,  7286,
       32042, 44601, 44225, 12805, 30681, 29489,  9202,  9074, 10615,
       38787,  5754, 46153, 14667, 34431, 22149,  5756, 46823, 43746,
        2056, 40121, 24108,  2911, 22183,  7043, 33666,  6894, 39005,
       13464, 27003, 33369, 16240, 13472, 29396, 44857, 10207, 27394,
       30268, 38511, 20192, 35563, 13145,  4438, 13498,  7100, 35517,
       26653,  3577, 19852, 12803, 24674, 49622, 38354,  3635, 35843,
       40665, 21949,  5909, 38805, 32776, 32095,  2123, 33480, 49077,
       33370, 21853, 31579, 11967,  8279,   951, 18065,  5063, 17338,
        3145, 34950, 31755,  5237,  6039, 49002, 44324, 40848, 27572,
       47928, 36915, 12179,  9967,   764, 45618, 42160, 46968, 43049,
       27935, 37719, 20904, 12645, 45397, 37977, 10067, 11481, 12590,
       45408, 14853, 20108, 24997, 26706, 13927, 27790,  3296,  2787,
       43172,  9782, 28913, 38762, 40562, 41810, 38755, 20435,  9155,
       14905,  3288, 10604, 32179, 40793, 47576, 45111, 41665, 14935,
       45340,  2088, 25620, 17877, 15990, 40842, 47664, 36167, 41573,
       12723,  1882, 36536, 25402, 38213, 13469,  7625, 31939, 22683,
       43974, 24419, 48878, 34699, 14152, 41417, 20363, 15737, 19441,
       38517, 49751, 16587,  3776, 46119, 19032, 39701,   162, 35228,
       12453,  2144,  3509,  8236, 28174, 12858, 35522, 45076,   816,
       23445, 21222, 28414, 19098, 11759, 47295, 28654, 27888, 22205,
        9153,  1069, 17556, 41382, 34038, 21041, 17077, 25250, 36096,
       41191,  6387, 11997, 16007,  3377, 49207, 47312,  8688, 34384,
       37741, 32960,  9761, 42312, 14649, 29939,  5180, 43683,  7642,
       40259, 17049, 37688, 12224, 46470,  4763, 45935, 11626, 40473,
        3587, 49984, 20659, 14104, 21176, 32535, 30061, 37342, 26638,
       19702, 19569,  8447, 27313,  6780,  8254,  5624,  6844, 23937,
       33015,  3189, 10105, 10136, 37294, 35956, 45170, 44172, 14377,
        6710,  4781, 43509, 25371, 21929, 22837, 14737, 47570, 27206,
       12416]), [8, 6, 1, 4]), (array([41098, 10258,   686, 36782, 17797, 34541,  4850,  1847, 40471,
       24160, 16797, 47065, 27893, 47289, 30393, 25230, 29432, 45728,
        6873,  6259, 19856, 36851, 44738,   426, 18346, 25465,  9301,
       11852, 13911, 33311, 47544,  3002, 22473, 16812, 48245, 10632,
        7160, 28877,  2794, 31264, 13257, 18764, 11314, 28661, 12326,
       26987, 14965, 46264, 18230, 26125, 33116, 48625, 21128, 25698,
       31047, 26024, 27771, 46720,  6958, 46842, 37252, 36556,  3017,
       11819, 18082,  7476, 30016, 28902, 27879, 14632,  4529, 44260,
       44781, 33295, 18729,  6760, 10397,  3421, 49327, 49680, 38529,
       43467, 18170, 44899, 45436, 23119, 30349,    70, 48645, 21278,
       49797, 47218, 20836, 33702,  8325, 31949, 20445, 34095,  2314,
         359, 11519,  1545,  6131, 17855, 37486,  5962, 31129, 49980,
       39002, 11770, 38695, 33435, 44706, 30622,  9196, 49753, 29110,
       31901, 36956, 22893, 34803, 11185, 12573, 22427, 18748, 13067,
       32511, 41499, 21780, 38854, 25311, 48673, 30662, 20292,   875,
        7094, 14227, 11662, 32184, 36224, 23335, 21077, 48383, 46112,
       45991, 44700, 29304, 29474, 46246, 47496, 28218, 45503, 11365,
       27598, 46494,  1656, 22483,  5393, 27292, 40351,  1849, 36603,
       24650, 41583, 25418, 37096, 36171, 48083,  5305, 23947, 12701,
        7472, 30235, 39228,  7157, 23976, 18061, 43703, 48883, 32530,
       22158, 20075, 45469, 22321, 24006, 29061, 25870, 36899,  5681,
       35165, 41046, 41069, 23146,  1786, 33306, 17346, 29558, 39929,
        8198, 30735, 23150, 44551, 21046, 28693, 39103, 27397, 44290,
       16653, 14875, 43999, 48573, 28442, 23949, 30684, 42337, 25057,
       43071, 39660,  1519, 11201, 33873, 27679, 16436,  2984,  7181,
       27945, 45553, 40091, 32719, 37946, 44555,  5500, 49720, 39587,
       15207, 35660, 49938,  6436,  5379, 21767, 41793, 20020, 21494,
       25610, 28995, 12912, 37813, 36669, 18985, 47625,  4686,  9201,
       11334, 21083, 19346, 29196, 16031, 42284, 16537,  6449, 13525,
       12099,   570,  2045, 11410, 21927, 30167, 42612, 20741, 25283,
         514,  6808,  3146, 31822, 10666,  3398, 29350, 33495, 15539,
       28715, 15081,  3272, 42593, 39425,  1071, 15283, 30030,  7819,
       13749, 10127, 43128,  8311, 32354, 22901, 38104, 11391, 37449,
       29026, 40837, 21967, 30447, 14947,  9082, 28132,  5774, 23880,
       37836, 40892, 42751, 37671, 37624, 41973,  5126, 26752, 49377,
       28150, 18095,  9789, 15679, 17989, 45861,  9795, 10887, 25553,
       40821, 44286, 47856,  8516, 35982, 33755, 32307, 11338, 17129,
       37902, 11846, 28965,   913, 34308,  1915, 37159,   318,   320,
       22725, 10734, 48467, 17997, 26991, 10730,   797, 22128, 43322,
       22934, 39395, 28945, 27568, 24916,  3955, 42668, 37496,  3715,
       45365, 18139, 45061,  8984, 22324, 16666, 15317,  5525, 37935,
       46211,  1135, 27036, 23928, 36364, 21718, 39107, 24091,  7372,
       17886, 16876, 24129, 16198,  4706, 23196, 18556, 10184, 20699,
       12925, 12994,  6752,  2616, 10695, 11796, 45874, 28853, 37458,
       12864, 47777,  9214, 29957, 39498, 11088, 32875, 19973,  7506,
       29727, 31719, 16843, 23547, 16149,  6699,  7609, 12838,  4903,
       12320,   329, 36352,  7939, 30996, 22526, 36136, 25724, 34782,
       28201, 25464, 16040, 18528, 12437, 17254, 43305, 48948, 17636,
       44101, 28243,  3637, 22198, 16022, 46391, 10269, 47372, 47673,
       36589, 46348,    11,   131, 26163, 15136, 38798, 18828, 10481,
       35158, 31036, 38500,  7023, 16589, 39485, 19250, 19361, 38955,
       21773, 32208,  7185, 48041, 34260, 36306, 10538, 43512, 33773,
       42936,  4809, 44151, 48490, 45087, 44694,   152,  3649, 23842,
       26291, 19200, 42909, 14398,  8904, 19794, 45758,  6759, 10034,
       14758,  9496, 37482, 36677, 31807, 40295, 26032, 37472,  4253,
       36691,  8202, 47039, 25598, 14537, 37327, 46213, 33112, 23895,
       47349, 22054, 29473, 25697, 10902, 21706, 49554, 49121, 16332,
        2286,  6928, 33686, 34595, 33114, 42666, 34097, 18698, 35401,
       31737, 43786, 33615, 12592, 36774, 19575, 43641, 42065,  9080,
       44993, 46649, 26652,  2339, 12894, 29409,  8480, 21234, 12560,
       39771, 34043, 44474, 48141, 10569,   396, 41254, 46956,  8119,
       20736, 35530, 38858, 16829, 31251, 20761,  8135, 29485,  1559,
       29494, 23936, 26915, 45602, 34263,  3078, 18331, 37468, 14764,
       25718, 29283, 45485,  1090, 22833, 30446,  7673, 38493,  4696,
       44936, 17126, 43178, 42860, 33693, 41579, 26127, 27268, 46889,
       21529,  4423, 41550, 40248, 34090, 15749, 28381, 47779, 16989,
        8191, 20866, 12903,  2455, 41563, 22217, 18594, 33250, 16412,
       22419,  9766, 39474, 11707, 22165, 31655, 41338,  9263, 38347,
       16549, 42925, 10022,  8771,  3827, 11109, 18649, 21020, 46463,
       35691, 15702, 42912, 42758, 15506, 21959, 18228,  7048,  7722,
       19018, 37309, 37385, 31830,  5207, 14378, 30234, 35338, 34081,
        3315, 49795, 18551,  4200, 28096, 35705,  6121, 18117, 12247,
       24247,  8794,   257,  3085, 23006, 17479, 33640, 28539, 36085,
       34493,  2184,  8884,  6068,  1052, 35674, 18093, 32121,   432,
       15632, 48138, 13899,  3020, 36343, 29230, 27627, 12038, 29203,
        6020, 19869, 36975, 40880, 31945, 20042, 36872, 30458, 40972,
       25619, 25910,  1304,  9809, 46099, 15557,  5810, 23500, 46893,
       30104, 24923, 28876, 47213, 44272, 11645, 19143, 11243,  3366,
       46547, 40713,  2039, 11075, 34068, 18625, 29974, 31138, 20840,
       35651,  3273, 37012,  1394, 43416, 19565, 13122, 23714, 30530,
       34349, 18303,  8432, 22372,  7284, 29633,   262, 49186,  2511,
        6639, 45977, 31599, 46078, 49849, 45317, 10362, 42318, 33970,
       32584, 46406, 45487, 38491, 19628, 18639, 25614,  7214, 27994,
        9442, 18333,    99,  9000, 27547, 32284, 28100, 31290, 37987,
       35538, 41286, 20137, 27050, 48892, 41897, 20730, 10888, 48367,
       47441, 27135, 31769, 27494,  1925,  1001, 46150, 30300, 33027,
       31775, 14397, 10766, 21188, 27631, 21327,   712, 29743, 32253,
       26417,   520, 44834, 27812, 23421, 45820,  2654, 46331, 18161,
       48211, 23072, 31954, 18637, 44850, 23187, 42080, 45782, 38146,
        8338, 27500, 38228, 15155, 35415, 40631,  3354, 24920, 17402,
       18384, 32553, 18056, 32634, 44963, 34751,  4104, 21382,  9147,
       43335, 41675, 44641, 21021,  7563, 21626, 27613, 19462,   489,
        5434, 24582,  5173, 40974, 44587, 37903,  9724, 35526, 23354,
       33129, 47534,  9887, 19256, 19178, 28260, 20906,  4741, 44942,
        9550, 47149, 10303, 36968, 43299,  3519, 33004,  7404, 17713,
       45773, 47644, 40531, 10179,  7295,  6168, 12810,  3217, 27718,
       47611, 37193,  7619, 14937,  9937, 11982, 13210, 30734, 20655,
       15909,  6603, 47907, 17199,  5931, 27469, 45515, 28892,  2106,
       38728, 29101, 25879, 13416,  4773, 39217,  3695, 21946,  7207,
       16527,  9529, 11116, 23252, 14359, 18493, 22137,  5581, 18130,
       48591, 18606,  8792, 47571, 20778, 31281, 44565, 44202,  4195,
       43044, 47738, 32018, 35276, 11994, 11250, 32094, 31515, 26586,
       17727, 45957,  2835, 15203, 19180, 32620, 36302, 43294, 32026,
       18734, 31053,  4695, 37778, 39768, 26819, 45962, 35450, 45496,
       15004, 17623, 27429, 23362, 48738, 33232, 46382, 24829,  2216,
        9962,  4998, 42315, 33971, 13513, 37963, 18011, 18110, 14521,
       24823,  7536, 48889, 43529, 17500,  4359, 17852,  3141, 26715,
       46446,  5298, 43337, 32590, 18374, 30801, 25798, 33401, 42662,
        1903,  4156, 30627, 37234,  7712, 47374, 29071, 43870, 17819,
       14726,  6298,  8131, 16073, 35514, 40705, 16328, 34936, 11757,
       44125, 23869, 30044, 29922, 10890, 26023, 42296, 33523, 15454,
       14704]), [5, 7, 1, 4])]
Collaboration
DC 0, val_set_size=1000, COIs=[3, 9, 1, 4], M=tensor([3, 9, 1, 4], device='cuda:0'), Initial Performance: (0.25, 0.04431799054145813)
DC 1, val_set_size=1000, COIs=[0, 2, 1, 4], M=tensor([0, 2, 1, 4], device='cuda:0'), Initial Performance: (0.252, 0.044353610038757325)
DC 2, val_set_size=1000, COIs=[8, 6, 1, 4], M=tensor([8, 6, 1, 4], device='cuda:0'), Initial Performance: (0.276, 0.04447247278690338)
DC 3, val_set_size=1000, COIs=[5, 7, 1, 4], M=tensor([5, 7, 1, 4], device='cuda:0'), Initial Performance: (0.218, 0.04446711611747742)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {9, 3}
D07: 1000 samples from classes {9, 3}
D08: 1000 samples from classes {9, 3}
D09: 1000 samples from classes {9, 3}
D010: 1000 samples from classes {9, 3}
D011: 1000 samples from classes {9, 3}
D012: 1000 samples from classes {0, 2}
D013: 1000 samples from classes {0, 2}
D014: 1000 samples from classes {0, 2}
D015: 1000 samples from classes {0, 2}
D016: 1000 samples from classes {0, 2}
D017: 1000 samples from classes {0, 2}
D018: 1000 samples from classes {8, 6}
D019: 1000 samples from classes {8, 6}
D020: 1000 samples from classes {8, 6}
D021: 1000 samples from classes {8, 6}
D022: 1000 samples from classes {8, 6}
D023: 1000 samples from classes {8, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO4', '(DO0']
DC 2 --> ['(DO2']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.375, 0.06283778327703476) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07227671495079994) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.07495411774516106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.349, 0.08856414425373077) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.284, 0.06856693160533905) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.293, 0.08170339134335518) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.452, 0.08742304396629333) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.09984482651948928) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.08577430075407028) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.357, 0.09504514583945274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.473, 0.11441597338020802) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.10870000609755516) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.435, 0.11007066434621811) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.11734175483882427) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.472, 0.16358603571355343) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.407, 0.13449031338095666) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1351322904229164) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.395, 0.13958969017863274) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.459, 0.17408120495080948) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.415, 0.1502700263261795) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO5', '(DO0']
DC 2 --> ['(DO4']
DC 3 --> ['(DO3']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.15643265687674285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.419, 0.1466355331838131) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.478, 0.2705200811526738) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.1623449475169182) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.1696251774523407) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.432, 0.15292382569611074) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.477, 0.2703526836093515) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.190651739038527) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19223802105896176) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.425, 0.16991872733086347) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3325524968125392) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.406, 0.1890639338158071) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.471, 0.19927251303382218) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.418, 0.17540518701449037) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.474, 0.3120443267803639) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.414, 0.2167533227801323) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.21156952209956945) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.424, 0.16317588014900683) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.475, 0.3092682206723839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.43, 0.2174901111423969) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[1, 4], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.885, 0.015315789431333542)
DC Expert-0, val_set_size=500, COIs=[9, 3], M=tensor([3, 9, 1, 4], device='cuda:0'), Initial Performance: (0.94, 0.005162857327610254)
DC Expert-1, val_set_size=500, COIs=[0, 2], M=tensor([0, 2, 1, 4], device='cuda:0'), Initial Performance: (0.848, 0.010477451875805855)
DC Expert-2, val_set_size=500, COIs=[8, 6], M=tensor([8, 6, 1, 4], device='cuda:0'), Initial Performance: (0.95, 0.00401176818087697)
DC Expert-3, val_set_size=500, COIs=[5, 7], M=tensor([5, 7, 1, 4], device='cuda:0'), Initial Performance: (0.86, 0.011385125815868377)
SUPER-DC 0, val_set_size=1000, COIs=[3, 9, 1, 4], M=tensor([3, 9, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[0, 2, 1, 4], M=tensor([0, 2, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[8, 6, 1, 4], M=tensor([8, 6, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[5, 7, 1, 4], M=tensor([5, 7, 1, 4], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x73f4c018fe20>, <fl_market.actors.data_consumer.DataConsumer object at 0x73f4c00ce940>, <fl_market.actors.data_consumer.DataConsumer object at 0x73f4c0318760>, <fl_market.actors.data_consumer.DataConsumer object at 0x73f4c0699130>, <fl_market.actors.data_consumer.DataConsumer object at 0x73f4cc85c100>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9205, 0.00700337478518486) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004069155035540462) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.010084012284874917) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.0022601318140514194) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.854, 0.013192485123872756) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.493, 0.06877092427015305) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.04253304976224899) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.568, 0.19154247342050076) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.576, 0.0452542989552021) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9595, 0.003736940904520452) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.003839136251248419) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.010469467058777809) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.0033707380143459887) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.856, 0.013479142040014267) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.551, 0.06357699976861476) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.03968214851617813) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.631, 0.05910591447725892) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.608, 0.03925286316871643) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.939, 0.004931474607205019) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.003260200880933553) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.9, 0.01115455187112093) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.964, 0.0032406924412352964) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.862, 0.012562235683202744) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.522, 0.07331850972771645) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.04547173196077347) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.639, 0.04505025185644627) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.589, 0.04402659529447556) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9675, 0.003435574807575904) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.003675205913372338) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.011647524736821652) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.97, 0.0034194872153457253) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.856, 0.015299662038683891) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.619, 0.045648057609796525) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.04911627414822579) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.696, 0.032997163578867915) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.635, 0.04167463457584381) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.963, 0.004109925538301468) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.00338606026628986) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.906, 0.011573557060211896) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.002799617609067354) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.872, 0.01287733966112137) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.637, 0.051765255719423295) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.055683192729949954) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.707, 0.03485109927877784) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.597, 0.04717367896437645) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.967, 0.003227353824564489) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0039316433419007805) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.013083107307553292) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.0037533194061252287) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.866, 0.015682212114334106) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.05681681329011917) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.583, 0.04722758716344833) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.726, 0.030485256358981132) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.622, 0.0438674236536026) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.00302935532660922) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004290018967585638) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.01132235400378704) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0030404160971738746) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.862, 0.01755492177605629) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.04070871785283089) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.572, 0.052106507569551465) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.748, 0.02927785524725914) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.646, 0.037822902500629425) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.0025943124776240437) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.005418662641197443) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.012673277743160724) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.978, 0.002739470494154375) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.876, 0.017047040559351443) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.05279240949451924) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.61, 0.04580461287498474) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.747, 0.02631702509522438) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.658, 0.035821168929338454) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.0031557132612506393) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004103368380572647) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.012216055914759636) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.0037803730735904537) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.87, 0.014189487494528294) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.674, 0.045594427347183226) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.061518533408641815) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.748, 0.029157835066318513) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.628, 0.04890474766865373) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.0036434932962656603) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.003933484741020948) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.014179089978337289) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.003654149867768865) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.868, 0.014844720959663391) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.63, 0.04903166784346104) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.059052694857120516) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.734, 0.03296150936931372) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.581, 0.05551099660620094) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9725, 0.002987668099536677) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0038483729583676904) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.013406147971749306) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.003607845736427407) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.864, 0.014235273018479348) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.664, 0.03864019519090652) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.62, 0.04884051117300987) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.719, 0.033443087458610535) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.614, 0.05390040984749794) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9775, 0.002789407314863638) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0037259942473028785) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.014214027673006058) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0025519447932601906) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.87, 0.01845723760128021) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.65, 0.03785724532604218) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.594, 0.04909640979766846) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.71, 0.03515646892786026) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.648, 0.04314301905035973) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9775, 0.002938138184927084) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0048941501178778705) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.013065782576799393) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0032918762050394433) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.856, 0.021733207523822785) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.042858728408813475) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.643, 0.041527205809950826) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.721, 0.0323769371137023) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.668, 0.037409817934036256) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9745, 0.003137180636818812) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004741856868378818) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.904, 0.013082249641418457) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.003041647222911706) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.866, 0.020260255247354506) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.624, 0.03915565149486065) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.595, 0.04959480908513069) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.687, 0.041332633540034296) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.576, 0.05988805808126926) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.977, 0.003121167989607784) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0050992997005814686) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.014549693986773492) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0030918251787079498) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.868, 0.01989436739683151) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.645, 0.03882952558994293) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.628, 0.05076582020521164) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.714, 0.03835951681435108) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.61, 0.05616506268084049) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9795, 0.0030150176932947944) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004929380341200158) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.01543989148736) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.002967744448018493) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.874, 0.016400175407528878) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.66, 0.04010048714280128) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.64, 0.04312045447528362) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.711, 0.03926869475468993) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.649, 0.04825782440602779) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.979, 0.002572940941216075) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004634199364809319) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.908, 0.01398347072303295) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.0035413782711621023) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.872, 0.016871565401554107) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.642, 0.0389470484405756) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.559, 0.0647230498790741) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.695, 0.039963028095662594) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.618, 0.058812302589416506) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9795, 0.002753591263313865) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004783200194011442) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.9, 0.014896325528621674) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.003534404121732223) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.858, 0.019275831311941147) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.679, 0.032663556098937986) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.611, 0.04734913951158524) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.736, 0.031632566846907136) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.623, 0.055042452663183215) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.977, 0.0030222831207793207) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.004246997718932107) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.91, 0.013912100568413735) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.0033832926045906787) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.878, 0.018864470183849336) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.675, 0.03699007697403431) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.05431496059894562) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.74, 0.03027404511719942) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.04769620212912559) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.979, 0.0030447706393970293) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004851196547970176) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.904, 0.014160466879606247) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.978, 0.0033446457161480795) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.858, 0.017122530877590178) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.574, 0.06735422637313604) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.592, 0.05257337540388107) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.743, 0.03251880694925785) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.606, 0.057850019238889216) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9795, 0.003133516364099705) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004315679356572218) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.914, 0.015638388648629188) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.972, 0.0034223212158758544) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.854, 0.017798050314188005) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.04898719666153192) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.571, 0.07037449923157692) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.728, 0.033747354313731194) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.611, 0.06296556792780757) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9775, 0.0027840329498940264) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005228524553938769) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.902, 0.013294066548347474) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0031875607972033323) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.858, 0.016833429843187333) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.038242701277136804) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.051204790204763415) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.719, 0.035929104059934616) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.605, 0.061484179269522425) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.98, 0.0031667096234737073) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004578270324098412) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.892, 0.013517882585525512) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.004690554295282709) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.872, 0.016590902298688887) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.637, 0.046429628521204) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.596, 0.049589404731988904) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.747, 0.029204720236361028) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.642, 0.050498197332024576) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.978, 0.00296947147339597) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004481907863286324) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.012897019565105439) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.978, 0.0032670297357981328) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.858, 0.01920526713132858) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.04499005073308945) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.06986290460824966) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.748, 0.029636311940848828) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.683, 0.03998360730707645) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9795, 0.0025212940636520215) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004276122919982299) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.902, 0.015031397566199303) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.004829465210704257) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.87, 0.01904634064435959) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.617, 0.04813580320775509) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.605, 0.06690938115119933) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.683, 0.04981767588108778) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.669, 0.04146476879715919) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.978, 0.0027570312818070305) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004871138953254558) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.9, 0.014350760504603386) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.97, 0.004668198282121012) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.87, 0.018835854679346085) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.04887381166219711) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.05958559662103653) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.717, 0.04109106588363647) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.643, 0.04363384929299355) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.976, 0.0034058722271656733) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.00471266557497438) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.015663456072099506) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.005324415561917703) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.868, 0.020330269753932952) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.598, 0.058735275849699974) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.05953613838553429) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.734, 0.03533580052852631) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.663, 0.03819269892573356) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.979, 0.0030119327621932826) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.005516958020743914) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.016308600097894668) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.003191430113192837) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.864, 0.01903877626359463) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.672, 0.04370612135529518) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.624, 0.05630675834417343) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.759, 0.029949706360697746) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.62, 0.049778420507907865) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.976, 0.0034659540965240013) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004322906028944999) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.015435117319226265) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0032853542924276553) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.886, 0.0177944473028183) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.045144939988851544) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.59, 0.052711895912885665) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.731, 0.035045566976070405) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.598, 0.05254534483700991) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9785, 0.0035337326629996824) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005479387810941262) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.896, 0.01492037597298622) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.0042690520293690495) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.876, 0.01858929118514061) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.594, 0.04952877194434405) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.596, 0.06345390844345093) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.743, 0.03187973867356777) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.631, 0.04482934571802616) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9795, 0.003592163919603081) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0057516068046097645) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.894, 0.014247060060501098) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.976, 0.004086349723193053) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.86, 0.01574574814736843) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.562, 0.07476220984756947) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.056228334367275236) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.75, 0.028487687602639197) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.628, 0.049908702876418826) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.978, 0.0034525704262537145) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004597054886398837) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.016204452499747276) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.982, 0.0032524480644897267) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.88, 0.018609683275222777) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.663, 0.041315625339746476) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.59, 0.052384893715381624) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.7, 0.041351185888051985) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.579, 0.06461219595000148) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.979, 0.003104420596649106) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004423958897881676) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.015469874054193497) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.978, 0.0031876735193363858) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.864, 0.022549447387456893) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.042073853418231014) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.629, 0.04716188010573387) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.733, 0.03254130059480667) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.666, 0.04038777196407318) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9815, 0.0032893701302346015) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005122632297861855) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.902, 0.014603105157613754) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.98, 0.003951826498350784) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.876, 0.019138867229223252) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.575, 0.06552984041906894) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.603, 0.05977516710758209) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.7, 0.04382590778172016) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.659, 0.03903001363575458) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9805, 0.0035113819870030054) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004763940429256763) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.015262342944741249) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.984, 0.0036087769772248065) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.884, 0.018696268811821938) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.643, 0.04560193594172597) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.617, 0.04918176981806755) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.757, 0.03493807961046696) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.637, 0.04522010359168053) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9785, 0.003072460367504391) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0047228346359916035) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.898, 0.015038471646606922) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.986, 0.002928475353764952) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.88, 0.021263154223561287) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.04803840634226799) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.59, 0.05486722731590271) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.747, 0.030864501670002938) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.649, 0.04310911579430103) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.975, 0.0037513692416389403) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.006363313512352761) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.904, 0.013490970470011234) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.982, 0.003099984307424165) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.866, 0.020768289238214492) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.578, 0.06299347914755345) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.618, 0.054897936940193175) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.753, 0.03285881543159485) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.673, 0.04100963048636913) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.979, 0.0039310636204616) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.00402207472914597) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.9, 0.014774016067385674) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.978, 0.004244128393847376) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.872, 0.020912739545106886) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.574, 0.06502964323759079) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.613, 0.072235974162817) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.681, 0.048297294169664384) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.673, 0.03914314013719559) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.978, 0.004367929886227784) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0065894885017405615) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.906, 0.015213511109352111) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.004496597370460222) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.878, 0.019131955325603484) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.050952113687992094) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.618, 0.051636664748191834) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.736, 0.02966964191198349) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.644, 0.04406986074149609) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.977, 0.004277237803207299) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004608091712376336) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.906, 0.014937215149402619) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.974, 0.004002049617971352) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.882, 0.01696758459508419) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.059141357317566874) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.61, 0.055561531275510785) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.723, 0.03506425069272518) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.665, 0.04086287470906973) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04431799054145813), (0.375, 0.06283778327703476), (0.284, 0.06856693160533905), (0.394, 0.08577430075407028), (0.435, 0.11007066434621811), (0.458, 0.1351322904229164), (0.452, 0.15643265687674285), (0.463, 0.1696251774523407), (0.471, 0.19223802105896176), (0.471, 0.19927251303382218), (0.47, 0.21156952209956945), (0.493, 0.06877092427015305), (0.551, 0.06357699976861476), (0.522, 0.07331850972771645), (0.619, 0.045648057609796525), (0.637, 0.051765255719423295), (0.629, 0.05681681329011917), (0.62, 0.04070871785283089), (0.62, 0.05279240949451924), (0.674, 0.045594427347183226), (0.63, 0.04903166784346104), (0.664, 0.03864019519090652), (0.65, 0.03785724532604218), (0.644, 0.042858728408813475), (0.624, 0.03915565149486065), (0.645, 0.03882952558994293), (0.66, 0.04010048714280128), (0.642, 0.0389470484405756), (0.679, 0.032663556098937986), (0.675, 0.03699007697403431), (0.574, 0.06735422637313604), (0.634, 0.04898719666153192), (0.67, 0.038242701277136804), (0.637, 0.046429628521204), (0.626, 0.04499005073308945), (0.617, 0.04813580320775509), (0.639, 0.04887381166219711), (0.598, 0.058735275849699974), (0.672, 0.04370612135529518), (0.628, 0.045144939988851544), (0.594, 0.04952877194434405), (0.562, 0.07476220984756947), (0.663, 0.041315625339746476), (0.644, 0.042073853418231014), (0.575, 0.06552984041906894), (0.643, 0.04560193594172597), (0.639, 0.04803840634226799), (0.578, 0.06299347914755345), (0.574, 0.06502964323759079), (0.6, 0.050952113687992094), (0.577, 0.059141357317566874)]
TEST: 
[(0.254, 0.043263681530952454), (0.37075, 0.060598327726125716), (0.28775, 0.06587039145827293), (0.39, 0.08190187042951584), (0.42925, 0.10396925833821297), (0.45375, 0.12755469673871994), (0.44575, 0.1482144956588745), (0.45525, 0.16008145165443421), (0.46375, 0.17392877840995788), (0.4635, 0.18666653883457185), (0.46075, 0.1964863825440407), (0.4855, 0.06645001435279846), (0.52975, 0.061046062663197516), (0.50825, 0.07089163936674595), (0.60725, 0.042934152752161024), (0.61125, 0.04709271090477705), (0.62025, 0.051290838837623594), (0.60625, 0.04053372709453106), (0.62625, 0.05025771944224834), (0.65875, 0.044375769674777986), (0.6385, 0.04701900459080934), (0.6475, 0.03752423126995563), (0.6415, 0.03809539729356766), (0.6455, 0.04168312749266624), (0.6345, 0.03932239039987326), (0.658, 0.03973392045497894), (0.6555, 0.041473017632961275), (0.64075, 0.04051320493221283), (0.66875, 0.03728974913805723), (0.65, 0.040029320299625394), (0.54875, 0.0666943255662918), (0.617, 0.050647670224308966), (0.6515, 0.040089271306991574), (0.63375, 0.047797355562448504), (0.611, 0.046422691725194455), (0.595, 0.04935607475042343), (0.60775, 0.04970155131071806), (0.579, 0.06138657892495394), (0.642, 0.047493823394179346), (0.608, 0.046118041388690474), (0.58375, 0.054137291625142095), (0.559, 0.075093106970191), (0.63475, 0.04495015858858824), (0.6285, 0.04590828387439251), (0.5445, 0.07225293281674385), (0.61325, 0.050413993030786516), (0.6195, 0.055134478464722636), (0.553, 0.06623735870420933), (0.56675, 0.06667972962558269), (0.577, 0.0531453148573637), (0.56325, 0.06339800889045)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.58      0.78      0.66      1000
           3       0.51      0.87      0.64      1000
           4       0.63      0.20      0.31      1000
           9       0.66      0.40      0.50      1000

    accuracy                           0.56      4000
   macro avg       0.59      0.56      0.53      4000
weighted avg       0.59      0.56      0.53      4000

Collaboration_DC_1
VAL: 
[(0.252, 0.044353610038757325), (0.25, 0.07227671495079994), (0.293, 0.08170339134335518), (0.357, 0.09504514583945274), (0.416, 0.11734175483882427), (0.395, 0.13958969017863274), (0.419, 0.1466355331838131), (0.432, 0.15292382569611074), (0.425, 0.16991872733086347), (0.418, 0.17540518701449037), (0.424, 0.16317588014900683), (0.523, 0.04253304976224899), (0.576, 0.03968214851617813), (0.54, 0.04547173196077347), (0.578, 0.04911627414822579), (0.532, 0.055683192729949954), (0.583, 0.04722758716344833), (0.572, 0.052106507569551465), (0.61, 0.04580461287498474), (0.532, 0.061518533408641815), (0.558, 0.059052694857120516), (0.62, 0.04884051117300987), (0.594, 0.04909640979766846), (0.643, 0.041527205809950826), (0.595, 0.04959480908513069), (0.628, 0.05076582020521164), (0.64, 0.04312045447528362), (0.559, 0.0647230498790741), (0.611, 0.04734913951158524), (0.598, 0.05431496059894562), (0.592, 0.05257337540388107), (0.571, 0.07037449923157692), (0.6, 0.051204790204763415), (0.596, 0.049589404731988904), (0.557, 0.06986290460824966), (0.605, 0.06690938115119933), (0.6, 0.05958559662103653), (0.604, 0.05953613838553429), (0.624, 0.05630675834417343), (0.59, 0.052711895912885665), (0.596, 0.06345390844345093), (0.604, 0.056228334367275236), (0.59, 0.052384893715381624), (0.629, 0.04716188010573387), (0.603, 0.05977516710758209), (0.617, 0.04918176981806755), (0.59, 0.05486722731590271), (0.618, 0.054897936940193175), (0.613, 0.072235974162817), (0.618, 0.051636664748191834), (0.61, 0.055561531275510785)]
TEST: 
[(0.267, 0.04317044323682785), (0.25, 0.06941601613163947), (0.3085, 0.07784869140386581), (0.37125, 0.09040846940875054), (0.41275, 0.11196270486712456), (0.39575, 0.1334228486418724), (0.41875, 0.14155897158384323), (0.43075, 0.1480341517329216), (0.4325, 0.16426597940921783), (0.4275, 0.16917636078596116), (0.4355, 0.1590513065457344), (0.5115, 0.042863746255636215), (0.593, 0.03938672478497028), (0.54875, 0.0452023508399725), (0.58475, 0.04876684045791626), (0.52575, 0.05626654788851738), (0.58375, 0.04779560935497284), (0.59175, 0.050300766304135325), (0.60075, 0.04647394475340843), (0.53725, 0.05939559453725815), (0.5735, 0.05584348315000534), (0.62875, 0.047014098539948465), (0.59175, 0.05012215998768806), (0.649, 0.04049223680049181), (0.60475, 0.048166441082954405), (0.6305, 0.0488050434589386), (0.634, 0.043149573862552644), (0.57325, 0.06212904752790928), (0.63375, 0.04554866489768028), (0.60575, 0.05253127309679985), (0.60675, 0.05150753559172153), (0.5825, 0.07039725434780121), (0.61925, 0.0479756556302309), (0.6035, 0.046942954391241076), (0.57625, 0.06542787948250771), (0.61125, 0.06487934632599354), (0.61125, 0.058701798617839814), (0.60425, 0.05903781466186046), (0.6115, 0.055769127890467646), (0.59275, 0.05224766920506954), (0.59225, 0.06391729924082756), (0.5935, 0.056800446659326556), (0.58225, 0.053144851297140125), (0.6285, 0.04756911253929138), (0.5975, 0.057903845235705374), (0.59675, 0.05026274357736111), (0.59525, 0.05532511186599731), (0.605, 0.05730046796798706), (0.60225, 0.07106845460832119), (0.6075, 0.05079489950835705), (0.6145, 0.054237738847732545)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.85      0.56      0.68      1000
           1       0.81      0.90      0.85      1000
           2       0.47      0.80      0.59      1000
           4       0.38      0.20      0.26      1000

    accuracy                           0.61      4000
   macro avg       0.63      0.61      0.59      4000
weighted avg       0.63      0.61      0.59      4000

Collaboration_DC_2
VAL: 
[(0.276, 0.04447247278690338), (0.436, 0.07495411774516106), (0.452, 0.08742304396629333), (0.473, 0.11441597338020802), (0.472, 0.16358603571355343), (0.459, 0.17408120495080948), (0.478, 0.2705200811526738), (0.477, 0.2703526836093515), (0.474, 0.3325524968125392), (0.474, 0.3120443267803639), (0.475, 0.3092682206723839), (0.568, 0.19154247342050076), (0.631, 0.05910591447725892), (0.639, 0.04505025185644627), (0.696, 0.032997163578867915), (0.707, 0.03485109927877784), (0.726, 0.030485256358981132), (0.748, 0.02927785524725914), (0.747, 0.02631702509522438), (0.748, 0.029157835066318513), (0.734, 0.03296150936931372), (0.719, 0.033443087458610535), (0.71, 0.03515646892786026), (0.721, 0.0323769371137023), (0.687, 0.041332633540034296), (0.714, 0.03835951681435108), (0.711, 0.03926869475468993), (0.695, 0.039963028095662594), (0.736, 0.031632566846907136), (0.74, 0.03027404511719942), (0.743, 0.03251880694925785), (0.728, 0.033747354313731194), (0.719, 0.035929104059934616), (0.747, 0.029204720236361028), (0.748, 0.029636311940848828), (0.683, 0.04981767588108778), (0.717, 0.04109106588363647), (0.734, 0.03533580052852631), (0.759, 0.029949706360697746), (0.731, 0.035045566976070405), (0.743, 0.03187973867356777), (0.75, 0.028487687602639197), (0.7, 0.041351185888051985), (0.733, 0.03254130059480667), (0.7, 0.04382590778172016), (0.757, 0.03493807961046696), (0.747, 0.030864501670002938), (0.753, 0.03285881543159485), (0.681, 0.048297294169664384), (0.736, 0.02966964191198349), (0.723, 0.03506425069272518)]
TEST: 
[(0.27075, 0.04346053540706635), (0.4375, 0.07203164887428283), (0.44975, 0.083502028465271), (0.4725, 0.10839265322685242), (0.47325, 0.15421055006980897), (0.45525, 0.16559282863140107), (0.48025, 0.25170764434337617), (0.478, 0.2528487293720245), (0.475, 0.3110364720821381), (0.478, 0.291153617978096), (0.48075, 0.29257084810733797), (0.54225, 0.18504786831140518), (0.59975, 0.05718805982172489), (0.611, 0.04460909809172153), (0.658, 0.03414805181324482), (0.704, 0.03469465823471546), (0.71125, 0.02990431587398052), (0.742, 0.02745420419424772), (0.72975, 0.026430241398513316), (0.74275, 0.028324541553854943), (0.7155, 0.03305344113707542), (0.7155, 0.03363097986578941), (0.707, 0.03532159914821386), (0.72375, 0.03183629083633423), (0.6895, 0.03918971597403288), (0.713, 0.03727865868061781), (0.70775, 0.0375650842487812), (0.70425, 0.036895533502101896), (0.746, 0.030008249700069427), (0.752, 0.029380967937409876), (0.73575, 0.03235940255224705), (0.71625, 0.035189159914851185), (0.7125, 0.03537224125117064), (0.71975, 0.03113263815641403), (0.73025, 0.031257781341671946), (0.66925, 0.050860028013587), (0.69875, 0.042058484368026255), (0.71075, 0.03654284294322133), (0.751, 0.030043132819235326), (0.73225, 0.03409600666165352), (0.7355, 0.03136793192476034), (0.74725, 0.02916856700181961), (0.71425, 0.039755246091634035), (0.7265, 0.032327590346336366), (0.677, 0.04355468565970659), (0.7455, 0.03260721373558045), (0.7395, 0.031300318248569964), (0.7265, 0.03290080380439758), (0.6825, 0.047162180460989474), (0.74225, 0.029846636161208154), (0.718, 0.03374384519457817)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.71      0.89      0.79      1000
           4       0.72      0.55      0.62      1000
           6       0.65      0.82      0.72      1000
           8       0.86      0.61      0.71      1000

    accuracy                           0.72      4000
   macro avg       0.73      0.72      0.71      4000
weighted avg       0.73      0.72      0.71      4000

Collaboration_DC_3
VAL: 
[(0.218, 0.04446711611747742), (0.349, 0.08856414425373077), (0.393, 0.09984482651948928), (0.407, 0.10870000609755516), (0.407, 0.13449031338095666), (0.415, 0.1502700263261795), (0.416, 0.1623449475169182), (0.416, 0.190651739038527), (0.406, 0.1890639338158071), (0.414, 0.2167533227801323), (0.43, 0.2174901111423969), (0.576, 0.0452542989552021), (0.608, 0.03925286316871643), (0.589, 0.04402659529447556), (0.635, 0.04167463457584381), (0.597, 0.04717367896437645), (0.622, 0.0438674236536026), (0.646, 0.037822902500629425), (0.658, 0.035821168929338454), (0.628, 0.04890474766865373), (0.581, 0.05551099660620094), (0.614, 0.05390040984749794), (0.648, 0.04314301905035973), (0.668, 0.037409817934036256), (0.576, 0.05988805808126926), (0.61, 0.05616506268084049), (0.649, 0.04825782440602779), (0.618, 0.058812302589416506), (0.623, 0.055042452663183215), (0.625, 0.04769620212912559), (0.606, 0.057850019238889216), (0.611, 0.06296556792780757), (0.605, 0.061484179269522425), (0.642, 0.050498197332024576), (0.683, 0.03998360730707645), (0.669, 0.04146476879715919), (0.643, 0.04363384929299355), (0.663, 0.03819269892573356), (0.62, 0.049778420507907865), (0.598, 0.05254534483700991), (0.631, 0.04482934571802616), (0.628, 0.049908702876418826), (0.579, 0.06461219595000148), (0.666, 0.04038777196407318), (0.659, 0.03903001363575458), (0.637, 0.04522010359168053), (0.649, 0.04310911579430103), (0.673, 0.04100963048636913), (0.673, 0.03914314013719559), (0.644, 0.04406986074149609), (0.665, 0.04086287470906973)]
TEST: 
[(0.2305, 0.0433762149810791), (0.34325, 0.08504497039318085), (0.39575, 0.09558765774965286), (0.4115, 0.10409454986453057), (0.40925, 0.12879934746026994), (0.41825, 0.14445116937160493), (0.41375, 0.15593986171483992), (0.4215, 0.18018277913331984), (0.41, 0.17879825294017793), (0.424, 0.20379313588142395), (0.42875, 0.2091999084353447), (0.59125, 0.041297776401042936), (0.63425, 0.03500947369635105), (0.6145, 0.03911129470169544), (0.62775, 0.03812130936980247), (0.61575, 0.04294694827497005), (0.634, 0.03940713281929493), (0.6715, 0.032762894704937935), (0.67775, 0.03229122216999531), (0.656, 0.04159155978262424), (0.6145, 0.050835995852947236), (0.64175, 0.04846529911458492), (0.6855, 0.03649193723499775), (0.70675, 0.03201777511835098), (0.6205, 0.05318630881607533), (0.6475, 0.048913976207375524), (0.678, 0.04312382143735886), (0.65025, 0.0518191042393446), (0.64425, 0.04885325215756893), (0.65125, 0.04432891246676445), (0.63675, 0.05245432414114475), (0.638, 0.05653269100189209), (0.63125, 0.055819482564926144), (0.65325, 0.04435623952746391), (0.6915, 0.03831293425709009), (0.68825, 0.037576796025037766), (0.656, 0.03957186204195023), (0.6795, 0.03562569515407085), (0.6505, 0.04437158355116844), (0.63075, 0.04672987376153469), (0.6615, 0.039649575650691986), (0.6455, 0.0451735220849514), (0.6045, 0.05906821100413799), (0.685, 0.036238209523260595), (0.69275, 0.034746557392179966), (0.6715, 0.03893587786704302), (0.682, 0.03810719981044531), (0.68875, 0.03696181803941727), (0.6915, 0.03387895780056715), (0.6805, 0.03857731568068266), (0.6915, 0.03634973229467869)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.89      0.92      0.91      1000
           4       0.51      0.61      0.55      1000
           5       0.63      0.67      0.65      1000
           7       0.79      0.57      0.66      1000

    accuracy                           0.69      4000
   macro avg       0.71      0.69      0.69      4000
weighted avg       0.71      0.69      0.69      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [72]
name: alliance-4-dcs-72
score_metric: contrloss
aggregation: <function fed_avg at 0x7d8672ba4c10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=72
Partitioning data
[[7, 9, 1, 2], [6, 8, 1, 2], [4, 3, 1, 2], [0, 5, 1, 2]]
[(array([ 2102, 37010, 13182, 10158, 47275, 19636,  1365, 30216, 31980,
       36407, 45523, 26661, 26983, 44664,  5047,  5597, 35724, 26606,
       14517, 47983, 18828, 43060, 47323, 24898, 36346,  3578, 40182,
        3284, 13516, 13280,  6320, 20701, 22128, 10684, 23000, 17462,
          68, 29789, 31528, 24853,  5774, 34010, 34440, 10045, 24682,
       25310, 45343, 18922, 26291, 29644, 16477,  7598, 31321, 19599,
       15067, 47898, 19909, 28764, 44892, 35176, 40465,  7291, 29962,
         440,   589, 43892, 40340, 16350,  6999, 14202, 34511, 12474,
       30876, 48837, 15999, 42355, 35082, 12844,  3251,  7717, 23621,
       43124, 41850, 24698, 38512,  4809,  8984, 14489, 49610, 37131,
       21051, 10573,  6726, 45708, 16224, 17876, 47372,  5313, 29047,
        6822, 21787, 20581, 24995,  6653, 45054, 11484, 35108, 37019,
       18417,   152, 44440, 31364,  7055, 24013, 18528, 45480, 21394,
       15472,  2207, 32660, 14261, 23682,  7798, 12320, 27529, 10347,
       45747, 44567, 12941, 44135, 31871, 35812, 16828, 43449, 28977,
        5101, 27043, 15013, 35220, 31589, 38616, 43435, 24327, 44661,
       48858,  8521, 29910, 35336,  1742, 12272, 16996, 15712, 40234,
       32211, 40360, 25701, 15324, 21553, 23561, 23627,  8076, 36895,
       10886, 18405, 34015, 37482, 39949, 17537, 18017, 32646, 38826,
       44718, 31156, 10666, 13268, 19159, 39246, 15292, 27697, 21396,
         641, 19940, 12864, 11068,  2366, 26567,  3213,  6239, 28695,
        8887, 38155, 49071,  9279,  4729, 38955, 29272, 30330, 22846,
       26032, 20094, 14144, 49592,   994, 37222,  2801, 34107,  5200,
       19821, 24944, 44262,  1806, 32282, 41319, 15720, 20681,  5241,
       39259, 47092, 17942, 14242, 41129, 11621, 25283, 30163, 49363,
       30030,  1445, 42117, 12744, 49760, 16022, 21876,   789, 40061,
       28832,  7450, 13818, 32061, 40942, 42457, 41784, 32633, 46301,
        3157, 38042, 15271, 23292, 30845, 10041, 11553, 41280,  9606,
       16482, 31378, 47743,  9626, 40599, 48816, 23169, 11312, 22643,
       29228, 38056, 35813, 29306,  2199, 18344, 11876, 30080, 43946,
       27591,  7698, 37993, 12427, 35984, 16601, 23336, 10163, 10144,
       17466, 10231, 14892, 14250, 27551, 20552, 39581, 11258,  6149,
       32040, 17359, 47574, 12555, 16684, 32552, 11472, 23720,  2935,
        1232, 13969, 41765,  8089, 31722, 42407, 43960, 37249, 25303,
       36291, 20947, 14224, 18020, 28189,   102, 39174, 16809, 14913,
       36413, 48332, 33856, 25387, 40043, 45465, 42278, 33813, 23303,
       36434,  8821, 40015, 26229, 44678, 35184, 24712, 30850, 11292,
       15694,  8125,  5332, 45547, 48284,  1765, 44692, 30966, 33437,
       21408, 22156, 19193, 16744, 28846, 46646, 19122,  1638, 40413,
       36475, 12835, 13380, 40059, 21330, 49398, 42638,  2637, 41614,
         668, 45906, 27184, 32945, 38280, 26574, 35667, 32820, 18703,
       12385, 23426, 42106, 24152, 25407, 16054, 40387,  5591, 32112,
       28053, 15263, 16999, 35498, 16092, 30678, 12562, 40790, 19053,
       41495,  8841, 25091, 38267,  7461, 13370, 13881, 42385, 43346,
       16773,  5306, 49823, 39841, 46059, 41630, 47148, 14599, 16170,
       37600, 13869, 36577, 34773, 16725, 10448, 14424,  3175, 23908,
        3143, 20481, 37400, 42208, 36210, 30932, 11889, 44881,  9091,
       16262, 11173, 26647, 14197, 40052, 25752, 26262,  3702, 13529,
       36179, 33417, 16699, 34024, 22802, 23023, 22464, 34474,  9959,
       17945, 48210, 28684, 42851, 37730, 30420, 24938, 26468,  2058,
       10943, 47121,  1987, 16535, 27784, 36918, 24718, 34290,  6708,
        8749, 20176, 12288, 25988, 17097, 42973, 30351, 35379, 16688,
       34485, 24086, 20718, 17526, 12709, 22296,  5668, 37780, 46190,
        3648, 14825, 21696, 34078, 28046, 22240, 29148, 13475, 36384,
       36031, 36649, 33868, 18567, 19120, 37285, 41408, 26620, 28549,
        8146, 34427, 47175, 48577, 23877, 40152, 22533,  7992,  6371,
       11075, 27228,  6813,   743, 20619, 28526,  7513, 46904, 41948,
       16755, 16089, 11791, 34400, 14912, 41695, 37592, 26290,  2469,
       10882, 41392, 37929, 41847, 40076, 18321, 39902, 13323, 43636,
       12169, 49286, 41262, 49085, 39391, 19866, 46741, 48746, 25050,
       12629, 22938, 37844,  2431, 11804, 33034, 24292, 27174, 12064,
        8920, 49196,  4696, 33748, 43847, 10236, 30463, 36334,  9327,
       28347, 38596, 46207,  7358, 32994, 31395, 31208, 49993,  3830,
        7841,  8244,  4654,  3059, 10627, 21593, 40449, 32418, 10522,
       46259, 22747, 36672, 37611, 49426, 25279,  5816, 44886, 38919,
       28559, 49221,  6971, 43297, 49060, 20453,  2286, 10747, 21585,
       24010, 25201, 25488, 10790,  3467, 12373, 31407, 12814,  4145,
       11403, 13523, 44475, 38270, 11130, 47616,   427, 29019, 19667,
       38204, 32011, 21283, 13332, 46674, 18844, 39405, 45289, 15669,
       19446, 35335, 36891, 24008,  2727, 21215, 45599, 43240, 21871,
       33105,  1946, 24024, 16223, 18246, 34673, 45105, 28187, 28688,
       16275, 35666, 30050, 30696, 12917,  5146,  2769, 43068, 35757,
       22145,  9252,  8570, 29315,  5257, 10569, 31552, 28393, 22788,
       21850,  3273, 24194, 44448, 23263, 33292, 37616, 21829, 22636,
       48275, 22833, 47279, 22552, 35058,  1389, 39118, 49151, 20865,
       32681, 16956, 22553,  5769, 22659, 12981, 40145,   848,  8124,
       17091, 42130, 29264, 19454,  1565, 41338, 35207, 31599, 19164,
       49411,    96, 38892, 43932,  7649, 42734, 41663, 26794, 21423,
       44235, 49849, 31300, 20336, 13858, 14844, 15950, 31957, 14717,
        5282, 30120,   226, 47495, 27730, 14538,  8938, 38464, 11822,
       28552, 48234, 18926, 17586, 31297, 16613,  1541, 45687, 28532,
         834, 11574, 34349, 37460, 23607,   262, 39333, 10953, 34145,
       20368, 40220, 13468, 38493, 25910, 22399, 46514,  8191, 43600,
       41087, 29522, 24828, 14423, 29042, 21654, 40327, 48651, 11630,
       20125, 12992, 30767,  9903, 31467, 25161,  6390, 47675, 37581,
        7783, 34566, 19998, 44053,  5603, 20149, 38186,   463, 22803,
       48090, 25695, 32387,  1777, 24457, 48388, 47730, 33699, 32570,
       48594, 30795, 27328, 36778, 36583, 12180,   403, 20876, 45550,
       28705, 35230,  1415, 27430, 45109, 44216, 41764, 10480, 27816,
       49156, 18149, 32420, 26992, 26990, 34293, 19874,  4896, 48012,
        4700, 30531, 45428,  5634,  9580, 33851, 27325,  4168, 34187,
       14588, 43856,  5384,  4220, 43823, 31307,  6558, 47846, 41469,
       42428,  4511, 14814,  8077, 39436, 29612, 35764, 33503, 15813,
       31090, 42155, 38830,  4398, 40231, 49342, 13179, 36245, 42158,
        7868, 16739,  6146,  9001, 27091, 42815, 44842, 25263, 36657,
       22280, 45229, 22006, 25530, 25831, 29715, 39765,  1789, 16123,
       26321, 21650, 15477, 12748, 14063, 23670, 39648, 36465, 44758,
       26982,  1995,   689, 46877, 40860, 12860, 37647, 35880,  3416,
        3760, 43469,   933, 30005, 36355,  5781, 45238, 21500, 42020,
       10935,  8148, 37947,  6616, 37280, 19070, 17344, 21102,  6917,
       18184, 21018, 15820, 27973, 46082, 18287, 22770, 21392,  4656,
       18241, 34181,  1676, 40221,  3484, 31475, 10018, 34608, 45074,
       35319,  9827,  1077, 38502, 36094, 35482, 39390, 28788, 28664,
       48870, 30705, 32166,  6502,  1497, 14834, 43287, 10474, 22375,
       43593,   281, 19714, 23241, 49328, 42564, 18756, 45342,  4242,
       48348, 45855, 49449, 11923,    41, 42234,  7480,  6388, 34497,
       38457, 11606, 38480, 24780,  9455, 15731,  6799, 25130, 46089,
        8713, 20138, 17538, 22864, 44328, 39594, 21208, 12462, 12584,
       49101, 21643, 27186, 22279, 20696, 14620, 48694, 30505, 27746,
       24064, 35823, 23388, 43041, 44121, 37374, 27542, 49395, 35673,
       24254, 44910, 14739, 35226,  6448, 37941, 20737,  1852, 15124,
        1530]), [7, 9, 1, 2]), (array([36373, 22530, 35781, 29310, 42872, 10211,  7403,  1317, 37261,
       41534, 34020, 12031, 12369, 30864, 31558, 30212, 44814, 46252,
        7091, 43390, 12321, 43174,  3716, 42712, 43691,  9653,  3041,
       47580, 46974, 10176,  6938,  5263, 11999, 49512, 21132, 25793,
       18103, 14162,  6025, 18870, 46317, 23370, 17282, 12604, 42584,
       33608, 17925, 43734, 26054, 23152, 34056, 12597, 12445, 15294,
       10600,  5661, 45693, 19184, 19308, 29687, 33173,  3926, 19763,
       28011, 42975, 19489, 16696, 17523, 41533, 48983, 47953, 25169,
       29241, 23087, 43431,  1876, 27626, 17287, 18021, 14660, 43761,
       16960,  1047, 16499, 36463, 36913, 21475, 32248, 39630, 42077,
       31142, 23366, 40206,  2796, 12874, 11392,   836, 13776, 13965,
        7064, 44224, 42859, 35672, 36522, 18615, 30816, 12796, 18554,
        3192, 33988, 18330, 18665, 34612, 20146, 16242, 41552, 16862,
       35891, 23579, 11675, 40316, 24992, 39357, 30001,  7967,  9423,
       46009, 37933, 46334, 20775, 42723, 27678, 45420, 37084, 27051,
       14830, 46050, 35600, 16782, 33436, 40437, 33758, 30644,  9275,
        8436,  7968, 49655,  8474, 34552, 35091, 28370, 21447, 46587,
       11066,  8363, 10517, 46765, 25761, 45597, 42577, 44096, 23593,
        2607, 25729, 13162,  5409, 22295, 33405, 49129,  6566, 29931,
       28504, 49289,    22, 18420, 38771,  7321, 34148, 48203, 27183,
        7172, 17143,  2898,  6442,  9323, 33791, 20008, 10757, 34762,
        3290,  8803, 23956, 29971, 25747, 22282, 33948, 44417, 24852,
       27117, 45300,  8679, 41785, 16611, 44518, 23987, 47858, 44489,
       36111,  1596,  5497, 16778, 45253, 42250, 32397, 30906, 25851,
       44708, 42748, 45719, 23771, 29339,  2276,  2777, 32759, 21125,
       34139,  6145, 12048, 29194, 12151, 24307, 14444, 36998, 18025,
       32080,  4678, 27741, 23373, 41196, 45385,  9112,  3912, 15193,
        4951,   355, 29550,  4143,  7632,  7036, 27367, 41867, 28507,
       32356, 45898, 44057, 32370, 39419, 27386,   958, 41218,  4779,
       41447,   897,  7029, 48805, 36214, 14442, 37692, 40669,  9786,
        1162,  5365, 39069,  9882, 46462, 20719,  5480, 22409, 30844,
       42270, 37953, 33264, 22920, 44043, 23125, 14281, 35617, 16044,
       17034, 23214, 16775, 23625, 48120, 12443, 21679, 24185, 24984,
       23838, 49897, 46543, 15948, 12206, 43858,  6788, 41957,  8414,
       40671, 48864, 28180, 24138, 33613,  4995, 40008,  5091, 34135,
       44594, 13369, 49899, 27727, 33861, 31367, 13587, 22661, 46223,
        7007, 38850, 34910, 48393, 49479, 42506,  1429, 47780, 42840,
        6916, 46710, 41712, 19251, 41008, 47400, 12513, 10840, 43644,
        5910,  3904, 14693,  7083, 23669, 16430, 47052, 28006, 12557,
       27243, 33756, 13576, 48297,  5820, 35825, 45649, 16607, 35707,
       38862,   507, 28066, 23440, 10537, 39038, 11244, 38162, 20727,
        5255, 27067, 44054, 47380, 12792, 42917, 41485, 24765, 39971,
        8657, 37711, 41292, 13481, 27550, 44420, 39362, 40495, 40292,
       37420, 32714, 37643,   609, 32783, 49521, 11490, 13095, 46985,
       38136, 36131, 24455, 28842, 29477, 49681, 27715,  3516, 36547,
       31182,  9188,  5941, 35885, 31714, 34892, 25756, 24083, 25754,
       22465, 24456,  2344, 38754,   259, 21134, 36498, 12249, 27858,
       37633, 17290,  9063, 26583, 46111, 13197,  2635, 42847, 11832,
        3305,  8531, 34319, 49887, 28707, 35540, 47776,  2381,  5985,
       37550, 43713, 49625, 29055, 27561, 38276, 47713, 28012, 30398,
       35150, 47376, 29788,  1794, 16086,  3258,  9669, 23960, 14613,
       46479,  5948, 25924, 21635,  1239, 45325, 36526,  7156, 18112,
       24588, 38734,  4955, 14369, 34695, 17799,  6134,  6219,  8267,
       47021, 20822, 36882, 25573, 30303, 42720,   786, 29877, 44701,
        7310, 41250,   291, 31188, 45243, 24328, 42884, 38797, 25533,
       32271, 47770, 37959, 41256, 15043,  6366,  6453,  3085,  4100,
       25071, 40655, 35129, 10133, 32679, 36813,  4354, 49503, 12903,
       39097, 18671, 18123, 13973, 22564, 28175, 31969, 24607,  8932,
       35041, 38719, 11427, 27194,   396,  2862, 42413, 47745, 33181,
        1559, 17515,  7261, 18649, 39476, 37799,  7019, 47060, 18181,
        6074, 17030, 35705, 34287, 40582, 11698,  6241, 25611, 30782,
       17366, 38785, 49594, 30673, 47839, 41627, 11664, 22358, 13277,
       40505, 15710, 35688, 47618,  4500,  6639, 27773, 31830, 22982,
       29932,  3773, 33257, 16197, 12743, 19018,   482, 27321, 38637,
       30865, 25206, 31539, 46544, 29318,    79, 34138, 11767, 44210,
        7428,  2584, 24678, 43638,  6668, 23933, 27089,  7378,  8977,
       49748, 33207, 18625, 26008, 12451, 35799, 30810,  6670, 28655,
        1304, 32883, 44198, 40987, 49356, 28784, 10450, 29546,  6515,
       24161, 40118, 41051, 26576, 32580,  2570, 13558, 22388, 39769,
       19869, 39692, 26320, 23529, 44168, 43948, 19983, 17893, 17647,
       30951, 23116, 37551, 49345, 40554, 28086, 36237, 40713, 22013,
       40386, 24923, 35339, 44571, 35933,  4858, 20856, 33085, 20680,
       32042, 10022, 44620, 22450, 37588, 13615, 12923, 44423,  2582,
       40880, 47178,  4299, 33897, 36703, 43605, 46865, 16310,  4562,
       32492, 24224,  7587, 44109,  3924, 46953,  7659, 29473,  6933,
       38333, 17593, 19341, 42753,  7198, 21529, 29283,  5076, 41378,
       21080, 49066, 12470, 20114,  4444, 43797, 24191, 44433, 28127,
       13509, 40615,  4200, 46162, 44217, 11707, 45322, 49383, 29901,
       30489, 31703, 39204, 12123,  9602, 19002, 39622, 28027, 15748,
       35567, 36593, 28220, 26168,  5149, 13217, 42735, 42448, 46928,
       23280, 23423, 31193, 37468, 21538, 24986, 40046,  5195, 39674,
        5247, 17128, 39679, 11683, 15776, 41532, 41964, 16989, 34530,
       28844, 26712, 41579, 49362,  8812,  5552,  9442, 36950, 49676,
       45866, 23139, 14336, 25912, 31394, 34968, 38234,  6583, 12721,
       44774, 21189,  3610, 11241, 21711,   425, 49870, 47589,  7074,
       21591, 32480, 34609,  1929, 12742, 19439, 17134, 32071, 42792,
       24636, 40032, 13154, 26181, 27743, 16472,   171,  9089,  4367,
       11992, 22322, 31343, 23843,  8182, 28443, 11137, 17786,  3440,
       45893, 34200, 32153, 34088, 40452,  3669, 21115,  8032, 18882,
        4730, 18477, 13639, 13637, 19916, 14640, 11342, 14918,     6,
        1793, 23138,  1936, 45709,  7951,  5151,  5506, 28518, 18242,
       27580, 31469, 35841, 12519, 27204,  7542, 13340,  9408, 47816,
       33793, 34827, 32377,  4703, 13542, 15522, 36756, 33881, 25830,
       38509, 16406, 13955, 43692, 30969, 37503, 40515, 46541, 25398,
       45710, 24016, 16530, 17682,  8178,  9802, 43476, 23315, 22676,
       35031,  4604,  1492, 48712, 14195, 23919, 42147,  4755, 16306,
       47411,   808, 15929, 42696, 39233, 13970, 12346, 38676, 19695,
       27594, 35259, 18496,  9900,  7056, 19977, 46125, 42661,  9440,
       31410, 42480, 27744, 31979, 35104, 39646, 40371, 46686, 23696,
       23692, 12945,  8616, 43607, 42345, 17571, 21940, 34463, 28348,
       44831,  5818, 38096, 49450, 42185, 48417, 41401,  7661, 33021,
        4376, 20633, 45818, 33396, 36169,  5333, 35958,  3119, 12958,
       38060, 27738, 12697, 22988, 19108, 19504, 48885, 26398, 14899,
       26541,  4776, 48323, 33042,  6976, 35168, 20318, 40305, 38766,
       48364, 27966, 37278, 40087, 17382, 30067, 14017, 27823, 44736,
       19359,  2875, 48973,   544, 44660, 22373, 49678, 45451, 26070,
       39312,  6660, 32582, 20712, 18143, 22843, 49693, 32278,  4811,
       17949, 20073, 13362, 14926, 15560,  6593, 43179,  4633, 12471,
       19220, 31035, 40487, 16165, 20029, 46589,  5226, 38016, 49736,
       46475, 13226, 15589, 42988, 16352, 17092, 32526,  8676, 41157,
        3769, 32430,  5609, 30287, 14317,  5922, 25222,  3856, 20850,
       14619]), [6, 8, 1, 2]), (array([44851, 11250, 43147, 48478,  2710, 43494, 44071,  9885, 11859,
       22133, 30037, 41178, 31893, 39244, 31615, 30425, 36096, 14263,
       15919, 45315, 12333, 23879, 26854,  1832, 10166, 29052, 43386,
       41093, 33586, 20137,  5051, 33279, 13896,  6103, 31775, 35250,
       45555, 10224, 26423, 23654, 36690, 15036, 43228, 49402, 24737,
        7602, 27276, 10282, 43473, 32413, 28219, 21389,  4104, 13513,
        5631, 39615, 31256, 21659, 46011, 13708, 14041,   378, 39451,
        5880, 26047, 32756, 11649, 28654, 48143, 28074,   946,  4193,
       43033, 40840,  6180,  2654,  8260, 34331, 16181, 41582, 30561,
       34727, 14656, 35601, 44525,   153, 23200, 16151, 29125,  2695,
        9352,  1866, 35228,  3723, 29862, 22173, 45122, 23455,  9761,
        5486, 12132, 11565, 43049, 42303, 14810,  4129,  6560, 38701,
       14447, 46251,  5625, 27135, 33703, 13778, 46591, 35634, 37288,
       31075,  4384, 42811,  3089, 10728, 41665, 23252, 27166, 15790,
       31365,  2999, 37294, 19510, 42640, 10006, 31627, 42138, 43584,
       10877, 39293,  3948, 46372,  2856, 27258,  2245, 40862, 29291,
       18705, 28002, 23814,  9260,  3433, 33215, 19528, 41305, 19516,
       32201, 33027, 35526, 39853, 34752, 28881, 42629, 36428, 10134,
       49154, 38161, 10207, 19452,  1795, 28903,  5143, 25893, 40591,
       37627, 33370, 44729, 15918, 14852, 12672, 25992,  8899, 46506,
       27455, 20192,  1212, 33740,  4057, 28755,  3116, 22536,  2418,
       33669, 43203, 28583, 24171, 17800, 27644, 33378, 48425, 45132,
       17045,  5669,  7407, 19614,   904, 32523, 17594, 47311, 14996,
       32459,  2216, 14090,  2144,  5957, 48889,  2900, 29278, 36536,
        8981, 18115, 29613,  7634, 29070, 26975, 45397,  9593, 21041,
       22765, 35954, 33408, 45189, 14060, 27631,  7531, 38124, 30734,
       10604, 48502, 46490, 39284, 19178, 37469, 24846, 19112, 44586,
       13385, 19315, 14262, 42096, 48521, 14316,  9147, 12642,  9943,
       43922, 18414, 36926, 15210, 13330,  8722, 12033,  2564, 13254,
       29561,  9371,   691, 25384, 12016, 24332, 14543, 13991,  5190,
       17455, 32504, 19830, 42585, 26859,  7253, 45128, 31934,  9300,
       46712,  7235,  3177, 38485, 19449, 27114,  9670,  3109, 33132,
       40839,  7852, 42837,  7804, 43123,  3568, 11387, 40446, 18949,
       13925, 14171, 49083, 40370,  9841,  1265,  4674, 17449,    36,
       49280, 30424, 35794, 11623, 22842, 15523, 27115, 32358, 28162,
       23197, 24364, 11836, 21501,  3875, 17821, 40365, 44852, 30772,
        5586, 37304,  9152,  1030, 29096, 38891,  6162, 18222, 10468,
       34136, 33956, 37804, 22866, 48638, 27722, 44354, 47132, 16301,
       42482,  5848,  4310,  9702, 30459, 20367, 49466, 15766, 35016,
       20194,  1098, 21103, 22110, 18329, 49563, 45897, 16120, 19276,
       11103, 36601, 44381, 10209, 43955,  7539, 45789, 11970, 48629,
       18127, 19799, 10255, 11222, 27698, 43551, 11805,  6584, 20153,
       36425,  4994, 25559,  1655, 12342,  9429, 36930,  2432, 31141,
       10506,  4346, 27021,  3807, 12750, 49140,  8330, 39921, 37939,
        6229, 24749, 19853, 22568, 22891, 48452, 46374, 20058, 14018,
        1963, 10587, 28423, 47892, 49408,  6306,  7161, 33358, 36474,
        6905,  6169, 16808, 10929, 36080, 14330, 31868, 39980, 21544,
       32982, 17047, 30960, 29139, 42357, 38992, 35124, 17578, 38066,
       48007, 13793, 28563, 12528, 12425, 21147,  2383,  7517,  5797,
       22223, 23189,  9792, 16604,  6684,  5963,  7153, 36990, 42373,
        8467, 31094,  5007, 13907, 25899, 22660,  2562,  8028, 16796,
       15321, 39158, 33976, 46286,  3709, 49078,  3190, 24652, 45319,
       44543, 14800,  6567, 14014, 44957,  1120, 42650, 10653, 15444,
        8680, 13393, 11668, 47933, 19875, 12546, 27275, 25713, 31460,
       27665,  5183, 39438, 44390, 36828, 37769, 44859, 33978, 32732,
        4108, 46503,  7519,  6135, 37204,  4739, 26877,  7357, 49234,
        6226, 19609, 39058, 44311,  5102, 14498, 24520, 46893, 23463,
       14927, 18890, 12755, 39911, 38710, 43818, 47349, 35753,  2825,
       12335,  1548, 45187,   168, 14967, 42666, 39040, 27802, 18117,
        7370, 23151, 11308, 33426, 40060, 22627, 35406, 28626, 34675,
       19063, 30923, 27994, 49230, 25596, 14593, 34233, 48806, 33838,
        4517, 14668,  2587, 28045, 49186, 37743,  6064,  6967, 18480,
       48899,  4769, 42031, 25061, 47113, 11473,  7821,  3452, 34940,
       14546, 21369, 21137, 33050, 30439, 43098, 23439, 45195, 26424,
       25459, 43918,   714, 36490, 26074, 38037, 33112, 23533, 42347,
       22762,  7390,  8696, 13849, 42171, 26344, 43646, 49392, 37689,
       17219, 23032, 22151,  4983, 44841, 36298, 44601,  6121, 31534,
       25069,  7777,  9070, 20245, 49539, 34453, 19125, 47566, 19546,
        7728, 36844, 24228, 49232,    65, 43164, 18698, 39828, 23332,
        5685, 22043,   432,  7827, 21893, 47423, 20994, 20273, 42860,
       29209, 46796, 22529, 42498, 27402, 44575, 31694, 17409, 32348,
       33643, 31209, 29986,    44, 27792,  6464, 25105, 45202,  5747,
       42292, 41710, 22818, 13994, 37461, 20162,   753, 38902, 27745,
       26532, 24385, 36388, 15577, 40472,  9765, 47001, 28538, 23564,
       18238, 17255, 16766, 39234, 36400, 17126, 48185, 39964, 32886,
       41489, 48003, 32462, 49759,   761, 26470, 31523, 24528,  6544,
        1551, 14209, 38650, 36251, 19598, 24462, 24425, 28694, 41849,
       22217, 25614, 33114,  9863, 45877, 46026, 36379, 43195, 47280,
       15425, 27447, 28081, 48681, 10157, 48840, 20844, 23601, 20222,
       37698, 33907, 16992, 12551, 24932, 41138, 31607, 14425, 37981,
       35674, 20381, 49873,  6673, 17701, 27576, 28192, 21604, 13207,
       34845,  2600, 24979, 14966, 44436, 28643, 35037, 34364, 49959,
       40484,  3505, 49075, 33742,  9781, 13317, 41230, 47288,  8346,
        3851, 49121, 13845, 48074, 48733, 41174,  3488, 15065, 25528,
       20538, 42706,  8157,  2044, 25089, 23537, 13286, 41333, 42120,
       29087,  8234, 33564, 24927,  2343, 47172, 33170, 18646, 39412,
       41667, 28667, 19932,  5717, 42168, 41084,  3245,  4493, 39901,
       11171, 32310,  1787, 17767,  6073, 16349, 28802, 28426, 20959,
       19039, 34873, 20457,   709, 14435, 30834, 26287, 15926, 10391,
       26605, 18162, 47152,  6685,   963,  6856, 14840, 38365,  2133,
        1609, 29229, 41599, 35853, 33379, 23123, 35310, 14473,  3629,
        5707,  9463, 18503, 35148, 11382, 44730, 45252, 33350,  6757,
       16215, 38546, 40134, 28497,  6236, 35834, 16616,  8019, 15326,
       37934, 23064,   630, 43019, 33448, 43545, 35653, 14127,  6069,
        7332,  9368, 38001, 29907, 24134, 14021, 47248, 31884, 19611,
        5340, 30096, 16232, 49858, 48370,  7664,  7340, 24962,   779,
       48300, 31002, 41273,  3982, 40348, 15649, 26730, 11270, 21332,
       25195, 33519,  7662, 38058, 36112, 38592, 40084, 39580,  8819,
       39269, 36293, 43680, 42217, 46053, 22780,  5926, 29354, 45360,
       17883, 37713, 32067,  2846, 25587,  1219, 44238, 12287, 39960,
        1918, 24496, 22960, 47452, 22035, 22432, 41990, 13506, 13814,
       29925,  6469, 30229, 23716, 48608, 28410, 18027,   790, 18234,
       42394, 36726, 35860, 12067,  2354, 46410, 39540,  6488, 36138,
       48790, 40244,  4634, 37488, 21987, 28862, 14459, 29126, 37615,
        8112, 11629,  7919, 15563, 13757, 36883, 16319, 43569, 39112,
       28686,    63, 26629,  1475, 46284, 33277, 35645,  3692, 32257,
       14720,  5638, 35353, 11385, 25491, 26215,  9660, 30025, 37895,
       25174, 42889, 15285, 23217, 46373,  6041, 22667, 17769, 32030,
       11030, 16693, 35157, 49573,  6467, 27470, 41562, 38705,  9992,
       14478,  1189,  4356, 49725,  4091,  8990, 42172, 38750, 11454,
       44574, 40661,  8740, 37932, 23158, 28641,  7449, 48194,  9404,
        9989]), [4, 3, 1, 2]), (array([38010, 31880, 23847, 43277, 35763, 14567, 39230, 48930, 48136,
       30383, 33530, 30645,  7547, 45927,  5404, 11939,   765,  9860,
       48788, 11721,  6400, 42151, 31757, 46303, 33933, 37879, 27609,
       18299, 40851, 40548, 34125, 43289, 22200,  5743, 39396, 10777,
       49387,  7731, 12375, 23240, 41664, 12944, 27425, 42587, 31412,
       47805,  4854, 23419, 48852,  8355,  6428, 32058, 22608, 13650,
       49559,  8660, 10796,   405, 14456, 42754,  2277, 26144, 32508,
       42878, 39739, 24764, 30324, 37181, 26704, 26488, 48908, 17107,
        3362, 46614,  9119, 47754, 20236, 30537, 29280, 29364, 38639,
       34012, 35263, 29173, 49303,  2287, 21360, 16225,  2066, 29844,
        5103,  4599, 36470, 16191,  5249, 13860, 30428, 29940, 48556,
       12328,  3197, 44616, 44884, 33542,  3842, 26391, 23021, 27000,
       28302, 37504,  8258, 43924, 43996, 27440, 34032, 30304, 23767,
       49528,  3049, 24543, 46041, 40287, 35603, 39966, 37061, 17221,
       28469,  5163,  9616,  7509,  1878,  7225, 15571, 46380, 29634,
       15304, 23352, 13457, 26656, 41410, 11114, 30959, 32107, 19445,
       38943, 43442, 11633, 10713, 37189,  4165, 42332,  4477, 24491,
        7203, 28179, 16075, 29515, 43483,  9657, 43475, 12270,  1205,
       33328, 44940, 16634, 17233, 39584, 48324, 29257, 30052, 10729,
       43368,  1178, 45304, 24844, 38387,  6132, 46872, 10146, 14782,
       28584, 44176,  7464,  5174, 35391,  4774, 15900, 34787, 17453,
        3607, 28033, 39138, 14867, 22264, 10987, 43814, 41243, 10955,
       38753, 28014, 19188, 37558, 37034, 12284, 25420, 46019, 41546,
       12052, 33662, 21639, 46369, 48554, 44303, 37102, 47332, 18591,
       42517, 17465, 20393, 33380, 42103, 28636, 44721,  2171, 18106,
       15115,  8655,  8549, 39392, 18271, 29620, 49628, 28833, 37945,
       39196, 33128, 28675,  7930,  2692, 48257,  3292, 47468, 22769,
       29927, 47324,  4552,  2683, 21288, 37062,  9435, 47018, 27671,
       23503, 22895, 45018, 26785, 32999,  9205, 24452, 42820, 32386,
       23432, 34635, 48817, 22387,  5593, 42813, 41501, 31561, 24702,
       11947, 19896, 10104, 33835, 12371, 14185, 10760, 31555, 34941,
       27805, 27141, 19889, 15968, 40783, 21780, 41875,  8683, 29796,
       28299, 43653,  6978, 32944, 32530, 39714, 47440, 44119, 49541,
       12485, 39458,    83, 33702, 26236, 10366, 40503, 49333, 16236,
       42384, 35842,  3002,  4255, 13990, 40269, 28144, 20292, 42691,
       36332, 27422, 44441, 27066,  1991, 23121, 44229, 21810,  5367,
        6575, 39225,  8269, 40205,  5729, 49185,  4548, 24796, 26845,
       47672, 38424, 29268, 27956, 19524, 48454,  6232,  5201, 44665,
       10319, 21957, 12545, 47059, 42987, 30701, 33142, 28854,  7417,
       34007,  4495, 37531, 44046, 49467,  8601, 36350, 33965, 23668,
        8768, 22840, 37676, 13252,  8960,  1538, 37216, 21059,  8954,
        3604, 14759, 21804, 43404, 14533, 38784,  5970, 10194, 25589,
       25517,  1567, 44312, 28845,  7189, 27150, 40905, 38417, 28843,
       28221, 26171,  1943, 20562, 22834, 44106, 15898, 18732, 29996,
       28550, 15602, 49565, 19012, 39147, 39748,  6436, 33989, 47768,
       36246, 18971, 10175, 12069, 28877,  2108, 29398, 26827, 36854,
       42731, 40004, 28069, 26954, 14908,  6815, 47223, 39539, 44345,
       38513,  1204, 31264, 36439, 29818, 15320, 13249, 41997, 11155,
       24226, 14734, 46667, 36487, 44085, 47485, 31973, 35949, 15403,
       15709, 45422, 20076, 19222,  1873, 33889, 16361, 35386, 48181,
       48083, 28286, 10181, 34207,  5510, 14530, 34220,  6082,   784,
       18981, 46222, 46621, 32044, 32804,  4740, 16767, 29603, 17259,
       18894, 21586,  3426, 48797, 32516, 26998, 26676, 32798, 20059,
       47038,  8511, 22650,  7113, 11267,  5760, 24867, 10555, 12573,
       25170, 33723, 40831, 24819, 14748, 12198, 43189, 28595, 13121,
       35254,  3015, 24363, 21834, 42178, 13218, 13843, 23100, 25044,
        2851, 28362, 36715, 30582, 38484, 15114, 40024, 21027, 26636,
       27905, 18518,   991,  3822, 41289, 37136, 27129,  9565, 12311,
       31147,  9209, 14302, 16272, 49163, 15740, 27871, 40537, 14061,
       26787,  4553, 48655, 11163, 41189, 45077, 22048, 23635, 13118,
       40407, 31102, 17312, 31072, 40250, 13505, 45769, 44225,  7124,
       20674, 46406, 33252,   261,  6709,  1631, 25874, 47774, 43369,
       22379,  6161, 44529, 49080, 28344, 21971, 24003,  8529, 34830,
       16410,  8216, 17580, 26471,  6851, 30135,  8090, 37264, 11242,
        4250, 41550, 37491, 34206,   772, 25209, 36594, 21280, 48269,
        5632, 18754, 38818, 14849, 24119, 23662, 44097, 12389, 33314,
       16180, 41455,  5224, 45787, 11196, 49616, 13296, 16087, 38549,
       48898,  8174, 18879, 42684,  2828,  3238, 28617, 39965, 13543,
       40358, 32311, 45859, 25306,  5805, 12964, 33910, 42402, 34063,
       16929, 24746, 24450, 41063, 13999, 48709, 34862, 43954, 48631,
       21720, 21094, 48528,  2259, 22884, 37620, 23203, 22067,  3661,
        2180, 42119, 25558,  9918, 46588, 41666, 11799, 27273, 21795,
        3922, 23379,  4528, 43006,  2280, 27130,  2545, 25881, 21407,
       13327, 18776, 25314, 11692,  1494,  7707,  2731, 11082, 44385,
         466, 35827, 42531, 41502, 16029,  5157, 36181, 29133, 42124,
        6895, 25608, 34809, 26241,  7381, 35651, 20056, 16563, 43145,
        4443, 36445,  8556,  5951, 10476, 28183,  9007, 17861, 21150,
       46598, 14518, 27607, 30840, 35291,  4648,  9400, 32488,  5627,
       15042,  3935, 34548, 46330, 14519, 35605, 14311,  4661, 19930,
       41133,  6120, 39275,  6889, 12893, 27084, 44589, 35953, 26467,
       44643, 38725, 17460, 34358, 46794, 18744, 47040, 20811,   250,
       24206, 17700, 32472, 47701, 37687, 15814, 18066, 12299, 17492,
       10630, 31138, 21985, 36591, 25567, 29188, 17471, 13766, 38878,
       27510, 22270,  3808, 12703,  2202, 22953, 31684, 48282,  4189,
       41498, 10860, 19729, 47133, 35493, 20403,  2619, 11825, 35765,
       13698, 26279, 32711, 45489, 30239, 24699, 15584, 28648,  2436,
       37560, 10406, 44351, 48232,  2784, 46563, 36494,   701, 12549,
       22974, 41143, 45681, 19157, 46642, 44797, 44327, 43826, 39729,
       34482, 39222, 36116, 20990, 32687,  9217, 36616, 27076,  5417,
       49008,  2090,  1050,   796, 28158, 15751, 48354, 35466, 27546,
       22587, 10926, 23264, 40213, 15837, 37008,  3101, 41856, 29636,
        9519, 13643, 21224, 35824, 15789, 37613,  3608, 36658, 29243,
       39308, 40941, 32151,  6499, 27750, 37485, 38989,  4109, 29079,
       33677, 35862, 34789, 44203, 16757, 13626,   778, 32951, 15910,
       15369,  4122, 37127, 29812, 44732, 13418, 31550, 47958,  8340,
       49666,  2719, 36009, 48589, 26449, 22215,   724, 30470, 26757,
       36377, 10866, 13435, 30396, 41454, 16312, 29682, 14715,  1675,
       19296, 34254, 29658, 10898, 12819, 10885,  4588, 39360, 36410,
       42765, 46795, 30377, 13727, 47171, 48205, 34268, 18464, 49248,
       39110,  7151, 34925, 30572, 19453, 11713, 18612, 10160,  7336,
       31454,  9712, 12582, 31687, 17902, 34508,  8173, 35785,  9420,
       36130, 18733, 31057, 40917, 46389, 25989, 36651, 18248,  7688,
       29470, 28291, 47853, 48270, 29389, 26976, 11194, 25030, 48598,
       47492, 49224, 24978, 48365, 39693, 38565, 45335, 30680, 47481,
       14601, 48745, 19806, 21630, 15847, 14174, 43106, 46088,    55,
       28031, 19130, 31707, 29269, 19435, 11197, 10152, 14088, 17850,
       27121,  9876,  5012, 49093,  2993, 29005, 31492, 43285,  8482,
       37121, 38389, 14135, 49329, 46870, 13524, 15922, 20061, 18385,
       39055, 44652, 42328, 24977, 25296, 39881, 12044, 41422,  8726,
       30746, 27326, 33457,  5576,  3259,  9933, 41512,  9448, 38129,
       37797,   646, 21668, 34685, 45367,   885,  3845, 15297, 16906,
       17117]), [0, 5, 1, 2])]
Collaboration
DC 0, val_set_size=1000, COIs=[7, 9, 1, 2], M=tensor([7, 9, 1, 2], device='cuda:0'), Initial Performance: (0.25, 0.04502433729171753)
DC 1, val_set_size=1000, COIs=[6, 8, 1, 2], M=tensor([6, 8, 1, 2], device='cuda:0'), Initial Performance: (0.26, 0.044506120443344115)
DC 2, val_set_size=1000, COIs=[4, 3, 1, 2], M=tensor([4, 3, 1, 2], device='cuda:0'), Initial Performance: (0.249, 0.04467034757137298)
DC 3, val_set_size=1000, COIs=[0, 5, 1, 2], M=tensor([0, 5, 1, 2], device='cuda:0'), Initial Performance: (0.248, 0.04460585105419159)
D00: 1000 samples from classes {1, 2}
D01: 1000 samples from classes {1, 2}
D02: 1000 samples from classes {1, 2}
D03: 1000 samples from classes {1, 2}
D04: 1000 samples from classes {1, 2}
D05: 1000 samples from classes {1, 2}
D06: 1000 samples from classes {9, 7}
D07: 1000 samples from classes {9, 7}
D08: 1000 samples from classes {9, 7}
D09: 1000 samples from classes {9, 7}
D010: 1000 samples from classes {9, 7}
D011: 1000 samples from classes {9, 7}
D012: 1000 samples from classes {8, 6}
D013: 1000 samples from classes {8, 6}
D014: 1000 samples from classes {8, 6}
D015: 1000 samples from classes {8, 6}
D016: 1000 samples from classes {8, 6}
D017: 1000 samples from classes {8, 6}
D018: 1000 samples from classes {3, 4}
D019: 1000 samples from classes {3, 4}
D020: 1000 samples from classes {3, 4}
D021: 1000 samples from classes {3, 4}
D022: 1000 samples from classes {3, 4}
D023: 1000 samples from classes {3, 4}
D024: 1000 samples from classes {0, 5}
D025: 1000 samples from classes {0, 5}
D026: 1000 samples from classes {0, 5}
D027: 1000 samples from classes {0, 5}
D028: 1000 samples from classes {0, 5}
D029: 1000 samples from classes {0, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO3']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.295, 0.06421255218982697) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.378, 0.06314386874437332) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.279, 0.0991387677192688) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.388, 0.08198602849245071) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.447, 0.07024952399730683) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.453, 0.0713477740585804) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.306, 0.1353405278623104) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.08995045691728593) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.43, 0.0827924791276455) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.07941373744606972) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.354, 0.1567333093881607) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.465, 0.112021663159132) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.1102066800892353) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.478, 0.09507753160595894) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.384, 0.17660580858588218) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.476, 0.1585041108801961) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.448, 0.1454920856282115) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.479, 0.1163012564405799) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.392, 0.18973852957785128) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.20079846078529953) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO4']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.14613931557536125) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.487, 0.129363997079432) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.20482386314868928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.22117121162824332) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.1731434683352709) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.485, 0.13845351364091038) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.19699945250153542) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2411895220540464) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.17792725160717965) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.486, 0.16328691100515424) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2086139908656478) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.25990544859599324) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.19964710243418812) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.489, 0.15808263261057437) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.422, 0.22641488587111236) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.472, 0.2669675034582615) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.477, 0.20615564992837607) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.483, 0.16297005883976817) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.20922902297973633) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.30454038487095386) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[1, 2], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.847, 0.015917466536164285)
DC Expert-0, val_set_size=500, COIs=[9, 7], M=tensor([7, 9, 1, 2], device='cuda:0'), Initial Performance: (0.954, 0.004131161715835333)
DC Expert-1, val_set_size=500, COIs=[8, 6], M=tensor([6, 8, 1, 2], device='cuda:0'), Initial Performance: (0.966, 0.0030926317796111107)
DC Expert-2, val_set_size=500, COIs=[3, 4], M=tensor([4, 3, 1, 2], device='cuda:0'), Initial Performance: (0.832, 0.01434116494655609)
DC Expert-3, val_set_size=500, COIs=[0, 5], M=tensor([0, 5, 1, 2], device='cuda:0'), Initial Performance: (0.934, 0.005315904927439987)
SUPER-DC 0, val_set_size=1000, COIs=[7, 9, 1, 2], M=tensor([7, 9, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[6, 8, 1, 2], M=tensor([6, 8, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[4, 3, 1, 2], M=tensor([4, 3, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[0, 5, 1, 2], M=tensor([0, 5, 1, 2], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7d8658044dc0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7d86647dbd90>, <fl_market.actors.data_consumer.DataConsumer object at 0x7d86641be3a0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7d86507955e0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7d8658044790>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.935, 0.006334980200976134) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004330671178176999) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.002942199419485405) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.013049502417445184) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.944, 0.004963145914487541) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.487, 0.09255493481457233) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.615, 0.044275046415627003) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.51, 0.05166408550739288) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.538, 0.1201990282908082) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.0035113336024805903) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004157401207834482) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.002765019295504317) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.013750713005661965) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004211559076793492) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.496, 0.059523570358753206) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.665, 0.04030325064063072) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.561, 0.045446689128875735) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.604, 0.06240568926185369) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.0030476746169151737) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004517898051068186) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0027702599181211552) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.856, 0.01436500258743763) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.004977886331267655) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.632, 0.03568078476190567) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.715, 0.031978302121162414) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.596, 0.04531221118569374) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.696, 0.037936606079339984) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.0031282596011878924) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0046994839124381545) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.00255238611320965) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.013220055103302002) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.004165235020685941) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.625, 0.03322945734858513) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.738, 0.02558685314655304) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.578, 0.04485312724113465) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.705, 0.0370280667245388) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9635, 0.0032554322173818945) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004231263095512986) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.002706480159540661) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.015606938421726228) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.004026004926068708) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.561, 0.061800545826554296) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.742, 0.03216676850616932) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.597, 0.042742857575416564) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.641, 0.050114810675382614) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9655, 0.00326139749790309) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004278309229761362) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.002605360841145739) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.015102042227983475) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.005229853085125797) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.045765645951032635) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.728, 0.029615534111857415) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.05540416586399079) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.704, 0.036140245735645296) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9585, 0.0034671895042411053) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005025780002819375) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0027944240708602593) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.01565276823937893) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.00445546766743064) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.592, 0.050471296191215514) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.743, 0.027157448351383208) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.599, 0.04782440584897995) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.691, 0.0382169189453125) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.965, 0.003026425651332829) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004546353803947568) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0028272026147460566) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.014144390225410461) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.0060604083458893005) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.575, 0.04916224528104067) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.778, 0.023444716796278953) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.592, 0.04734405216574669) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.727, 0.03323284375667572) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.963, 0.004161817841551965) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.004271695319097489) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0030466475859284403) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.014887997359037399) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.95, 0.00528891110420227) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.673, 0.036004799097776416) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.745, 0.024930493906140327) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.563, 0.05181194987893105) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.709, 0.03513875421881676) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9635, 0.003377107796535711) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004621291158720851) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0034185612213332205) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.017597798094153405) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.0052325274425093085) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.635, 0.04285764172673225) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.76, 0.023894026786088943) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.603, 0.04322230035066604) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.723, 0.03358262304961682) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9645, 0.0035015088942818694) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004686413192539476) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.004075136409213883) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.856, 0.018846873477101327) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006682079893769697) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.658, 0.03506489610671997) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.761, 0.02805743768811226) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.598, 0.04913363891839981) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.671, 0.040687834233045575) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.0034469281497149493) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004933944856747985) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0031656119519611822) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.016521833822131158) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.005846861316327704) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.05178230503201485) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.755, 0.02678404365479946) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.625, 0.04250983327627182) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.716, 0.035467515110969544) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9655, 0.00317169181079953) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004901531061972491) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0035420461197500116) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.017868235170841216) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.007140775905165355) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.040136510908603665) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.77, 0.02850631467998028) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.598, 0.05177110171318054) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.692, 0.03964764228463173) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.0037002598625113024) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004177804378792643) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003267784903640859) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.01781517419219017) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.005317021423979895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.64, 0.05121419411897659) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.746, 0.028313815847039223) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.577, 0.054027207493782044) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.709, 0.03704192736744881) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.0035629080161234013) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005624534115486313) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0036090557058778357) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.016962111055850982) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.004710258760547731) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.646, 0.03957973083853722) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.728, 0.03369829134643078) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.586, 0.05082072949409485) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.691, 0.03691163596510887) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9685, 0.003538049901388149) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005403815612720791) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0028786128881911283) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.018202820241451263) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.007195745400342276) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.555, 0.06385636730492115) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.652, 0.046806669156998396) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.584, 0.0567541692852974) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.667, 0.044251493036746976) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.003493863342126133) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005143847239669413) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003196704176836647) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.018472925394773482) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006197083230828866) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.625, 0.04523317288607359) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.743, 0.031296284437179564) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.593, 0.04987446594238281) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.701, 0.04088004288077354) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.973, 0.0037604956192062674) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004702484054490924) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0030991087485454046) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.018055744111537934) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.00532552866806509) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.604, 0.04761206229031086) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.738, 0.02941493719816208) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.552, 0.05914287483692169) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.67, 0.04060187396407127) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.0034489914172736464) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.004053778160363436) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0027868769047781827) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.01685335850715637) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.005697840269654989) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.615, 0.047204912334680556) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.71, 0.03652881418168545) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.588, 0.051364448189735415) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.704, 0.0389387965798378) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.0034544352009688735) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.974, 0.004203337448649108) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0030613857481075683) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.016062335699796676) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.005970812145620585) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.058190276950597766) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.706, 0.03790628883242607) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.575, 0.05399363034963608) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.659, 0.04483589345216751) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.00354394768486236) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.004427930383011699) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003007323193422053) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.016771577566862105) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.006674592652678257) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.601, 0.046207492716610434) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.746, 0.030705698370933534) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.623, 0.0473079674243927) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.691, 0.043341827273368834) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.003518140962192774) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.004959613096201792) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0033273122793470973) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.01879065039753914) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.0064096750545722895) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.631, 0.04892246620357037) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.742, 0.030363019078969957) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.06151811325550079) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.678, 0.044704005658626554) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9675, 0.003803782817232786) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.974, 0.004892418649513275) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003455012258520583) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.01760625648498535) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.006093914650322404) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.05482286502420902) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.743, 0.03432208546996117) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.538, 0.0625423269867897) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.655, 0.044166576147079466) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9655, 0.003872911718317482) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.976, 0.004750354958639946) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003490030093045789) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.018133401840925215) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.005720984535641037) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.04070509439706802) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.72, 0.03284953987598419) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.053300109684467314) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.649, 0.0519112366437912) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.003313756072901015) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.974, 0.004601334782317281) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0040645132761274) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.020123577758669853) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.946, 0.006033604060299695) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.04689635996520519) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.726, 0.037894655741751194) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.555, 0.0577879119515419) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.629, 0.05546288761496544) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9675, 0.003721363240354549) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.978, 0.0038734060025308282) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003918150365880137) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.016079694807529448) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.004907808429561555) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.042864707946777346) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.767, 0.02630090096592903) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.62, 0.04874168473482132) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.673, 0.05376061901450157) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9675, 0.0041076738730616855) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.005017378071090207) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.0035899996288935652) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.015537854075431824) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006347889259050134) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.603, 0.04429099741578102) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.674, 0.04110608645156026) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.597, 0.0546313384771347) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.68, 0.04534341478347778) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9685, 0.0036043168877113205) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005902468436979689) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.00385215434285783) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.017219540983438492) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006137902111979202) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.615, 0.05264648960530758) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.676, 0.04856567415595055) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.563, 0.059030362904071806) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.687, 0.04210517156124115) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.0036382373415162874) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005258666300331242) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003612902048189426) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.015259453773498535) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.006489517567912117) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.597, 0.04931300504505634) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.709, 0.03770454765856266) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.598, 0.05221340924501419) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.672, 0.05625690743327141) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9685, 0.00376092232598603) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.004300222561927513) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.98, 0.0037957496116869153) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.020676939338445662) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.00531078579928726) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.56, 0.04852891202270985) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.659, 0.05731403946876526) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.569, 0.06138270491361618) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.68, 0.04327588802576065) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.003991149415263863) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.004249443041160703) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003968255244020839) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.01721110385656357) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.005881651711824816) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.038726362079381946) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.702, 0.0384599839001894) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.05478143829107285) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.662, 0.045156591475009916) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.0034715996186650954) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004751409147167578) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.004077414684230462) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.015455190628767013) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.006862629771989304) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.603, 0.04829297912120819) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.678, 0.049627564318478104) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.575, 0.057674789786338804) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.647, 0.05765736904740334) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.004531547039925499) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.005651842153631151) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.004292562329152133) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01573483496904373) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.006985190342529677) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.561, 0.057703790370374916) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.72, 0.03773105150461197) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.504, 0.06952627098560334) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.642, 0.049792878329753876) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.003927809601209447) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.005510474695009179) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004985358443100267) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.018619304478168487) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.008318951296125306) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.04288591653108597) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.7, 0.04836172634363174) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.566, 0.06218312901258469) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.626, 0.06388346421718598) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.004377190882732066) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.974, 0.005587793952785433) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.00506034036844585) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.019294600859284403) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.00813551343769359) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.63, 0.03986516207456589) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.636, 0.05998890519887209) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.571, 0.05831019181013107) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.613, 0.06578141671419144) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.00391548550783591) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.974, 0.004781891909427941) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.004036163935670629) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.018111396163702012) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006265007918016636) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.605, 0.04671021690964699) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.67, 0.054020491167902945) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.561, 0.06597041410207749) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.655, 0.05244132381677628) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.004229553833429236) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.005411833129779552) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.984, 0.004004291108707548) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.01912748572230339) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.007392224127310328) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.607, 0.05033475238084793) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.699, 0.05149827750399709) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.548, 0.06195019835233689) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.627, 0.06837549617886543) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.00395048176263208) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.972, 0.005870802205448854) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.978, 0.003785395205050008) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.017381709337234496) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.007191301196115092) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.587, 0.050766116440296175) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.659, 0.05837053133547306) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.06325946962833405) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.62, 0.06720847879350185) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.0037810600802076805) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.005664284283295274) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.005449064398751346) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.017022663414478303) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.006557029316434637) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.053755159229040145) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.655, 0.0669657881706953) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.537, 0.0625254977941513) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.627, 0.058761735498905183) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.0035629179250609012) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.005533341544913128) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.005428024625713988) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.017661312907934188) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.008336268346232828) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.583, 0.05731514431536198) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.628, 0.0650823487341404) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.546, 0.07031012427806854) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.601, 0.07320794896781445) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04502433729171753), (0.295, 0.06421255218982697), (0.447, 0.07024952399730683), (0.43, 0.0827924791276455), (0.458, 0.1102066800892353), (0.448, 0.1454920856282115), (0.472, 0.14613931557536125), (0.477, 0.1731434683352709), (0.47, 0.17792725160717965), (0.477, 0.19964710243418812), (0.477, 0.20615564992837607), (0.487, 0.09255493481457233), (0.496, 0.059523570358753206), (0.632, 0.03568078476190567), (0.625, 0.03322945734858513), (0.561, 0.061800545826554296), (0.644, 0.045765645951032635), (0.592, 0.050471296191215514), (0.575, 0.04916224528104067), (0.673, 0.036004799097776416), (0.635, 0.04285764172673225), (0.658, 0.03506489610671997), (0.583, 0.05178230503201485), (0.644, 0.040136510908603665), (0.64, 0.05121419411897659), (0.646, 0.03957973083853722), (0.555, 0.06385636730492115), (0.625, 0.04523317288607359), (0.604, 0.04761206229031086), (0.615, 0.047204912334680556), (0.628, 0.058190276950597766), (0.601, 0.046207492716610434), (0.631, 0.04892246620357037), (0.577, 0.05482286502420902), (0.627, 0.04070509439706802), (0.634, 0.04689635996520519), (0.629, 0.042864707946777346), (0.603, 0.04429099741578102), (0.615, 0.05264648960530758), (0.597, 0.04931300504505634), (0.56, 0.04852891202270985), (0.629, 0.038726362079381946), (0.603, 0.04829297912120819), (0.561, 0.057703790370374916), (0.609, 0.04288591653108597), (0.63, 0.03986516207456589), (0.605, 0.04671021690964699), (0.607, 0.05033475238084793), (0.587, 0.050766116440296175), (0.577, 0.053755159229040145), (0.583, 0.05731514431536198)]
TEST: 
[(0.25, 0.04384468254446983), (0.3, 0.06179358550906181), (0.45025, 0.0672670782506466), (0.438, 0.07863861978054047), (0.46075, 0.10416459986567497), (0.4575, 0.13624865448474885), (0.471, 0.13515578013658525), (0.473, 0.1617032497525215), (0.46975, 0.16537044155597685), (0.477, 0.18476930224895477), (0.475, 0.19292769503593446), (0.49675, 0.08719056990742684), (0.51825, 0.05684522117674351), (0.64875, 0.034328589379787444), (0.64075, 0.03315312897413969), (0.559, 0.05826252084970474), (0.64725, 0.04491850784420967), (0.59425, 0.04886842955648899), (0.578, 0.047772850707173346), (0.66525, 0.03650044986605644), (0.6535, 0.04167886297404766), (0.64325, 0.03516037694364786), (0.5855, 0.050808347649872306), (0.634, 0.041624305084347724), (0.66025, 0.047336338624358175), (0.6465, 0.04025773073732853), (0.573, 0.060659882605075835), (0.639, 0.04375768904387951), (0.60025, 0.04586043214797974), (0.62025, 0.04660024406015873), (0.6325, 0.055276305109262465), (0.61375, 0.043129806213080886), (0.62425, 0.047730790391564366), (0.58825, 0.05463239195942879), (0.65025, 0.038222273826599124), (0.63725, 0.04670052184909582), (0.6125, 0.04318508081138134), (0.60325, 0.04347361402213573), (0.61375, 0.05318811801075935), (0.61475, 0.0463733756095171), (0.598, 0.04575777007639408), (0.64075, 0.0380927893742919), (0.615, 0.04827784024178982), (0.568, 0.05638510137796402), (0.62875, 0.04046683941036463), (0.63075, 0.039594087101519106), (0.615, 0.047523539111018184), (0.616, 0.051637866877019406), (0.58425, 0.053970692530274395), (0.5615, 0.05565710456669331), (0.583, 0.059078896299004556)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.80      0.18      0.29      1000
           2       0.55      0.88      0.68      1000
           7       0.81      0.44      0.57      1000
           9       0.51      0.84      0.63      1000

    accuracy                           0.58      4000
   macro avg       0.67      0.58      0.54      4000
weighted avg       0.67      0.58      0.54      4000

Collaboration_DC_1
VAL: 
[(0.26, 0.044506120443344115), (0.378, 0.06314386874437332), (0.453, 0.0713477740585804), (0.447, 0.07941373744606972), (0.478, 0.09507753160595894), (0.479, 0.1163012564405799), (0.487, 0.129363997079432), (0.485, 0.13845351364091038), (0.486, 0.16328691100515424), (0.489, 0.15808263261057437), (0.483, 0.16297005883976817), (0.615, 0.044275046415627003), (0.665, 0.04030325064063072), (0.715, 0.031978302121162414), (0.738, 0.02558685314655304), (0.742, 0.03216676850616932), (0.728, 0.029615534111857415), (0.743, 0.027157448351383208), (0.778, 0.023444716796278953), (0.745, 0.024930493906140327), (0.76, 0.023894026786088943), (0.761, 0.02805743768811226), (0.755, 0.02678404365479946), (0.77, 0.02850631467998028), (0.746, 0.028313815847039223), (0.728, 0.03369829134643078), (0.652, 0.046806669156998396), (0.743, 0.031296284437179564), (0.738, 0.02941493719816208), (0.71, 0.03652881418168545), (0.706, 0.03790628883242607), (0.746, 0.030705698370933534), (0.742, 0.030363019078969957), (0.743, 0.03432208546996117), (0.72, 0.03284953987598419), (0.726, 0.037894655741751194), (0.767, 0.02630090096592903), (0.674, 0.04110608645156026), (0.676, 0.04856567415595055), (0.709, 0.03770454765856266), (0.659, 0.05731403946876526), (0.702, 0.0384599839001894), (0.678, 0.049627564318478104), (0.72, 0.03773105150461197), (0.7, 0.04836172634363174), (0.636, 0.05998890519887209), (0.67, 0.054020491167902945), (0.699, 0.05149827750399709), (0.659, 0.05837053133547306), (0.655, 0.0669657881706953), (0.628, 0.0650823487341404)]
TEST: 
[(0.2605, 0.04348890659213066), (0.38275, 0.06072431656718254), (0.45925, 0.06845111122727394), (0.451, 0.07635300573706627), (0.479, 0.09213218602538109), (0.482, 0.1134857217669487), (0.4865, 0.12708065676689148), (0.48525, 0.1371110547184944), (0.48575, 0.1597578677535057), (0.4875, 0.15451923394203185), (0.48675, 0.16020039916038514), (0.58475, 0.04555307303369045), (0.646, 0.041171626687049864), (0.6915, 0.03169973509013653), (0.747, 0.02516709729656577), (0.708, 0.032153388530015946), (0.7205, 0.029612935699522495), (0.723, 0.02837946430966258), (0.7425, 0.0256789926327765), (0.75075, 0.024583229526877404), (0.75625, 0.026246374823153017), (0.7455, 0.029545267436653377), (0.7445, 0.026039678655564787), (0.73925, 0.028283161580562592), (0.73225, 0.029107665464282034), (0.7245, 0.032425511740148065), (0.6435, 0.0465229998677969), (0.72775, 0.03157576302438975), (0.7245, 0.0326140623614192), (0.70325, 0.03768766286969185), (0.70925, 0.03593936764448881), (0.73275, 0.03135877864062786), (0.73025, 0.03150221636146307), (0.72775, 0.03366834180802107), (0.7125, 0.03507175599783659), (0.69625, 0.03950827609747648), (0.75425, 0.026570012174546718), (0.6695, 0.042447334937751297), (0.6595, 0.05023451596498489), (0.70625, 0.03714323717355728), (0.643, 0.05736223530769348), (0.6895, 0.03870321971923113), (0.6635, 0.0513367827385664), (0.72525, 0.03670702514797449), (0.69075, 0.04887994679808617), (0.629, 0.05905147795379162), (0.66375, 0.05337190797179937), (0.6775, 0.053471218578517436), (0.65, 0.05858505554497242), (0.63875, 0.06713428059220314), (0.62725, 0.06617300444841386)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.91      0.30      0.45      1000
           2       0.73      0.38      0.50      1000
           6       0.61      0.90      0.73      1000
           8       0.55      0.93      0.69      1000

    accuracy                           0.63      4000
   macro avg       0.70      0.63      0.59      4000
weighted avg       0.70      0.63      0.59      4000

Collaboration_DC_2
VAL: 
[(0.249, 0.04467034757137298), (0.279, 0.0991387677192688), (0.306, 0.1353405278623104), (0.354, 0.1567333093881607), (0.384, 0.17660580858588218), (0.392, 0.18973852957785128), (0.403, 0.20482386314868928), (0.403, 0.19699945250153542), (0.404, 0.2086139908656478), (0.422, 0.22641488587111236), (0.416, 0.20922902297973633), (0.51, 0.05166408550739288), (0.561, 0.045446689128875735), (0.596, 0.04531221118569374), (0.578, 0.04485312724113465), (0.597, 0.042742857575416564), (0.558, 0.05540416586399079), (0.599, 0.04782440584897995), (0.592, 0.04734405216574669), (0.563, 0.05181194987893105), (0.603, 0.04322230035066604), (0.598, 0.04913363891839981), (0.625, 0.04250983327627182), (0.598, 0.05177110171318054), (0.577, 0.054027207493782044), (0.586, 0.05082072949409485), (0.584, 0.0567541692852974), (0.593, 0.04987446594238281), (0.552, 0.05914287483692169), (0.588, 0.051364448189735415), (0.575, 0.05399363034963608), (0.623, 0.0473079674243927), (0.558, 0.06151811325550079), (0.538, 0.0625423269867897), (0.576, 0.053300109684467314), (0.555, 0.0577879119515419), (0.62, 0.04874168473482132), (0.597, 0.0546313384771347), (0.563, 0.059030362904071806), (0.598, 0.05221340924501419), (0.569, 0.06138270491361618), (0.576, 0.05478143829107285), (0.575, 0.057674789786338804), (0.504, 0.06952627098560334), (0.566, 0.06218312901258469), (0.571, 0.05831019181013107), (0.561, 0.06597041410207749), (0.548, 0.06195019835233689), (0.558, 0.06325946962833405), (0.537, 0.0625254977941513), (0.546, 0.07031012427806854)]
TEST: 
[(0.2515, 0.043640657246112824), (0.267, 0.09514398372173309), (0.29675, 0.12978019261360169), (0.3455, 0.14937431919574737), (0.3785, 0.16722273707389831), (0.389, 0.17958619666099548), (0.39425, 0.19564056634902954), (0.3885, 0.18772674351930618), (0.39475, 0.1980825604200363), (0.409, 0.21704438626766204), (0.412, 0.19847762775421143), (0.47825, 0.05237614369392395), (0.563, 0.04325985950231552), (0.5825, 0.045084277987480166), (0.5765, 0.04637067887187004), (0.5785, 0.042697843432426455), (0.541, 0.055518782779574394), (0.57475, 0.04994022639095783), (0.581, 0.04737866662442684), (0.56225, 0.054606202512979506), (0.5845, 0.044484860479831696), (0.588, 0.05053098511695862), (0.61325, 0.044373545140027996), (0.57075, 0.0520419597774744), (0.567, 0.055236873865127566), (0.56675, 0.054336007818579675), (0.5565, 0.05837572929263115), (0.588, 0.04985306851565838), (0.535, 0.05799818900227547), (0.568, 0.05376194609701634), (0.56925, 0.053753197386860845), (0.612, 0.046642730444669725), (0.55675, 0.06010617373883724), (0.52925, 0.0644878940731287), (0.5565, 0.055200252681970596), (0.5415, 0.05915440341830253), (0.59875, 0.05130714888870716), (0.57175, 0.056312860161066056), (0.55075, 0.0584530046582222), (0.58475, 0.0505833685696125), (0.56325, 0.0599534392952919), (0.5765, 0.05254241071641445), (0.58025, 0.05570658123493195), (0.5065, 0.0686188227981329), (0.54025, 0.061368899673223495), (0.54025, 0.0586700292378664), (0.5445, 0.06409216848015785), (0.5415, 0.06065055675804615), (0.54575, 0.061918072313070296), (0.5185, 0.06558985856175423), (0.522, 0.0713140609562397)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.94      0.44      0.60      1000
           2       0.39      0.44      0.42      1000
           3       0.50      0.49      0.49      1000
           4       0.50      0.72      0.59      1000

    accuracy                           0.52      4000
   macro avg       0.58      0.52      0.53      4000
weighted avg       0.58      0.52      0.53      4000

Collaboration_DC_3
VAL: 
[(0.248, 0.04460585105419159), (0.388, 0.08198602849245071), (0.468, 0.08995045691728593), (0.465, 0.112021663159132), (0.476, 0.1585041108801961), (0.46, 0.20079846078529953), (0.457, 0.22117121162824332), (0.463, 0.2411895220540464), (0.468, 0.25990544859599324), (0.472, 0.2669675034582615), (0.467, 0.30454038487095386), (0.538, 0.1201990282908082), (0.604, 0.06240568926185369), (0.696, 0.037936606079339984), (0.705, 0.0370280667245388), (0.641, 0.050114810675382614), (0.704, 0.036140245735645296), (0.691, 0.0382169189453125), (0.727, 0.03323284375667572), (0.709, 0.03513875421881676), (0.723, 0.03358262304961682), (0.671, 0.040687834233045575), (0.716, 0.035467515110969544), (0.692, 0.03964764228463173), (0.709, 0.03704192736744881), (0.691, 0.03691163596510887), (0.667, 0.044251493036746976), (0.701, 0.04088004288077354), (0.67, 0.04060187396407127), (0.704, 0.0389387965798378), (0.659, 0.04483589345216751), (0.691, 0.043341827273368834), (0.678, 0.044704005658626554), (0.655, 0.044166576147079466), (0.649, 0.0519112366437912), (0.629, 0.05546288761496544), (0.673, 0.05376061901450157), (0.68, 0.04534341478347778), (0.687, 0.04210517156124115), (0.672, 0.05625690743327141), (0.68, 0.04327588802576065), (0.662, 0.045156591475009916), (0.647, 0.05765736904740334), (0.642, 0.049792878329753876), (0.626, 0.06388346421718598), (0.613, 0.06578141671419144), (0.655, 0.05244132381677628), (0.627, 0.06837549617886543), (0.62, 0.06720847879350185), (0.627, 0.058761735498905183), (0.601, 0.07320794896781445)]
TEST: 
[(0.23875, 0.043380969196558), (0.3875, 0.07882354038953782), (0.4575, 0.0859538711309433), (0.45425, 0.10616131201386451), (0.46675, 0.1488635730743408), (0.45475, 0.19176117146015167), (0.45075, 0.21221389746665956), (0.46, 0.22604575574398042), (0.46225, 0.24731591022014618), (0.46675, 0.25240520119667054), (0.4685, 0.2846794567108154), (0.55, 0.10123259568214417), (0.60275, 0.05686019140481949), (0.6805, 0.03520761100947857), (0.7055, 0.034424609541893005), (0.6315, 0.04712331308424473), (0.695, 0.03482965931296349), (0.691, 0.03554078992456198), (0.7115, 0.03133914592862129), (0.71175, 0.03304112755507231), (0.7105, 0.03205165863037109), (0.67775, 0.03807177782803774), (0.711, 0.03228652051836252), (0.688, 0.03694746915251017), (0.708, 0.034355539202690126), (0.692, 0.03528948800265789), (0.671, 0.041547619380056855), (0.6865, 0.04111723116785288), (0.671, 0.04012353710830212), (0.67425, 0.038046577580273154), (0.665, 0.04411188168823719), (0.677, 0.04061052816361189), (0.67375, 0.04362953639030456), (0.65225, 0.04348217997699976), (0.646, 0.050287915870547296), (0.63075, 0.05122134277224541), (0.67025, 0.04873013196885586), (0.67525, 0.042386322245001794), (0.67225, 0.04207817162573337), (0.67025, 0.05210671101510525), (0.691, 0.0404725681245327), (0.6695, 0.0423650783598423), (0.64925, 0.053386668369174), (0.6425, 0.04778941570222378), (0.61875, 0.06068338929116726), (0.60575, 0.06347536163032055), (0.6555, 0.04883168229460716), (0.63, 0.06174752356112003), (0.62725, 0.06389705291390418), (0.62475, 0.05562630160152912), (0.61, 0.06990818731486798)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.64      0.85      0.73      1000
           1       0.90      0.41      0.56      1000
           2       0.61      0.28      0.38      1000
           5       0.51      0.90      0.65      1000

    accuracy                           0.61      4000
   macro avg       0.67      0.61      0.58      4000
weighted avg       0.67      0.61      0.58      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [95]
name: alliance-4-dcs-95
score_metric: contrloss
aggregation: <function fed_avg at 0x73e8a2093c10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=95
Partitioning data
[[2, 3, 8, 9], [0, 1, 8, 9], [4, 6, 8, 9], [5, 7, 8, 9]]
[(array([46451,  2561, 21623, 18088, 24694, 25512, 35140, 16671, 31785,
       37613, 18952, 26960, 37745, 26051,  8019, 41312, 44181, 19057,
       28834, 28377, 33205,  4293, 43484, 25415, 24165, 33699, 13141,
       33849, 36141,  7008, 21765,  2993,  4008, 26765, 29673, 35157,
       11882, 33204, 43422, 29048,  1670, 35466,  4426, 17883, 12140,
       30849, 25626, 31704, 18477, 27191,  4109, 30690, 37972, 19039,
       25982, 12992, 22618, 17188,  9629, 18496, 12782,   288,  6628,
        8234,  4888, 31677, 48501, 20285, 47632, 47846, 31809,  5686,
       28934, 37247, 21859, 47563, 43545,  9386, 41362,  2702, 48404,
        3673, 22307, 29779, 11385,  5384, 39594, 13071, 41763, 19950,
       12871, 18586, 30680, 34608, 22864, 37280, 27484,  6230,  3068,
       20864, 12346, 16906, 44340, 36883, 24199, 36726, 32257, 19694,
        2193, 44053,  3331, 14926, 46907, 32106, 39339,  6583, 16747,
       11089, 43476, 43503, 15589, 44542, 33621, 45359, 31428, 28443,
        6836,  9038, 19359,  2393,  2693, 19811, 30239, 18888, 37811,
       23642, 43961,  8871,  2033, 40716, 30466, 36450, 42328, 48784,
       24773, 38434, 33535,  8485, 20490, 21168, 27131, 21446, 18101,
        6944, 11486, 30572,  3786, 11171,  8165, 47727,  3253, 19163,
        9827, 21819, 38565, 37015,  7363, 34130, 40965, 42605, 22931,
         849, 42894, 22275,  7178,  3814, 30110, 45533, 33851,  2535,
       13300, 46457,  7009, 39675, 17329,  3338, 30096, 26265, 20563,
        2093, 36469, 34219, 28281, 27973, 32956, 26730, 41604,  8219,
       32457, 12287, 21746, 40849,  9740, 36622, 18138,  5219,  3791,
         586, 46887, 10767,  1676, 17273, 20570,  6069, 37198, 29354,
       46837,    18,  5012, 30454,  4604, 14472, 16593, 37848, 40583,
        7410, 49700,  6305,  2041, 47273,  9896, 13985, 45131, 26001,
       18234, 40115, 36860, 18485, 27499, 46516, 28158, 49725,  5151,
       43738, 39051,  9678, 40480,   924, 17267,  1050, 30013,  2521,
       36431, 26797,  1553, 41117, 44322, 49819, 11960, 29478,   861,
       31676, 34334, 38449,  5945, 30754,  6135,    21, 35525, 18942,
       32215, 13204,  8104, 22273, 42504, 38068, 21103, 13167, 46496,
       44635, 29607,  6847, 36990,  1496, 26485, 26126, 22914, 26675,
       41446, 46561, 46109, 47075, 32813,  6492, 14032, 34042, 17661,
       42133,    91,   806, 48957, 40852, 37410,  4628, 40086, 34379,
       20593, 17831,  6208, 42646,  8255,  7065, 43017, 36772, 28808,
       31460, 37399, 41116, 32371, 20704, 43773,  9564,   774,  9152,
       47410, 14941, 31380, 17047,  5733, 22278, 20425, 48614,  4010,
       47067,  7584, 37095,  1070, 41323, 32914,  7075, 28423, 49982,
       27525, 46290,  7872, 16904, 40226, 47439, 41993, 11359, 39095,
       44267,  9305, 47217, 20770, 25649,  3081, 19580,  7128, 10575,
        4982, 30476,  6556, 30571, 39552, 49233,  5445, 24022, 15154,
       27090, 10135, 25705, 12639, 16131, 20639, 27628,  7213, 33964,
        7180,  6782, 20330, 25218, 15836, 15388,  9107, 42354,  2930,
       48798, 45850, 48561, 17155, 41511, 28712, 24903, 35215, 25492,
       30122,  2562, 39543, 34649, 32520,  5423,    26, 10137, 49443,
       23417, 20163, 49563,  5601, 45053, 42783, 17545, 41493,  2903,
       17912,  1965, 16932, 39325, 14264, 29082, 49463, 42192,  7086,
        9930, 35307, 26598, 34710, 25953, 44973, 18259, 30270, 38066,
       24775, 11564, 48262, 16635, 24949,  6185, 38444, 43438, 13093,
        5534, 21712, 22226, 44530, 19577, 12241,  2525,    33, 38679,
       41053, 15411, 12155, 12076,   494, 21373, 13813,  8288, 44276,
       19727, 42445, 15967, 32379, 35293, 16754, 47014, 17430,  2011,
       46121, 47970, 27250, 23763, 42122, 40985, 33642, 30960, 33915,
       16308, 31974, 44868,  2581,  6433, 48494,   342, 12758, 31204,
        1864,  2565, 37204, 43257,  7389, 25443, 38149, 14232, 49835,
       14419, 37562, 18233, 46424, 15974, 15618, 22682, 38835, 23736,
       10848, 47380,  4292, 19768, 28782, 14369, 29647, 10876, 15237,
       43408, 14804, 13633, 21165,  8120,  8057,  2732, 16820, 16676,
       47430, 27728, 49852, 38754, 22696, 38666, 20650, 32887,  3516,
       47635, 49446, 10245, 31340, 38239, 25374,  1087, 16304,  5807,
       35115, 29371, 30745, 11676,  1957, 11149, 14464, 15835, 30990,
       25096, 47894, 40389, 43344, 34387, 34743,  2325, 19131, 42050,
       48552, 24085, 17418, 11180, 47259, 47202, 16473, 32421, 47094,
        5073,  8919, 26925, 27958, 22492,  7508, 34093, 14969, 35361,
       32644,  8429, 19979,  8892, 31183, 35671, 34641, 44373,  6909,
       33005, 29674, 35730,  6948,  4380,  9188, 38443, 36803, 38162,
       40780, 15046, 23405, 18892, 39114, 36560, 11693, 31332, 36405,
        2229, 28435, 49679, 24608, 13641, 36670, 32051, 42515, 44458,
        5949, 10445, 16457,  6397, 43100, 14497, 47971, 21919,  3980,
       42135, 14790, 44681, 10088, 26188,   410, 34471,  9651, 42371,
       32876, 45045, 41753, 23960,  2817,  2018, 11886, 31732,  9743,
        5297, 42203, 16257, 32760, 37953, 30438,  2534,  3832, 12418,
        2798, 23672, 21451, 14010,  3380, 40990, 20461, 46737,  6773,
       25093, 34814, 33869, 11309, 44236, 47291, 30164, 37072, 27842,
       17653, 48393, 42236, 24878,  9684, 42389,   192,  3359, 20620,
       35995, 26981, 12125, 11345, 29903,  8471, 21172, 21812, 13565,
       39627, 44649, 38881,  6830, 45718, 21457, 42693, 26105, 38817,
       18122,  1688, 24185, 36985, 11266, 33992, 30018,  5732, 29476,
       37641,  7876, 34296, 19081, 26579, 17962, 45830, 12951,  8031,
       35540, 11042,  2743, 19648,  2918, 23534, 14832, 40500, 29316,
       46386, 15239, 38436, 20228, 22920,  9527,  2298, 26149, 18660,
        8526, 48562,  3310, 32676, 15780, 40136, 36742,  3753, 47181,
       20596, 31097,  2754, 39278, 26510,  7149, 48611, 24787, 49897,
       23258, 34893, 13874, 31490, 24343,   369, 15752, 33563, 29352,
       35262,  7698, 21690, 26489, 28709, 37139, 37730, 39934, 34943,
        4321, 39856, 24188, 38494, 49997, 44996, 43039, 26384, 44188,
       15652,  1829,  5861, 30056,  9869, 18361, 11677, 49502, 45740,
       12385, 38928, 28065, 17974, 15817, 43910, 32605, 30796, 19625,
       23770, 44871, 33486,  1633, 37780,   706, 10559, 34587, 15857,
        9097, 18789, 38740, 40381, 22614,  2020, 39879, 23397,  9382,
       27791, 34903, 16910, 23569, 26610, 43376,   798, 26875, 33432,
       37499, 20799,  8650, 27237,  2192, 41363, 42638, 48435, 31911,
        5938, 21658,  3179, 27040,  5662, 13669, 16078, 27168, 10118,
       16174, 10309, 13199, 42025, 19674, 18423, 33248, 47700, 17716,
       25009, 12700,  2935, 29334, 24603, 34502, 18778, 40229, 11828,
       37444, 26942,  5591,  1075, 17941, 11388, 39583, 15142, 12718,
       14599, 39984, 39713, 41495,  2845, 34078,  1353,  8743, 48713,
       12948,  9026, 29721, 34811, 23726,  8245, 27420, 38405, 23299,
       41245, 47765, 34586,  7414, 25604,  2761, 15750, 24331, 37314,
       49057, 29820, 14179, 24214,  3381, 26769, 40819, 25897,   668,
       31742, 11111,  6984, 13000, 18860,  8807, 28560, 43341, 18785,
       17968, 21645, 17470, 17139, 35085,  4215, 43619, 11931,  7708,
       46039, 25091, 31039, 21766, 49634,  2495,  9432, 44829, 18337,
       21209, 39876, 15150, 21121, 49601, 28907, 49417, 41267,  4735,
       19353,  6571, 31106, 29186, 26021,  3672,  5310, 47159, 26511,
        3312, 42632,  1987, 17163, 47419, 44155, 38370,  9213, 29148,
       13665, 38644,   768, 38561, 28591, 22196,   188,  1796, 41282,
       44669, 24114, 24182, 44469, 13343,  1262, 19640, 11485,  1652,
       27570, 49072,  3316, 16454,  5275, 31862, 20430, 13919, 23243,
       41503, 21632,  6644, 47084, 30404, 16735, 33593, 12775,  1201,
       32221, 31324, 11678, 22995, 47636, 29043, 25433, 23291, 27695,
       34290]), [2, 3, 8, 9]), (array([32290, 12297, 10399, 24605, 13194, 11963, 30605, 31086, 32220,
       45004, 33249,  1831, 19630, 17456, 47253, 36924, 32190, 39601,
       28515, 33732, 16297,   116, 26412, 14898, 37324, 12294, 20454,
       40649,  9287, 10776, 32703, 43996, 28080, 39396, 20265, 27128,
       40343, 19066, 47220, 49799, 18745,  7444, 34964,  3304,  4746,
       40906, 48257, 12820, 38185, 47404, 22112, 31991,  9696, 28179,
       14486, 10867,  8086, 33919, 40328, 16316, 11509,  1715, 24802,
       28921, 17178, 32857,  4906, 36007, 25615, 40401, 22167, 37841,
       10989, 34279,   983, 48054, 15707, 13382,  5642, 22190,  1147,
       25616, 40418, 20398,  9657,  4411, 40542, 41324, 17395, 47689,
       46046, 22118, 21903, 12139,  8733, 37758, 30837, 45349,  5472,
       26662,  4141, 34123, 21467,  9157, 21000,  6240, 34009, 35758,
       24447, 26495, 19942,  5924, 38581, 17080, 38748, 28653,  9004,
       27431, 24639, 44946, 48059, 41609,  7731,  3842, 20574, 25016,
        6479, 44925, 22724, 44742, 28791, 11643, 18368, 49050, 43564,
        4192, 40890, 49735,  6425, 41266, 35763, 38791, 38326, 24641,
       12037,  4348, 28650, 40830,  1144, 48430, 38683, 11958, 23822,
       40319, 27749, 10787, 20257,  6165,  7801, 29917, 30888, 45088,
       14056, 40746,  1811, 23924, 15949, 24349,  5174, 40245, 42073,
       30609,  4271, 29893, 46137, 46343, 27155, 17432, 11937,  3066,
       35524, 15620, 34537, 13420,   822,  1514, 17303,  6682, 34855,
       37962, 47267, 27375, 49245, 15594,  9373, 41179, 16392, 37265,
       34125, 20816, 22449, 25644, 28216, 13467, 48355, 23854,  8759,
       44975, 28408, 35042,  3918, 44031,  7592,  7965, 24488, 16160,
       10637, 42175, 39012, 34173, 25820, 32263, 44319,  4060,  6008,
       26846, 33999, 11229, 42945, 44875, 45484, 39875,  1338, 41258,
        2435,  9390, 22123,  1270, 16011, 37881, 36415, 30595, 10990,
       31412, 24549,  9619, 40190,   115, 18854, 26311, 33207, 38037,
       25852,  3992,  6515, 42954,  7289,  7019, 27509,  4360, 30103,
       25915,  7102, 19382,  2994, 17534,  2597, 27404, 35070, 38958,
       46993, 18413, 25910, 26637, 18716, 23463, 48322, 33097,  2569,
       14247, 40283, 39828, 29536, 13722, 13852, 23391, 23248, 43847,
       42282, 14048, 17479, 21080,   275, 44818, 41241,  4496,  3851,
       23425, 31251, 11037, 41568, 31079, 17553, 43655,  7442, 17701,
       16813,  2582, 17539, 24068, 23901, 43015, 24762, 46956, 42976,
       11831, 19351, 12150,  6922,  9668, 10276, 18160, 28245, 26467,
       49589, 30486, 42143, 22983, 31338, 44185, 35344, 28822, 20731,
         354, 33939, 36086,   815,  8232, 46140, 31102, 30166,  6895,
       47362, 46213,  3212,   676, 49411, 34720, 31445,  2084, 34249,
        2318, 29765,  3327,  8197, 23198, 26008, 14851, 38333, 47824,
       28098, 25463, 24269, 45016, 26471, 41254, 42758, 28175, 42318,
       28192, 45268, 15867,  8350,  2180,   330, 13156, 34301, 25105,
       30119, 21593, 21540,  9043, 46173, 12348, 32319, 21992, 24725,
       42697, 33237, 38691, 21003, 28220, 35335, 39405, 11776, 24044,
       12303, 37145,  4464, 29791,  2185, 12523, 16774,  2067, 28810,
        6411,  9889, 23263, 40000, 17205,  9535, 12516, 49435, 47035,
       38086, 40595, 27814,  1377, 24041,  4332, 15779, 31523, 38515,
       22859,  2841, 32226,  8566,  5822,  6356, 40024, 13509, 34721,
       43453, 48791, 46649, 39195, 18583, 46238, 16850, 42834, 39404,
       42307,  8221, 43711, 46731, 31539, 18639, 39805, 29546, 19181,
       32886, 41398, 33252,  4385, 28089,  1574, 19450, 47745, 13088,
       20856, 35531,  5572, 42294, 43859, 32994, 22659, 46240, 10535,
       18890, 35134,  7778, 33569,  4185,  8189, 36542, 43132,  9318,
        3907, 28383, 29143, 29555,  9380, 47453, 29675, 17802, 29252,
       20897, 46461, 12047,  8735, 40394, 23789, 18961,  7352, 15710,
       21270, 44824, 37512, 49354, 42305, 41403, 48168, 39915, 25580,
       42870, 10475,  2464, 17624, 46854,  4848, 40671,  9324, 36267,
       15648, 24039, 19720,   139, 35388, 26863,  7941, 26328, 11141,
       19309, 23031,  6044, 42280,  6379,  2857, 41889, 19413,  4758,
       14891, 24905, 48877,   602,  3039, 40169, 49851, 38322, 46582,
       34094,  9648, 40191, 35416,  8348,  1673, 36579, 48774, 12442,
       25501, 26170,  5360, 21842, 12024, 35345, 11598, 23319, 23267,
       32574, 10171,  4812, 10572, 30035, 26977, 24131, 45062, 39999,
       11577, 19887, 37104, 42521, 31313, 32178, 43913, 31950, 21267,
       39658, 45907, 11529,  7045,  8952, 26369, 31121, 48225, 21596,
        9055, 44888, 18378, 49976, 43400, 24140, 13562, 22849,  9109,
        4304, 36882, 20491, 21830, 21042, 10962, 18840,  3430, 45461,
       45243, 46944,  8544, 13582,  8982,   460, 15011, 16942, 29786,
       36628, 27241, 11674, 29055, 30472, 39590, 42810, 40192,  3518,
       25002,  1671, 32896, 27056,  9326, 25176, 17168, 44205, 20530,
       11010, 32322, 43409, 49967,  4575, 28918, 42162, 13388, 15914,
        5794, 48821,  3898, 21157, 39207, 37659, 20182, 10620, 36892,
       40926,  6004, 15007, 11576,  5948,  3469, 30294,  1391, 21377,
       35864, 21484, 38809, 29287, 46128,  6160,  5787, 47886, 25888,
       34522, 38421, 21036, 27261, 28115, 23399, 35938, 34712, 46048,
       49789, 11265, 14872,  9469, 31644,  9645, 13810, 41727, 46615,
       36874, 26131, 15346, 22064, 37858, 21295,  3170, 43858, 18327,
       20509, 25669, 36636,  5339,  6250, 13015, 23307, 39605, 10483,
       31498, 47628, 49757, 25124, 19547, 21741, 17696,   567, 41042,
       17516,  6192, 49378, 16167,  3067,  7737, 17601, 17327, 41867,
       48286,  7468, 37524, 26856, 36262, 32014, 15276,  9488,  7104,
         860, 34894, 19020, 21608, 35917,  4066, 27692, 31456, 41059,
       12277, 17705, 21823, 33078, 35894, 25364, 39298, 47376, 26694,
       40368, 35736, 18236,   915,  4603, 10829, 30276, 40735, 27524,
       38777, 21883, 15901, 29169,  7793, 36456, 34796, 26152, 17240,
       13609,  4371, 46629, 47663, 25960, 48949, 24234, 28285, 47511,
        6178, 39453,  5519, 11717, 23444, 17765, 19601, 23229,  5801,
        3407, 21537, 33786, 23749, 34538, 18943, 24615, 33930, 17435,
       48827,  2529, 46833, 43121,  3671, 13586,  5013,   225, 13024,
        5332, 18947, 27622, 26597, 22152, 12182, 37993, 32595,  4665,
       34483, 21866, 39801,  8428,  8517,  8575, 29514, 14695, 20187,
       42681, 22296,  7457,  4701, 19377,  9511,  1223, 41905, 43281,
       40068, 23287, 20943, 41348,  6152, 19593, 13705, 16744, 14100,
       28046, 26447, 48051,  7691, 26508,  5671, 37141, 30412,  7042,
       33900, 29094, 41640, 35498, 19661, 22433, 13577,  4289, 37737,
       47690, 21065, 14077, 29013,  9106,  2379, 38056, 17731, 19289,
        8811, 27439,  6286, 19410, 46345, 29609,  1034, 31638, 27198,
       25883, 32841, 37490, 33668, 10421, 32778, 49238, 40157, 11104,
        1706, 38602, 29335, 42387, 17201, 33671, 21555, 11213,  8951,
         360, 30073, 44157, 10071,  1767, 35330, 38516, 32717, 24152,
        6038, 27566, 31869, 32163, 46298,   657, 40742, 37538, 48743,
       28959, 10533, 15027, 48868, 42278,  7746, 13148, 41634, 32158,
       33856, 43377, 35206, 35961, 10859, 37454, 17406, 21648, 29349,
       35047,   666,  3023, 49694, 37540, 46774, 27675,  3923,  4587,
       47637, 37031,  3697, 36401,  3097, 35657, 20472,  2351, 34158,
       30179, 33639, 40083,  9739,  8159, 29938,  5978, 47944,  9244,
       27588, 48734, 14626, 27589,  9461,  4704, 33839, 13904, 36288,
       35138, 20925,  1774,  8644,  4252, 46656, 16909, 25001, 30678,
       12383, 11533, 37535, 16347, 28166, 40722,   884, 21583, 45860,
       47083, 27997, 17021, 11373, 46781,  6171, 42458, 30215,  3643,
       33982, 39522, 37600, 47448,  2437, 10252, 13889, 48751,  8126,
       31115]), [0, 1, 8, 9]), (array([24658,  3862,  9010,   693, 29770,  6021, 16461, 25898, 18340,
        6527, 37075,  6586, 21753,  2691, 41953, 40195, 39146, 39656,
       46415, 21838, 41770, 14996, 18559, 33434, 39009, 49727, 39772,
       31349,  5940, 15656, 14815, 23455, 42403, 29663,  3587, 42512,
       35624, 25949, 29466, 17033, 45773,  6924,  4053, 43683, 14415,
       30815, 48478, 15503, 41023,  7574, 28489, 23029, 39740, 26653,
       18099, 16254,  5289, 17585, 25441, 38995, 12179, 27853, 20575,
       41518, 36142, 46153, 39878, 40510, 27923,  7407, 38353, 35113,
       28226, 15582, 44473,  9041, 33215, 36960, 25017, 30907, 30911,
        2712, 12103, 26602, 13186, 22536, 20810, 33027, 29629, 25803,
       17937, 33223,  3690, 46526, 20409,  4296, 41305, 33080, 18699,
       25529, 28040,   930, 33735, 23420, 36225, 42303, 21978, 15737,
       24769, 17632,  2327, 38123, 39177, 22750, 20798, 36003, 18279,
         434, 28803, 28654,  3252, 12334, 43222,  1256, 23460, 41003,
       25992, 13412, 18690, 15802, 15428, 42544, 20782,   951, 27455,
       39209, 33501, 45571, 12416,  1158, 18293, 41822,  3089,  3687,
       42695,  3776, 47126, 28334, 20608,  7404, 19627, 44780, 24967,
       22318, 10150, 23057,  4763, 10661, 47716, 44558, 14704, 21341,
       49595, 34365, 27466, 26894, 37827, 31290, 18734,  7775,  4660,
       41699,  2830, 44760, 35634, 43793, 40028, 35250, 13025, 22018,
       34051, 30522, 14181, 32786, 30790, 48812,  1348, 19098, 38660,
       27860, 16195,  5619, 27193, 22361, 18566, 38852, 31143, 30499,
        7382, 25785, 26844, 14937, 44978, 45512, 18836, 25941, 45586,
       38386, 10350, 38103,  5491,  5347, 28100, 17728, 24883, 40665,
       24265,  8523, 37281, 37124, 35180,  8404, 19833, 49597, 36923,
         677,  2216, 17613,  4043,  6475,   543, 25834, 10561, 10970,
       25620,  2155, 23743, 31470,  8131, 25147, 46644,   925, 33266,
       11586, 46416, 29102, 35429, 30225, 31840,  6844, 21155, 31301,
       24242, 38698, 49412, 33953,  8474, 28925, 32859, 31792, 29751,
       16367, 24531, 31270, 19511,  3048, 40041, 31984, 33630,   854,
       21435, 17562, 39680, 19308, 42679, 26197, 28607, 43269, 27346,
         204, 45918,  1342, 36922, 16687, 38771, 16685, 49771, 36913,
       15830, 18282, 35535, 37270, 38061, 31135, 10792,  7025, 32880,
       37940, 11602, 31672, 46271, 21072, 36069, 32869,  8603, 48897,
       31570, 29284,   645, 36602,  9256, 18988, 14864,  9978,  2774,
       42391, 20146, 41916,  6582,  7034, 25892,   488, 45443, 38508,
        3262, 22746,  4824, 37380, 29217, 40396, 31080, 31928, 15497,
       17847, 21146, 23154, 48619, 17427, 41969,  7524, 14065, 49285,
       44590, 10704, 32648, 35891,  3266, 38738, 14188, 11751, 22290,
         103, 44194, 49364, 28776, 26190, 43026,  4926, 33967, 27022,
       38475,  9029, 20293,  9521,  1842, 25964, 23780,  8873, 23050,
        8564, 26387,  4751, 15962, 31508, 25511, 14443, 48400, 34033,
       31312, 14984, 35455,  9381, 33425, 49129, 18396,  4550, 36164,
       10649, 49798, 36232, 44018, 48401, 23230, 26335, 41668,   680,
       30350, 14638,   132, 43182, 47792, 35225, 36373, 16789, 26192,
       39415, 46991, 34805, 16389, 36638, 19870, 13619, 33733, 27688,
        8460, 24298, 22082, 11634, 29009,   655, 31852, 16998,  9953,
       48277,  7264, 26415, 34926, 16287, 38583, 49928, 12459, 23896,
       20853, 24732, 28663, 10600, 19665, 19281,  9924, 46622,  2650,
       19420, 40313, 20025, 29327, 13878, 24424, 23917, 36515, 16562,
       41558, 23216, 14513, 11911, 30089, 14411, 38665, 42641, 23539,
       18224, 41272, 49061, 39544, 17342, 21264, 18090, 27830,  7422,
       42145, 42724,  8956, 28490, 46995, 25835, 13291, 15039,  5443,
        4570, 32958, 23135, 38773, 15498,   437, 20687,  4752,  2925,
        7959,  5474,  9323, 37457, 49404,  3698, 10117, 43074,  9706,
       21719, 10444, 27450,  3567, 21192, 21972, 31652, 44984, 22471,
       18693, 24414, 47021, 29446, 44685, 12426,  2670, 20727, 36371,
       48911, 29175,  2923, 15795, 36390, 49358, 44659, 15676,   888,
        5245, 14823, 46636, 49712, 29530,  1300, 12867, 30639, 19817,
       16435, 33145, 33029, 49389, 13684, 24860, 19618,  1822, 16572,
       24286,  4507,  1174,  1032,  7484, 16147, 10810,  4892, 28295,
       49543, 48164, 29344, 33537, 10864,  9693, 10923, 46029, 19123,
       13115, 23321, 36856, 20298, 35078, 42571, 21546,   870, 27621,
       12206, 27057,  9674, 41314, 19167, 46787,  7534, 47960, 19658,
        9483, 15241, 26214, 18959, 40344, 48127, 34582,   987,  1138,
       48281, 46008, 44438, 28537,  7012, 31872, 18141, 48133, 21164,
        1375, 19219, 44807,  4611, 43612, 29661, 12041, 33372, 11591,
       42202, 34683, 31745, 40408, 49778, 31085, 21515, 30647, 39776,
       16902, 21679, 27124, 43623,  2471, 10524, 39702, 13197, 18125,
       43713, 28287, 18797, 43357, 10511, 23080, 47694, 46985, 22583,
        5643,   892, 47562,  3765, 10068, 40429, 16156, 32611, 37044,
       36949, 17935, 14123, 21427, 19733, 35171,  1236, 33756, 39072,
       21312, 30504, 36942, 32726, 14901, 38625, 17783,   244, 36214,
       26004, 26402, 32351, 15416,  7614, 19605, 28706, 22760, 16084,
        3330,  4210, 38274, 41002, 34212, 28204, 21611, 47854, 12207,
       32265, 15022,  9913, 38619,  4469, 19214, 26938, 35912,  7329,
       49501, 34104,  7310, 41507, 41674,  7281,  2220, 12725, 10975,
        9160, 40110,  4988, 31069, 17685, 23567, 32037,  8766, 12258,
        4916, 26454, 15617, 26608,  6418, 17020, 26239, 28642,  2272,
        2974, 48905,   170, 39956, 23669, 34004, 10817, 25399, 49568,
       18700,  1014, 35536, 12487, 47019, 20786,   795, 22551,  8926,
       38089, 10997, 14046, 10016, 39094, 32234,  1732, 17891, 16532,
       34815, 40179, 16620, 19464, 38008, 42677, 13139, 23765,  9796,
       28012, 30529, 32622, 26209, 24408, 29990, 17404, 29565, 14458,
       46955, 18000, 22898, 48140, 43946, 14510, 32851, 37126, 31014,
       27146, 16302, 13963, 22925, 20765,  7461, 12816,   756, 39330,
       44511, 16317, 24066,  4136,  9358,  1378, 43787, 47876, 44919,
       29972, 15694, 10312, 29926, 12085, 33028, 49793, 21738, 35625,
        8838, 21562, 16601,  2508, 46190, 33806, 33896, 22637, 13358,
       31845, 44410, 30733,  4340, 46269, 33149, 43137, 42764,  2853,
       12008,  2956, 47124,  4131, 19801, 17918, 21219, 41128,  3090,
       44723, 28318,  4169,  1744, 28601, 33208, 18786, 46687, 41409,
       39743,  6784, 15085, 37427, 24372, 19424, 16255, 12835, 37433,
       15337, 37793,  4115,  2199, 24055, 38300,  3129, 16480,  9911,
       10351, 37071, 17798, 42560, 27380, 21129,  5203, 44380, 12902,
        3876, 44056, 18244,  5637, 12405, 46852, 23284, 45993, 49931,
       25051,  5172, 40507,  4541, 40315, 31089, 46861, 29472,   749,
        8850, 49065, 36381,  8539, 30823, 44332, 30392, 11232, 15096,
       16362,  1277, 49662, 21221, 35521, 40750, 29073, 41396,  3479,
        2548, 41395, 36837, 23341, 45755, 16373, 16923, 36875,  3477,
       22150, 23550, 17313, 29418, 35974,    14, 28534, 33858, 22220,
       46090, 22694, 28572,  1638, 25991, 19824, 13767, 45431, 20995,
       19709, 23647,  7920, 42554, 24133,  9211, 49461, 17042,  3705,
       10144, 28879, 19122, 33794,  2001, 41906, 27365, 41126,  2767,
        4111, 10257, 46186, 34645, 10827, 48442, 39894, 26627,  6834,
       14083, 37201, 43942, 36640,  9036, 49945, 42406, 16092, 40885,
       42343, 24731, 47296, 38687, 36866, 23471, 38153, 28781, 14170,
       38324, 16725, 17176, 16917, 21817, 44289, 43696, 45183,  4028,
       17869, 48461,  3793, 30615, 30985, 39183, 49520, 22346,  4223,
       37363, 48178, 14220, 23303, 24391,  8762, 42349, 23403, 15690,
       42213, 39955, 29384, 18488, 20462, 48033, 37132, 17973, 10208,
       35618]), [4, 6, 8, 9]), (array([18202, 17830, 41371,  4579, 25845, 41499, 38106,  6723, 19999,
       10715, 11826, 47524, 17931, 39475, 35543,  5108, 37364, 34846,
       18921, 30290, 27892,  4002, 47659, 18245, 30698, 29077, 21318,
       25606, 39381, 16436, 10597,  1072, 37632, 25886, 33259,  3060,
       38860, 49541, 29615, 40657, 36350, 47977, 24909, 46422, 20809,
       19071, 26700, 33878,  7750,  8756, 15527, 17375,  1274,  8591,
        9999, 49118, 31409,  2305, 34188, 39229,  2013, 43330,  4009,
       19024, 42745,  7406, 33800, 46393, 19362, 11185, 18521, 24585,
       49508, 41680, 11328, 30099, 45417,  4463,  4692, 41034,  1152,
       47158,  1993, 12198, 29321, 10242, 31038, 22470, 18985, 25065,
       14184, 21879, 27947, 14246, 22225, 12728, 44756, 24380, 47783,
       24663, 33629, 19096, 18626, 31059, 29040, 21179, 39403, 46903,
        5100, 48882, 42293, 48736, 18059, 21077, 34790, 24703, 22021,
       20484, 10549,  1588, 26560, 24309, 17921, 17354,  8407,  1111,
       32427, 21678, 39456, 46289, 38807, 11043, 26735, 32877, 48673,
       12839, 27828, 25284, 11145, 13190, 41475, 22671,  5613, 15488,
       45440, 47090, 35155, 20747, 38305,  1662, 44388, 22430, 15549,
       40044, 30140, 33989, 34444, 18721, 46980, 35547, 18598, 45674,
       27835, 27804, 44223, 30684, 37568, 44284, 32862, 40538, 29251,
        9772, 13142, 33172, 32835, 26917, 47485, 19925, 25919,  5175,
       27805, 16798, 15312, 20315, 30085, 44337, 32764, 22590, 40014,
        8928, 38839, 41817, 44377, 18685,  4316, 43543, 49267,  4662,
        2059,  9132, 27401, 19595, 10831, 19757, 18212, 47160, 36532,
        3794, 11507,  3714, 30718, 35510, 26010, 15301, 31129, 46716,
       27885, 45831, 32935, 37396, 29890, 20207, 35819, 44496, 44792,
       44676, 45181, 18966, 39718, 36471, 22174, 11165, 10104,    83,
        2231, 44988, 26313, 11603, 21502, 39647,  3022, 11610, 20348,
       14391, 12716, 38172, 35652,  9235, 37795, 21233, 22647, 44098,
       45147, 47308, 10384, 22626, 39767,  8187, 23988,  3279, 35623,
        2201, 42955, 34900, 32343,  1896, 38890,  5295, 36303, 14503,
        2397, 20099, 45596, 35005,  3272, 21100, 39717, 40026, 22115,
       34733, 26562, 33585, 26702, 35698, 42078, 36270, 14400, 11278,
       28152, 18823, 20565, 42126,  1931, 11697, 31871, 34010, 38252,
       18258, 13409, 29886,  1582,  3261,  9560, 10905, 24237, 40912,
       40744,   492,  6271,  7874,  9719, 46117, 17929, 34412, 41390,
       37800,  6808, 17503,  4396, 49048, 20903, 21244, 29733, 38780,
       19391,  5047,  2140, 29727, 30617,  4480, 36608, 19064, 27161,
       28884, 43360,   113, 48837, 18869, 37601,  2008, 20998, 25617,
       28553, 44076, 40426,  1149, 45945, 32487, 16355,  4264, 18914,
        2813, 34291,   739, 29622, 32399, 27381, 17804, 19119, 48823,
       48741,  1891, 26358, 32815,  5241, 47923,  5564, 24853, 10735,
         191, 45561, 49310,  6821, 10496, 45058, 47246, 15543, 49912,
       25968,  5126,  9184, 14837, 45540,  4736, 47864, 10994, 22198,
        8115, 47346,  9443, 24439,  8038, 46671, 12759, 16198, 38142,
        8897, 44839, 33652, 22595, 33605, 18219,  9689, 36936, 28735,
       23621,  7613,  4325, 35483, 19263, 21113, 12985, 19940, 31839,
       23873, 25804, 20738,  4898,  9486, 19956, 32579, 17643, 10459,
       30925, 47286,  6460, 22267, 47285, 44664, 42500, 29326, 20707,
       40692, 43066, 29778, 42996, 43544, 21410, 43844,  7611,  7577,
       22739,  3104, 40107, 33636, 34599, 22612, 17944, 16764, 42137,
        6680, 34339, 28240, 38597,  9958, 18524, 10778, 47323,  9082,
       44353, 21599, 46167, 40866, 28977, 28206,  3573, 45610, 27998,
       17398, 34868, 35284, 15889,  8277, 18916, 25811, 27682,  6164,
        7831,  1716, 28914, 33254, 37423, 43587, 17467,  4820, 41416,
       37467, 29137,   595,  2195,  5958, 37190, 48055, 35020, 41973,
       38962, 16881, 45874, 21101, 14356,  7437, 12646, 25513,   106,
       22132, 26623,  8235, 48125,  5185, 19706, 25906,  9975, 34983,
       14011,  5546, 48156, 27612, 43048, 49144,  6176, 48462, 26112,
        1255, 45917, 42917, 30664, 17162, 31903, 18397, 39653, 39317,
       40019, 33190, 32897, 27643, 38607, 12172,  8455, 17519, 36766,
        5365,  9206, 35210, 42265,  8830, 32663, 14624,  3094,  3244,
       24127, 31018,  2483, 27252, 25258, 38904, 43198,  6880, 21257,
       46926, 30941, 19283, 14149, 17799, 11244, 17813, 44357, 31692,
       18171, 26098, 26172, 27068,  9023, 14045, 46102,   901, 49681,
       24740, 19748,  4303, 25566, 24444, 13893, 13623, 18076, 31725,
       14826, 32448, 39462, 28599,  1970, 35150, 20089, 18950, 36484,
       17022, 29556, 44490,  3614, 35586, 28829, 26182, 27618, 37420,
       45784, 35169, 28929, 41732, 40147,   503, 25088, 11977, 48597,
        4823, 15429,  8180,  2954, 48737, 37534, 46395, 25987,  6598,
       44577, 19028, 25728, 32560, 47784, 18312,  7678, 42367, 12490,
       30123, 32765, 10003,  3121, 10408, 16430, 47646, 30444, 27061,
       44569,  8940, 38689, 15135,  1821, 17933, 26948, 29014, 39015,
       40415,   943, 23827, 36733, 11001,  9159, 13355, 47814,  9567,
       30788, 23731,  3250, 46477, 12422, 33571, 24855, 41413, 21943,
       33661, 17120, 39210,  1782, 16330, 42770, 24588, 22177, 25744,
       28323, 35562, 13095, 33356,  4580,   627, 11461, 23890,  5987,
        4828, 28153,  9844,   609, 36678,  9951, 19342, 16729, 29479,
        2658, 44414, 49955, 25072, 47881, 24629, 38371, 12452,  1506,
       17630, 22010, 27861, 32578,  7334, 28103, 28066,  2721, 39454,
       46699,  2454,  2523, 32506, 35035, 22524, 38901, 40609, 13923,
       30252, 29690, 36253, 22584, 43807, 26703, 38969, 16847,  5283,
       38158, 30514, 20479, 15422, 46028, 25040, 20662, 21483, 48175,
       39142,  3541, 35617, 47793, 41163, 24347,  4029,  4742, 44382,
       44927, 14866, 46068, 27395, 49405, 44010, 18968, 27975, 37606,
       18467, 36309,  9787, 32249, 41920, 45735, 26506, 39993, 34730,
       11457, 28846, 17977, 20248,  7694, 48123, 25036, 40082, 39586,
       25507, 14278, 18166,  1940, 11547, 39746, 49158, 40923, 18030,
       45465, 39427, 23091, 34772,  6288, 40059, 20176, 29909, 31662,
       32273,    76, 27005, 45829, 36118, 18792, 34573, 27603, 24537,
       19563, 46280, 21954, 30343, 26650, 12653, 39841, 13717, 25988,
       44066, 36521, 49637, 43199,  8567, 41765, 29401, 42454, 45197,
       33739, 10306,   214, 45388, 49398, 32146,  3006, 21507, 47656,
        8402, 46024, 34251, 14564, 10246,  9261, 42452,  8788,  6708,
       30247, 11984, 17296, 15159, 47002, 27575, 17371, 31056, 44212,
       21592, 25994,  5071,  5352, 46243, 45241, 31815, 34175, 36475,
       25285, 12531, 49278, 49244,  1773, 49385, 40237, 26013, 40063,
        3345, 21273, 31637, 15420,  9513, 20441,  8565, 11275, 33656,
       38393, 21300, 22233, 25393, 31162,  8075, 48455, 28159, 46830,
        2757, 39781, 10448, 45405, 32115, 34753, 49300, 13125, 22915,
       23464, 39564, 49453, 34077, 45167, 31483, 21182, 36266,  2477,
        2218, 13531,  5191, 26970, 20069, 32040, 32940, 41786,   541,
       34957, 21010, 30178, 14610, 16293,  8896,  1026, 19248,   306,
       33316, 21542, 45577,  3597,  9683, 42556, 47592,  8821, 22815,
       28888, 29574, 29529, 43325, 18642,  5056, 49125, 35288, 25506,
       46788,   219, 35178, 17052, 27784, 25338,  9971, 49074, 15597,
        7703, 16051, 20013, 40222, 28656,  1602, 20600, 43882, 17336,
       43900,  8394,  1456, 26218, 44810, 23961,  3216, 16107, 42140,
       18394, 16728, 39546, 18252, 16631, 46302, 36818, 25907,  8012,
       11720,  4454, 31905,   883, 18989, 31205,  6149, 37751,  4826,
       41415, 15031, 35280, 42714,  3685, 16234, 17061, 29825,  3719,
       49284, 20250,  3526, 10658, 11311, 48766, 24027, 18846, 10187,
       43634]), [5, 7, 8, 9])]
Collaboration
DC 0, val_set_size=1000, COIs=[2, 3, 8, 9], M=tensor([2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.226, 0.04464438664913178)
DC 1, val_set_size=1000, COIs=[0, 1, 8, 9], M=tensor([0, 1, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04425914561748505)
DC 2, val_set_size=1000, COIs=[4, 6, 8, 9], M=tensor([4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.244, 0.04420206964015961)
DC 3, val_set_size=1000, COIs=[5, 7, 8, 9], M=tensor([5, 7, 8, 9], device='cuda:0'), Initial Performance: (0.25, 0.04452222609519958)
D00: 1000 samples from classes {8, 9}
D01: 1000 samples from classes {8, 9}
D02: 1000 samples from classes {8, 9}
D03: 1000 samples from classes {8, 9}
D04: 1000 samples from classes {8, 9}
D05: 1000 samples from classes {8, 9}
D06: 1000 samples from classes {2, 3}
D07: 1000 samples from classes {2, 3}
D08: 1000 samples from classes {2, 3}
D09: 1000 samples from classes {2, 3}
D010: 1000 samples from classes {2, 3}
D011: 1000 samples from classes {2, 3}
D012: 1000 samples from classes {0, 1}
D013: 1000 samples from classes {0, 1}
D014: 1000 samples from classes {0, 1}
D015: 1000 samples from classes {0, 1}
D016: 1000 samples from classes {0, 1}
D017: 1000 samples from classes {0, 1}
D018: 1000 samples from classes {4, 6}
D019: 1000 samples from classes {4, 6}
D020: 1000 samples from classes {4, 6}
D021: 1000 samples from classes {4, 6}
D022: 1000 samples from classes {4, 6}
D023: 1000 samples from classes {4, 6}
D024: 1000 samples from classes {5, 7}
D025: 1000 samples from classes {5, 7}
D026: 1000 samples from classes {5, 7}
D027: 1000 samples from classes {5, 7}
D028: 1000 samples from classes {5, 7}
D029: 1000 samples from classes {5, 7}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO4']
DC 1 --> ['(DO2', '(DO5']
DC 2 --> ['(DO0']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.0756532991528511) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.265, 0.07162128928303718) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.346, 0.10060960775613785) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.262, 0.09178792214393616) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.338, 0.08844312906265259) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.304, 0.08061573424935341) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.407, 0.13195230773091315) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.319, 0.1186123097538948) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.373, 0.09776843130588532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.428, 0.0912200716137886) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.432, 0.15404716596007348) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.397, 0.14389754155278206) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.382, 0.116108429312706) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.1028368262052536) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2008201377093792) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.436, 0.19386132979393006) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.383, 0.11486499130725861) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.447, 0.12005862887203693) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.371, 0.2020772139430046) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.439, 0.1812336931824684) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO4', '(DO0']
DC 1 --> ['(DO1', '(DO3']
DC 2 --> ['(DO5']
DC 3 --> ['(DO2']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.396, 0.12786139059066773) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.12526907271891832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.411, 0.2141107092946768) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.188261329382658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.391, 0.12239557676017285) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.1390012750029564) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.363, 0.259074219936505) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.443, 0.19005238442867994) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.397, 0.1381242759525776) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.456, 0.1493007049560547) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.442, 0.21411252587661148) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.44, 0.2096376870945096) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.393, 0.1398952288478613) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.464, 0.16985845598578453) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.241391664955765) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.444, 0.22949583377689123) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.411, 0.15190104061365128) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.1669399950876832) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.437, 0.2810998134780675) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.447, 0.2314559705518186) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[8, 9], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.7195, 0.0195660545527935)
DC Expert-0, val_set_size=500, COIs=[2, 3], M=tensor([2, 3, 8, 9], device='cuda:0'), Initial Performance: (0.822, 0.014769307374954223)
DC Expert-1, val_set_size=500, COIs=[0, 1], M=tensor([0, 1, 8, 9], device='cuda:0'), Initial Performance: (0.922, 0.006159715548157692)
DC Expert-2, val_set_size=500, COIs=[4, 6], M=tensor([4, 6, 8, 9], device='cuda:0'), Initial Performance: (0.874, 0.011269920859485865)
DC Expert-3, val_set_size=500, COIs=[5, 7], M=tensor([5, 7, 8, 9], device='cuda:0'), Initial Performance: (0.894, 0.011586012534797192)
SUPER-DC 0, val_set_size=1000, COIs=[2, 3, 8, 9], M=tensor([2, 3, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[0, 1, 8, 9], M=tensor([0, 1, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[4, 6, 8, 9], M=tensor([4, 6, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[5, 7, 8, 9], M=tensor([5, 7, 8, 9], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x73e888069280>, <fl_market.actors.data_consumer.DataConsumer object at 0x73e8887cc940>, <fl_market.actors.data_consumer.DataConsumer object at 0x73e88862f160>, <fl_market.actors.data_consumer.DataConsumer object at 0x73e888186b50>, <fl_market.actors.data_consumer.DataConsumer object at 0x73e88871fa30>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.7895, 0.013854958780109882) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.01611533400416374) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.005598380114883185) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.008661510519683361) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.894, 0.01401566119492054) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.494, 0.04856345897912979) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.05775300109386444) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.507, 0.06674806496500969) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.515, 0.06437698121368884) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.885, 0.008950041275471448) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.01516148892045021) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.005953201491385699) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.908, 0.008369173347949982) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.013772100150585175) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.597, 0.03729934859275818) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.621, 0.04246030887961388) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.583, 0.05346182940900326) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.635, 0.04255699399113655) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9225, 0.006422457146923989) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.015600555688142776) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.0062740762643516065) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.008198176868259906) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.894, 0.01379973892122507) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.042283935725688934) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.621, 0.04399972575902939) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.667, 0.038022797286510465) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.659, 0.03561469155550003) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.932, 0.005875492110382766) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.014881011456251144) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.0066034297654405235) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.009103518664836883) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.902, 0.015082862254232169) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.611, 0.045791570007801055) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.65, 0.038084795743227005) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.644, 0.03884149700403214) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.627, 0.038583864569664) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.931, 0.0058097853171639145) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.01683548381924629) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006291927687823772) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.00849106627702713) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.904, 0.013361722275614739) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.0386363996565342) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.636, 0.042357877373695375) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.688, 0.03399552446603775) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.616, 0.04370503574609756) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9385, 0.005790396075695753) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.016668522387742996) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.006276091039180756) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.008445201188325882) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.015436428621411324) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.03957413753867149) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.647, 0.041745519518852234) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.661, 0.04109418612718582) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.646, 0.03989719778299332) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.936, 0.0060089164299424735) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.016484076082706453) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.0072714916914701465) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.008167082488536835) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.014351934991776944) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.654, 0.03815848037600517) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.656, 0.04137351074814796) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.708, 0.03261804114282131) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.665, 0.03648252922296524) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.005269972040550784) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.01803592875599861) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.007155164390802384) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.009054871715605258) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.015106510281562804) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.642, 0.03933895581960678) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.649, 0.04189026537537575) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.713, 0.03331783059239388) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.656, 0.04049820411205292) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9345, 0.005920911294408143) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.017244751870632173) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.008145197694655508) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.01031267772614956) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.906, 0.015982563741505147) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.65, 0.039423394545912746) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.633, 0.04797288990020752) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.718, 0.03255673772096634) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.679, 0.03685330587625504) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9435, 0.005683884490979835) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.017622363775968552) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.007970278657972812) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.010264527402818203) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.902, 0.01778389918804169) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.664, 0.041997080743312834) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.635, 0.043608181029558185) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.681, 0.037767027407884596) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.694, 0.03783651277422905) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9385, 0.00584253396524582) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.02088955146074295) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.00710465357452631) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.010026176400482655) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.9, 0.018274705126881598) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.03702389788627625) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.622, 0.04673974350094795) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.694, 0.036062782794237136) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.651, 0.0406122467815876) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9335, 0.0070373250407865275) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.017313517421483994) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.006333221290260553) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.916, 0.010773812845349312) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.01951501749828458) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.04729506565630436) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.616, 0.05222092871367932) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.695, 0.03557918144762516) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.664, 0.04032594457268715) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9325, 0.007050318311317824) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.020377063050866128) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.00635194310452789) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.914, 0.009499180518090724) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.016244877308607103) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.63, 0.04417530930042267) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.636, 0.05075003823637962) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.687, 0.03783576726913452) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.663, 0.041057499140501025) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.007395472860065638) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.0221250915043056) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.0076508666416630145) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.009562886089086533) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.019035976715385913) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.625, 0.0473710655272007) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.591, 0.06381241969764233) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.645, 0.045030857898294925) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.664, 0.04317493858933449) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.939, 0.007174068672229623) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.022592011079192162) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.007490156103856862) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.010033565293997525) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.017686619520187376) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.04889915303885937) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.607, 0.06555135120451451) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.694, 0.04016150605678558) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.639, 0.04739527928829193) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9415, 0.006003274522197898) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.848, 0.022498680993914603) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.007500383408274501) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.011230029943399132) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.01708840398490429) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.04225961092114448) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.617, 0.06028944385051727) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.707, 0.03579248285293579) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.631, 0.04888680443167687) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.937, 0.0072948731227515965) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.02155680626630783) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.0066792073659598826) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.010934856863692402) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.884, 0.02137607368081808) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.616, 0.05310979264974594) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.07385703096911311) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.707, 0.03661813400313258) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.66, 0.040448685973882674) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.00673969248731737) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.852, 0.020807569935917855) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.0070256017372012135) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.916, 0.009373335301876068) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.02325995334144682) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.05241055706143379) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.621, 0.06503870090842247) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.68, 0.0417577041387558) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.664, 0.043471432119607925) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.008757081842013577) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.02084646874666214) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.00691211443208158) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.010850105479359627) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.896, 0.01987466939166188) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.584, 0.059593795374035835) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.06958035192638636) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.689, 0.045467913672328) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.645, 0.04490999816358089) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.00835742525068963) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.858, 0.01976092091947794) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.00739125331863761) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.012024296611547471) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.01865878998115659) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.05539454360306263) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.575, 0.07518544685095549) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.678, 0.0480157582461834) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.626, 0.04885062563419342) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.008554185866442823) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.86, 0.021819643154740335) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.0070166475493460894) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.010842033065855503) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.906, 0.017734882533550263) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.613, 0.062317116998136045) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.575, 0.07741427361406386) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.662, 0.049918055094778535) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.656, 0.046709011614322664) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9425, 0.007179361655245884) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.02336936992406845) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.008461886439472438) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.009751907385885715) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.878, 0.017352203398942948) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.625, 0.05251530459523201) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.06415885627269745) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.704, 0.03895705892145634) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.648, 0.048517700910568236) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.006935273148257693) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.828, 0.021679307967424392) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.007595783473923802) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.010829988799989223) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.02170840135216713) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.04928330120444298) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.591, 0.06294019909203052) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.707, 0.03835894790291786) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.674, 0.04251325815916061) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9475, 0.0076994191438279815) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.02295267128944397) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.007847258530557155) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.009302427809685468) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.894, 0.019189293697476385) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.055339501604437825) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.06951949489116668) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.678, 0.04388792404532432) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.631, 0.056636410668492315) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9395, 0.008483358598817176) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.02156433555483818) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.007691651910543442) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.010745854958891868) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.886, 0.019235778510570525) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.613, 0.05636703669279814) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.0726780073940754) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.694, 0.044073669716715815) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.66, 0.044687388688325884) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.934, 0.009209775640971201) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.020928442388772964) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.007317754091694951) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.918, 0.010586566552519798) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.9, 0.020018943533301352) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.598, 0.061445217579603195) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.588, 0.0759041441977024) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.667, 0.04818380978703499) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.632, 0.05176793386042118) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9395, 0.007910219989338657) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.024519413337111474) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.008442826384212822) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.92, 0.010124554827809334) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.886, 0.019995488241314887) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.641, 0.04751089960336685) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.608, 0.06833687137067318) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.716, 0.03731236502528191) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.64, 0.05188358369469643) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9395, 0.007167463914120162) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.026805192828178406) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.007849375575780868) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.010415746876969934) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.022156578987836836) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.638, 0.04855518412590027) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.613, 0.07149866551160812) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.708, 0.03950303134322167) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.653, 0.04688811030983925) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.943, 0.00802499926037126) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.858, 0.022396369010210037) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.007671679519116879) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.912, 0.011636090309824794) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.01850672718882561) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.635, 0.05622700636088848) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.0738663121163845) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.668, 0.0494796102643013) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.655, 0.04641459731757641) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9415, 0.0077129619586849005) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.848, 0.0218947072327137) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.007661665670573712) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.010854230888187886) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.019449574261903763) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.622, 0.058589112550020216) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.609, 0.0662911048680544) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.677, 0.042600028965622184) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.666, 0.04615631645917893) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9395, 0.00875025089085284) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.848, 0.019298189163208006) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.007678400129079819) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.01026935138553381) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.884, 0.018901188731193543) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.633, 0.05092135375738144) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.596, 0.06641110345721245) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.663, 0.04861419452726841) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.646, 0.050927485048770905) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.008828418883000268) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.836, 0.023959481835365296) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.0084137790100649) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.011119897607713938) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.894, 0.025866157960146664) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.626, 0.05441561706364155) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.554, 0.09028231303393841) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.619, 0.05949425715953112) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.626, 0.05589402700960636) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9395, 0.007923069689997647) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.026530385389924048) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.00917273596301675) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.010672381045296789) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.02361471363902092) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.633, 0.04569559298455715) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.07805563773214817) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.686, 0.04236575628817081) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.655, 0.048500156074762345) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.94, 0.008627697216390515) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.02087590715289116) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.008584077144041658) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.010635095611214639) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.886, 0.01906744708120823) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.633, 0.0520187791287899) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.605, 0.07082722844183445) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.68, 0.04578724517673254) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.65, 0.04373833790421486) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9435, 0.00761912729528558) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.84, 0.02264769721031189) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.008028528740629554) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.924, 0.011496756382286548) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.019001660749316217) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.63, 0.054936578661203384) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.576, 0.07514764010906219) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.666, 0.05093208111822605) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.636, 0.05465048253536224) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9435, 0.008578143687722332) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.027442907497286798) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.00726955862576142) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.011800011665094643) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.01862743226438761) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.621, 0.053849437564611434) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.579, 0.08704217623919248) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.66, 0.05086052019149065) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.651, 0.05364901241660118) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.945, 0.006997294019383844) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.842, 0.022991457387804986) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.0077372971307486296) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.01051897044479847) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.021821938201785088) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.614, 0.05344673807919025) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.61, 0.06522460593283176) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.705, 0.04037196981906891) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.645, 0.049560083091259005) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.941, 0.008767385895895131) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.021100271955132484) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.009522559973411261) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.011892969273030757) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.886, 0.019232542380690575) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.601, 0.060441058307886125) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.572, 0.07926745111495256) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.677, 0.047405364245176315) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.618, 0.06256959341466427) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.944, 0.007887844869765103) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.025153926491737366) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.008517663558828645) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.010506046822294593) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.88, 0.019871525079011916) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.605, 0.055949025958776474) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.566, 0.07878828290104865) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.649, 0.05217573642730713) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.621, 0.05959118384122848) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.009771130937047928) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.025128429651260376) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.009265168894082307) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.01002837304212153) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.02142167326807976) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.596, 0.055930633813142776) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.582, 0.06949489895999432) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.658, 0.05092763935029507) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.05843600375205278) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.226, 0.04464438664913178), (0.25, 0.0756532991528511), (0.338, 0.08844312906265259), (0.373, 0.09776843130588532), (0.382, 0.116108429312706), (0.383, 0.11486499130725861), (0.396, 0.12786139059066773), (0.391, 0.12239557676017285), (0.397, 0.1381242759525776), (0.393, 0.1398952288478613), (0.411, 0.15190104061365128), (0.494, 0.04856345897912979), (0.597, 0.03729934859275818), (0.62, 0.042283935725688934), (0.611, 0.045791570007801055), (0.644, 0.0386363996565342), (0.629, 0.03957413753867149), (0.654, 0.03815848037600517), (0.642, 0.03933895581960678), (0.65, 0.039423394545912746), (0.664, 0.041997080743312834), (0.67, 0.03702389788627625), (0.62, 0.04729506565630436), (0.63, 0.04417530930042267), (0.625, 0.0473710655272007), (0.628, 0.04889915303885937), (0.634, 0.04225961092114448), (0.616, 0.05310979264974594), (0.6, 0.05241055706143379), (0.584, 0.059593795374035835), (0.6, 0.05539454360306263), (0.613, 0.062317116998136045), (0.625, 0.05251530459523201), (0.639, 0.04928330120444298), (0.626, 0.055339501604437825), (0.613, 0.05636703669279814), (0.598, 0.061445217579603195), (0.641, 0.04751089960336685), (0.638, 0.04855518412590027), (0.635, 0.05622700636088848), (0.622, 0.058589112550020216), (0.633, 0.05092135375738144), (0.626, 0.05441561706364155), (0.633, 0.04569559298455715), (0.633, 0.0520187791287899), (0.63, 0.054936578661203384), (0.621, 0.053849437564611434), (0.614, 0.05344673807919025), (0.601, 0.060441058307886125), (0.605, 0.055949025958776474), (0.596, 0.055930633813142776)]
TEST: 
[(0.236, 0.04354009944200516), (0.25025, 0.07285457700490952), (0.325, 0.08505733188986778), (0.3635, 0.09408129823207856), (0.36725, 0.11148307979106903), (0.381, 0.10974129223823548), (0.39125, 0.12281110650300979), (0.38925, 0.11663438200950622), (0.393, 0.1320787268280983), (0.38825, 0.133778289437294), (0.39625, 0.14531275349855424), (0.476, 0.0446248359978199), (0.5755, 0.03840452505648136), (0.5875, 0.0444328389018774), (0.575, 0.04760470287501812), (0.6295, 0.03870695433020592), (0.6025, 0.04255019210278988), (0.613, 0.042276706859469416), (0.62, 0.04099082833528519), (0.62975, 0.04134761287271976), (0.61825, 0.04566247607767582), (0.6405, 0.03920516735315323), (0.6045, 0.048277901992201805), (0.62525, 0.04264290514588356), (0.60775, 0.048372026443481445), (0.616, 0.049210998639464376), (0.622, 0.04299621890485287), (0.6135, 0.05199300406873226), (0.60425, 0.05194907122850418), (0.58, 0.058964142352342604), (0.58, 0.05621748694777489), (0.58275, 0.06234143313765526), (0.611, 0.05272980417311192), (0.62975, 0.04870757357776165), (0.6075, 0.05593009501695633), (0.6005, 0.05593914756178856), (0.59025, 0.05863719180226326), (0.631, 0.047213637799024585), (0.62425, 0.04853785246610642), (0.598, 0.05618861874938011), (0.5935, 0.057262028262019156), (0.611, 0.052549171403050425), (0.59775, 0.05982791268825531), (0.60725, 0.04888702274858952), (0.61625, 0.05443276722729206), (0.59875, 0.056265929311513904), (0.61175, 0.05495806746184826), (0.60325, 0.05387122170627117), (0.588, 0.06376299086213112), (0.59625, 0.0587509910017252), (0.59075, 0.05724058219790459)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.59      0.29      0.39      1000
           3       0.65      0.48      0.55      1000
           8       0.55      0.87      0.67      1000
           9       0.60      0.72      0.66      1000

    accuracy                           0.59      4000
   macro avg       0.60      0.59      0.57      4000
weighted avg       0.60      0.59      0.57      4000

Collaboration_DC_1
VAL: 
[(0.25, 0.04425914561748505), (0.265, 0.07162128928303718), (0.304, 0.08061573424935341), (0.428, 0.0912200716137886), (0.443, 0.1028368262052536), (0.447, 0.12005862887203693), (0.446, 0.12526907271891832), (0.459, 0.1390012750029564), (0.456, 0.1493007049560547), (0.464, 0.16985845598578453), (0.461, 0.1669399950876832), (0.525, 0.05775300109386444), (0.621, 0.04246030887961388), (0.621, 0.04399972575902939), (0.65, 0.038084795743227005), (0.636, 0.042357877373695375), (0.647, 0.041745519518852234), (0.656, 0.04137351074814796), (0.649, 0.04189026537537575), (0.633, 0.04797288990020752), (0.635, 0.043608181029558185), (0.622, 0.04673974350094795), (0.616, 0.05222092871367932), (0.636, 0.05075003823637962), (0.591, 0.06381241969764233), (0.607, 0.06555135120451451), (0.617, 0.06028944385051727), (0.589, 0.07385703096911311), (0.621, 0.06503870090842247), (0.576, 0.06958035192638636), (0.575, 0.07518544685095549), (0.575, 0.07741427361406386), (0.598, 0.06415885627269745), (0.591, 0.06294019909203052), (0.578, 0.06951949489116668), (0.604, 0.0726780073940754), (0.588, 0.0759041441977024), (0.608, 0.06833687137067318), (0.613, 0.07149866551160812), (0.58, 0.0738663121163845), (0.609, 0.0662911048680544), (0.596, 0.06641110345721245), (0.554, 0.09028231303393841), (0.604, 0.07805563773214817), (0.605, 0.07082722844183445), (0.576, 0.07514764010906219), (0.579, 0.08704217623919248), (0.61, 0.06522460593283176), (0.572, 0.07926745111495256), (0.566, 0.07878828290104865), (0.582, 0.06949489895999432)]
TEST: 
[(0.25, 0.04342002448439598), (0.266, 0.06890167579054833), (0.31225, 0.07728723338246346), (0.42825, 0.08714401468634606), (0.44875, 0.09824884018301963), (0.45125, 0.11498481222987175), (0.44725, 0.11927160546183586), (0.4605, 0.13327452754974364), (0.4625, 0.14331845676898955), (0.4655, 0.1620286877155304), (0.467, 0.16024554711580277), (0.52425, 0.05446592140197754), (0.59725, 0.04136319744586944), (0.6095, 0.04490765075385571), (0.6405, 0.03724949318170547), (0.6245, 0.043118545547127726), (0.6395, 0.04231675399839878), (0.6375, 0.04229618288576603), (0.62825, 0.042606346115469935), (0.61175, 0.05067700362205505), (0.62525, 0.045978790760040286), (0.62125, 0.04576956170797348), (0.6075, 0.05282519364356995), (0.611, 0.05267890112102032), (0.5805, 0.06522898587584496), (0.5935, 0.06667052394151687), (0.599, 0.06170080944895744), (0.57175, 0.07722441527247428), (0.5925, 0.06513847336173058), (0.5665, 0.07307805049419402), (0.56425, 0.07821164458990097), (0.57675, 0.07963573509454727), (0.57175, 0.06843507698178292), (0.587, 0.06563581040501594), (0.5705, 0.075279423058033), (0.58975, 0.07607554250955581), (0.57425, 0.0798494335114956), (0.579, 0.06883716014027595), (0.598, 0.07235558000206947), (0.561, 0.0785711773633957), (0.58, 0.07198747865855694), (0.56475, 0.07024887284636497), (0.539, 0.09292638012766838), (0.57875, 0.08015666970610619), (0.57475, 0.07580422502756119), (0.55325, 0.07812835761904717), (0.55025, 0.09253749725222588), (0.57075, 0.07138298985362053), (0.5505, 0.08271553999185562), (0.5515, 0.08171992257237434), (0.55425, 0.07089007133245469)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.65      0.22      0.33      1000
           1       0.70      0.38      0.49      1000
           8       0.50      0.88      0.64      1000
           9       0.54      0.74      0.62      1000

    accuracy                           0.55      4000
   macro avg       0.60      0.55      0.52      4000
weighted avg       0.60      0.55      0.52      4000

Collaboration_DC_2
VAL: 
[(0.244, 0.04420206964015961), (0.346, 0.10060960775613785), (0.407, 0.13195230773091315), (0.432, 0.15404716596007348), (0.437, 0.2008201377093792), (0.371, 0.2020772139430046), (0.411, 0.2141107092946768), (0.363, 0.259074219936505), (0.442, 0.21411252587661148), (0.431, 0.241391664955765), (0.437, 0.2810998134780675), (0.507, 0.06674806496500969), (0.583, 0.05346182940900326), (0.667, 0.038022797286510465), (0.644, 0.03884149700403214), (0.688, 0.03399552446603775), (0.661, 0.04109418612718582), (0.708, 0.03261804114282131), (0.713, 0.03331783059239388), (0.718, 0.03255673772096634), (0.681, 0.037767027407884596), (0.694, 0.036062782794237136), (0.695, 0.03557918144762516), (0.687, 0.03783576726913452), (0.645, 0.045030857898294925), (0.694, 0.04016150605678558), (0.707, 0.03579248285293579), (0.707, 0.03661813400313258), (0.68, 0.0417577041387558), (0.689, 0.045467913672328), (0.678, 0.0480157582461834), (0.662, 0.049918055094778535), (0.704, 0.03895705892145634), (0.707, 0.03835894790291786), (0.678, 0.04388792404532432), (0.694, 0.044073669716715815), (0.667, 0.04818380978703499), (0.716, 0.03731236502528191), (0.708, 0.03950303134322167), (0.668, 0.0494796102643013), (0.677, 0.042600028965622184), (0.663, 0.04861419452726841), (0.619, 0.05949425715953112), (0.686, 0.04236575628817081), (0.68, 0.04578724517673254), (0.666, 0.05093208111822605), (0.66, 0.05086052019149065), (0.705, 0.04037196981906891), (0.677, 0.047405364245176315), (0.649, 0.05217573642730713), (0.658, 0.05092763935029507)]
TEST: 
[(0.2445, 0.04316330614686012), (0.35275, 0.09631808972358703), (0.40675, 0.12594314509630203), (0.4285, 0.14750396132469176), (0.43075, 0.19383546972274782), (0.37875, 0.19488197696208953), (0.4085, 0.20684573304653167), (0.3695, 0.2504873937368393), (0.435, 0.20733263194561005), (0.43075, 0.2325922714471817), (0.43575, 0.27226708149909973), (0.497, 0.06579066443443299), (0.55, 0.05356515258550644), (0.64325, 0.03744403468817473), (0.6395, 0.03773209834098816), (0.68375, 0.03348658727109432), (0.6385, 0.040929766297340395), (0.69275, 0.033096628345549106), (0.69, 0.033671315036714076), (0.71275, 0.0311436922326684), (0.67125, 0.03693717008829117), (0.6845, 0.03658236344903708), (0.67425, 0.03647785557806492), (0.66925, 0.038705649197101594), (0.645, 0.04327755117416382), (0.69075, 0.037023250065743925), (0.70475, 0.0351971091106534), (0.7055, 0.03635401351004839), (0.67225, 0.042903995230793954), (0.663, 0.04551930013298988), (0.66575, 0.04812571561336517), (0.66375, 0.04834489731490612), (0.6985, 0.03843709173053503), (0.70525, 0.037364767357707024), (0.6615, 0.04618387080729008), (0.69275, 0.042797661520540715), (0.66625, 0.04612796571850777), (0.69275, 0.038921538159251215), (0.693, 0.040128808684647084), (0.65875, 0.04761722658574581), (0.6765, 0.04236235635727644), (0.66775, 0.04830386185646057), (0.61, 0.06283233942091465), (0.67975, 0.044720578826963904), (0.67475, 0.046363602593541144), (0.63875, 0.05217969526350498), (0.658, 0.04891992197185755), (0.68975, 0.041151219338178634), (0.64125, 0.04854271011054516), (0.61825, 0.05607594084739685), (0.6505, 0.051712786249816414)]
DETAILED: 
              precision    recall  f1-score   support

           4       0.76      0.40      0.52      1000
           6       0.81      0.54      0.65      1000
           8       0.63      0.86      0.73      1000
           9       0.56      0.80      0.66      1000

    accuracy                           0.65      4000
   macro avg       0.69      0.65      0.64      4000
weighted avg       0.69      0.65      0.64      4000

Collaboration_DC_3
VAL: 
[(0.25, 0.04452222609519958), (0.262, 0.09178792214393616), (0.319, 0.1186123097538948), (0.397, 0.14389754155278206), (0.436, 0.19386132979393006), (0.439, 0.1812336931824684), (0.441, 0.188261329382658), (0.443, 0.19005238442867994), (0.44, 0.2096376870945096), (0.444, 0.22949583377689123), (0.447, 0.2314559705518186), (0.515, 0.06437698121368884), (0.635, 0.04255699399113655), (0.659, 0.03561469155550003), (0.627, 0.038583864569664), (0.616, 0.04370503574609756), (0.646, 0.03989719778299332), (0.665, 0.03648252922296524), (0.656, 0.04049820411205292), (0.679, 0.03685330587625504), (0.694, 0.03783651277422905), (0.651, 0.0406122467815876), (0.664, 0.04032594457268715), (0.663, 0.041057499140501025), (0.664, 0.04317493858933449), (0.639, 0.04739527928829193), (0.631, 0.04888680443167687), (0.66, 0.040448685973882674), (0.664, 0.043471432119607925), (0.645, 0.04490999816358089), (0.626, 0.04885062563419342), (0.656, 0.046709011614322664), (0.648, 0.048517700910568236), (0.674, 0.04251325815916061), (0.631, 0.056636410668492315), (0.66, 0.044687388688325884), (0.632, 0.05176793386042118), (0.64, 0.05188358369469643), (0.653, 0.04688811030983925), (0.655, 0.04641459731757641), (0.666, 0.04615631645917893), (0.646, 0.050927485048770905), (0.626, 0.05589402700960636), (0.655, 0.048500156074762345), (0.65, 0.04373833790421486), (0.636, 0.05465048253536224), (0.651, 0.05364901241660118), (0.645, 0.049560083091259005), (0.618, 0.06256959341466427), (0.621, 0.05959118384122848), (0.625, 0.05843600375205278)]
TEST: 
[(0.25, 0.04352640879154206), (0.2605, 0.0880089095234871), (0.3135, 0.11392097294330597), (0.38675, 0.1381194160580635), (0.41975, 0.18566381484270095), (0.4255, 0.17218308329582213), (0.42525, 0.18001394498348236), (0.42625, 0.17942561841011048), (0.4315, 0.19832437139749526), (0.43375, 0.21836428129673005), (0.42825, 0.22052164554595946), (0.50875, 0.06152540382742882), (0.60025, 0.04330872677266598), (0.64675, 0.03524215993285179), (0.623, 0.03882304675132036), (0.59925, 0.04364308626949787), (0.61225, 0.04114578464627266), (0.652, 0.036860820293426515), (0.63675, 0.040488010704517366), (0.658, 0.037154193364083765), (0.66325, 0.038548713251948356), (0.6395, 0.040810409262776376), (0.656, 0.037603556990623475), (0.65175, 0.038857447624206544), (0.639, 0.04357705250382424), (0.6255, 0.0479299580976367), (0.63875, 0.043860778659582135), (0.66, 0.037974684707820415), (0.65725, 0.04018404546380043), (0.62025, 0.0468248732239008), (0.622, 0.04762698917090893), (0.6495, 0.04570268416404724), (0.6345, 0.046845172613859176), (0.66775, 0.03963151258975267), (0.60725, 0.05929427394270897), (0.63425, 0.04561792168021202), (0.62525, 0.04916901625692845), (0.62325, 0.051071969375014306), (0.63325, 0.045993142992258075), (0.62075, 0.047436657279729844), (0.63525, 0.045299449831247326), (0.622, 0.05104577240347862), (0.63425, 0.05343429456651211), (0.63, 0.049893088653683665), (0.6445, 0.0436043147444725), (0.607, 0.05567264184355736), (0.61925, 0.05498903676867485), (0.632, 0.049258489772677425), (0.587, 0.06361268340051174), (0.60375, 0.060665566071867946), (0.606, 0.061142144560813905)]
DETAILED: 
              precision    recall  f1-score   support

           5       0.75      0.47      0.58      1000
           7       0.68      0.31      0.43      1000
           8       0.69      0.86      0.76      1000
           9       0.47      0.78      0.59      1000

    accuracy                           0.61      4000
   macro avg       0.65      0.61      0.59      4000
weighted avg       0.65      0.61      0.59      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [46]
name: alliance-4-dcs-46
score_metric: contrloss
aggregation: <function fed_avg at 0x7f323b190c10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=46
Partitioning data
[[0, 5, 1, 6], [8, 3, 1, 6], [4, 7, 1, 6], [2, 9, 1, 6]]
[(array([41745, 36201, 46585,  2513, 33328, 10243, 10637, 36506, 39196,
        5426, 33363, 17115, 40873,  6858, 31004, 43504, 13419, 38618,
       35855, 20383, 35417,  1214, 20045, 49799, 26565, 31391, 28380,
       36491,  9171, 37841, 22680, 42044, 48869, 35529, 15439,  3295,
        6015, 39619, 39286, 17717, 47865, 49525,  1913,  8072, 44866,
        2707, 32661, 21633, 33950,  5888, 44968, 43828, 29444,  3184,
       44074, 11762, 18451,  9124,  1147, 32290, 33438, 22264, 46287,
         695, 12355, 26049,  9905, 21044, 32369, 12659, 45288, 12553,
       19082, 19106,  2355, 31654, 20862, 41606, 30749, 27434, 33647,
       40761, 39471,  1470, 24383,  5557, 39995, 44430,  3644, 42204,
       44849, 41068,   600, 32839, 29634, 42566, 47645, 10578, 31339,
       30709, 42517, 39793, 32233, 27301, 47215,  1950, 40018, 47969,
        5984,  5765, 12056, 14520, 14025, 34567, 29219, 24695, 25191,
       29286, 41751, 11356, 21143, 24507, 31048,  7972, 39691,  9430,
        1988, 35934, 12325,  3197, 12419,  1329,  4176, 19859, 24101,
       30811, 19634,  7511, 17166,  4165, 35626, 44116, 22320, 34040,
       28865, 30233, 23449, 19332, 12328,   189, 46763,  8062, 39589,
       14666, 14567, 33516, 26018, 32931, 44618, 35414, 16253, 44666,
       15546, 23377,  7251,  2413, 21248, 24351, 41200, 24931,  5371,
       10686,  7971, 29620, 43012, 30240, 18768, 22034, 17107, 20452,
       18572, 17316,  2885, 10146,   457, 43542, 27415, 34783, 14213,
       41238,  6399, 32662, 41821, 47617, 26723, 39329, 49717, 21272,
       35422, 14023, 48378,  7095, 28487,  1338, 18782, 45423,  3136,
       38478,  7581, 36545,  1044, 49507, 33502, 40649, 11913, 22286,
       36052, 46706, 16559, 13457, 19409, 49263, 29104,  5414, 14069,
       25160, 12492,   199, 12704, 49868, 14210, 13894, 35570, 43639,
       48671, 29977, 29518, 33934,  6090,  4524, 20349,  8136, 44476,
       28092,   866, 47379, 17959, 34381, 43378, 18203, 15713, 40957,
        2013, 16199,   984, 38507, 45181, 19437, 33634, 27472, 16949,
       24831, 30356, 30162,  4467,  2884, 46441,  2182, 48890, 39463,
       22941, 11725, 14992, 35543, 18016, 42494, 48135, 36139, 42218,
        1519, 11143, 46112, 48110, 43994, 21977, 12371, 36692, 35534,
       33295, 28283, 15229, 25603, 11861, 43273, 47038, 20732, 23185,
       34074, 13257,  4456, 22879, 48395,    81, 30965, 20043, 26138,
        3021, 37375, 23090, 18212, 44832, 16185, 22681, 32864, 20809,
        6794,  1224, 29037, 42491, 41915, 19024, 14929, 46318, 35194,
       28484, 22434,  7425,  8039, 38529, 23238, 31551, 22445, 27219,
       33477, 13073, 41942, 19933, 32255, 18280,  9383, 12300, 35166,
       40456, 31243,  5645, 11735, 22003, 32926, 10835, 37531,  5208,
       43535, 26589, 30308, 31109, 36235, 22731, 16431, 34884, 43654,
        3069, 36057, 25840, 41886, 19786,  7115,  1157,  3810, 16979,
       14841, 26277, 45142, 36912, 31631,  7729, 44033, 38792, 33067,
       40887, 30188, 34634,  7562, 11077, 40889, 37894,  6058,  1199,
       46525, 30189, 30063, 12682, 25699, 17708, 28999, 36335,  5850,
       13461, 33965, 23437, 16496, 18096, 23556, 29358, 46201, 42323,
       13680, 15378,  5448, 34112, 11274, 30847, 16835, 18189, 46606,
       11004, 25925, 27359, 39456, 24741, 45915, 12832, 44089, 49715,
       11491,  5369, 29804, 27397, 44561, 44687, 24867,  3660, 44740,
       14001, 28505, 38784, 28425, 30126, 21839, 12089,  4319,  3017,
        3005,   999, 30662, 36769,  1280, 43115, 15207,  3420, 48910,
       22671, 35394, 10541, 45581, 42538,  5233, 22483, 33927,  1942,
        2916, 23249, 23544, 30523, 33504, 35604, 35822, 49431, 15177,
       42613, 37341,  3022, 13252, 35390, 27345, 11441, 21075, 35602,
       34048, 22895,  4355,  4968, 40871, 26680,  3060, 17165, 11043,
       31901,  4694,  5698, 13604,   535, 46083, 35748, 18894, 36120,
        8902, 47773,  1998, 47720, 21586, 34493, 23895, 46674, 17384,
       27731, 39792, 12594,  8556, 10571, 37491, 20057, 20366,  5311,
       37799, 11482, 23714, 48021, 16197,  5683,  7683,  6656, 16412,
       43372,  7649, 14096, 39824,  5236,  5301, 35041,  1305, 30486,
       38257, 30912, 49435,  1380, 48496, 29252, 48612, 16250, 47040,
       31776, 35174, 10790,  8149, 35399, 47310, 14448,   547,  9459,
       40505, 11336,   257,  4250,  4533, 45923, 11367, 33686,  7005,
       17606,  7912, 43976, 15386, 22859,  1651, 11515, 15195, 22090,
       42709, 47256,  9766, 48263,  4528,  7358, 41378, 37847, 44269,
       38911, 29792, 38078, 26344,  5663, 28906, 38744,  5149,  6161,
        5099, 18583, 37007, 35803, 47030, 46140, 31534,   855, 11130,
        9232, 26740, 30752, 35847, 23423, 45500, 16077, 45377, 43089,
       31300, 38795, 18388, 47288,  8307,  1064, 16810, 20856, 15662,
       10964, 34906, 18902,   893, 40172, 41519, 31737, 28352, 20616,
        3346, 25799, 49469, 34595, 36400, 41607, 13763, 14643, 21149,
        3661, 43618, 48275, 28027, 45714, 20304, 49973,  1694, 25750,
       19363, 18754, 23142, 11716, 14201, 32913, 26614, 16598, 41579,
       24220, 11683, 22957, 47237,  5392, 26450, 25196, 32937, 30182,
        4945, 40060,  6420,  3808, 21152,  5197, 24566, 15306, 49509,
       14546, 15710, 14040,  8449,    96, 48791, 42143, 22943,   311,
       20155,  2756, 17701, 25582, 45769, 12766,  6244,  7378, 20376,
       25323, 42710,  3444, 43251,  9878, 47900, 24463, 31151, 28848,
        8848, 35322, 45488, 39800, 43416, 31619,  3886, 34809, 36879,
       19181, 11971, 25135, 36838, 25652, 34080,  6967, 45084, 43609,
         282, 19616, 23198, 28580, 28780, 23338, 12875,  7324,   879,
       20074, 43186, 40427, 23407, 37092,  9080, 37779, 28790,  9480,
       35884, 11767, 25208, 12169, 27769, 18932, 42666, 28784, 25874,
       26433,  1631, 46135, 37235, 46448, 42121,  7659, 49777, 28441,
       35757, 41502, 30088,  2475, 44011, 21216,  9953, 49129, 33738,
       42509, 33178, 38769, 13168, 35830,  9727, 26492,  2439, 45944,
       43912, 49279, 37714, 20527, 26232, 36089,  3686, 26618, 34286,
       33733,  4208, 22550, 12302,  4875, 49952, 18604, 36720, 30812,
       48361,  3942, 45889,  3986, 10649, 43166, 24671,  3521, 14443,
       47472, 38475,  3861, 18974, 49550, 37045, 10554,  7393,  2729,
       22066, 24431, 15664, 49046,  7277, 12367, 19508,  3098, 26333,
       47649, 15726, 29973, 39939, 35370, 21917, 29381, 35950, 15771,
       10939, 42762, 16670, 10176,  9360, 22356, 17776, 15291, 11459,
       10635, 25373, 41160,  3403, 29391,  7553, 34662, 34056,  9877,
        5148, 25680, 24143, 22525,   770, 20666, 32359, 21644, 33123,
       31639, 42404, 17317, 17417, 21717, 41297,  7976, 43250,  6599,
        8069, 49928, 10364, 13029, 12796,  4126,  3095,    19,  8956,
        3437, 34684, 48947, 22295, 47465, 40038, 30727, 39203, 19588,
       22631, 14024, 22428, 38230,   807, 30536, 33623,  7675, 22786,
       24918, 40436, 20377,   249, 29217, 42742, 44194, 21497, 24888,
       49516,  2411, 36564,  5680,  8293, 36290, 35571,  2777,  6267,
       25382, 36360, 37048, 18978, 19763, 24155, 25622, 43027, 30212,
       37261, 42049,  2684,  7650, 35170, 35458, 23366, 43157, 32125,
       45622, 13127, 22236, 13479,  4810, 24531, 35745,   235, 35312,
       15193, 46991, 48650, 11101, 28421, 29360, 34139, 20022,  2924,
       40313, 14916, 43298, 23005, 30953, 45019,  4279, 13988, 47331,
        2690, 15823, 14845, 16639, 24354, 32856, 23539, 18682, 37134,
        7918, 29627, 31279, 10374, 18029, 28492,  3341, 15612, 17723,
        4550, 36304, 31118, 32812, 17697,  3739, 38290, 13866, 49804,
       18338,  3683, 41448, 38662, 38397, 12066, 10600, 39205, 27464,
       25234, 18270, 19268, 16218, 31128, 38845,  8256, 40498, 33627,
        3401, 38336, 13596, 27928, 40613,  6832, 38026,  6517, 43414,
       37694]), [0, 5, 1, 6]), (array([10313,  1060, 22492, 47609, 49025, 35505,  9527, 40702, 34894,
       48611, 49501,  2942, 22714, 24062, 26623,  1325,  3258,  4313,
       47376, 39971, 47430, 14149, 29933, 12249, 31714, 27643, 29362,
       25412,  3123, 23399, 31917, 31183, 39299, 17010, 17170,  4283,
       18395, 14166, 20617, 41335, 12907, 43623, 39605, 20259, 49388,
       24260, 18950, 40048,  3240, 11378, 47181, 10369, 12866, 31370,
        5546,  1239, 36295, 39433, 32671, 30510, 42917, 11248, 30433,
       34296, 47793, 21451, 45095, 35766, 34369, 32714, 35730, 22776,
       49712,  6717, 45461, 33954,   723, 27692, 34892, 23497, 37002,
         580, 13929, 40578, 42162, 37036, 41247, 41413, 15164, 20909,
       18081, 42898, 23960,  9427, 29075, 39243, 47212, 25513, 10096,
       32619,  7610, 41078, 44659, 25163, 29954, 40470, 37959, 24328,
       46223, 16942, 41727, 23757,  5280, 15022, 13386, 11063, 21050,
        3319, 38315, 45398,  9922, 30997, 10810,  4204, 29437, 35416,
       46028, 11965, 49379, 25691, 48168, 42713, 40785, 16978, 39335,
       48124, 37722, 29868, 28230, 24526, 13504, 40881, 10856, 29895,
       12207, 13859, 30443, 11674,  1141, 11352, 46543, 47561,  3777,
       36299, 36444, 39577, 27846, 36498,  7693, 29559,  1398, 17168,
       14281, 10923, 23723, 46975, 10172, 35911, 42467, 13886,  5658,
       12772, 23567, 25126, 29490, 17482, 35074, 30509, 29782,  6166,
       29676, 13338, 48432, 37072, 27808, 49654, 47342, 28436,  5378,
       17820, 18492, 45104, 46101, 20749, 28768,  7129, 46309, 37642,
       32532,  1751,  7083, 47978, 49127, 20984, 19186, 40402, 10702,
       12087,  2819, 16630, 37307, 12699,  4001, 23672, 22920,  8006,
       30691, 25237, 19097, 16792, 34274, 29479, 38689, 20726, 12543,
       24045, 29371, 25072, 46580, 14693, 23872, 39198, 12615, 13388,
        9122,  5348, 26487, 30770, 49851, 27068, 30664,   170, 39653,
        6529, 26720, 48261, 23595, 35491, 36438, 43381, 43490, 15872,
       35459, 25934, 40670, 13189, 11743,   785, 44451, 47100, 38193,
       36080, 43606, 38894,  7171,  2406,   395, 46230, 27441, 39871,
        8710, 14623, 38445, 37415,  7432,  3486, 26657, 42604, 26097,
       13259, 45928,  6783, 46950, 25705,  4672, 40712, 13201, 40249,
       45282, 48592,  9973,  3795, 45914, 26862, 20210, 18780, 40954,
       13873, 16983, 47067,  2051, 38472,  3222, 23389, 23763, 38360,
        3320,  6284, 22979, 45362, 36425, 30339, 30452, 30687,  4350,
       12786, 31479, 45433, 20721,  9051, 10753, 47062,   869, 10468,
        3447, 44103, 28368, 42646,  4628, 15467, 27608, 32134,  3108,
       19414, 40651, 43762, 48762, 45117,   740, 15470,   922, 17658,
       46682, 33600, 49478, 33236, 15873, 35472, 21655,  5553, 19061,
       44120, 38338,  4876, 18324, 36140,  6332, 26964, 11103, 39420,
       22913,  6199, 16583,  4697,  1895,  4392, 19255, 28697, 17214,
       33345,  3218,  9667,  4428, 38999,   241,  6714, 10452, 26707,
       41827,  9760, 12798, 15388,  4405, 33238,  2132, 42953, 41421,
       48540, 34199, 20425, 44944, 23286, 43123, 12973,  4558, 47897,
       41853, 25899, 44943,  1841,  7411, 21703,  2082, 17062, 39604,
       17565, 24991, 23702, 10186, 37194, 16335, 22042, 46604, 46977,
        2447, 39865, 33653, 34517, 22861, 25218, 47586, 14107, 21274,
        7667, 26482,  8846, 33033,  6151,  4519, 41271,  8635,  7107,
        5838, 47950, 22377, 26849, 22919, 35790, 22244, 30192, 42122,
       49742, 46058, 17096, 35307, 15545, 32230,  7615, 43487,  5502,
       37421, 35697,  3242, 15860, 23584, 21835, 25627, 45949, 23409,
       48758, 17655, 43598, 34160, 30318,  9846,   792, 25545, 28657,
       45933,  7122, 49726,  8686, 44197, 47603,  5539, 12904, 22484,
       21898, 14986, 46605, 21085, 16526, 45667, 27796,  5260, 36930,
       38764,  2359,  9008, 33709, 35380, 18462,  4110, 49032, 48368,
       48085, 25509, 23992, 30947, 34170, 13999, 12164, 40936, 21796,
        5125, 25046,  8058, 41710, 24268, 36848, 27419,  9584, 34364,
        6383, 31599, 44889,  5282, 27174, 10107, 43520,  6779, 46099,
       35688, 15669, 39964, 49022, 15577, 14662, 37727, 17772, 40964,
       40713, 30696, 37830, 44415, 11134, 30039, 17964, 43274, 10795,
       32737, 35863,  6915, 41072, 23263,  4132, 22500,  8476,  5759,
        2433, 49225, 27953, 24544, 19886, 43806, 38064, 44051, 14967,
       14376, 31890, 35344, 33181, 41153, 43480, 41051, 15114, 31720,
       27260,  2204, 28671,  6411, 34802, 14384, 16613, 38658, 29710,
       16543, 39322, 24292, 20674, 22227, 41075, 33085, 38710, 27132,
        7148, 25331, 30019, 46904, 12977, 41189,  9335, 37455,  9998,
       36067, 30545,  7390,  4993, 14238, 48465, 31665,  2771, 40220,
        5616, 46547, 17169, 39340,  6226, 32462, 22788, 47670,  1790,
       42437, 43655, 42438, 12516,  9085, 34400, 37203, 41760, 38482,
       15315, 14367, 41445, 32584, 34258, 34905,  9646, 16736, 35576,
        2362, 10168, 12306,  4958,   250, 44204,  4648, 35910, 29874,
       25600, 23768,  4200, 47989, 22060,  7371, 38420, 12340, 28268,
       45289,  3025,  8127,  9668, 27405,  9317, 16855,  7875, 18908,
       31645, 44266, 30183, 43954, 48970, 18178, 30083, 25165, 44273,
       47190, 26368, 41439, 45821, 47423, 30999,  4354,  7284, 12629,
       39892, 45308,  7887, 30650, 40449,  1736,   593, 43608, 12451,
       45068, 40046, 12292, 34278, 29429, 10826, 13044,  4244, 47226,
       19571,  4099, 23439,  8189, 45972,  3827, 22790, 38223,  7428,
        6819,  6121, 47264, 17460, 34794, 40076, 24075, 15183, 48269,
       13311, 35465, 26350, 10564, 18825,  7124, 21706, 45014, 41946,
       49847, 33658, 40472, 17031, 47033, 49977, 49769, 17205,  3142,
       18907,   761, 44601,  3131,    60, 39204,  1413,   176, 14676,
       12247, 42163, 13523, 16211, 49959, 16286,  6851, 45317, 11799,
       33832, 21416, 33118, 43482, 24673, 46043, 15028, 24872, 41260,
       33566, 31748, 19877, 28042, 35535, 31943,  7091,  6025, 29332,
        1063,  4682, 34322, 16493,  3994,  1876, 10828, 21636,  3865,
       11239,  7843, 47984,  8557, 17675,  2152,  9233,  9428, 31232,
       35221, 45355, 49464, 27295, 15657,  3168, 47752,  3307, 16768,
       26727, 18407, 25851, 42619, 18533,  7025, 16696, 24275, 23412,
       18866, 47976, 38834, 36666,  3091, 45836, 33990, 26192, 12641,
       24397, 19366, 18024, 25709,  4430, 38677, 39678,  6768, 30864,
       31598,  4966, 39323, 13213,  9974, 47878, 37593, 12635, 25139,
       15592, 47790, 39159, 20482, 29846, 27367, 25913, 48103, 40438,
       44128, 35090,  9543, 38069, 35332, 33787,  1017, 20628, 44696,
       40274,  2937, 14084,  2479, 22746, 17966, 37426, 40879, 25612,
       41328,  9530,  1031, 11911, 39464, 33491, 17938, 23888, 10117,
       16789, 30302, 46835, 23771, 11147, 41188, 31928, 45078, 20663,
       16915, 29136, 36274, 47451, 31570, 19107, 24973, 36285, 17562,
       29752, 42510, 29436, 41656, 33364,  2810, 30130, 30481,  3588,
       20885,   931, 13600, 42261, 13216, 34804, 10510, 48065, 12879,
        4086, 32228,  9847, 33192, 40836,   473, 37922, 18542, 31142,
       43269, 44612, 33948, 29357, 20106, 31396,  8873, 36842, 46340,
       36645,  9131, 14951,  9103, 19239, 46612,  5682,   488, 12609,
       20027,  6672, 36704,  1248, 10001, 19927, 41534, 10026, 27765,
       19562,  7272,  4899, 33758,  3775,  1989, 30202, 41293, 17796,
       20003, 33704, 13331,   819, 21572, 45607, 16404, 46473, 10718,
       32010, 18261, 46066, 49754,  1175,  3018, 38396, 33759,  7669,
       10379, 12293, 24321, 14864,  9515, 49636, 18342, 13076, 23036,
       17715, 31181, 33812, 26383, 20113, 44917, 27610, 13956,  9808,
       21756, 14621, 14694, 13066, 14065, 11429, 23628, 36847, 29103,
       25828, 23680, 29516,  8091, 46721, 24701, 33256, 34922, 12048,
       11751]), [8, 3, 1, 6]), (array([ 1937, 21260, 10485, 22369, 36168,  1657, 33408,  4753, 15238,
       19471,  5742, 28170, 40846,  6270, 40739, 23468,  5570, 19360,
       32799, 39182, 11765, 41618, 47630,  2573, 35228,  8423,  1169,
       11810, 16889, 42096, 13012, 42967,   665, 35324, 12406, 33401,
        8765,  6206, 27469, 15587,  9852, 15342, 27604, 21304, 16702,
       19340,  8462,  8379, 48087,    86,    20, 16173, 28658,  5880,
       42716,   816, 33749, 21966, 16878, 30037, 26838, 43509,  3908,
        8487, 38492, 18115,  4199,  3897, 44397, 37288, 48172,  1158,
       33209,  7984, 29065, 10477, 18293, 39947, 45605, 48484, 47857,
       19702, 29445,  1826,  3560, 32593, 37850, 23869, 21197,  2734,
       18493, 46790, 25871, 28555,  5631, 10629, 38123, 46251, 30561,
       42314,  9234,  6652, 33016, 11499,  3189,  1018, 18099, 21364,
        2792, 21016, 42104, 28049, 43151, 48945, 11435, 39265, 46164,
       33574, 30869, 22148,  7382,  4750, 29723, 20782, 16936,   934,
       15718, 42863, 22990,   844, 27649, 46036, 15781, 18821, 30384,
       31932, 26434, 28755,  1746,  3635, 14134, 30358, 41777, 15508,
       43241, 43637,  8046, 30527, 18690,  7100, 33245,  2155, 34777,
       49097, 19441, 49488, 14532, 18766,  3973,  8523, 20978, 43640,
        2486, 26301, 36552,  6432, 28892, 27390, 26233, 23611, 43055,
       23255, 21693, 24464, 47239, 15253, 18418, 36517, 37089, 12691,
        5653, 26823, 48699, 33947, 33272, 12843, 15656, 37313, 27479,
       27923, 18052, 43486, 11254, 11957, 42769, 30225, 40650,  6844,
       12282, 31505, 39125, 39036, 42442, 44629, 16659, 41246,  1264,
       13855,  3717, 38821, 30242, 42303, 31349,  7625, 42653, 19284,
        4394, 15663, 15538,  1182, 27614, 10426,   381, 24884, 25813,
       19790,  2943, 13896, 40665, 43044,  4290,  4781, 19833, 38281,
       48787, 14440, 27017,   563, 48801, 21254, 14397,  6452, 15760,
        1866, 43925, 24067, 23634, 32069,  8890, 45611, 44845, 37302,
       49763, 29158, 23880, 43531, 16260, 41624, 42423, 28759,   178,
        2275, 36495,   948, 39810, 40541, 42500, 32546, 15824, 49648,
       10305,  4216, 15002, 11935, 22136,  2807, 44986, 44798, 48857,
       23860, 45476,  8595, 37262, 38196, 36037, 16088, 11323, 21660,
       34733, 24050, 44338, 44858, 46747,   824,  5842,  3914,  2015,
       48423, 44105, 47082, 21317, 30032,  3135, 47263,  7385,  8498,
       24547, 18938, 40447, 43316,  6966, 49470, 49162, 45758, 35316,
       11656, 35922, 25724,   954, 39465,  3104, 48104, 19062, 19259,
       37893, 11428, 24129, 37472, 43498, 41418, 18667, 26165, 25310,
        5035, 22541, 46691, 42126, 28812, 39297, 46300, 44447, 33681,
       19772, 20900, 33019, 23304, 36680, 19642, 40461, 20846, 32668,
       19461, 30671, 23784, 35814, 23356, 18258, 35277, 19321, 17463,
       11761, 10528, 49231, 38131, 39246,  2601, 30195, 33220, 48727,
         163, 39961, 48759, 29733,  9462,  5809, 30036, 34788, 10852,
        7758,  3606,  3147, 44295, 44378,  6588, 41956, 40637, 34807,
       35407, 38178,   825, 41381, 37292, 39762,   636, 38358, 25889,
       26133, 30993, 27930, 32779, 21927,   641, 10573, 18443, 45561,
        7611, 29648, 13540, 32383,  8461, 16171,  6010,  5854, 31212,
        2783, 38220, 29592, 13482, 37882, 25090, 23196,  8367, 23298,
        6032, 19909, 43748, 41438, 14375, 37763, 17110, 17886, 33804,
       13373,  6235, 44297, 12364, 21787, 44249, 40010,  4554,  3487,
        6759, 41717, 48486,    11, 48837, 39494, 17503, 31262,  8351,
       27145, 22291,  5269,   131, 17872, 27672, 25778, 27400, 32660,
       10998, 20960,  7016, 46353, 29196, 12466, 48436, 27157,  9369,
       18997, 35164, 36654, 28936, 38688, 13185, 23829, 13083, 23993,
       29355, 20369, 12864, 49529,  3534, 25156, 31982,  3592,  4714,
       13493, 17589, 40694, 16824, 41185, 41877, 47828, 10602,  7101,
       26669, 22626, 28727, 41626, 28152, 38330, 24194,  9565, 19351,
       13852,  6668, 17312, 44725, 44436,  8856,  5827, 13323, 37257,
       39692,  1494,  3238, 29325, 29522, 43061, 12920,  2455, 38908,
        9854, 33897,  6371, 21752, 20312,  6295, 47529, 28183, 33126,
        1660, 16848, 29675, 34723, 11444,  7147, 10576, 16474, 12424,
       27073, 45600, 33230, 35914, 25718, 28938, 20313, 46238, 12100,
       46376, 40207, 33532, 15903, 32847, 14425,  2037,  9048, 34052,
        8965, 11910,  5257, 37505,  2615, 27601, 16925, 11163, 18160,
       32827, 16953, 46051, 14978, 47377, 32941, 42490, 21933, 42927,
       41291,  4101, 26298, 27284,  8625, 44591, 26937, 27195,  5287,
        9263, 41262, 49554,  7640, 37687,  4230, 41984, 35850, 32262,
       10359, 44767, 41654,  5917, 24975, 15051, 19667, 27802,  1037,
         978, 30489, 38822,   137, 38262,  6130, 43195, 41948, 22175,
       16089, 46731, 24680,  5722, 33845, 39250, 44841, 21331,    45,
        1079, 28655, 38725,  3907, 40935, 11571, 23959, 48528, 10035,
       23058, 26315, 11804, 15770, 29494, 45296,  8600, 10495, 17534,
        7832, 47568, 26899, 28005, 29485, 29285, 44541, 25209, 46717,
        3483, 14182, 48901, 32418, 35933, 19522, 34571, 30104, 32652,
       24398,  5552,  7374, 26026,  6639,  3992,  4334,  6589, 37801,
         493,  1571, 15885, 25611, 28896, 14502, 37551, 23648, 46649,
       44225,  7738, 40594, 17984, 21218, 40422, 35454,  4326,  9494,
       28265, 44272, 47606, 17626, 16400,  5422, 47470, 40081,  3020,
       36388, 49060,  9745,  3034,  9017, 19581, 22055, 13264, 44625,
       47088, 38579, 39654, 30458, 49629, 39679, 39771, 10569,  3285,
       38211, 46396,  7338,  6931, 17492, 16365, 29963, 45968, 39668,
        4636,  2545, 41563, 24527, 13006,  7312, 25279, 10022, 16937,
       44589, 22833,  8174, 21651, 13667, 32458, 21992, 21142, 15248,
       13608, 12335, 47162, 49121, 37920,  2762, 36376,   991, 44679,
       47370, 30227, 37630, 16003, 16069,  2888,  1159, 48400,  5833,
        8171, 40322, 28258,  7064, 44915, 43021, 14178, 30146,    25,
       27660, 14457, 36147, 21072, 10356, 40767, 17143, 12369, 49171,
       18224, 16389, 36620, 44985,  6747,  6941, 18816, 48866,  5398,
       32191,  5380, 39190, 35435, 13120,   863, 20717, 17908, 34786,
       44417, 30922, 47792,  6127,  4881, 24660,  9828, 46306, 17132,
       11038, 37997,    95, 45793, 41399, 42671,  5169,   103, 47396,
       35600, 34245,  7252, 42334, 13573, 23244, 26245, 34092, 30685,
       32637, 28432, 25657, 21560, 25747, 18025, 48664, 31500, 38936,
       10738, 36625, 32397, 49624, 48325, 27504, 29976, 14889, 10444,
       12396,  9762, 20362, 24418, 42027, 44163,  9732, 31820, 25406,
       21704, 39986, 26524, 11904, 31112, 36493, 37865, 40551, 18021,
        2019, 43933, 12014, 29214, 18325, 27577,  2915, 14547, 20878,
       13838, 48043, 32880, 43284, 10063,  6395, 40804, 47302, 11823,
        9126,  3589, 16822,  2965, 16762, 32105,  7339, 45878, 32762,
        4663, 45009, 34157, 26419,  7591,  6777, 12433, 38462, 46856,
        5708, 38298, 26798, 18334, 10783, 32739, 43431, 39759,  8215,
       22027, 36638, 29009, 12019, 28672, 37918,  2872,  3716, 33367,
       16097, 48977,  1023, 45225,  3603, 31616, 40047, 19731, 44626,
       12168,  9246, 30222, 23915, 48176, 10579,  8815, 27486,  3041,
       48819, 26054, 10936, 48710, 24519, 17597,  5558, 23881, 28501,
       17688, 45247, 22610,  7232, 29327, 14824,  2602,  7881, 14973,
       15343, 45443, 31891, 42327,  4751, 34542, 23470, 46791, 36586,
       39052, 19128, 36808,  7172,  5402,  8545, 19823, 11212, 31135,
        9356, 47953, 31784,  6664, 13806, 43353, 13143, 21722, 32212,
       15517,  2874, 43995,  3439, 43390, 39343, 15232, 31575, 23697,
       29426, 30033, 42358,  6035, 43439,  1176, 15231,  8827, 45663,
       46952,  8832, 18621, 18884,  7638, 14884,  3192, 22968, 40323,
       34564]), [4, 7, 1, 6]), (array([  742,  9067, 29220, 49888, 47450,   910,  6227, 41198, 43122,
       20779, 10433, 40281, 15176, 13786, 12711, 44392,  4588, 48893,
        3037, 12669, 21819, 44075, 41681, 45864, 28497, 41619, 32274,
       47632, 25041, 13433,  8543,  1492, 36125, 49397, 40701,  9552,
       38745, 12504,  7166, 38096, 26321,  7948,  5979, 14192, 29678,
       46114, 26541, 40398, 12742, 35311, 26186,  8881, 49802, 31495,
       34088, 38594,  7303, 28802, 14172, 26807, 43465,  9033, 21942,
       12583, 27825,  9001, 32951, 27498, 48589, 39062, 42267,  9989,
       32278, 29912,  1768,  1788, 26893, 13081, 43478,  3848, 37742,
       20547,  8186, 39019,  9526,  9455, 48545, 10160, 18652, 37745,
       24170, 30454, 22667, 48833,  3629, 47956, 44658, 48365, 41542,
       20580, 25449, 19964,  1530, 36025, 17844, 25491, 41933, 45074,
       28705,   108, 43010,  7853, 37560, 14229, 19950, 40177,  6408,
       22297, 42803, 40372, 15522, 33051, 34396,   820,  1444,  3462,
       11443, 31308, 18863, 13648, 20572, 48531, 42302,  1122, 43118,
       17955, 12046,   975, 31333, 28994, 28164, 19914, 43469,  1693,
       15696, 42648,  1040, 42468, 39991,  3476, 39051, 31813, 46819,
        6484,  5727, 28708, 19932, 49104,  7807, 40087, 33519, 45926,
       46522, 14829, 47491, 23911, 29728, 18027,  3119, 29448, 23530,
       19137, 12148, 22602, 33032, 10018, 37321,  2372, 16919, 35681,
       36450, 40583, 43766,  8871, 39436, 31562, 25859, 12602,  8843,
       18092, 32470, 25830, 23692, 20660, 47016, 31026, 31519, 24636,
       31759,  8800, 13564,  1533, 33449, 32816, 35482, 40403,  3101,
       22228,  9437,  3889, 41362, 16266, 18101, 31040, 14270,  1606,
       24694, 39687, 44739,  6228, 29181, 23453, 46530, 24496, 40225,
       44277, 15182, 36339, 12156, 27580, 37278, 27557, 41255, 38058,
       37816, 10767, 39928,  2033, 32321, 21167, 43134, 16279,  4426,
        6448, 20526, 19695, 38704,  8418, 49297, 17735, 29776,   369,
       22921, 42739, 21786, 10208, 38363, 28543,  6066,  7127, 45231,
       25319, 48017,   886, 17716,  3513, 27548,  3820,  6937, 29992,
       26069, 13881, 38379, 39884, 40699,  5437, 14099, 11230,  5611,
        1390, 39583, 40068, 47073,  2281, 41560, 13155, 27190, 30175,
       43039, 17468, 25009,  5671, 27420,  7793, 49926, 40303,  9213,
       17905, 22016, 27523,  3477, 24718, 10494, 49637, 49785, 46685,
        7457,    31, 31566,  1706, 25631, 36050, 24086, 26955,  2214,
       43511, 25159, 48371, 42346, 11312, 30853,   883, 34943, 32810,
       25837, 15750, 32242, 41976, 31287, 17218, 44052, 19842, 45906,
       10138, 21203,  3628,  5453, 35746, 42627, 37074,  1115, 37678,
       33200, 28391, 20587, 14809, 26061, 37709, 16641, 12835, 39806,
       27983, 39816, 26053, 40117,  2477, 24391, 33896, 43591, 11104,
       26496, 44132, 10178,  9025, 33865, 17228, 27784, 38380, 33025,
        6579, 43803, 36807,  6480, 36859, 46777,  8501, 15838,  6237,
       39984, 19302, 43346, 20713, 24542, 10390,  8505, 34796,  2061,
       27957,  5161, 23075, 27799, 41183, 46346, 36529, 38961, 22614,
       16720, 14224,  3850, 45063, 10467, 26057, 16347, 48192, 13795,
        6539, 32129,  1572, 41226, 14661, 39688, 19670,  3838, 29784,
        3409,  7010, 44058, 22651, 13827, 44716,  1231,   186, 20995,
       12104,  4364, 29193,  3652,  5465, 17090, 40450, 49442, 11755,
       45008, 38789, 35542, 39418, 31162,  5300, 40833,  8650, 35183,
       46781, 13096, 46269, 11777, 27336, 15142, 47512, 39453, 44339,
       42140, 23225, 11373, 34867, 47765, 36684, 30615,  4473, 45198,
       32115, 33417, 49147, 21584, 44469,  2908, 10312, 45498, 17798,
        9807, 13421, 11555, 28879, 32041, 15384, 19367,  3846, 24408,
       17068, 49003,  1972, 31403, 27380, 42973, 43619, 23137, 31490,
       34872, 14220, 14462, 41228, 27196, 12086, 46645,  3537, 26446,
       16591, 18878, 10781, 46861, 49385, 43646, 30814,  6353,  2143,
       43669, 43267, 48995, 46507,  9549, 29299, 46966, 13509, 19810,
       29473, 43290, 44487, 30821,  4856, 30006, 14593, 49663, 27404,
       45946, 16180, 49283, 14048, 47213,  2851, 13766, 40554, 24294,
       35866, 10868, 27130, 43710, 23313, 18670,  8427, 28963, 49426,
       29049,  5337, 46634, 28685, 43984,   834, 49585, 38157, 18527,
       14716, 31935,   325, 44185, 32811, 47706, 16471, 24336, 44235,
       35827, 27510, 30316, 30049,  4172, 19575, 36850, 18744, 31193,
       17319, 28383, 38958, 14964,  8729,  2428, 23060, 19745, 49326,
       13327,  7774, 11418, 38735, 41307, 40808, 47349, 19565, 34519,
       13191, 33615, 29959,  8893, 35207, 24151, 19678, 12981, 42237,
       47824,  4360, 47390, 49989, 41063, 11920, 32189,    97, 31502,
       45565, 10377, 44966,  2849,  5356, 35948,  5816, 31407,   134,
       34238, 41739, 23275, 10771,  6325, 19341, 10832, 21877, 22552,
       12158, 14455, 42753, 48917, 14074, 27538, 40290, 20047, 37309,
       42085, 49401, 43132, 41532, 43918,  2038, 27002, 28335,   364,
        9066,   936, 38452, 44474, 27582,  6429, 21947,  2844, 38542,
       36991, 26578,  1731, 11109, 37440,  4654, 38902, 19202, 21003,
       14249, 27730, 31583, 26818, 20680,  6272,  6831, 35508,  2101,
       15797,  4423,  9583, 16962, 35058, 16252, 29908, 32486, 34128,
        2318, 24338, 35531,  7297, 29999,   323, 36853,    79, 21150,
       27970, 14381, 28541, 29555, 34618, 17860, 29469,  3734, 21181,
        8124, 18495, 40431,  7019, 24923, 22499, 14101, 26158,  9113,
       39256, 37620,  6347,  8389, 26115, 43205, 45332, 43547,  7375,
       26331, 18238, 28122,  4171, 30009, 14855, 13997, 49148, 22906,
       30119, 27509, 38858, 19481, 46588, 23789, 33097, 42802, 40655,
       16777, 49933, 35756, 16981, 10693, 31655, 25300, 46889,  5971,
       28552, 26310, 22930, 29149, 48639, 38028, 34426, 41392,  2149,
       18425, 24807, 38818,  7137, 13890, 11654, 15390, 40298, 42291,
       24510, 31257, 34422, 47546, 15498, 13018,  6145, 18871, 36164,
       14392, 45806, 42660,  2989, 33701, 37907, 37926, 12359, 44431,
       20934, 11748, 24279, 44342,  5022,  3707, 31448, 47047, 12508,
       44018, 23025, 11066, 40640, 25704, 29749,  7848, 20332, 41379,
        2939, 17283, 48448, 41799, 37603, 29889, 11699, 21956, 40455,
       11090, 17238, 49076, 48789, 28194, 33069, 31812, 34926, 16584,
        4590, 31753,  1842, 45863, 40627, 46126, 19967,  5745, 17245,
       37119, 26335, 12077,  1837, 39252,  1678, 32922,  6754, 19701,
       40617, 29310, 49704, 27379, 12852, 15360, 11369, 28395, 11672,
       12305, 33543, 34550, 30193, 44684, 26190,  9477, 29506,  3833,
       30910, 15116, 45266, 11827,   326, 29223, 32385, 36880,  1955,
       40568, 33663, 15534, 15994, 22686,  9406, 46009,  5019,  7036,
       41537, 32859, 45958,  8820, 11875, 45585, 40153, 41909, 33594,
       38844, 35480,  6545, 31508, 13550, 10220, 34033, 35349,  3873,
       13484, 39337, 29457, 17484, 15804,  9367, 46429, 18380, 12434,
        6473, 38508, 42156, 10128, 29434, 48067, 17766,  7041, 23747,
       36576, 29802, 16337, 22266, 13188, 47454, 40982, 21084, 39046,
       24928, 21362, 35831, 11346, 13617, 45169, 12897, 22362, 35506,
        3926, 16177,  7267, 26206, 46092,  4789, 44712, 34942, 48337,
       12657, 39010,  8009, 15110,  4144, 23411, 22065,  7491, 34829,
       18742, 25519, 46913, 34031,  2267, 15179, 21177, 35351, 29408,
       35817,  7950, 21800, 35136, 11303, 18992,  4786, 25168, 22752,
       15991, 33726, 26872, 12273, 32381,  5025, 23563, 12933, 21935,
        4234, 43308, 25396, 27376, 40874, 40927,  5374,  4415, 33044,
       28869, 30469, 23618,  3241,  7820,  3070, 30569, 41749, 23328,
       22528, 28188, 31270,  8212, 22101, 48888, 28956, 38125, 33791,
       27937, 28454, 46948, 17680,   960, 10774, 48532, 31808,  4377,
       26066]), [2, 9, 1, 6])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 5, 1, 6], M=tensor([0, 5, 1, 6], device='cuda:0'), Initial Performance: (0.259, 0.0446454519033432)
DC 1, val_set_size=1000, COIs=[8, 3, 1, 6], M=tensor([8, 3, 1, 6], device='cuda:0'), Initial Performance: (0.249, 0.04491214168071747)
DC 2, val_set_size=1000, COIs=[4, 7, 1, 6], M=tensor([4, 7, 1, 6], device='cuda:0'), Initial Performance: (0.246, 0.04495431101322174)
DC 3, val_set_size=1000, COIs=[2, 9, 1, 6], M=tensor([2, 9, 1, 6], device='cuda:0'), Initial Performance: (0.251, 0.04425755798816681)
D00: 1000 samples from classes {1, 6}
D01: 1000 samples from classes {1, 6}
D02: 1000 samples from classes {1, 6}
D03: 1000 samples from classes {1, 6}
D04: 1000 samples from classes {1, 6}
D05: 1000 samples from classes {1, 6}
D06: 1000 samples from classes {0, 5}
D07: 1000 samples from classes {0, 5}
D08: 1000 samples from classes {0, 5}
D09: 1000 samples from classes {0, 5}
D010: 1000 samples from classes {0, 5}
D011: 1000 samples from classes {0, 5}
D012: 1000 samples from classes {8, 3}
D013: 1000 samples from classes {8, 3}
D014: 1000 samples from classes {8, 3}
D015: 1000 samples from classes {8, 3}
D016: 1000 samples from classes {8, 3}
D017: 1000 samples from classes {8, 3}
D018: 1000 samples from classes {4, 7}
D019: 1000 samples from classes {4, 7}
D020: 1000 samples from classes {4, 7}
D021: 1000 samples from classes {4, 7}
D022: 1000 samples from classes {4, 7}
D023: 1000 samples from classes {4, 7}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO4']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.418, 0.06142205399274826) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.411, 0.06509904915094375) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.0901500660777092) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.08961409723758698) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.46, 0.06931790328025818) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.07178343132138253) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.285, 0.12618956410884857) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.273, 0.1021047906279564) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.467, 0.0813574980199337) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.08620012563467026) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.322, 0.1603555630147457) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.12693029174208642) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.09731255877017975) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.462, 0.11036875423789025) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.379, 0.1944475924372673) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.402, 0.15093016832321882) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.458, 0.12286224584281445) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.15348628028482197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.21103904109448193) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.411, 0.1675949716344476) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO0']
DC 1 --> ['(DO3', '(DO5']
DC 2 --> ['(DO2']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.462, 0.13750439224019648) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.15476095771044493) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.418, 0.21013511917740108) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.459, 0.21405988819152116) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.452, 0.14960554386116565) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.17682396379858256) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.404, 0.2106591824889183) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.462, 0.2431645128428936) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.453, 0.15374461515340954) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.18366238740086555) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.419, 0.24523430716246367) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.27802466233447193) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.16089225522428752) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.47, 0.19264739524573088) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.417, 0.2547052926644683) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.2662605528570712) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.472, 0.17724084822554143) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.468, 0.1974615651369095) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.416, 0.2696862921901047) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.458, 0.29413447110913693) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[1, 6], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.882, 0.01609993854165077)
DC Expert-0, val_set_size=500, COIs=[0, 5], M=tensor([0, 5, 1, 6], device='cuda:0'), Initial Performance: (0.944, 0.0053529364764690395)
DC Expert-1, val_set_size=500, COIs=[8, 3], M=tensor([8, 3, 1, 6], device='cuda:0'), Initial Performance: (0.936, 0.005041519492864609)
DC Expert-2, val_set_size=500, COIs=[4, 7], M=tensor([4, 7, 1, 6], device='cuda:0'), Initial Performance: (0.832, 0.01533126512914896)
DC Expert-3, val_set_size=500, COIs=[9, 2], M=tensor([2, 9, 1, 6], device='cuda:0'), Initial Performance: (0.916, 0.007196049313992262)
SUPER-DC 0, val_set_size=1000, COIs=[0, 5, 1, 6], M=tensor([0, 5, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[8, 3, 1, 6], M=tensor([8, 3, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[4, 7, 1, 6], M=tensor([4, 7, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[2, 9, 1, 6], M=tensor([2, 9, 1, 6], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x7f32203b91f0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f322047afa0>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f322c117e80>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f322076bc70>, <fl_market.actors.data_consumer.DataConsumer object at 0x7f322cc89f70>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.006445269078016281) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.0058631349164061245) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.926, 0.007369664035737514) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.015653792336583136) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.932, 0.009287841427372769) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.547, 0.050795930441468955) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.552, 0.07709999161958694) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.502, 0.05031405353546142) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.489, 0.1797991826236248) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9505, 0.004316524667665363) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.005520698878914118) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.006747136060148477) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.015064482599496842) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.93, 0.007247727146372199) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.595, 0.0411306115090847) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.594, 0.056574709050357344) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.601, 0.04102335196733475) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.507, 0.09343599458411336) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.0041415336290374395) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.005289361756760627) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.00612120446562767) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.852, 0.01525129397213459) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.94, 0.006716595630627126) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.731, 0.024238391131162645) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.649, 0.03722249153256416) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.59, 0.04546395319700241) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.584, 0.051403731115162374) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.002973830804694444) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.006243309191195295) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.007770657600834965) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.014036418348550796) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.936, 0.007545819563325494) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.733, 0.02567674732208252) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.623, 0.061965431459248065) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.666, 0.035792200952768326) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.613, 0.03938980630040169) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.963, 0.0037012425282155165) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.0059140135084744545) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.934, 0.00783152299374342) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.0154050003439188) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.936, 0.0077189549780450764) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.733, 0.026639016449451446) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.644, 0.053913314312696456) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.644, 0.036955202460289) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.612, 0.04521228820085525) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.003245991866569966) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.005932647455018014) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.008320747688412667) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.852, 0.015875172965228557) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.936, 0.007942992221564055) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.725, 0.0269991315305233) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.655, 0.047488531082868574) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.658, 0.038300505489110945) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.646, 0.03519877073168755) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.966, 0.003406046995020006) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005851992154843174) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.007513638071715832) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.848, 0.015663083389401436) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.928, 0.00904262419999577) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.705, 0.02961384901404381) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.662, 0.04092483478784561) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.675, 0.03676444536447525) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.632, 0.041200685143470764) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.0040050423211396265) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.0057749702429864555) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.009379917684709653) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.016127162113785745) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.932, 0.00861980075982865) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.693, 0.03469860502891243) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.644, 0.05707345283031463) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.635, 0.04647459475696087) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.041467139229178426) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.003153658877679845) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.006984461134066805) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.009132064283825457) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.016566954016685485) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.934, 0.008302702634828164) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.745, 0.026326444387435914) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.668, 0.04178254196047783) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.658, 0.03983251619338989) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.639, 0.039694714099168776) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.0028582476929877886) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005336791640263982) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.010567793183028699) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.019334185689687727) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.936, 0.00796994288568385) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.722, 0.029249932929873466) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.607, 0.06753907273709774) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.697, 0.03400136277079582) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.671, 0.037321833819150926) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9725, 0.0034007300738157936) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007122862907010131) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.008876426480710506) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.852, 0.01777329236268997) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.932, 0.008548626611474902) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.732, 0.02716569750010967) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.64, 0.04455553811788559) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.659, 0.04066576704382897) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.616, 0.045795697957277295) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.0037815659220868836) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005875545090297237) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.009662198450416326) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.01970019755885005) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.93, 0.009710031008347868) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.742, 0.026637971714138984) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.637, 0.0522280433177948) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.623, 0.049525929749011995) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.645, 0.03872542467713356) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.0032321451612951934) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.007528890931804199) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.008258563332259656) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.02100730648636818) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.924, 0.008774555820738897) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.704, 0.03359039270877838) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.622, 0.06581331197917462) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.696, 0.035649652063846585) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.04385368824005127) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.0040873937707401635) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006367735988809727) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.008792135320836678) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.86, 0.019217968486249445) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.928, 0.010553744820528664) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.723, 0.030828103631734848) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.653, 0.047078115284442903) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.655, 0.04307818266749382) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.629, 0.043171667397022245) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.004000014912428014) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.005931953891180456) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.008568479967070743) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.844, 0.020136353511363267) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.934, 0.008995208046399058) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.646, 0.04713644970580935) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.574, 0.06920741181075572) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.649, 0.041760530188679694) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.638, 0.048794257763773205) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.974, 0.003989766134716774) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.944, 0.006675112831755542) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.011047195712570101) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.016319036655128002) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.928, 0.010823996834922581) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.717, 0.0313970894664526) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.0647716373205185) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.66, 0.04442910447716713) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.604, 0.05042414002120495) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.004037625695182214) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.0064291227797511966) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.011077205649227834) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.016372607797384264) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.932, 0.008193422585027293) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.725, 0.032839372184127566) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.639, 0.045868532568216325) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.663, 0.040677474722266196) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.622, 0.04988641519099474) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.0037453659458478797) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.934, 0.0070760561813949605) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.010515685725025833) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.01921632458269596) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.93, 0.010151172986079473) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.744, 0.028571440357714892) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.62, 0.05610110062360764) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.649, 0.04073577809333801) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.621, 0.043325917199254035) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9725, 0.004262971116490008) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.00655973504926078) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.008889835293171928) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.019615273267030716) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.934, 0.007826351794879883) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.685, 0.0364359865905717) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.596, 0.05366685727238655) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.671, 0.043482642993330954) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.622, 0.0525171035900712) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9745, 0.004324698249227367) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.938, 0.006708329575136304) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.008696995710313786) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.01898470105230808) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.932, 0.009487985744839535) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.681, 0.03776028340961784) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.06484624594449998) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.681, 0.03732343593239784) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.648, 0.04025281718373298) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9755, 0.003726706563917105) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.006852858778089285) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.009116775594651698) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.854, 0.022314638733863832) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.932, 0.009332376032427418) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.703, 0.03314370623603463) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.601, 0.06062482556700707) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.649, 0.04746254348754883) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.632, 0.044553053125739095) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9735, 0.0033601955703488782) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.942, 0.0070590453611221165) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.00835190072003752) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.019660787366330625) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.938, 0.009631985758634982) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.718, 0.03130707121267915) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.564, 0.06858906842768192) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.611, 0.05405946865677833) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.597, 0.056035902373492714) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.004521376990769568) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006212637046584859) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.008451730409637094) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.017075588505715132) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.944, 0.008784913062350824) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.657, 0.044318756341934204) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.586, 0.06999039927497507) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.671, 0.03945934942364693) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.65, 0.04275301340222359) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.003694759226833412) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006730022837058641) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.009401152236387133) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.85, 0.02197173860669136) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.94, 0.007523030549287796) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.689, 0.03834298157691956) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.595, 0.06135682947188616) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.622, 0.06203891667723656) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.611, 0.050044659845530984) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.0039987148940363116) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005891998519073241) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.010096957305446267) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.852, 0.017434510976076126) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.944, 0.00758543219731655) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.694, 0.03846064468286931) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.07371649321913719) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.636, 0.04491434951126576) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.62, 0.04674349550157785) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9735, 0.004253238522241645) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006888928944827058) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.009104902544990181) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.858, 0.02075147121399641) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.938, 0.009523193307803013) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.68, 0.038660501297563314) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.08283391879778355) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.616, 0.054957449540495876) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.603, 0.05540685974061489) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.0048268325655585614) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.00511895182725857) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.010250031858682632) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.019316941864788533) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.006411076900083572) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.651, 0.046326700419187544) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.07908149050548673) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.604, 0.06354393991827965) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.614, 0.058002731740474704) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.973, 0.004173516445936912) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006077810584567488) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.936, 0.014188420291087822) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.862, 0.020472938790917397) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.942, 0.008903759190114215) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.72, 0.031928596768528224) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.606, 0.057230159670114515) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.622, 0.05435780891776085) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.652, 0.043589647948741914) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.973, 0.004208288342979359) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.94, 0.007463137209415435) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.009453282161615789) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.01938334011659026) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.95, 0.009662201651735813) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.044848692521452904) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.613, 0.056404383689165116) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.654, 0.04235048687458038) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.632, 0.04885575746744871) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.004129204218211271) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.005887427314533852) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.010384163242124487) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.020426824778318406) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.938, 0.009878711487166583) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.699, 0.03546481452137232) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.633, 0.05018753683567047) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.617, 0.059147969394922256) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.623, 0.04795552572607994) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.003879994339778932) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.006262978773796931) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.010915309060830623) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.856, 0.01781584835797548) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.94, 0.00925665114633739) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.707, 0.030841476574540137) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.663, 0.04546616262197495) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.658, 0.03992126697301865) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.638, 0.05368132098764181) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.004213575460526044) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.007458271195471752) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.011495733319781721) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.01831636341661215) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.942, 0.00912500031106174) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.711, 0.03379925364255905) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.571, 0.07896297708153725) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.657, 0.04242586433887482) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.641, 0.04519142985343933) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.003877565552900705) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.946, 0.008647486928472062) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.013290050853975117) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.019717701822519303) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.938, 0.010212708079779986) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.67, 0.04183044853806496) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.592, 0.0607987744435668) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.629, 0.054665666073560715) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.598, 0.06773023445159197) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9685, 0.004537969423968207) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.005263385591446422) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.011887148929876275) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.868, 0.018356940388679504) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.932, 0.010412546317093074) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.677, 0.040702689379453656) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.639, 0.04985949888825417) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.652, 0.04315918904542923) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.649, 0.04453566288948059) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.004456207509916567) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006285605265060439) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.938, 0.014172440468333661) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.020273000363260508) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.934, 0.009213439290761016) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.672, 0.0363576976954937) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.615, 0.04713082504272461) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.649, 0.042778992295265196) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.677, 0.038530497208237646) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.005214471088901519) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.006063428221619688) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.010779245989397168) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.021038888706825673) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.936, 0.007962681191042065) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.695, 0.04090597329288721) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.06406189392507076) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.638, 0.0485488515496254) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.649, 0.04780976603925228) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.005099144968894052) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.006500333520991262) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.010530963988974691) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.018403369769454003) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.934, 0.010503384063777049) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.641, 0.049504916523583235) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.06615509567409754) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.638, 0.049913161642849445) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.592, 0.06392533089220524) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9735, 0.004150885646857205) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006917652946562157) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.012343623287975789) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.020723763052374125) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.936, 0.01159300668763899) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.599, 0.07473216200899332) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.625, 0.048843277022242546) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.629, 0.05067667219042778) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.629, 0.05431710361829028) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.968, 0.0058497768698002805) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005745627001713729) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.014522174863144756) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01808152089267969) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.009485841430840082) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.708, 0.0335091001316905) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.591, 0.0638135938346386) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.657, 0.03972252354025841) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.601, 0.05497646573185921) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.004959492393625169) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.006769994451649836) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.013360178124625236) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.020450633559376) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.938, 0.010385309595323633) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.68, 0.040842896204441786) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.618, 0.059538896352052686) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.678, 0.04091231420636177) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.635, 0.04371427950263023) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.259, 0.0446454519033432), (0.418, 0.06142205399274826), (0.46, 0.06931790328025818), (0.467, 0.0813574980199337), (0.461, 0.09731255877017975), (0.458, 0.12286224584281445), (0.462, 0.13750439224019648), (0.452, 0.14960554386116565), (0.453, 0.15374461515340954), (0.469, 0.16089225522428752), (0.472, 0.17724084822554143), (0.547, 0.050795930441468955), (0.595, 0.0411306115090847), (0.731, 0.024238391131162645), (0.733, 0.02567674732208252), (0.733, 0.026639016449451446), (0.725, 0.0269991315305233), (0.705, 0.02961384901404381), (0.693, 0.03469860502891243), (0.745, 0.026326444387435914), (0.722, 0.029249932929873466), (0.732, 0.02716569750010967), (0.742, 0.026637971714138984), (0.704, 0.03359039270877838), (0.723, 0.030828103631734848), (0.646, 0.04713644970580935), (0.717, 0.0313970894664526), (0.725, 0.032839372184127566), (0.744, 0.028571440357714892), (0.685, 0.0364359865905717), (0.681, 0.03776028340961784), (0.703, 0.03314370623603463), (0.718, 0.03130707121267915), (0.657, 0.044318756341934204), (0.689, 0.03834298157691956), (0.694, 0.03846064468286931), (0.68, 0.038660501297563314), (0.651, 0.046326700419187544), (0.72, 0.031928596768528224), (0.67, 0.044848692521452904), (0.699, 0.03546481452137232), (0.707, 0.030841476574540137), (0.711, 0.03379925364255905), (0.67, 0.04183044853806496), (0.677, 0.040702689379453656), (0.672, 0.0363576976954937), (0.695, 0.04090597329288721), (0.641, 0.049504916523583235), (0.599, 0.07473216200899332), (0.708, 0.0335091001316905), (0.68, 0.040842896204441786)]
TEST: 
[(0.259, 0.043567125290632246), (0.40925, 0.05918567603826523), (0.46025, 0.06647206515073777), (0.46125, 0.07821208986639977), (0.4535, 0.09296726769208909), (0.4575, 0.11661085388064385), (0.45925, 0.12995803904533387), (0.44975, 0.14063709473609926), (0.44975, 0.1442626576423645), (0.4685, 0.15252220165729521), (0.4685, 0.16789883202314376), (0.56375, 0.04744403874874115), (0.609, 0.039006187424063685), (0.73525, 0.023789378546178342), (0.73375, 0.025072583347558974), (0.74375, 0.025279256962239743), (0.73775, 0.02620972215384245), (0.7085, 0.029939078994095324), (0.716, 0.03413687486946583), (0.7425, 0.026001547887921334), (0.7385, 0.027952223155647517), (0.74725, 0.026229475609958173), (0.74625, 0.026513108301907777), (0.71725, 0.031793577767908573), (0.747, 0.029840050034224987), (0.6635, 0.0453214595913887), (0.73025, 0.030628673039376737), (0.725, 0.03181673077307642), (0.74575, 0.029446162644773722), (0.7175, 0.03514555843546987), (0.70675, 0.03723326659947634), (0.7115, 0.032387587759643795), (0.72725, 0.03074799384176731), (0.652, 0.0423173062056303), (0.69125, 0.03639524351805448), (0.693, 0.03779797717183828), (0.6905, 0.03641696514934301), (0.65725, 0.044706114932894704), (0.71725, 0.032857639940455555), (0.673, 0.04388725976645946), (0.69, 0.035472718171775344), (0.73525, 0.030151826161891222), (0.71975, 0.03347749733552337), (0.66475, 0.04236150775104761), (0.67, 0.04271894580870867), (0.6945, 0.0352633601538837), (0.67725, 0.04152569975703955), (0.6445, 0.047566931933164594), (0.6035, 0.07210479241609573), (0.70525, 0.034095645152032374), (0.68025, 0.040022138997912404)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.91      0.52      0.66      1000
           1       0.72      0.86      0.78      1000
           5       0.63      0.63      0.63      1000
           6       0.58      0.71      0.64      1000

    accuracy                           0.68      4000
   macro avg       0.71      0.68      0.68      4000
weighted avg       0.71      0.68      0.68      4000

Collaboration_DC_1
VAL: 
[(0.249, 0.04491214168071747), (0.411, 0.06509904915094375), (0.459, 0.07178343132138253), (0.462, 0.08620012563467026), (0.462, 0.11036875423789025), (0.467, 0.15348628028482197), (0.47, 0.15476095771044493), (0.467, 0.17682396379858256), (0.467, 0.18366238740086555), (0.47, 0.19264739524573088), (0.468, 0.1974615651369095), (0.552, 0.07709999161958694), (0.594, 0.056574709050357344), (0.649, 0.03722249153256416), (0.623, 0.061965431459248065), (0.644, 0.053913314312696456), (0.655, 0.047488531082868574), (0.662, 0.04092483478784561), (0.644, 0.05707345283031463), (0.668, 0.04178254196047783), (0.607, 0.06753907273709774), (0.64, 0.04455553811788559), (0.637, 0.0522280433177948), (0.622, 0.06581331197917462), (0.653, 0.047078115284442903), (0.574, 0.06920741181075572), (0.589, 0.0647716373205185), (0.639, 0.045868532568216325), (0.62, 0.05610110062360764), (0.596, 0.05366685727238655), (0.589, 0.06484624594449998), (0.601, 0.06062482556700707), (0.564, 0.06858906842768192), (0.586, 0.06999039927497507), (0.595, 0.06135682947188616), (0.562, 0.07371649321913719), (0.545, 0.08283391879778355), (0.545, 0.07908149050548673), (0.606, 0.057230159670114515), (0.613, 0.056404383689165116), (0.633, 0.05018753683567047), (0.663, 0.04546616262197495), (0.571, 0.07896297708153725), (0.592, 0.0607987744435668), (0.639, 0.04985949888825417), (0.615, 0.04713082504272461), (0.604, 0.06406189392507076), (0.6, 0.06615509567409754), (0.625, 0.048843277022242546), (0.591, 0.0638135938346386), (0.618, 0.059538896352052686)]
TEST: 
[(0.25425, 0.04382993084192276), (0.417, 0.06269155278801918), (0.465, 0.06890970841050148), (0.4675, 0.0824232129752636), (0.466, 0.1052005781531334), (0.47175, 0.14798526340723037), (0.474, 0.14918128669261932), (0.47475, 0.1711675282716751), (0.474, 0.17799552154541015), (0.4715, 0.1876283633708954), (0.47575, 0.19261900579929353), (0.5425, 0.07415766942501067), (0.57025, 0.05470945443212986), (0.657, 0.03329551127552986), (0.621, 0.05546140693128109), (0.638, 0.047618829369544986), (0.66125, 0.041609659284353256), (0.67875, 0.036226610489189626), (0.64625, 0.053739584110677245), (0.67875, 0.037436376705765724), (0.6035, 0.06116001629829407), (0.65325, 0.039987615399062634), (0.6395, 0.04872925935685635), (0.6185, 0.061039509862661365), (0.6515, 0.04613944286108017), (0.582, 0.06456459525227547), (0.5875, 0.05936129716038704), (0.6385, 0.04375018447637558), (0.62775, 0.0529689554721117), (0.615, 0.051193055555224416), (0.5945, 0.06154584170877934), (0.60575, 0.056198512613773344), (0.5795, 0.06484541717171668), (0.58975, 0.06550524561107159), (0.6135, 0.05667273646593094), (0.5775, 0.06762153024971485), (0.54875, 0.07948398408293723), (0.5615, 0.07633242385089398), (0.6145, 0.05517745618522167), (0.61925, 0.05225618189573288), (0.64125, 0.046765637323260306), (0.671, 0.03947292345017195), (0.5835, 0.07024767120182514), (0.597, 0.05567786987125874), (0.639, 0.04593996322154999), (0.66125, 0.03921687413752079), (0.61125, 0.05759962084889412), (0.58575, 0.06357991530001164), (0.63675, 0.04604561376571655), (0.593, 0.05819135357439518), (0.6045, 0.058238862842321396)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.59      0.88      0.70      1000
           3       0.64      0.38      0.47      1000
           6       0.53      0.81      0.65      1000
           8       0.90      0.35      0.50      1000

    accuracy                           0.60      4000
   macro avg       0.66      0.60      0.58      4000
weighted avg       0.66      0.60      0.58      4000

Collaboration_DC_2
VAL: 
[(0.246, 0.04495431101322174), (0.25, 0.0901500660777092), (0.285, 0.12618956410884857), (0.322, 0.1603555630147457), (0.379, 0.1944475924372673), (0.396, 0.21103904109448193), (0.418, 0.21013511917740108), (0.404, 0.2106591824889183), (0.419, 0.24523430716246367), (0.417, 0.2547052926644683), (0.416, 0.2696862921901047), (0.502, 0.05031405353546142), (0.601, 0.04102335196733475), (0.59, 0.04546395319700241), (0.666, 0.035792200952768326), (0.644, 0.036955202460289), (0.658, 0.038300505489110945), (0.675, 0.03676444536447525), (0.635, 0.04647459475696087), (0.658, 0.03983251619338989), (0.697, 0.03400136277079582), (0.659, 0.04066576704382897), (0.623, 0.049525929749011995), (0.696, 0.035649652063846585), (0.655, 0.04307818266749382), (0.649, 0.041760530188679694), (0.66, 0.04442910447716713), (0.663, 0.040677474722266196), (0.649, 0.04073577809333801), (0.671, 0.043482642993330954), (0.681, 0.03732343593239784), (0.649, 0.04746254348754883), (0.611, 0.05405946865677833), (0.671, 0.03945934942364693), (0.622, 0.06203891667723656), (0.636, 0.04491434951126576), (0.616, 0.054957449540495876), (0.604, 0.06354393991827965), (0.622, 0.05435780891776085), (0.654, 0.04235048687458038), (0.617, 0.059147969394922256), (0.658, 0.03992126697301865), (0.657, 0.04242586433887482), (0.629, 0.054665666073560715), (0.652, 0.04315918904542923), (0.649, 0.042778992295265196), (0.638, 0.0485488515496254), (0.638, 0.049913161642849445), (0.629, 0.05067667219042778), (0.657, 0.03972252354025841), (0.678, 0.04091231420636177)]
TEST: 
[(0.2375, 0.04377405685186386), (0.25, 0.08629248949885368), (0.2875, 0.1204801225066185), (0.33075, 0.15272677546739577), (0.383, 0.18404866141080856), (0.396, 0.1998128536939621), (0.4165, 0.19864237356185913), (0.41075, 0.20010809701681137), (0.4185, 0.232751971244812), (0.4215, 0.2435290548801422), (0.42125, 0.25688072097301484), (0.49325, 0.04898611567914486), (0.5845, 0.04019800987839699), (0.598, 0.042788520053029064), (0.6505, 0.03443526145815849), (0.62425, 0.03779074862599373), (0.63775, 0.039274704530835154), (0.67175, 0.0363140382617712), (0.62525, 0.046440820485353466), (0.64675, 0.03836131717264652), (0.68475, 0.03278308605402708), (0.65075, 0.0386874770373106), (0.60725, 0.04686629457771778), (0.6795, 0.035481575064361094), (0.6315, 0.04404978582262993), (0.63925, 0.0417269741743803), (0.636, 0.04741902785003185), (0.649, 0.04106434446573257), (0.635, 0.04042578058689833), (0.65825, 0.044794448778033256), (0.67675, 0.03620380161702633), (0.64125, 0.0439026505202055), (0.6265, 0.048909996300935746), (0.67175, 0.038201782196760174), (0.62375, 0.056347114562988285), (0.65275, 0.041342015780508515), (0.61525, 0.050922768235206604), (0.60575, 0.058146410569548605), (0.61425, 0.05104665443301201), (0.65075, 0.03993709869682789), (0.61875, 0.05493184846639633), (0.64475, 0.041280170150101186), (0.65525, 0.040205401480197904), (0.63225, 0.04995908857882023), (0.654, 0.04149900634586811), (0.653, 0.03998517183959484), (0.648, 0.04429467509686947), (0.6365, 0.045279942557215694), (0.629, 0.04605379563570022), (0.663, 0.03969324767589569), (0.66725, 0.041299662843346595)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.85      0.87      0.86      1000
           4       0.59      0.41      0.48      1000
           6       0.49      0.78      0.61      1000
           7       0.87      0.61      0.71      1000

    accuracy                           0.67      4000
   macro avg       0.70      0.67      0.67      4000
weighted avg       0.70      0.67      0.67      4000

Collaboration_DC_3
VAL: 
[(0.251, 0.04425755798816681), (0.25, 0.08961409723758698), (0.273, 0.1021047906279564), (0.392, 0.12693029174208642), (0.402, 0.15093016832321882), (0.411, 0.1675949716344476), (0.459, 0.21405988819152116), (0.462, 0.2431645128428936), (0.46, 0.27802466233447193), (0.463, 0.2662605528570712), (0.458, 0.29413447110913693), (0.489, 0.1797991826236248), (0.507, 0.09343599458411336), (0.584, 0.051403731115162374), (0.613, 0.03938980630040169), (0.612, 0.04521228820085525), (0.646, 0.03519877073168755), (0.632, 0.041200685143470764), (0.625, 0.041467139229178426), (0.639, 0.039694714099168776), (0.671, 0.037321833819150926), (0.616, 0.045795697957277295), (0.645, 0.03872542467713356), (0.625, 0.04385368824005127), (0.629, 0.043171667397022245), (0.638, 0.048794257763773205), (0.604, 0.05042414002120495), (0.622, 0.04988641519099474), (0.621, 0.043325917199254035), (0.622, 0.0525171035900712), (0.648, 0.04025281718373298), (0.632, 0.044553053125739095), (0.597, 0.056035902373492714), (0.65, 0.04275301340222359), (0.611, 0.050044659845530984), (0.62, 0.04674349550157785), (0.603, 0.05540685974061489), (0.614, 0.058002731740474704), (0.652, 0.043589647948741914), (0.632, 0.04885575746744871), (0.623, 0.04795552572607994), (0.638, 0.05368132098764181), (0.641, 0.04519142985343933), (0.598, 0.06773023445159197), (0.649, 0.04453566288948059), (0.677, 0.038530497208237646), (0.649, 0.04780976603925228), (0.592, 0.06392533089220524), (0.629, 0.05431710361829028), (0.601, 0.05497646573185921), (0.635, 0.04371427950263023)]
TEST: 
[(0.2505, 0.04323132234811783), (0.25, 0.08614569437503815), (0.2725, 0.09757168874144555), (0.40125, 0.1206749073266983), (0.39925, 0.14345088809728623), (0.415, 0.15829271918535232), (0.46625, 0.20534850442409516), (0.4725, 0.2301350491642952), (0.46875, 0.26313387537002564), (0.4705, 0.2506633816361427), (0.4695, 0.2793130035400391), (0.4905, 0.16111542946100235), (0.52775, 0.08562729161977768), (0.579, 0.05034293141961098), (0.61475, 0.036521432399749754), (0.6315, 0.043586676880717275), (0.65775, 0.03341469150036573), (0.6385, 0.038644304506480695), (0.6315, 0.040433348506689074), (0.64125, 0.036715415567159654), (0.678, 0.034963262140750885), (0.6215, 0.04498926636576653), (0.64575, 0.03766694256663322), (0.6435, 0.04186118514835834), (0.626, 0.040680077105760576), (0.612, 0.04714854320883751), (0.61975, 0.048045546054840085), (0.611, 0.04976771302521229), (0.6195, 0.04263566344231367), (0.60775, 0.051047194555401805), (0.6365, 0.040496151626110075), (0.63925, 0.04073861303925514), (0.592, 0.05423267094790935), (0.645, 0.0408120673596859), (0.60725, 0.04790275691449642), (0.606, 0.0469625151604414), (0.60575, 0.05089420691132546), (0.60875, 0.053917662903666494), (0.64875, 0.038858568727970125), (0.6315, 0.04451594713330269), (0.629, 0.04576030793786049), (0.6255, 0.04904253387451172), (0.631, 0.04540519236028195), (0.58325, 0.0664161945283413), (0.6595, 0.041565906524658205), (0.67375, 0.03666859177500009), (0.643, 0.04500670407712459), (0.59525, 0.06049029250442982), (0.606, 0.05122097222507), (0.5985, 0.05342416615784168), (0.625, 0.04331011462211609)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.57      0.75      0.65      1000
           2       0.64      0.66      0.65      1000
           6       0.66      0.70      0.68      1000
           9       0.68      0.39      0.50      1000

    accuracy                           0.62      4000
   macro avg       0.64      0.62      0.62      4000
weighted avg       0.64      0.62      0.62      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [42]
name: alliance-4-dcs-42
score_metric: contrloss
aggregation: <function fed_avg at 0x7491316d0c10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=42
Partitioning data
[[6, 3, 1, 0], [4, 9, 1, 0], [2, 7, 1, 0], [5, 8, 1, 0]]
[(array([44073, 21183, 23596, 47728, 22362, 44695, 31128, 11101, 38769,
        9468,  7705, 30033, 16760, 33704,   313,  4389,  8937, 40664,
       11672, 12356, 23451,   326,  9481, 31835,  9625, 38853, 40544,
       13754, 33212, 15185,   347, 14513, 38677,  1596, 13137, 27554,
       36188, 47513, 39889, 26364, 46971, 40041, 15983, 12491, 13390,
       18884,  5526, 42015, 31564, 19941,  8679, 39252,  7516,  1529,
          22, 22337, 32010, 20734,  8171, 26966, 24289, 36360, 15498,
       17372, 18655, 10058, 47115, 15052, 26940, 36841, 46052,   525,
       17596, 18621, 22795, 25168, 27678, 40685, 41088,  7964, 47396,
       31706, 33824, 13479, 35439, 48841,  8597, 37887, 15612, 15492,
        7392, 25819, 46429, 22384, 18257, 10532,   124, 35928, 12804,
       33731, 14231,  7823, 42460, 39637, 37574, 20717, 42186, 49537,
       22058, 31948,  9367, 34028, 37332,  3833, 25967, 48565, 29798,
        6810,  4143,  2888, 48753,  1360, 43697, 19959, 42690, 11363,
       34805, 20106, 25904, 43581,  3290, 38662, 46188, 29538,  2623,
       23876, 12238, 17317, 34912,  2674,  6749,  7215,  2411,  3048,
        4644, 19955,  9320, 37974,  7867, 45247, 18656, 45900, 34706,
       12550, 40128, 17680,  7265,  2503, 24261,  5223, 32339,  7092,
        2997, 37682, 29241, 11048, 14042, 49200,   154, 32198, 27385,
       34563,  8983, 30143, 13331, 35672, 25067, 35830,   327,  3567,
       40836,  1733, 30193, 29680, 11644, 28322, 45305, 10938, 22065,
       26240, 36788, 32082, 11466, 41641, 42778,  5921, 49082, 14257,
        3861, 30323,  9126, 23771, 48658, 49094, 26054, 39986,  7003,
        7968, 14894, 48607, 29123,  1228, 35732,  6125,  3227,  3780,
       43661, 49059, 38166, 17759,  3849,  6859, 10029, 39678, 33488,
         409, 15192, 48047, 36285, 37033, 49829, 10828, 24852, 35904,
       11875, 40462, 30074, 23787, 14749, 17151, 48446, 40797, 17603,
        3266,  2115, 36486, 12996,   125, 14889, 36282, 11192, 42141,
        3178, 42131,   315, 42949, 20520, 39924, 26609, 38931, 33925,
       32215, 37175, 37162, 30708, 48686, 20868, 39609,  3763, 45152,
       19465, 40314,  2525, 34407, 28717, 43813, 10869, 31626,  3320,
       39166, 22922, 39130,  7049, 41918, 11211, 26357, 31535, 39500,
       33422, 36212,  9304, 43123, 28472, 21070,  4745, 38317, 10967,
       32451,  9976, 10843, 25882,  8656, 15912,  4657,  7909, 37770,
        6557,  6167, 36664, 43283, 33371, 37187, 30838, 35009,   159,
       42647, 36781, 10254, 23995,  5389, 30683, 15175,  1030, 32392,
       21574,  7412, 25492, 41952, 46568, 29692, 21909, 37215, 35163,
       10468,  1573, 18452, 32145,  6071, 27270, 13552, 35124,  5610,
       44047, 13471, 35071, 11028, 29235,   895,  9694, 38702,  7230,
         367, 34682, 23322, 15844, 10611, 14125, 22701, 38879, 44859,
        5578, 49468,  9052, 48315, 26154, 14574,  4439, 11449, 39822,
       47819, 18889, 15083, 46402,  7906,  8781, 39871, 49855, 31141,
        9710, 42789, 12821,  1059, 46594, 39150, 20601, 21415, 20631,
       10746, 36930,  4449, 42206, 29177, 46880,  4428, 47217, 12601,
         258,  2723, 14578, 21524,  1265, 22030, 47413, 35804, 46109,
       31261, 14155, 26711, 16827, 29950, 47523, 42223, 39420, 29004,
       30479, 43744, 19187, 49582, 20496, 15954, 31479, 44103, 32043,
        5859, 30586, 45379, 47851, 27619, 40584, 12013, 42865, 13678,
       17449, 46232,  7460,  9609, 24082, 16705,  5870, 39300, 19352,
        6304,  3447, 12776,  3591, 35976,  8255, 37109, 15383, 10948,
        6200, 26126, 17841, 39220, 26472, 17018, 36745, 43554, 23977,
       41369, 26575,  3289, 21015, 29074, 36051, 19847, 17124, 39575,
        8330, 17879, 43034, 26859, 34756, 17814,  8507, 21156, 30737,
       12174,  4564, 25933, 21147, 15397, 25535, 46838,  9738,    74,
        8056, 42633, 19820, 14122,   334,  5065, 48601, 37863, 17565,
       22008, 41057, 28731,  7128, 12454, 36929, 21685, 43908, 46396,
       12773, 48122, 38084, 42606,  6751, 18908,  8812, 30135, 19522,
       30182,  9189,  2180, 15583, 45214,   354, 11750, 30154, 24725,
       24463, 47109, 15770, 48358,  7255, 24400,  8232, 10190, 23921,
       24620, 47266, 28671, 28615, 36641, 38577, 21971,  7379, 41922,
        8667, 29188, 37801,  4047, 11407,  2582, 35861, 48595, 38596,
       39953,  6064,  9690,  2143, 35576,  9646, 47616, 20994, 35910,
       13277, 38514, 22399,  2204, 21528, 38725, 38682, 48740, 10292,
       10755,  4648, 25150, 22072, 15867,  9658, 19481, 37588, 12982,
       22111,  1559, 49409, 47470,  4983, 42805,  5197, 37030, 47863,
       42918,  1304, 46674, 30227, 13908,  9138, 16726, 10644, 28910,
       14859, 44448,  3064, 24284, 31737, 25323,  1660, 31512, 41439,
       19533, 30457, 49356, 21094, 12127, 45470, 34794, 35850, 12276,
       26304, 10363, 36791, 12920, 15299, 34347, 17752, 35649,  4100,
       22212, 12451, 13895, 24288, 17309,  9318, 20079,  9595, 28937,
       32087,  9509, 36028, 30747, 19886, 33059, 13608, 47772,  3934,
        8107, 36879,  6779, 40410,  9878, 30570, 11824, 29347, 16766,
       20746,  1907, 20753, 46840, 24774,  5137, 47987, 26141, 13849,
       44146, 23151,  2286,   568, 19354, 49447, 47980, 24833, 31274,
        7147, 31268,  9946, 48820,  9889, 40250, 40713,  4334,  1731,
       36040, 35106, 26467, 48264, 47151, 42413, 37618, 13755, 37179,
       46237, 22564, 33126, 46544, 43655, 24527, 38347, 12628, 43935,
       41701, 38174, 10824, 41471,  7130, 23578, 23864,  4993, 40958,
       18311, 13940, 49438, 16810, 24224, 33198,   536, 25459, 16012,
       21631, 16853, 24520, 36774, 13462, 35366, 24999, 48845, 45228,
       49998, 30560, 32684, 27488, 14549, 34336,   917, 42658, 16723,
        5420, 26794,  5202, 36736, 23232, 30047, 14048, 32665, 16986,
       42437, 10910,   815, 40204, 40595, 39735,  6831,  2775, 39387,
       34809, 30344, 37468,  5969, 40275, 43245, 18009, 23386, 29944,
       29364, 47106, 44599, 48378, 12469, 29596, 26162,  2718, 44906,
         332, 27961, 22026, 28972, 31164, 47678, 12030, 40747, 46576,
       17485, 23478, 36858, 27026, 38344,   349,   628, 47320, 31837,
       45242, 34856, 47691, 16238, 10553, 30977, 47379, 20265,  8382,
        7727, 27104, 26430, 20759, 11989, 18857, 31521, 44294,  6165,
       21881, 23194, 15473, 13326,  5873, 10394, 25337,   752, 26842,
       38551, 40945, 36750, 40287, 32247,  4295,  9014,  9140, 26546,
       18218, 33596, 47991, 17196, 30743,  4957,  4859, 25763,  4759,
       37558, 39716, 28978, 40644, 28208,  4645, 24387, 48470,  4504,
       47802, 31654, 15594,  2490, 23964, 10215,  6085,  2707, 15886,
       29480, 47579, 25350, 10334, 22962, 38855,  9813, 21874,   165,
       13357, 19294, 30378,  3318, 36149,  6242, 44715, 16840,  7660,
       42476, 19945, 36055, 34106, 36163,  8857, 18291, 41238, 39739,
       21825, 48671, 22847,  4861, 41300, 46898, 10043,  6396, 20924,
       23786, 21144, 49420, 23825, 29819, 36836, 42399, 10686, 13051,
       38977, 13835, 31459,  6276, 47550, 14949,  4765, 21195,  2578,
       43292, 45096, 17490, 46200, 19860, 31083, 36623, 12836, 38224,
        3549,  8383, 12767, 32592, 15863, 44167,   940, 42755,  1988,
       14534, 11897, 37628, 19237, 49556, 41258,  3362, 14213, 38656,
       49856, 38187,  8623, 11416, 21469, 23684, 12010, 47711, 36254,
        3618, 40411, 15541, 17054, 11572, 46614,  9493, 34017,  2513,
       47560, 48516, 14194, 25748, 42678, 32194, 42205, 14441, 40022,
        7347, 41731, 37646, 36786, 12429, 39670, 18953,   695, 28456,
       16100, 34919, 18047,  6904, 31488,  2435, 14594, 37729, 29246,
       11783, 24521,   694, 35758, 18677, 37806, 33750, 28636, 40479,
       16230, 49018, 29540,  5533, 18955, 43375,   748, 31119,  8199,
       35343, 39179, 10732,  5699, 35209, 21119, 26269, 27749, 32267,
        1039]), [6, 3, 1, 0]), (array([ 4666, 12143, 39947, 36083, 20608,  5946, 45609, 19936, 21684,
       29799, 48902,  8772, 28174, 30790, 33066, 20342, 43758, 26854,
       43923,  7100,  7062, 42938,  8117, 23393,  7383, 41270, 34261,
       48008, 19957,  5143, 26417, 49207, 14303,  4773,  6475, 27914,
       20485, 30341, 24377, 49024, 48879, 24933, 14664, 13787,  2922,
       35630, 41927, 30616, 16936, 48429, 31799, 18014,  3566, 45311,
       16085, 10385,  2710, 41635, 29131, 32026, 22727, 44473, 14082,
       13292, 44790, 12213, 16155, 22139, 30881, 37234, 43974,  7154,
       27469, 27213, 36896, 24236, 46940, 10629, 25949, 38802, 31433,
       24827, 22619, 29613,  6311,  5364, 26897,  1557, 12797,   982,
       25346, 27663, 28414, 30384,  2976, 32803, 15909, 34784, 27787,
       38353,  3802, 28885, 35925,  9452,  1639,  3393, 41788, 29368,
       22480,  3985, 48533, 44254, 29922, 14385, 14923, 10426, 17179,
       17819,  8523,  5594, 14810,  1535, 47305, 41820, 48478, 49114,
        8378, 31993, 28260, 31514,  2444, 31848,  6594, 33222, 40844,
        8208, 16294,  3970,  8379, 26653,  1148, 26351,  3556, 46339,
        3957, 24241,  7099, 49758, 39061, 27817, 41669, 42969, 22519,
       20956, 23823, 47441, 41097, 48945,  9848,  5721, 48163,  4943,
        6844, 22133, 18912, 34384, 26638, 42418, 12898, 26540, 20192,
       14041,   420, 18821, 36843,  6437, 49352, 34808, 44846, 37871,
       11092, 29756, 34082, 25846, 48884, 13282,  3688, 39734, 15018,
       28649,  3383,  5625, 15105, 26475,  4497, 47928, 23474, 18669,
       38755, 39202,  3141,  1407, 27214, 36533, 34215, 27429,  3326,
       27596, 36914, 45170, 18060, 41547,  6421, 33094,  9771, 30719,
       15674, 46255, 42087, 22246,  1349, 36777, 48521, 18320, 12106,
       36247, 27417, 29083, 21771, 17845,   398, 10865, 36600,   945,
        6532, 36199, 15487, 35066, 31549, 22944, 35809, 18300, 41453,
       49645, 27647, 36976,  1366, 35522,  6144, 27180,  4865, 36734,
       44242, 28459, 22989, 48431, 39556, 23908, 47690, 27717, 29073,
        8323, 29438, 19525, 31909, 36807, 26530, 19486, 48672, 36223,
       11678, 12708, 10071, 25499,  3876, 13588, 43596,  1117, 24609,
       40931, 32273, 16143, 20231, 44013, 26439, 24893, 40315, 34995,
       22009, 32952,  4584, 10092, 23645,  5595, 37600,  9673, 10030,
         749, 35178, 14471,  5300, 42058, 46685, 31458,  8506, 14723,
       20517, 26150, 36887, 32778, 27569, 35203, 39721, 19076, 14954,
       18235, 26050, 41904, 25510, 45055, 18668, 45197, 29238,   443,
       47343, 11258,  1827,  4643, 26627, 13609, 16803, 40693, 23302,
       43325, 18982,  8402,  8754, 28215, 18681, 49833, 40324,  9277,
       41899, 11388,  9959,  4142,  1767,  2935, 44315,  2251, 21887,
       30285, 27581, 10717, 35251, 21067, 36878, 46335,  4665, 29197,
       19432, 16642,  8126, 15221, 23886,  2219, 41539, 11620, 21801,
       23624, 22285, 16152, 21478, 42704, 49181, 31638, 45059, 33547,
       36834,  2129,  5133, 21714,  9027, 30175, 23979, 26121, 16718,
       49427, 23325, 14868,  1437, 40885, 27369, 33746, 28901, 34370,
       26848, 47755, 18968, 42624, 44380, 17747, 37347, 32439, 16773,
       48568, 15978, 25285, 31517, 23961, 41505, 26681, 34774, 42911,
       15384, 19891,  5801, 32084, 45735, 25945, 43104,  8567, 33676,
        9513, 48868, 32041, 27040, 23798, 35676, 44669, 10762,  7629,
       48165, 26763, 43760, 43560, 31768, 36557, 45174,  9936, 14198,
       39930, 24453, 23840, 34402,  3819, 21093,  5394, 46438, 28235,
       15350, 20296, 25936, 11430, 31790, 34423, 48919, 43616, 12986,
         683, 37604,  5382, 10467, 28379,  7583, 44056, 48449, 13208,
       19826, 17600,  3510, 15501, 48921, 45302, 38687, 26466,  6934,
       38035, 36794, 44469, 46351,  5485, 23471, 31012, 49904, 31618,
       13962, 43652, 46852, 35852, 41156, 41436, 47419, 37115, 10781,
       29384, 18488, 31771,  4028, 13780,  7587, 38858,   238, 33505,
       20327, 23431, 41963, 32311, 46519, 28876, 26008, 25389,  9620,
        4354, 10145, 22656, 34595, 10747, 12311, 21418, 48269, 29829,
       44412, 28530, 27802,  2771, 18670, 30912,  2339, 30086, 12360,
       40297,  8191, 38658, 11196, 41105,  7738, 14167, 21283,  7019,
       30103, 22490, 32121, 49674, 42239, 29763, 40431, 33845, 26298,
       40185, 14721, 19575, 13323, 17726, 38549, 37145,  8149, 28008,
       25689, 49652, 36046, 33314, 15261, 35666, 25463, 14900,  1287,
       37301, 32474, 31969,  8189, 37260,  8559, 12123, 27538, 15306,
       46953, 16286, 44876, 18331, 49705, 35757,  3735, 33646, 43520,
       35406, 25544, 45856, 15934, 16774, 40936, 43797, 33292, 39902,
       22217, 49269, 29794, 37995, 25105, 34128, 49769, 44466, 45099,
        4803, 28054, 39912, 18238, 26093, 21215,  6969, 47370, 16309,
       25750,  9478,  6639, 13240, 33561, 38078, 49286, 32003, 18562,
       43988, 42130, 31890, 36086,  9251,  4927, 22586,  2630, 17275,
       31300,  4099, 33951, 31147, 23100, 32235,  5827,    94, 44824,
       24936, 17205, 24146, 18456, 33643, 46794, 49121, 37183, 25069,
       34618, 23524, 26742,  5195, 40964, 19095, 10642, 11515, 23311,
       39990, 27330,  6813, 18844, 48639, 16412, 37327, 34862, 41946,
       28268, 10249, 16956, 14358, 25619, 45988, 39208,  2039,  5635,
       45910, 16958, 17017,  8757, 30236, 24297, 26550, 10458, 19574,
        5895, 21152, 44183, 37512, 25682, 20463, 41862,  4949, 20475,
       21429, 28617,  1586, 38637, 26368,  1790, 43984, 43585, 15855,
       25048, 27260, 49381, 31552, 18649,   874, 48263, 20595, 37461,
       22545, 38463, 49777, 27700,  8628, 42335, 13056,  7349, 46786,
       17106,  2615, 23357, 38231, 13191,  4132,   561, 24075, 19760,
       42976, 43290, 39437, 18160, 15783,  6269,  9584,  2727, 11580,
        2227, 42709, 25007, 34913, 13922, 38157, 42912, 44936, 29283,
       20336,  8085,   947, 27460, 45188, 44319, 40590, 40194, 28133,
       45348, 24735, 36889, 14689, 12192, 29666, 40131,  7470, 25362,
       10855, 29380, 21674, 43002, 47483, 18767, 14488, 22326, 40861,
        8453, 17200, 29854, 36370, 29367, 28653, 32076,  7168,  6374,
       40928, 47805, 44307, 32149, 41355, 47912,  8376, 10323,  4387,
       19632, 19386, 46910, 23767,  9860,  7826,  2006, 24939, 40607,
        1119, 17692, 25198, 32484, 42110, 45123,  2895,   341, 17520,
       17237, 20682,  7547, 11280, 21231, 29378, 24445, 28681,  3940,
        3536, 31728,  5529, 39959, 22800, 30539, 14334,  6015,  3903,
       16838, 15554, 38233, 37049, 39396, 37504, 28311,  7093,  4524,
       21760, 12181, 25457, 19329, 20257,  2613, 18525,   317, 26124,
       41179, 35028, 39431, 34173, 32874, 49510,  4516, 22521, 29444,
        9945, 49656, 37519,  7360,  5642, 40635, 41019, 35927, 45615,
       31450, 10487, 29641, 20233, 31927, 28068, 46839, 24750,  3180,
       48729,  6663,  2478,  1376, 43564, 45106, 30487, 48851,  4281,
        3564, 19277, 44645, 34780,  6328, 23024, 26178, 13817, 26296,
       16559,  4176, 23126, 15236, 21522, 24661, 25422,  5346, 12944,
       42241, 32683, 20473, 30913, 28865, 44552, 21774, 33003,  8478,
       26599, 19662, 39927, 16346, 10031, 43160,  1329, 35599, 18999,
       28665,  8549, 35377, 21227, 37032, 44785, 26391, 39504, 37905,
       13656, 35883,  4574, 35509, 23522,   598, 35763, 43667,   129,
       38307, 27080, 45384,  5094, 24795, 34189, 40343, 36363,  3128,
       48618, 21439, 27703, 13613, 21782, 48071, 46046, 35626, 49423,
       40556, 36415, 34646, 32631, 33509,  5924, 19445, 12764, 49174,
       15044, 12916, 36862, 38478, 43838,  7647,  4348, 47274, 33542,
       19730, 29189, 37250,  2169, 41792, 44619,  6735, 30408, 27315,
       31556,  8569, 29492, 41414, 18211,  3184, 24592,  3042, 17093,
       46367, 30321, 23491,  1382, 48958, 16253,  7196, 36632, 15414,
       41456]), [4, 9, 1, 0]), (array([37315, 14106, 29220, 39412, 37615,  8641, 37821, 20651, 44177,
       33293, 46673, 25455, 25038, 31269, 17779, 48074,  8219, 47587,
       19204, 10417, 43190, 48493, 40189, 33550, 25478, 40256, 15303,
       26990, 13290, 16221, 39931, 34529,  9319, 15751, 40385, 48146,
       31926, 13671,  2557,  8121,  1107, 31941, 18837, 42588, 28403,
       41312,  5922,    54,  3639, 28834, 40148, 15477, 13837, 22618,
       38476,  2247, 15563, 48518,  5016, 32499, 44302, 20880,  7844,
         830, 34495, 15522,  9424, 28224,  4902, 22045, 32141, 39508,
        8341, 43116, 28935, 37403, 14926,  7410, 46491, 14404, 12448,
        8234, 43312, 17826, 36585, 44670,  1334, 18086, 12782, 30102,
       40213, 22287, 42144, 12141, 30874, 26551, 14299, 19343, 43873,
       37613,  4023,  8776,  4719,  7807, 37352, 47335,   218, 30542,
        7919, 34630, 12346, 35460, 27560, 28948, 33183, 30466, 17817,
         411, 31026, 11921, 30023, 33564,  7948, 31040, 13556, 49224,
       18964, 29150, 13727, 40371, 31882,  1110, 19163,  7178, 49397,
       19789,  9227, 18863, 38007,  1789, 48244,  6946, 24902, 35537,
       36726,  3963, 39550, 25901, 15638,  9577, 21836, 41538, 31041,
       30478, 24739, 29581, 39976, 14146,  4491, 40115, 38022, 24706,
       19287, 24904, 17980,  8543, 33081, 19231, 21521, 24978,  9660,
       12721, 47449,  9240, 27580, 12536, 47481, 35734,  9740, 21442,
       25379, 42821, 13408,  5131, 26696, 32103, 14768, 25200,  7452,
       39514, 17206, 13786, 17380, 10474, 19655, 30654, 37549, 31420,
        8032, 23692, 17070, 47492, 14120, 43162, 23237, 43513, 31361,
       17262,  7194, 47533, 48287, 17834, 17844,  6392, 46810, 45341,
       45021, 29637,  7633, 20957, 44218, 23493, 14317,  3254, 32047,
       30669, 35230,  9954, 25743, 35294, 45238, 11528,  6502, 37766,
       31058, 35022, 13448, 43631, 24429, 11991,  7576, 34373, 35862,
       46179,  5527, 49991, 11813, 12656, 19964,  4588, 33450,  9224,
       24908, 39518,  3135, 39755, 30761, 14400, 28945, 26702,   623,
       28446,  4157, 32296,  8681, 18258,  7182, 38611,  8533, 31990,
       47092, 48767, 27748, 39494, 43919, 32673, 11881, 27634, 10259,
       44578, 14503, 22782, 10496,  1898, 47665, 36133, 19849,  2969,
       12658, 45851, 40930, 47153,  6739, 37211, 43671, 18941, 21482,
       18411, 43491, 10077, 48649, 14680, 41700,  1650, 34487, 39378,
       35814, 13124,  4736, 14745,  8814, 29588, 33921, 33151,  3487,
       23621, 14144,  2392, 23506, 20186, 46454, 38252, 30987, 21025,
       30319,  2813, 34899, 13268,  4933, 46348, 16025, 14977,  1276,
       14319, 25213, 32660, 21001, 23860, 45027, 37200, 35108, 40300,
       46988, 23834, 43083, 40912, 19962, 33981, 43580, 25968, 41052,
        7894, 37601, 38955, 49407, 32335,  2000,  4790, 44222, 43515,
         440, 13376, 14333, 21108, 13272, 18491, 41624, 40723, 29137,
       44356, 32696,  5058, 38110, 35975, 47170, 43163, 22031,  1054,
       29916,  6218,  1365,  4238, 40604, 19250, 33397, 47241, 49942,
       27781, 26796, 15296, 33496, 41001, 13423, 45058, 38474, 20016,
       43544, 48826, 19956,  6326, 33349, 14095,  3348, 34584,  6748,
       27916, 24012, 40439, 49264, 31520, 25365, 26032,  1488, 47748,
       19794, 36513, 10386, 10562, 43629,  2554,  6726, 43791, 14983,
       23744,  5737, 10348, 24970, 23053, 34228, 12381, 16914, 39345,
       23292, 30216,  5560, 47455, 48853, 25788, 27915, 17956, 37908,
       30030, 34372, 27086, 43293, 28150, 12683,  3835,  5771, 13058,
       41000, 22267, 14556, 37884, 24547, 15081, 17023,  1490,  7204,
       18601,  8751, 41959, 29062, 23880, 24698,   178, 33855, 22566,
       19461, 45722,   181, 28639, 19326,  5890, 24982,  9778,  8817,
       21822, 35623, 13818, 33684, 32507, 47185,  4076, 34583, 25410,
        5317, 41843, 30935,  3651, 12763, 27930, 29669, 26259, 26343,
       41805, 26017,  8461, 44826, 24954, 15662, 39696, 45859, 41519,
       29044, 46463, 28761,   565, 33532, 10630, 13973, 27024,  7875,
       19226, 44282,  9066, 33625,  3404, 40472, 29485, 28526, 31522,
       37412, 44198,  9822,  6703, 36751, 30446, 33539, 38327,  9365,
       25596, 23514, 30676, 34358, 45613, 24398, 39769, 32319,  9127,
       15609, 38491, 26937, 22822, 46730, 15100, 23379, 47256,  5311,
       23317, 27009,  8801, 13701, 40822, 41133, 25363, 39974, 32083,
       23139, 48515, 29616,  6429,  7102, 45475, 13740, 11007,  1631,
       17746, 24008, 43089, 23164, 34364, 41336, 14311, 11264, 31765,
        1689, 31711, 13509, 16882, 28896, 27545,  7495, 24807, 42113,
       20057, 46679, 24220, 28476,  8475, 39674, 22421, 45977,  4533,
       35635,  3230, 30183, 20811, 39352, 12260, 19898,  8825, 44960,
       40075, 42171, 28187,  9809,  5831, 42669, 11767,  5112, 22316,
       46956,  6683, 13244, 10377, 17209, 32402, 35508, 32486, 48749,
       37937, 20088, 49494, 42925, 14129, 22145, 28297,  3025, 11640,
       26471, 34802,  5515, 19564, 26474, 16278, 34069, 16938, 35264,
       18438, 18907,  8938, 25196, 10655, 25799, 39622, 10090, 36015,
       43547,  8181, 25857, 49993, 22261,  4101,  3922, 20661,  5811,
       21398, 20764, 30083, 45332, 42143,  8977, 22372, 41051,  2568,
       40808, 22037,  1820,  6130,  6668, 28394, 24157, 45317, 44835,
       28163, 27194,  8573, 12849, 26331, 26456,   617, 43618, 22818,
       48128, 13534,  1020, 49973,  5146, 29120, 21280, 21720, 19526,
       45403,  8809, 49977, 15512,  4171, 47839, 27607, 28858, 35357,
       45329,  1320, 25611, 17002, 39490, 40615, 35070, 25697, 27512,
       44751, 21003,  6494, 30120, 29875, 40090, 42150, 43480, 43061,
       29066, 32847, 30166,  4609, 20353, 24228,  8307, 31158, 25135,
       24151,   962,   991, 41248, 37687,  3483, 31925,   206, 18415,
       36593,  5627, 45427, 39275, 22055, 14040, 16953, 31319, 23275,
        5552, 41663,  6959,  5010, 49507, 19858, 10399,  8522, 36386,
       45604, 47468,  9859,  1335,  4991, 25439, 14520, 42309,  5477,
       24543, 42622, 10617, 19749, 36311,  1960, 26045, 35470, 16625,
         179, 46554,   284, 10516, 13649, 44637, 49633,  8292, 44932,
       44350, 12131, 11176, 25489,  9619, 40752, 22993,  2391, 10265,
       47271,  6570,  8889, 14754,  9616, 39628,  2863, 13260,   407,
       11474, 28305, 27646, 47456, 18847, 40190, 36925, 42754,  6428,
       46287,  4096, 39904, 29295, 36491, 44037, 15581, 13436, 22109,
       24212, 28512, 40525, 30426, 17203, 10115, 29099, 25546, 37432,
       21217, 17107, 30895,  1338, 12785, 42934, 17076, 21721, 18630,
        9124, 16564,  9059, 11107,  1514, 34989, 17328, 44609,  2946,
       45358, 32885, 21308, 49439, 15003, 21892,  9181, 44770, 41628,
       47163,  7299, 38618, 42952, 43700,  7925, 11304, 48327, 41164,
       38943, 11122,  9966, 22692, 33229, 25037, 12363, 11255, 49671,
       46876, 12078, 48119,  6373, 44116, 29732, 32403, 32460,   557,
        6920, 22129, 33249, 47969, 47916, 48197, 37493,  9328, 12328,
       32033, 32629,  3053, 38479,  5955, 41740, 44027, 44662, 42784,
       17183, 10811, 38613, 18381, 24258, 14350, 12133, 36252, 35042,
       10847, 40241,  7788, 24360,  7183, 28063,  3859, 28400, 27751,
       15030,  7060,  2967, 46553, 48289,  3382, 10024, 45716, 48359,
       44082, 12660,  3842, 24021, 43811,  3918, 43951, 39104, 25531,
       31339, 34270, 31214, 24802, 28267,  6640, 42867, 15396, 26427,
        3665, 22716, 12435, 25106, 32659, 14023, 17100, 13864, 39926,
       13850, 39899, 36886, 17011, 34568, 45509, 39113, 42563, 35286,
       16892, 32669,  4726,  1473, 10690, 42456, 20537, 20831, 45232,
       36068, 45262, 44145, 30172,  5640, 11224, 48691,  8118, 13876,
        5414, 22458, 45982, 48852, 11276, 12270,  4906, 38617, 25190,
         843,  6954, 14528, 43219, 18603,  9696, 48986, 10326, 16214,
       24367]), [2, 7, 1, 0]), (array([17346, 10744, 37508, 24120, 21089,  9397, 47629, 28022,  8204,
       25333, 15019, 41531, 10119, 39649, 15499,  2963, 31064, 29603,
        8400, 25248, 41098, 46780, 23268, 17186, 37880, 21379,  5201,
       42776, 37589, 47524, 48403,  1411, 49638, 27445,  1545,  5305,
       36378, 22430, 46767, 16698, 11496, 11174, 11636,   784, 48625,
       31132, 26889, 33319, 43991, 45653, 32079, 26543, 12573, 26173,
        5057,  9035, 36308, 13352, 20122, 20615, 37395,  4067, 44154,
       30757,  4424,  3152, 17375, 13111, 10541, 15485, 35748, 39677,
       16643, 24953, 20647, 20020, 49715, 26010, 48645, 13252, 31631,
       44024, 47905, 18939, 39558, 33295, 28754, 22390, 17773, 21148,
       17425, 21961, 14681, 28099, 35390, 28340, 44632, 11753, 18262,
        8502, 20322, 23639, 48161, 34216, 16212, 48135, 25202, 10586,
         450, 24656, 42494, 48208, 26917, 46471, 42152, 25057, 49052,
       30140, 27968, 34757, 47334, 44211,  1331, 20672,  5288, 36127,
       32636, 33795, 45278,  2167, 19105,  7864,  1786, 10294,  4893,
       20939, 32889, 24128, 15915, 14696, 17670, 10740, 49644, 27093,
       11189, 47672,  1810, 24037, 45222, 49333,  1298, 28967, 44112,
       15133, 33768,  6807,   607, 10759, 11396, 18001, 20185, 28306,
       12791,  8276, 22424, 21534, 38851, 31543,  5547, 21075, 27671,
       24006, 45831, 47012,  3748, 33552, 46165, 16496, 28902, 32782,
       20776, 34859, 47781, 28756, 43467, 12326, 21689, 17619, 40681,
        8457, 14663,  4874, 29774, 19252, 18003, 21266, 37706, 20092,
       44033,  1717, 30310, 48477, 17354, 48048,    40, 46010, 19537,
        9987, 20484, 48139, 44229, 43455, 44832, 15544, 20495, 24130,
       48341, 38788, 40310, 39176, 31248, 42645, 45184, 43338, 29660,
       26738,  1033, 21436, 47621, 31240,  8768, 17882, 34403,  8671,
       34887, 27890,  8287, 11259, 38629, 44306, 32452, 36030, 26073,
       12581, 24452, 13190, 42384,  4817, 29816, 40528, 41403,  7668,
       25618,   240, 26980, 25104,  8668, 21036,  4594, 42275, 20962,
       25472, 44720,  9648, 29695,  7496, 15676, 40881, 40137, 26481,
       26437, 30148, 16435, 22255, 23572, 31663, 22920, 12258, 34814,
        3740, 15241,  6756, 10702, 15223,  4652, 10810,  9324, 41314,
       21905,  1771, 44384, 14402, 14883, 16676, 18312, 12172,  2586,
       41264, 42048,  7306,  2166, 18382, 17854, 20150, 45017, 34983,
       28807,  2754, 20492, 42207, 34096, 23962, 34712, 37823, 29493,
        5523, 37994,   777, 11141, 47793, 16738, 32322, 49393, 16381,
        1357, 19195, 34963, 23538, 33231, 49830,  8278,  8445, 16483,
       38458, 43713,  1391, 23839, 12353, 33554, 49184, 22635, 31370,
        5732, 34276, 21456, 27490,  4272, 13004, 12891, 38525, 12201,
       34521, 24093,  9262,  7384, 13388, 13883,  9226, 10096, 41288,
        1994, 37002, 49960, 35150, 40500,  1616, 10098, 32698, 11842,
       17899,  5054, 47212, 29903,  5911, 38524, 43348, 30788,  8045,
       28037, 36584, 26220, 38002, 24787,  4974, 34152, 35864, 14587,
       29661, 45777, 13227, 46001, 40093, 48678, 26923, 34523, 27621,
       49744, 23080,   741,  3065,  2434, 27034, 38436, 40789,  7156,
       22056,  3541, 14245, 16159, 42032, 16898, 20198, 35374, 43899,
       23560, 20209, 35736, 45966, 14711, 41078, 37483,  3881,   456,
       42693, 42325, 19557, 44659, 36553, 27692, 38408, 45626, 25450,
        1720,  1250, 15743,  7790,  8337,  1686,  9826, 25521, 38120,
       15276, 14176,  5360,  7934, 20219, 31996, 36939, 21759, 47992,
       13693, 10088, 30514, 23219, 45648, 32819,  1914,  5427,  8489,
       41225,  1506, 36765, 15927, 16846, 20594, 39507, 30762, 45650,
        9883, 26951, 24271, 40124,  6912, 47445, 22862, 42557, 32543,
       39766, 24281, 22758, 21823,  8968, 23625, 30164, 36893, 39079,
       48299, 12954, 30745, 35274, 18182,  2284, 32800, 14934, 13958,
       43913, 47960, 21579, 38857, 48164, 30949, 12917, 32168, 48496,
       27262, 11571, 40358, 42507,  4050, 21137, 14855, 29791, 14481,
       12977, 45677,  7448, 48915, 17112, 21407, 30902, 49151, 14776,
       25201,  8090, 41489, 29007,  3120, 25488, 34442,  8944, 18594,
       16925,   942, 17255, 12080, 46259, 23901, 41733, 37027,  6851,
       42753, 41445,  6161,  8174, 36237, 31445, 13218, 42292, 39749,
       37203, 25209, 13217, 30712, 41602, 12648,   140, 16543, 17995,
        7913, 10826, 19865, 26288, 45655, 38950, 44726, 25306, 48075,
       28027,  4858, 14145,  7597, 18495, 42652, 46547, 25210, 38919,
       15172, 22710, 30374, 21661,  4565,  2928, 10248, 28810, 31534,
       49435, 17223, 40178, 38482, 10251, 32305, 42833, 27794,  3212,
       48030, 19446, 15740, 33970,  2844, 48294, 14381,  1570, 23662,
       11966,  7288,   997, 32366,  1476, 39039,  8300, 34153, 12283,
       43186, 39940, 35465, 31749, 13924, 21150, 45488, 17750, 43167,
        5076, 26519,  1547, 45923,  4040, 37938, 32252, 12981, 48574,
       26905, 31241, 15655,  1037,   105, 21498, 12158,  6922, 11558,
       37799, 40762,  2795, 19581, 22395, 36537, 29410, 12841, 36868,
       24255, 31288, 20914, 18276, 24561, 11464, 18698, 26424,  8875,
        8600, 22522, 16128, 41463, 30281, 27868, 34778,  3399, 12927,
       45866, 16576,  7640, 36672, 33034, 23564,  6786, 19484, 43602,
        3257, 38929, 18321,   743, 22984, 34162, 38630, 39532,  3954,
       44411, 21627,  4379, 22396, 34923, 21328, 36039, 28818, 13332,
       23635,  2600, 19628,  1611, 15710,  4953, 11791,  3072,  6020,
        4360, 28098, 10711,  1464, 30439,  5301,   840, 20197, 18527,
       46859, 16089, 47226, 35587, 15195, 41110, 45466,  6453, 24263,
       38522, 41844, 16594, 33624, 37491, 41525, 46966, 24979, 16829,
       27871, 19539, 20244,  6122, 47968, 19869, 12183, 46078, 16385,
       49045, 20546, 26881, 46147,  2422, 33206, 33907, 30104, 14964,
        6731,  9549, 15248, 21564, 47981, 23854, 41274, 11963,  3689,
       21488,  2421,  5266, 45373, 19210, 26081, 28428, 47771, 15502,
        7530,  4523,  8833, 23866, 32251, 14309,  4468,  2258, 14407,
       41751, 24534, 13660,  1950, 27155, 34828,   637,  7757, 18765,
       13880, 49169, 46420,  2598, 31880, 12915, 37324,  5516,  5037,
       46548, 42674, 33242, 14567,  8160, 40654, 44001, 40807,  2824,
       44296,  7700, 17998, 32378,  7663, 13561, 34746,   371, 12461,
        2355, 23449, 19682, 15931, 16529, 43442, 11440, 22308, 22907,
       31726, 34381, 11238,  5702, 18605, 26007, 27638, 49530, 38683,
       22665, 21815, 44468, 27873, 37165, 39213, 46113, 30749, 35524,
       11305, 38586, 36180, 39723, 31246, 47401, 34547, 46434, 44866,
       47763, 17963, 27522, 34676, 18315, 38898, 20606, 34711, 15884,
       40516,   189, 38254, 49245, 20622, 23096, 31600, 27071, 33700,
       12922, 46782, 16002, 27426,  3361, 36286, 30078, 20694, 20916,
       33510,  2451, 40649, 17717, 31888, 20980,  1243, 38937, 21605,
        4030,  3713, 33336,  5194, 44470, 38518, 41848, 17947, 44742,
       45085, 28273, 35285, 36134,  2171, 39080, 42844,  1926, 43867,
        7537, 19527, 36573,  7972,  3979, 30904, 16553, 13043, 13225,
         415,  9384, 38052,  8102, 16225, 40802, 37881, 36004, 26764,
       19103, 44884,  2855, 40876, 19347,  9495, 20985, 18200, 49555,
        2932,  2413, 31648,  2652,  1999, 16420,   687, 20708, 40843,
       46294, 28408, 11721, 27179, 22080, 20611, 24589, 40612, 35059,
       18659, 42231, 28401,  2010, 14413, 28126,  4559, 39477, 20035,
        5719, 41467, 34998, 44143, 13594, 21647, 47799,  5216,  1216,
       25288,  8842, 14727, 19639, 33076, 36358, 47089,  9759, 15240,
       39372,  3900, 24721, 11762, 22628, 17763, 28467, 12993, 41234,
        9287, 26722,  6923, 25083, 31947, 17838, 16920,  2060, 34722,
       27147, 16939,  8098, 43944, 21533, 20105, 16371, 31004, 18680,
       14124]), [5, 8, 1, 0])]
Collaboration
DC 0, val_set_size=1000, COIs=[6, 3, 1, 0], M=tensor([6, 3, 1, 0], device='cuda:0'), Initial Performance: (0.242, 0.04443864643573761)
DC 1, val_set_size=1000, COIs=[4, 9, 1, 0], M=tensor([4, 9, 1, 0], device='cuda:0'), Initial Performance: (0.218, 0.044824156880378725)
DC 2, val_set_size=1000, COIs=[2, 7, 1, 0], M=tensor([2, 7, 1, 0], device='cuda:0'), Initial Performance: (0.243, 0.04442102932929993)
DC 3, val_set_size=1000, COIs=[5, 8, 1, 0], M=tensor([5, 8, 1, 0], device='cuda:0'), Initial Performance: (0.244, 0.044828352451324466)
D00: 1000 samples from classes {0, 1}
D01: 1000 samples from classes {0, 1}
D02: 1000 samples from classes {0, 1}
D03: 1000 samples from classes {0, 1}
D04: 1000 samples from classes {0, 1}
D05: 1000 samples from classes {0, 1}
D06: 1000 samples from classes {3, 6}
D07: 1000 samples from classes {3, 6}
D08: 1000 samples from classes {3, 6}
D09: 1000 samples from classes {3, 6}
D010: 1000 samples from classes {3, 6}
D011: 1000 samples from classes {3, 6}
D012: 1000 samples from classes {9, 4}
D013: 1000 samples from classes {9, 4}
D014: 1000 samples from classes {9, 4}
D015: 1000 samples from classes {9, 4}
D016: 1000 samples from classes {9, 4}
D017: 1000 samples from classes {9, 4}
D018: 1000 samples from classes {2, 7}
D019: 1000 samples from classes {2, 7}
D020: 1000 samples from classes {2, 7}
D021: 1000 samples from classes {2, 7}
D022: 1000 samples from classes {2, 7}
D023: 1000 samples from classes {2, 7}
D024: 1000 samples from classes {8, 5}
D025: 1000 samples from classes {8, 5}
D026: 1000 samples from classes {8, 5}
D027: 1000 samples from classes {8, 5}
D028: 1000 samples from classes {8, 5}
D029: 1000 samples from classes {8, 5}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO1', '(DO2']
DC 1 --> ['(DO3', '(DO4']
DC 2 --> ['(DO5']
DC 3 --> ['(DO0']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.354, 0.07661269408464431) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.07654814468324185) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.09640297073125839) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.392, 0.07325242793560029) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.275, 0.07945466989278793) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.252, 0.08400962515175342) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.278, 0.12523775127530098) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.46, 0.09349438312649727) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.377, 0.09021134239435195) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.381, 0.09025510634481906) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.396, 0.15705580693483354) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.474, 0.14240596529841423) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.41, 0.11596032911539078) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.412, 0.11271193671226501) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.42, 0.19727019335329532) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.16480454625189303) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.408, 0.13193382388353347) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.434, 0.12182052563875914) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.426, 0.20265718460083007) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.21888766702078283) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO1', '(DO3']
DC 1 --> ['(DO2', '(DO0']
DC 2 --> ['(DO5']
DC 3 --> ['(DO4']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.398, 0.12846777415275573) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.467, 0.140342478916049) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.421, 0.21342508929222823) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.47, 0.24219384985789658) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.394, 0.12581581400334835) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.14166802729293704) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.43, 0.2270521096549928) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.27305388302356004) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.378, 0.11846597637236118) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.474, 0.15023153780028223) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.427, 0.24203452307730913) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.471, 0.26939495775196703) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.388, 0.12462650395929814) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.469, 0.17387545125558973) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.435, 0.2579819469116628) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29477349990280344) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.39, 0.14789085798710586) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.466, 0.19303536528348922) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.429, 0.267256860435009) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.29640268966881556) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[0, 1], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.707, 0.018712581679224967)
DC Expert-0, val_set_size=500, COIs=[3, 6], M=tensor([6, 3, 1, 0], device='cuda:0'), Initial Performance: (0.78, 0.015118347033858299)
DC Expert-1, val_set_size=500, COIs=[9, 4], M=tensor([4, 9, 1, 0], device='cuda:0'), Initial Performance: (0.932, 0.004831772923469544)
DC Expert-2, val_set_size=500, COIs=[2, 7], M=tensor([2, 7, 1, 0], device='cuda:0'), Initial Performance: (0.858, 0.010821685314178467)
DC Expert-3, val_set_size=500, COIs=[8, 5], M=tensor([5, 8, 1, 0], device='cuda:0'), Initial Performance: (0.954, 0.0033387430375441908)
SUPER-DC 0, val_set_size=1000, COIs=[6, 3, 1, 0], M=tensor([6, 3, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[4, 9, 1, 0], M=tensor([4, 9, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[2, 7, 1, 0], M=tensor([2, 7, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[5, 8, 1, 0], M=tensor([5, 8, 1, 0], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x74912402a160>, <fl_market.actors.data_consumer.DataConsumer object at 0x74909c4b5460>, <fl_market.actors.data_consumer.DataConsumer object at 0x74909c5da610>, <fl_market.actors.data_consumer.DataConsumer object at 0x74909c5788e0>, <fl_market.actors.data_consumer.DataConsumer object at 0x74911019db80>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9, 0.008632618889212608) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.012914453119039536) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.0033784635411575436) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.010653720825910568) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.004116409907932393) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.591, 0.039245903074741365) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.559, 0.06244843313097954) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.487, 0.06595959118008614) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.536, 0.10922989162430168) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.928, 0.005650037975981832) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.858, 0.013831778928637505) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0032628414072096348) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.012776300966739654) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.004064155930886045) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.629, 0.03739896178245544) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.662, 0.04286584801971913) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.561, 0.0435549099445343) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.627, 0.04272077882289887) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.938, 0.005140694806352258) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.858, 0.015395931102335452) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.0033223910946398975) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.011941022008657456) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.00373567250079941) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.657, 0.03489218163490295) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.681, 0.0389595872759819) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.636, 0.0358351212143898) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.686, 0.03445350693911314) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9385, 0.005001224967185408) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.015319034159183502) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.976, 0.0030976673716213553) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.011530145943164826) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.003959205056540668) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.658, 0.03655926707386971) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.649, 0.045713587254285815) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.603, 0.04188470071554184) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.681, 0.03576340690255165) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.943, 0.004905794050078839) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.86, 0.014714618176221848) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.0035547210697550327) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.011379104733467102) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.003973412634222768) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.03781917169690132) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.674, 0.037697251439094547) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.64, 0.037711730390787124) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.698, 0.03375894003361463) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9455, 0.005520377396373078) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.862, 0.016680112183094023) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0035091941698919982) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.010449654936790466) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004212400732300011) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.672, 0.0345491324365139) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.681, 0.03419839730858803) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.69, 0.03140047264099121) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.679, 0.04082447551190853) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9465, 0.004759415034437552) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.018369799703359602) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.0030292272071819754) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.014355455383658408) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.004881396369251888) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.684, 0.03333987963199615) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.685, 0.03680878172814846) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.621, 0.04395344015955925) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.698, 0.034618877783417705) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.004876774820266292) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.858, 0.01747446046769619) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.0039624456990277395) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.014115231096744537) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004823503433406586) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.694, 0.03184683001041412) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.683, 0.03643240690231323) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.644, 0.0412138714492321) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.718, 0.03074655210971832) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.946, 0.004971898794989101) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.017084759175777436) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.004725999776361277) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.015381988495588302) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004313835218577878) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.681, 0.035640123575925824) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.662, 0.04134149795025587) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.615, 0.041545124769210816) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.721, 0.029874087125062943) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.005708199748245534) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.01856556245684624) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004627041732193902) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.013945229709148407) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.00460391952811915) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.671, 0.036242692410945895) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.683, 0.0364826300740242) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.661, 0.0367469522356987) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.735, 0.02947990697622299) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.005338299359369557) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.862, 0.0166447711288929) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.00374487885273993) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.014014615386724471) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.004434113155177328) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.667, 0.03511771750450134) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.644, 0.0465121997743845) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.646, 0.03652799659967423) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.725, 0.03132780465483666) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.00561462546151597) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.856, 0.020285947501659393) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.004591075043193996) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.014913425147533417) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.004613521847735683) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.688, 0.033872486770153046) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.655, 0.04614466489106417) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.648, 0.03697805631160736) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.715, 0.031624788001179696) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.955, 0.005733108260174049) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.862, 0.018565232053399085) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.004040223881485872) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.011917881935834885) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.0050221358446433445) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.674, 0.03705120849609375) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.654, 0.04577952402830124) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.628, 0.04264143812656403) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.705, 0.034430406108498575) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9485, 0.006046676384052261) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.858, 0.017816388845443724) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.974, 0.003149560026358813) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.013959281221032143) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.005816056361072697) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.633, 0.03920442000031471) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.638, 0.049110173478722575) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.615, 0.0491052620112896) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.672, 0.04739785156585276) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.005729031799244695) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.862, 0.018472537398338318) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003880093701649457) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.013814518421888352) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.004398104915788281) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.675, 0.03382257866859436) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.656, 0.046830353662371635) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.617, 0.045850910127162935) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.714, 0.0369552187025547) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.958, 0.005696584656230698) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.86, 0.017011636555194855) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003890669686254114) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.014584281384944916) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.0051183907719096165) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.665, 0.0356386694163084) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.612, 0.05047098395228386) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.622, 0.051062924891710285) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.714, 0.035093184202909466) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.005971949133265298) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.020739055007696152) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.0036340624541044237) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.014591597154736518) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.004720326395225129) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.03834119921922684) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.63, 0.05100584727525711) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.571, 0.0636042622923851) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.666, 0.043972707197070124) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.006094307638995815) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.019021487474441528) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004686488806386478) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.0168807530105114) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.004793157054315088) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.65, 0.036651596009731294) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.595, 0.08059557291865349) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.591, 0.04913302314281463) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.679, 0.040109265848994255) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.0056583800668013285) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.022547281980514526) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004411491175182164) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.014095140635967255) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.004882648680038983) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.645, 0.039764077544212344) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.63, 0.050672761529684064) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.622, 0.0459683013856411) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.711, 0.03506801946461201) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.006088648788238061) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.86, 0.021518414914608) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004127453699649777) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.012954420819878578) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.004199369754627696) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.671, 0.03612388345599175) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.638, 0.056532887250185015) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.634, 0.040430772960186004) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.675, 0.04278933121263981) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.007240186054008518) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.856, 0.020633308649063112) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003736209570735809) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.013425598844885827) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.0051095656524121295) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.669, 0.03417376089096069) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.624, 0.055180048435926436) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.622, 0.04189634421467781) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.692, 0.04001474630832672) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9505, 0.006189705314296589) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.832, 0.020497987508773803) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004375617472804152) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.896, 0.013486722469329833) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.004776266503758961) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.673, 0.0336833875477314) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.578, 0.08337091606110335) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.588, 0.051919824421405796) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.643, 0.052857256446033715) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9545, 0.00564561643532943) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.019928624391555787) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.004130918347625993) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.014253667950630189) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.004212109267828054) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.654, 0.034993755757808685) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.634, 0.05124758216738701) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.625, 0.04019127494096756) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.681, 0.043364764586091044) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9515, 0.006231633709816379) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.838, 0.02026232498884201) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.004050075232633389) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.014424564063549042) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.003930093006638344) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.667, 0.03620791605114937) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.591, 0.07589882418513298) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.608, 0.05136546391248703) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.628, 0.05834645319730043) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.006415521740389522) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.01818370768427849) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.005591336105688243) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.011757458418607711) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.004819625587420887) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.682, 0.03587639282643795) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.07179918286204338) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.598, 0.050828558087348936) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.663, 0.0485813292004168) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.006181192361662397) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.834, 0.021744977742433547) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.97, 0.003683782126288861) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.013673119843006135) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.005053911219816655) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.672, 0.037589016184210775) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.673, 0.04496958231925964) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.622, 0.04113403046131134) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.668, 0.04628809627890587) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.006554301141819451) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.856, 0.02099411904811859) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.003919943287910428) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.013995560571551323) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.004502288799732923) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.693, 0.035433118626475336) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.654, 0.04854700493812561) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.57, 0.061531874805688856) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.636, 0.05586849132739007) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.005554319996270351) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.852, 0.02277780020236969) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.00516642867246992) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.014157386988401414) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.972, 0.003629692783113569) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.03877690029144287) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.604, 0.06455575546622276) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.593, 0.051465123295783996) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.709, 0.03562054541707039) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.006069099718602956) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.017379299215972425) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.005309212298576313) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.01524731969833374) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.00391730128115887) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.676, 0.03546821853518486) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.582, 0.07192348971217871) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.6, 0.056195162564516066) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.668, 0.05089805757254362) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.007691939101207026) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.02049256107211113) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004801734356209636) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.016611801087856294) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.005640466445584025) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.681, 0.03456679373979568) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.612, 0.06256063602864742) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.597, 0.05152093920111656) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.657, 0.05058272358030081) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.00722922558373466) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.854, 0.018290117979049683) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.005377800419013511) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.01801358824968338) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.005695538291520279) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.681, 0.03960353821516037) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.619, 0.061770573519170285) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.591, 0.04545621418952942) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.682, 0.036153842389583586) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9585, 0.00781372385843224) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.846, 0.021201269388198853) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.004697281321306946) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.014482921093702316) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.966, 0.005053887113492237) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.641, 0.042013502657413486) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.623, 0.06148028811067343) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.608, 0.046466359168291095) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.727, 0.03312612400949001) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9535, 0.007185461258313808) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.866, 0.019542309403419493) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.964, 0.004412654356819985) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.014697239249944687) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.005667431861846125) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.0499170771241188) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.605, 0.06996203712373972) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.622, 0.050161685422062875) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.66, 0.053417035430669786) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9545, 0.006540728349165874) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.02146287202835083) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.004606976118317107) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.017348676800727846) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.0045631231031293285) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.637, 0.04257824158668518) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.61, 0.07686142027378082) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.545, 0.07385149359703064) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.05740388075262308) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.957, 0.006450202003234154) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.844, 0.021742142856121062) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.005667926242080284) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.01519612717628479) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.964, 0.005656174484684016) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.596, 0.051967937529087065) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.586, 0.08547626613453031) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.627, 0.07122523209452629) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.668, 0.0428003453463316) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.006246278971149878) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.862, 0.020897127836942673) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.005020811084192246) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.015393248111009597) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.004639475921838311) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.646, 0.042688113927841186) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.595, 0.07445346596091985) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.605, 0.05477771770954132) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.68, 0.042894622281193735) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.956, 0.006530363072457476) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.86, 0.019775819838047028) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.968, 0.004998128016261034) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.01640181338787079) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.96, 0.006176834208497894) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.66, 0.03738777595758438) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.624, 0.0716229165866971) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.585, 0.06058100089430809) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.669, 0.04955772555992007) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9545, 0.007493821373464016) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.852, 0.0172198526263237) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.006198951747501269) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.9, 0.013981979817152023) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.962, 0.006330549106038233) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.625, 0.049890837788581846) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.611, 0.08016563611477613) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.634, 0.05127499383687973) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.69, 0.04449742098897696) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9535, 0.006194284363577026) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.85, 0.019834417700767517) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.966, 0.006266591518041196) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.01675564832985401) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006008858219292961) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.639, 0.04121125745773315) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.627, 0.05681398414075375) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.638, 0.05643644595146179) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.658, 0.05522468635812402) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9555, 0.007108010303458286) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.858, 0.019471308827400207) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.972, 0.005301317840931006) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.016320516288280486) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.968, 0.00441865784020456) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.046131529808044434) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.0713766059577465) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.634, 0.05079439923167229) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.683, 0.04658884197473526) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.242, 0.04443864643573761), (0.354, 0.07661269408464431), (0.275, 0.07945466989278793), (0.377, 0.09021134239435195), (0.41, 0.11596032911539078), (0.408, 0.13193382388353347), (0.398, 0.12846777415275573), (0.394, 0.12581581400334835), (0.378, 0.11846597637236118), (0.388, 0.12462650395929814), (0.39, 0.14789085798710586), (0.591, 0.039245903074741365), (0.629, 0.03739896178245544), (0.657, 0.03489218163490295), (0.658, 0.03655926707386971), (0.644, 0.03781917169690132), (0.672, 0.0345491324365139), (0.684, 0.03333987963199615), (0.694, 0.03184683001041412), (0.681, 0.035640123575925824), (0.671, 0.036242692410945895), (0.667, 0.03511771750450134), (0.688, 0.033872486770153046), (0.674, 0.03705120849609375), (0.633, 0.03920442000031471), (0.675, 0.03382257866859436), (0.665, 0.0356386694163084), (0.644, 0.03834119921922684), (0.65, 0.036651596009731294), (0.645, 0.039764077544212344), (0.671, 0.03612388345599175), (0.669, 0.03417376089096069), (0.673, 0.0336833875477314), (0.654, 0.034993755757808685), (0.667, 0.03620791605114937), (0.682, 0.03587639282643795), (0.672, 0.037589016184210775), (0.693, 0.035433118626475336), (0.644, 0.03877690029144287), (0.676, 0.03546821853518486), (0.681, 0.03456679373979568), (0.681, 0.03960353821516037), (0.641, 0.042013502657413486), (0.612, 0.0499170771241188), (0.637, 0.04257824158668518), (0.596, 0.051967937529087065), (0.646, 0.042688113927841186), (0.66, 0.03738777595758438), (0.625, 0.049890837788581846), (0.639, 0.04121125745773315), (0.634, 0.046131529808044434)]
TEST: 
[(0.2285, 0.04340292808413505), (0.35375, 0.07347347635030746), (0.27775, 0.07620753061771393), (0.3925, 0.0865502817928791), (0.4165, 0.11079292529821395), (0.403, 0.12526093649864198), (0.401, 0.12224474203586579), (0.3945, 0.11966084939241409), (0.38425, 0.1124050145149231), (0.38725, 0.11857743835449219), (0.3925, 0.13983969384431838), (0.6115, 0.035766317769885064), (0.651, 0.034539789348840715), (0.67125, 0.034004875630140305), (0.6865, 0.033517979592084884), (0.659, 0.035831274896860124), (0.68475, 0.03215356098860502), (0.69575, 0.03082096792757511), (0.7035, 0.030778899013996126), (0.69175, 0.033898005083203314), (0.67975, 0.034605421133339406), (0.68225, 0.03315177556872368), (0.698, 0.03206210586428642), (0.6865, 0.03525170353055), (0.6635, 0.03410609523206949), (0.6945, 0.031667162977159026), (0.69725, 0.03360237967222929), (0.66, 0.03656319651007652), (0.67875, 0.034344693370163444), (0.6535, 0.037434977494180205), (0.70125, 0.03213863291218877), (0.69225, 0.032919121898710726), (0.68925, 0.032244139410555366), (0.6765, 0.03313158547878265), (0.683, 0.03381147871166468), (0.698, 0.03378494467586279), (0.69525, 0.03666114236414433), (0.689, 0.03525634141266346), (0.6615, 0.037944876179099085), (0.6995, 0.03427111343294382), (0.6895, 0.03576166521012783), (0.7035, 0.036902409084141254), (0.6485, 0.04041345401853323), (0.63175, 0.04659488952159881), (0.644, 0.03859517304599285), (0.61875, 0.048016271382570266), (0.64825, 0.041446134388446805), (0.69725, 0.03489396867156029), (0.62325, 0.047409602895379065), (0.65975, 0.038076350323855875), (0.654, 0.04112118235975504)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.62      0.55      0.58      1000
           1       0.72      0.77      0.74      1000
           3       0.54      0.65      0.59      1000
           6       0.78      0.65      0.71      1000

    accuracy                           0.65      4000
   macro avg       0.66      0.65      0.66      4000
weighted avg       0.66      0.65      0.66      4000

Collaboration_DC_1
VAL: 
[(0.218, 0.044824156880378725), (0.25, 0.07654814468324185), (0.252, 0.08400962515175342), (0.381, 0.09025510634481906), (0.412, 0.11271193671226501), (0.434, 0.12182052563875914), (0.467, 0.140342478916049), (0.461, 0.14166802729293704), (0.474, 0.15023153780028223), (0.469, 0.17387545125558973), (0.466, 0.19303536528348922), (0.559, 0.06244843313097954), (0.662, 0.04286584801971913), (0.681, 0.0389595872759819), (0.649, 0.045713587254285815), (0.674, 0.037697251439094547), (0.681, 0.03419839730858803), (0.685, 0.03680878172814846), (0.683, 0.03643240690231323), (0.662, 0.04134149795025587), (0.683, 0.0364826300740242), (0.644, 0.0465121997743845), (0.655, 0.04614466489106417), (0.654, 0.04577952402830124), (0.638, 0.049110173478722575), (0.656, 0.046830353662371635), (0.612, 0.05047098395228386), (0.63, 0.05100584727525711), (0.595, 0.08059557291865349), (0.63, 0.050672761529684064), (0.638, 0.056532887250185015), (0.624, 0.055180048435926436), (0.578, 0.08337091606110335), (0.634, 0.05124758216738701), (0.591, 0.07589882418513298), (0.589, 0.07179918286204338), (0.673, 0.04496958231925964), (0.654, 0.04854700493812561), (0.604, 0.06455575546622276), (0.582, 0.07192348971217871), (0.612, 0.06256063602864742), (0.619, 0.061770573519170285), (0.623, 0.06148028811067343), (0.605, 0.06996203712373972), (0.61, 0.07686142027378082), (0.586, 0.08547626613453031), (0.595, 0.07445346596091985), (0.624, 0.0716229165866971), (0.611, 0.08016563611477613), (0.627, 0.05681398414075375), (0.598, 0.0713766059577465)]
TEST: 
[(0.22, 0.043847873479127886), (0.25, 0.07381470426917076), (0.2505, 0.08048938992619514), (0.386, 0.08601839923858642), (0.4165, 0.10769178631901741), (0.44125, 0.11695740818977356), (0.4685, 0.13551646131277084), (0.4675, 0.13753553491830825), (0.4745, 0.14645577955245973), (0.47025, 0.1688867705464363), (0.47025, 0.18691782051324846), (0.54375, 0.0629473647326231), (0.65075, 0.04351591534912586), (0.67625, 0.03869277563691139), (0.643, 0.04529958030581474), (0.6595, 0.03990296484529972), (0.6745, 0.03566778135299683), (0.6695, 0.0368082809150219), (0.6625, 0.036494349099695685), (0.6345, 0.04148555554449558), (0.67725, 0.037198576897382735), (0.63475, 0.04705282573401928), (0.62725, 0.04678571470081806), (0.6475, 0.04717335732281208), (0.625, 0.04994049634039402), (0.638, 0.04901583532989025), (0.592, 0.052978135511279105), (0.62525, 0.05103157339990139), (0.587, 0.08571654652059078), (0.626, 0.052927921399474144), (0.6175, 0.058705235704779625), (0.593, 0.05951480023562908), (0.56125, 0.08689821821451187), (0.6245, 0.05272288878262043), (0.5955, 0.07557699091732502), (0.5825, 0.07524344113469124), (0.668, 0.0472400287091732), (0.625, 0.052788905039429665), (0.591, 0.06838190402090549), (0.56725, 0.07574271121621132), (0.60375, 0.06549193927645683), (0.61125, 0.06498875775933266), (0.6005, 0.06385980181396007), (0.59, 0.07274448445439338), (0.60275, 0.0809358102530241), (0.57225, 0.09010305163264275), (0.591, 0.08136642421782017), (0.619, 0.07312687203288078), (0.58475, 0.08375588139891624), (0.611, 0.05757623913884163), (0.58725, 0.07264903700351714)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.72      0.36      0.48      1000
           1       0.68      0.25      0.36      1000
           4       0.63      0.93      0.75      1000
           9       0.49      0.81      0.61      1000

    accuracy                           0.59      4000
   macro avg       0.63      0.59      0.55      4000
weighted avg       0.63      0.59      0.55      4000

Collaboration_DC_2
VAL: 
[(0.243, 0.04442102932929993), (0.25, 0.09640297073125839), (0.278, 0.12523775127530098), (0.396, 0.15705580693483354), (0.42, 0.19727019335329532), (0.426, 0.20265718460083007), (0.421, 0.21342508929222823), (0.43, 0.2270521096549928), (0.427, 0.24203452307730913), (0.435, 0.2579819469116628), (0.429, 0.267256860435009), (0.487, 0.06595959118008614), (0.561, 0.0435549099445343), (0.636, 0.0358351212143898), (0.603, 0.04188470071554184), (0.64, 0.037711730390787124), (0.69, 0.03140047264099121), (0.621, 0.04395344015955925), (0.644, 0.0412138714492321), (0.615, 0.041545124769210816), (0.661, 0.0367469522356987), (0.646, 0.03652799659967423), (0.648, 0.03697805631160736), (0.628, 0.04264143812656403), (0.615, 0.0491052620112896), (0.617, 0.045850910127162935), (0.622, 0.051062924891710285), (0.571, 0.0636042622923851), (0.591, 0.04913302314281463), (0.622, 0.0459683013856411), (0.634, 0.040430772960186004), (0.622, 0.04189634421467781), (0.588, 0.051919824421405796), (0.625, 0.04019127494096756), (0.608, 0.05136546391248703), (0.598, 0.050828558087348936), (0.622, 0.04113403046131134), (0.57, 0.061531874805688856), (0.593, 0.051465123295783996), (0.6, 0.056195162564516066), (0.597, 0.05152093920111656), (0.591, 0.04545621418952942), (0.608, 0.046466359168291095), (0.622, 0.050161685422062875), (0.545, 0.07385149359703064), (0.627, 0.07122523209452629), (0.605, 0.05477771770954132), (0.585, 0.06058100089430809), (0.634, 0.05127499383687973), (0.638, 0.05643644595146179), (0.634, 0.05079439923167229)]
TEST: 
[(0.246, 0.04339280295372009), (0.25, 0.09244508910179138), (0.2815, 0.12015084260702133), (0.38875, 0.15123850136995315), (0.42325, 0.18991599303483964), (0.4235, 0.19383455294370652), (0.42825, 0.20367060768604278), (0.42975, 0.21929274678230284), (0.4355, 0.23072633934020997), (0.44425, 0.245837830722332), (0.442, 0.25503220057487486), (0.50825, 0.06184557110071182), (0.576, 0.04011393597722054), (0.63625, 0.03400801792740822), (0.593, 0.04041153518855572), (0.63625, 0.037198223516345025), (0.679, 0.03185368682444096), (0.626, 0.042754730090498926), (0.62475, 0.042482139244675636), (0.6095, 0.04393058118224144), (0.65675, 0.03470205997675657), (0.64975, 0.03584851756691933), (0.64575, 0.03592451193928719), (0.626, 0.04213952097296715), (0.62575, 0.046111497409641745), (0.633, 0.04239871050417423), (0.62125, 0.04982917757332325), (0.57175, 0.06200853446871042), (0.5985, 0.048838822960853574), (0.62125, 0.045285975635051726), (0.624, 0.04078788089752197), (0.6325, 0.04011537420749664), (0.6105, 0.04852174738794565), (0.6445, 0.03841149841248989), (0.6085, 0.04983474050462246), (0.5835, 0.05048595617711544), (0.61175, 0.04066818735003471), (0.57275, 0.06089350001513958), (0.58275, 0.05346718791127205), (0.59475, 0.05449843026697636), (0.59525, 0.04995629049837589), (0.5955, 0.04460326737165451), (0.61225, 0.045286590665578844), (0.6205, 0.04825430315732956), (0.55775, 0.07018812762200832), (0.6115, 0.06701888446509838), (0.60825, 0.05448742188513279), (0.5755, 0.05878107361495495), (0.62775, 0.04909538466483355), (0.62375, 0.053775921896100046), (0.64175, 0.04649720744788647)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.48      0.26      0.34      1000
           1       0.87      0.78      0.83      1000
           2       0.47      0.76      0.58      1000
           7       0.82      0.76      0.79      1000

    accuracy                           0.64      4000
   macro avg       0.66      0.64      0.63      4000
weighted avg       0.66      0.64      0.63      4000

Collaboration_DC_3
VAL: 
[(0.244, 0.044828352451324466), (0.392, 0.07325242793560029), (0.46, 0.09349438312649727), (0.474, 0.14240596529841423), (0.469, 0.16480454625189303), (0.469, 0.21888766702078283), (0.47, 0.24219384985789658), (0.477, 0.27305388302356004), (0.471, 0.26939495775196703), (0.477, 0.29477349990280344), (0.477, 0.29640268966881556), (0.536, 0.10922989162430168), (0.627, 0.04272077882289887), (0.686, 0.03445350693911314), (0.681, 0.03576340690255165), (0.698, 0.03375894003361463), (0.679, 0.04082447551190853), (0.698, 0.034618877783417705), (0.718, 0.03074655210971832), (0.721, 0.029874087125062943), (0.735, 0.02947990697622299), (0.725, 0.03132780465483666), (0.715, 0.031624788001179696), (0.705, 0.034430406108498575), (0.672, 0.04739785156585276), (0.714, 0.0369552187025547), (0.714, 0.035093184202909466), (0.666, 0.043972707197070124), (0.679, 0.040109265848994255), (0.711, 0.03506801946461201), (0.675, 0.04278933121263981), (0.692, 0.04001474630832672), (0.643, 0.052857256446033715), (0.681, 0.043364764586091044), (0.628, 0.05834645319730043), (0.663, 0.0485813292004168), (0.668, 0.04628809627890587), (0.636, 0.05586849132739007), (0.709, 0.03562054541707039), (0.668, 0.05089805757254362), (0.657, 0.05058272358030081), (0.682, 0.036153842389583586), (0.727, 0.03312612400949001), (0.66, 0.053417035430669786), (0.625, 0.05740388075262308), (0.668, 0.0428003453463316), (0.68, 0.042894622281193735), (0.669, 0.04955772555992007), (0.69, 0.04449742098897696), (0.658, 0.05522468635812402), (0.683, 0.04658884197473526)]
TEST: 
[(0.2495, 0.044014592796564105), (0.40075, 0.07023401990532875), (0.46475, 0.08952063956856728), (0.47275, 0.135820614695549), (0.476, 0.15784210336208343), (0.47525, 0.20845930409431457), (0.47475, 0.2282492750287056), (0.48075, 0.2596552656888962), (0.47775, 0.25685111624002455), (0.48, 0.28122049593925474), (0.47975, 0.28348508703708647), (0.5315, 0.09778806620836258), (0.6365, 0.040541463688015936), (0.704, 0.0315441619604826), (0.69325, 0.03331603585928679), (0.70175, 0.031324221223592755), (0.691, 0.037037048377096655), (0.71125, 0.03204648957401514), (0.7255, 0.027338344432413578), (0.73975, 0.027464084506034853), (0.7465, 0.02653638254851103), (0.72925, 0.02906020449846983), (0.7175, 0.029060219831764697), (0.72025, 0.030573720261454582), (0.68, 0.04400294736772776), (0.703, 0.03384724712371826), (0.7235, 0.03132687573134899), (0.6785, 0.040640360660851), (0.68725, 0.03746877162158489), (0.71575, 0.03240059216320515), (0.68925, 0.03804807269573212), (0.697, 0.036133420817553996), (0.66225, 0.04712652350962162), (0.6955, 0.037051461145281794), (0.651, 0.05247035178542137), (0.67125, 0.04204827547073364), (0.68525, 0.0403596750497818), (0.6365, 0.052241074308753015), (0.72225, 0.032110308580100534), (0.66225, 0.04456866380572319), (0.67775, 0.04275924457609653), (0.686, 0.03408854621648789), (0.74525, 0.028469264276325703), (0.653, 0.049499625787138936), (0.633, 0.05196864593029022), (0.675, 0.04080855195224285), (0.69275, 0.03792367754876614), (0.675, 0.04468322549760342), (0.68375, 0.043487366005778315), (0.659, 0.05072874104976654), (0.688, 0.0417072451710701)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.67      0.28      0.40      1000
           1       0.84      0.74      0.79      1000
           5       0.86      0.87      0.87      1000
           8       0.51      0.85      0.64      1000

    accuracy                           0.69      4000
   macro avg       0.72      0.69      0.67      4000
weighted avg       0.72      0.69      0.67      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [31]
name: alliance-4-dcs-31
score_metric: contrloss
aggregation: <function fed_avg at 0x776002648c10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=31
Partitioning data
[[2, 8, 0, 7], [5, 3, 0, 7], [1, 9, 0, 7], [4, 6, 0, 7]]
[(array([32045, 13820, 18987, 26795, 19845,  5574,  2772, 11006, 30325,
       24699, 30286, 16712, 47074, 45079,  7576, 28872, 34555, 22733,
        8186,  4375,  9827, 25022, 17379, 46652,  2007, 28176, 28479,
       39508, 20226, 26282, 39765, 49710, 20651,  2387, 25296,  8627,
       32706, 31375, 19963, 21987, 15380,   300, 48344, 44660,  3019,
       29517,  2133, 48242, 46589,  4183,  1122, 31707, 36186, 48270,
       13655, 43715, 20563,  4806,  7868, 46214, 29351, 48835, 20038,
        1523, 19163, 49756, 36242, 31293, 40832, 24734, 48314,  1129,
       48472, 46244, 15888, 34203, 17817, 19231,  6805, 46633,  5531,
       47632, 43795,  6614, 43469, 48739,    42, 42854, 34747,  4209,
       44351, 11050, 27399,  7962, 35686, 37855, 33183, 43541, 33137,
       27169,  5836, 44186, 12720, 46810, 14085, 44938, 46759, 17329,
       30751, 16530, 22732, 35958, 15754, 41604, 43106, 19364,  9498,
       33134,  9255, 20850, 40720, 34580, 11500, 23775, 32730, 43959,
       27980, 13698, 40281, 17981, 39466, 35740, 40791, 23650, 32981,
       27592, 15054,   271, 21034, 31501,  1789, 27877, 37754, 43930,
       20323, 26648, 27091, 22275, 48549, 27368, 25130, 46870, 43287,
        7662, 39145, 21162, 48300, 49972, 42856,  2938,  9579, 28568,
       25998, 40112, 45359, 34363, 47411,  3796, 18733, 48989, 14423,
       16165, 23653, 22570,  9089,  7761, 12752, 36245, 40917, 21057,
       24555, 15944,  6976,  9553,  1372, 32332, 49274, 19296, 20212,
       31614, 42988, 28164, 18489, 17481, 20308, 22376, 47593, 23074,
        6445, 23038, 43658, 35901, 35787, 23705,  4189, 34050, 33550,
       30003, 15820, 29076, 12070,  5554, 29682, 34959, 30596, 13413,
       22877, 29678, 34222, 13177, 28291, 27744, 18855, 49395,  9403,
       14887, 16786,  1576,   924, 29925,  2224, 34663, 18248, 34266,
        3769, 45006, 26730, 48095,  3446,  8395, 12694, 46972,  2833,
       36469, 23237, 15477,  1438,  7807,   303,  1838, 12258, 12614,
       16697, 33195, 24083, 34342,  7716,  2735, 10613, 13641, 39658,
        5687, 29782, 40255, 28424, 32034, 10763,  4863,   716,  7973,
       43755, 23583, 15011, 47993, 26563, 31950, 28154, 48911, 32138,
       30076, 23839, 10663, 48168,  1060, 39011,  9844, 19910,  5272,
       48698,  2344,  9226, 34655, 35266, 34718,  8961, 13674, 20612,
       45045, 44577, 48604, 22426, 15743, 30788, 36670, 19788,  1957,
       45446, 36571, 41413, 33322,  8852, 14969, 46138, 44594, 47713,
       45017, 20500, 18274, 29056, 36484, 21919,  2797, 45372, 45161,
       35078, 14971, 43916, 15648, 24142, 49379, 20898, 24140, 26981,
       27919, 10785, 26703, 18177, 12206, 15721, 25472, 15768, 31258,
       35617,  7668, 49042, 15617, 26337, 42506, 15927, 32618, 48959,
       42236, 18055, 17063,  7109, 11095,    69,  7248, 19622,  9623,
       21880, 25088, 46210,   259, 16479, 48457, 25118, 39106, 34636,
       49696, 42799, 37858,  4287, 28269, 48541, 37681, 44059, 22504,
        3268, 29747, 11368, 40070, 14183, 14402, 23793,  1720, 31787,
       46582, 24260,  3800,  2687, 13116, 37023, 16476, 27896,  9682,
       41002, 47652, 46926, 35489, 19149,  1229,  6646,  8495, 36939,
       14208, 18236, 21608, 48107,  3269,  6346, 11265,  2333, 12605,
       26055, 21840, 33109, 24748, 31182, 18036, 29868, 42316, 39579,
       25513, 48774, 27057, 37420, 30504, 40093, 18565, 40122, 20596,
        9514, 22551,  3670, 37411,  6435, 48670, 36933,  1397, 13266,
       13425,  6823, 40344,  3881, 37398, 45025, 18718,  3359,  2740,
       34319, 28450, 17729, 35236,  2368, 28396, 32663, 17345,  1439,
       17416, 42822,   723, 12935,   901,  4979, 18675, 27864, 17588,
       42247, 12731,  8787, 38332, 27337, 35177, 10346, 32562, 30872,
       29530, 43334, 38322, 14728, 26038, 30147, 41714, 27839, 33815,
       43381, 49654, 34208, 48752, 42245, 28940, 16156,  4303, 44981,
       48481, 19415, 11832, 26672,   566, 40241, 36771, 14708, 44961,
        1878, 20267, 46486, 32905, 23010,  8941, 34250, 49869, 13707,
       18925,  3184,  8102, 46586, 39959, 19974, 24764, 22769, 14456,
        1454, 24792, 35059,  7647, 46224, 47242, 40159, 49535, 39288,
       12009, 43944,  7464, 35354, 39709, 41222, 32483, 10493, 13016,
       45827,  8759,  1243, 46041, 25352, 15628, 18402, 41741,  3128,
       23182,  1188, 31087,  4721, 41842,  4141,  9800, 45364,  5371,
         757, 10548, 38586, 34856, 20831, 39793, 33767, 16004, 16920,
       34259, 42044,  6015, 28653, 34601, 24971, 29412, 46749, 13578,
       46277, 39553, 36337, 13549, 25144, 39780, 34998, 33923, 25816,
       14698, 29944, 22622,  1205, 16313, 31358, 10741, 17103,  9860,
       36501, 46116, 19395, 35281, 24285, 47486, 29140, 36289, 10872,
       17387, 41040, 18538, 36981, 49891, 35731, 48850, 29666, 23371,
       37322, 38479,  2446, 35738, 32346,  4571, 34780, 43292, 15580,
       17080, 49513, 33243, 41030, 42364, 27426,   694, 11240, 44875,
       24589,  4079, 43211, 48246, 47444, 35858, 40022, 10812, 33363,
       38342, 38635, 30205, 23507, 43831,  9390, 22249, 32365, 27855,
       24859, 10365, 19004, 47771, 27431, 33700, 42151,  9292, 42808,
       42797, 17438, 46689, 33229, 23175,  2169,  3073, 10690, 16497,
       22293,  4577, 12010, 24661, 48968,  4097, 14407, 33087, 34454,
       16169, 31249, 37664, 43992, 25930, 47799, 30280, 47274, 30429,
       24383, 46200, 24600, 42621, 42272, 17026, 15044, 35700, 24404,
       33943, 10401, 26412, 30408, 43054, 26913, 36148, 36634, 10713,
       30415, 41792,  1168,   752,  8623, 35079, 42883,  5828, 40906,
       35411, 46839,  9096, 26286,  5969, 29293, 28400, 34607,   348,
        8242, 11849, 36789, 47215, 38098, 41266, 42945, 17458, 11691,
       43301,  4621, 31976, 19639, 26178,  4778, 26007,  8200, 27104,
       19982, 13011, 25212, 38716, 18080, 10516,  6947, 25023,  8072,
       34026, 16840, 12130, 18973, 35316, 15120, 38286, 45034, 39231,
       48041, 31851, 41462, 21410, 25917, 39767, 37082, 19250, 28566,
        3606, 31965, 45365,  5200, 12535, 43717, 12980, 39164, 22599,
       27505, 32700, 13333, 27327, 43272,  5058, 33902, 41959, 22541,
        3844, 13146, 32586, 38803,  8115,  2360, 20062, 41704, 19300,
       39944, 37724, 42330, 44374, 37286, 32282, 29461, 38104, 25411,
       19074, 17913, 40055, 33299, 38938,  9279, 23699, 16224, 13039,
       18227, 43227, 24794, 44406, 36619, 36773, 22554, 45409,  3920,
       18723, 31980, 22725, 22324, 48596,  8966, 41660,  2611, 45643,
        3284,  2834, 34487, 15079, 12838,  7945, 32704, 11611, 28117,
       48720, 27998,  8498,  4747, 42299,  1931, 19636,  9456, 37980,
       41449, 49451, 23794,  7291, 28249, 12658, 19010,  8640, 31871,
        9079, 28296,  6839, 24612, 46711, 37763, 42052, 37802,  8694,
       33884,  5996, 35164, 24406,  2612, 32995, 49772, 12220,  1679,
       40374,  6998,  4238, 42126, 32032, 30498, 24499,  8901,  2502,
       34113, 34239, 11621, 14671, 22592, 42381, 32948, 29588, 28911,
       38110, 33343, 33382, 21317,  7624,   589,  4900, 22723, 12529,
       13284,  8595,  8909, 43445, 34380, 13336, 17587, 27039,   739,
       26389, 10672, 44958, 16565, 26037, 20309,  5044,  9422, 19037,
       45352,  5989, 42137, 38611,  3611,  7611,  1650,  7185, 29245,
       23643, 16091,  4820, 29789, 35767, 33843, 16000, 19121, 11741,
       12842, 49341, 14630, 15167, 27574, 22204,  1561, 28914, 19691,
       11323, 32646, 18481,  7939,   842,  8461,  4499, 42555, 38249,
        2458, 39314, 35982, 30401,  8604, 12566, 29823,  7641,  9958,
       28710, 12415, 15332, 22359, 34708,  4386, 44413, 26661, 14955,
       13818, 44262,  2397, 33648, 35120, 21843, 20356, 17775, 39835,
       36017, 31873,  2201, 33074,  8924, 26820,  6372, 23585, 43844,
       33527, 35988, 46098,  2776, 28987, 18326, 17489, 11377, 13525,
       48420]), [2, 8, 0, 7]), (array([16110, 17711, 49675, 41400,  8457, 27977,  8989,  1634, 18317,
       36308, 37436,  8497, 19007, 22355, 20644, 42846, 23335, 17305,
        8824, 46381,   515, 48083, 28438, 45583, 28475,  8204, 31877,
        1346, 31129,  3060, 15133,  9677, 34859, 29683, 39845, 44583,
       45326, 41703, 38199, 27903, 45134, 21436, 38313, 30072, 30359,
       23689,  9337, 25390, 48220, 22978,  8795, 42038, 27397, 30482,
       31273, 36150, 49867, 18732, 25141, 46853, 19042, 38854, 48630,
       27292, 35452, 45184,  9301, 47625, 24160, 38471,  6222,  3040,
       42731, 12687, 20031, 48882, 47192,  3746, 40800, 27956, 31376,
       12931, 41221, 47662, 38302, 33304, 35821, 41942, 30519, 33629,
       23733, 16079, 38632, 10315,  8274,  5840, 40421,  5981, 40717,
       45973, 46911,  1661, 16351, 16534, 36335, 44929,  3767,  1847,
       18966, 35495, 19825, 11463, 49531,  5690,  1580, 23233, 33720,
       15006, 14931, 48495, 19293, 43345, 29304, 11551,  7435, 26914,
       45882,  9274, 16444,  1976, 27590, 17152, 17065, 47480,  1072,
       31064, 40351, 14530, 27771,  8768, 25205, 22099, 45501,  8805,
       45153, 26581, 49988, 32255,  3206, 22079, 47223, 10319, 17102,
       13829, 20361, 40272, 30042, 11819, 18516,  9264, 19319, 27711,
       35802, 37484, 18945, 32924, 17721, 29836, 33800, 43878, 19362,
       30395,  6455, 36617,  5305, 48139, 28659, 21678, 46997, 46918,
       17882,  1983, 41559, 33161, 28999, 25886,  2906, 45271, 40997,
       26751, 14627, 42194, 28047,  4728, 37151, 25894,  4990,  2384,
       14734,  3604, 38343, 34391, 24714,  1134, 28756, 30502, 33295,
        9622, 46811, 27345, 48750, 21379, 31347, 28982, 18708, 38564,
        5205, 18459, 24006, 29896, 25157, 19487, 23285, 37708, 36962,
       36217, 45629, 35604, 14465, 36769, 23039, 19663,  7364, 17795,
       28070, 40871, 19177, 34117, 13610,  6017, 15775, 22679, 19271,
       47629, 16166, 46112, 47768, 47913, 13111,  5905, 17785, 13463,
       35026, 40712, 38931, 42501, 31260, 49430,  4128, 20345, 33113,
       20965, 23168, 40249, 13941, 37297, 27235, 47722, 48524,  9421,
       48933, 40314, 22463, 43462, 38679, 45542, 43456, 47933, 40129,
       37408, 39614, 42206, 35154, 37756, 16301,   992, 20806, 18393,
        4697, 21921, 37809, 26711, 36140, 33267, 32509,  6151, 37223,
       48376, 11476, 35581, 41144,  1546, 22042, 11294, 33355, 49609,
        8386, 30318, 25745, 10960, 25447, 37038,  1499, 16120,  5147,
       23700, 19234, 38066, 39438, 39294,  8790, 41497, 41182, 14380,
       31150,  3749, 25273,  4329, 23884,  8420, 49227, 23270, 46542,
        6905,  9185, 49835,   922, 20153,  7122, 44635, 49545, 33944,
       37215, 39175, 25997, 45931, 44354, 15778, 35935,   377, 26472,
       43971, 10418, 29038,  1281,  3646, 20685, 41154, 44543, 47100,
       12952, 10427, 33101, 32219, 38982, 29692, 25103, 13064, 16449,
       21510, 15345, 25378,  1345, 40922, 37135, 45251, 32649, 17919,
       10993, 40365, 23761, 38432,  5088, 29923, 49713, 43779, 20625,
       37429, 24961, 45319, 37300, 13928,  8319, 34742,  2785,  6422,
       40668, 25706, 26323, 40007, 21373, 18089, 44550, 12601,  6167,
       35770, 46402, 46349, 31861, 47866, 33463,  6637, 46799,  9343,
       47840,   494, 39681, 28839, 18412, 45236,  9904, 25062, 27413,
        5354,  3758, 33607, 33211, 15181, 29213,  8381, 49970,    80,
        8164, 39979, 15996, 11544, 19449, 17058, 30479,  8864, 36970,
       37417, 20917, 33346, 42647, 10341, 48019, 32938,  3617,  3813,
       14422, 17036,  4982, 34658, 33393,  8876, 47204, 33238,  8916,
       46337, 37733,  2414, 20968, 43827, 15321, 19327, 33779, 49054,
       36772, 19093, 43318, 22759, 47475,  8151, 28592, 46812, 29397,
       11291, 25639,  6071, 13082,  5800, 42417, 26064, 24538, 37904,
       47823, 24839,  4657, 49325, 43313, 15279, 30777, 10501, 41012,
       34160, 46727, 34901, 28880, 14703, 23061, 11169, 11879, 31556,
       32016, 19094,  1306, 31481, 13594, 21874, 26376, 41971, 43451,
        9385, 16559, 32318,   448, 39027, 47969,  4787, 49671, 15542,
       34938, 43989, 18215, 28742, 23924, 38828, 22924, 26137, 18047,
       43252, 45102, 17232, 16002, 22408, 11238, 12757, 31015, 48327,
        8392, 21469, 28300, 23999, 19730, 13473,  8292,  5314, 11975,
       10425, 18657,  3721,  2855, 14079, 15102, 13084, 47525, 13522,
        1626, 22557, 10553, 18765, 45156, 44618, 28178,  7179, 47685,
       26493,  4202, 30517, 39196, 45919,   401, 32217, 36195,  2613,
       32033, 35594, 25905, 25004, 44484,  6682, 35608, 13289,  1340,
       40194, 16015, 31635,  5266, 26592, 29468, 44275, 37869,  7592,
       32662, 36763, 17328, 38900, 17140, 24931, 12345, 33130, 30272,
       34831, 32839, 21243, 39301, 11424, 23746, 37619, 33347, 42148,
       30160, 45096, 24887, 21272,  7328, 11002, 45064, 32062, 33558,
       41960,  7970, 25763, 24447, 35434,  9121, 11682, 25814, 12420,
        2804, 27686,  1477, 34948,  1980, 13225, 24690,  6008, 16793,
        8889, 32316, 27931,  7655, 37083,   284, 10010, 23462, 19860,
       11014,  1144, 33451, 45564,  2885,  9687, 26040, 32206, 43245,
       21105, 48355, 20817, 24212, 16253, 45845, 25160,  8190, 18381,
       31970, 34018, 32076, 25422, 28080, 27301, 21467, 36322, 40211,
        1270,  9819, 24676, 27522,  4523, 23113, 25316, 20709,  9064,
        1473, 25167, 12469, 44122, 47048, 10689, 37432, 31904, 49164,
       35403, 28223, 35952, 35546, 22326,  2773,  3466, 40334, 23667,
       42205, 20924, 44968, 25546, 11400, 17056, 11263, 30400, 28794,
       33732,  8778, 13668, 35761, 48317, 22991,  7196,  3180, 22609,
        5614, 17969, 15008, 37165, 37103, 42166, 28428, 47332, 15309,
       30383, 24695, 16553, 11919,  4338, 44785, 11237, 13629, 33011,
       14712, 19804,   783, 23863, 32267,  8963, 14460, 45387,  2617,
        8147, 40401,  3618, 37556, 44815, 10278, 43748, 21589, 35008,
        7598, 19832, 46117, 42964, 42184, 34528, 37810, 34717, 31073,
       19349,  3573, 10879, 47189, 21378, 40977, 26358, 32000, 30419,
        5563, 20179, 27423, 31982, 32152, 30617, 43246, 42951, 12770,
       24129, 44789, 11414, 44921, 23842, 37919, 47372,  9294,  2783,
       46034, 40563, 47856, 27320, 12848, 40306, 31468, 26259, 28152,
        3598, 20757, 35987,  3209, 40141, 27672, 17627, 26104, 33763,
       24091, 12099, 15644,  3627, 36248, 29208,  9044, 32888, 12502,
       24439, 23909, 23993,  9186, 39180,  4088, 39518, 36932, 44115,
       11527, 47110, 22846, 38425,    84, 36680, 25741, 25080,  6441,
       17463, 16881,  9823, 18712, 45054, 36535, 39717, 29671, 17472,
       24768,  5887, 45172, 13900, 41985,  7833, 16764,   926, 14233,
       26910, 44249, 38062, 20417, 41898, 23381, 37660,  1445, 18504,
       11870, 22198, 38798, 35890, 26826, 26733, 10932,  7686, 30993,
       20369, 31529, 40238, 41795, 36637, 35962, 15099, 30803, 37458,
       27182, 37430, 48927, 49456, 47724, 47996, 33981, 32633, 47308,
       33450, 38667,  6239,  2746, 32546, 17575, 31513, 24613, 17467,
         492, 49203,  4119, 38435, 43247, 44762, 38505, 23676,  3543,
        2396, 32287, 36776,  7304, 29619, 47748, 26673, 49794, 43320,
       14057, 37655, 32097,  2135, 29618,  6992, 43536, 46946, 36346,
       16114, 47270,   133, 10358, 36936, 41791, 18443, 17521, 10858,
        2801, 22128, 23402, 16144, 48829, 43128, 19802, 47696,  8142,
       13638,  1835,  1054, 37273, 30556,  6552, 45724, 10816, 24052,
       37010, 38713,   256, 12722, 48466, 13494, 23034, 49814,  5121,
       21773,  2151, 44559, 15514, 32329,  5854, 31364, 41039, 44025,
       21120,  4357,   652, 29533, 24930, 13632, 47522,  9581, 47286,
        6748, 24508, 17037, 28398, 21430, 28392, 40465, 29359, 36730,
       12439, 14745, 42887,  5809, 49139, 40360, 16031, 29998, 33618,
       11334]), [5, 3, 0, 7]), (array([31193, 38816, 44841, 42424, 19783, 23139, 26637, 45118, 38211,
        3620, 46233,  9322,  8305, 11130,  9372, 16012, 15528, 20742,
         978, 22270,  8973, 35106, 14129, 34832, 33970,  9849, 47642,
       38522, 35004, 41404, 12164, 43711, 45157, 44575,  5822, 19351,
       35454, 14764, 39192, 26550, 49875, 40076, 47365, 15704, 33198,
       43098, 34300,  6899, 29283,  7401, 37937, 47113, 37689,  5149,
        4050, 37494, 11804, 46259, 38047, 12551, 25874, 35587, 11336,
       36379, 48917, 18345,  8107, 11645, 11850,  7358, 46127,  6779,
        2389,  5247,  5102, 47264,  8576, 49572, 31345, 26115, 49733,
       12100, 27037,  2101,   160, 16989, 28054, 17334, 17666, 31665,
       16598,  2844, 31100, 40204, 28640, 30553, 21088, 27794, 16682,
        6420, 35605, 47618, 15646,  9863, 15583,  7600, 49919, 16958,
       16256, 10495,   176, 19574, 19865, 25910,  3231, 33599, 36237,
       47377,  2289, 30198, 43207, 15069,  7841,  7381, 41807, 46865,
       45296,  4261, 47616, 43684, 23275, 29117, 49847,  2023, 38177,
       41837, 30457, 20680, 37792, 23648, 46928, 30570, 41637, 22063,
       10479, 25731, 42163,    45, 41061, 35048, 32860, 31158, 49959,
       35068, 42352, 10544,  9392, 37401, 11619, 32135, 39638, 10292,
       48639, 21073,  4172, 17673, 42912, 45014, 18354, 24484, 21020,
       22298, 49021, 41896, 24110, 20391, 42188, 34385, 44671, 41733,
       38804, 11109, 43413, 44751, 33941, 16577, 31300,  6871,  1446,
       30006, 42916,  8932, 44415,  3922, 23607, 41844, 22303, 19399,
        3808, 26498,  8389, 36838, 42060, 33897, 22212, 22952, 24520,
       32319, 40207,  5722, 48375, 11037,  9610, 16172, 15783, 21871,
       28089, 22975, 33050, 49489, 33611, 23357, 34206, 38637, 45070,
       17586, 38078, 39566, 25405, 49759, 24590, 36208, 24561, 31352,
       42921, 11692, 44960, 38919,  8812, 45602,  1455,  3505, 35810,
        3483, 28981, 43668, 18551, 31072, 11543, 34890,   202, 22685,
       30566, 37295,  2845,  3228, 29438, 46236,  4865, 45547,  2935,
        5562, 39548, 45841, 15480, 11834, 13370, 41436, 46335, 18343,
       23019, 36717,  8532, 14436,  1034, 46755,  9955, 11688, 42407,
        8281, 22166, 20176, 21355, 49833, 42208, 37538, 24133,  5002,
       33746,  4407, 26447, 16143, 29142, 41173, 13148, 14387,  8103,
        4246, 12662,  3930, 47343, 49073, 33856, 27365, 34175, 11773,
       18805, 48034, 19114, 47154, 11227, 46278, 45943,  1241, 33766,
       45426, 32072, 35384,  3392, 23001, 25361, 47076, 38752,   511,
        9919,  8975, 44564, 32552, 23679, 14467, 32550, 43614, 13208,
       28359, 37443, 27570, 30360, 16378,  7556, 47896, 35618, 41427,
       38324, 20481, 18800, 14401, 37634, 43900, 16488,  4071, 30343,
       37678,   166,  8413, 23336, 46355, 31662, 15758,  7453,  5692,
        1114, 45860,   438, 34474, 40079,  3613,  6546, 36117, 21489,
       31387,  2991, 42106, 31986,  8428, 29519, 42362,  9701, 25839,
       18513, 43169,  7854, 19199, 36508, 38056, 36859, 23243, 37968,
       26888, 13577,  7167,  3983, 26229, 46298,  5203, 13381, 27811,
       35498,  6002, 49260, 24078, 19048, 30364, 36397,  5382, 49785,
       24627,  6937, 42733, 37132, 19534, 48996,  8840, 36413, 23287,
       17171, 19367, 16454,  1472, 45612, 29956,  5602, 45302, 23284,
        5974, 11918, 24477, 41605, 37070, 42727,  7479, 23975, 20720,
       40233,  2655, 47767, 32156, 40494, 12986,  2002, 47086, 23819,
       10178, 49278, 43176, 41560, 47641, 32612, 35233, 14197, 36878,
       15821, 36507, 19190, 15005, 35019, 46685, 49864, 26021, 17367,
        4041, 17228, 13343, 41813, 37031, 29179, 29167, 32952, 28351,
       27889,  7657, 33088, 39816, 31298, 32209, 33991, 31210, 15096,
       36461,  8673, 44157, 40157, 27622, 18692, 21470,  6036, 14475,
       29551,  8575, 44809, 24731, 15056, 29805, 16517, 29700, 17990,
        5668, 26061, 31186, 21999,  5300, 44178, 23822, 32107, 34989,
       46992,  5426, 36886, 19009, 35283, 32721, 21044, 18803, 43628,
       16541, 16918, 26656, 36491, 34270,  9314, 27278, 46367, 36993,
       25578, 32081, 24104, 48560, 32713,  2660,  9890, 20035, 49856,
       46876, 37949,  6663, 19273, 19216,  8882, 17865,  2429, 37519,
       39927, 19196,  5702, 27155, 34040, 47381,  5331, 40418, 14138,
       16103, 39121, 34123, 31023, 45556, 12817, 48476, 12095, 25624,
        1664, 42456, 11280, 21023, 46756, 42630, 34799, 17303, 38368,
        9281, 26000, 42784, 40485, 32903, 49556, 27315, 25339, 36632,
       40343, 48389, 17093,  7256,   605, 28548, 37747, 15483, 45166,
       31144,  8365,  7377, 15622, 36482, 47243,  5873, 15636, 46576,
       45666, 12740, 45304, 32232, 28828, 49263, 33179, 28917, 24319,
       20233, 15707, 29879, 44476, 15949, 24834,  2675, 29099, 21440,
        6954, 19173, 32369, 37328, 41220, 42399, 44067, 36909, 22350,
       10911, 48105,  8444, 43138, 42628, 14325, 19664, 37358, 40448,
       20824, 39480, 37334, 43368, 38003, 46046, 48174,  4726, 29596,
       44749, 40420,  5216, 21903, 29634, 29444, 19812, 12764, 47456,
       21217, 34676, 20242, 38796, 33905, 44970,  2322,  4935,  7015,
       36506,  4941, 37738, 20124, 19370, 24814, 12730, 13660,    77,
       10267,  9619, 11643, 23224, 41457, 35490, 35548, 48096,  4337,
       47865, 19682, 34715, 23254, 46922, 15502, 18630, 46934,  5323,
       32444, 19828, 26027, 13223,  8524, 35524, 36415, 15473,  6919,
       36183, 24851, 40607, 17948, 49359, 37566, 47227, 21288, 31213,
         373, 22716, 25532, 10782, 19210, 25862, 27286, 33000, 44721,
       43220,  9697, 47754,  4311, 45457, 16554, 32761,  7528, 27358,
       42332, 18905, 16450, 22684,  6396, 33780, 17233, 34009, 38633,
       43667,  1142, 26744, 13650, 13055, 13649, 27533,  4229,  5589,
       48979, 48363,  4869,  6711, 40479, 43993, 30093,  6440, 35690,
       24051, 46987, 27434,  3958,   847, 38780, 23344, 27915, 42238,
       48697, 43150, 17398, 38595, 25539, 42996, 48204, 18017,  3364,
       27421, 40684, 39168, 22380, 38220, 27187, 46783, 34046, 30805,
       49183, 13058,  5784,  7621, 23970, 30385, 23456,  6876,  7076,
       16839, 15296, 38067, 43435, 40061,  1742, 29281, 31034, 43848,
       11974, 14983,  7325, 31316, 45103,  4399, 24898,  8817,  3434,
       17782, 13026, 15433, 44135, 46167, 39181, 46875, 19973, 33254,
       16589, 35093,  6776,   952,  1698, 22179, 27230, 12097, 32640,
       26494, 19064, 25122,  7836,  7756, 42360, 13703,  5117,  7030,
       34067, 16109, 15712, 48265, 32239, 27086, 25809, 40502, 21508,
       12690, 23778, 39307, 30195, 44874, 47556, 22046,  2049,  7849,
       18577, 28719, 25711,  8580, 12683, 25948, 26285, 41239, 40570,
       23265, 20550, 33191, 24974, 41626, 22782, 32298, 42685, 16227,
       34446, 48759, 47046, 24758, 32898, 30296, 22219, 10891,  8312,
        4903, 26249, 42659, 17314, 17181, 35523, 11059, 34329, 11469,
        8996,  1215, 17581, 39494, 24941, 38767, 16025,  8360, 37293,
       38440, 11520, 21852, 47184, 45027, 17469,  9741,  6398, 11393,
       12163, 41416, 22222, 17979, 49728, 27078, 11502, 44922, 26697,
       36839,   972,  6696, 47673, 18339,  5934, 26628,  6864, 18411,
       35253, 47039,  3413, 30972, 18823, 28150,  3208, 10884, 16171,
       40106, 41781,  8202, 21891, 29621, 42390, 24786,  5991, 11950,
       14692, 24995, 20893, 44457, 18108, 37930, 38176, 39077, 46754,
       48695, 28240, 16504, 33082, 22669,  8744, 19346, 43130,  2319,
       36065,  3989,  7986, 25637, 29370,   478, 39276, 37837, 13950,
        8646,  2311,  3191, 29016, 36897, 30004, 34277, 23547, 42959,
        8369, 40198, 33945, 31847, 39425, 19777, 19467,  5401, 46301,
       36265, 36409, 48453,  3553, 41496, 34226, 31263, 25329,   181,
       30984, 38394, 27936, 30781, 39547, 30053, 43255, 35133, 23486,
       32568]), [1, 9, 0, 7]), (array([15790, 19345,  2235, 18272, 28885,   247,  2385, 49024,  9395,
        9845,  9528,  7634, 37691, 31393, 49598, 47922, 32368,  7536,
       35429, 33666, 41294,  1923, 15533, 10861, 35526, 40375, 12108,
       41659, 16085, 21171, 22820, 30499, 32525, 41202,  4224, 44629,
       36630, 31124,  1728, 16968, 16426, 14957, 12094, 45255, 36105,
       44525, 18566, 25692, 28313, 42553, 28271, 45315, 28620, 25474,
       39740, 17498,  6652,  3981, 35978,  7110, 37954, 24883,   420,
       42310, 22749, 27199, 35395, 42795, 14429, 10697, 25371,  5042,
       19784, 49187,  4695, 21346,  3322,  2442, 23604, 17323,  4905,
       19168, 30958, 27205, 23071, 41836, 21693, 43032, 37903, 27801,
       47293, 49138, 17145,   632, 17552, 27757,  3556, 31585, 43620,
        3612, 40550, 18065, 43575, 31998, 37053,  5075, 30291, 19406,
       15773,  4158, 27276, 14284, 35450,  1644, 27923, 32224, 42656,
       29835, 41216, 40071,  4032, 37214, 38375, 27812, 30885, 13038,
       45571, 28577, 28649, 36896, 41737, 39265,  5756, 30289, 32643,
       11872, 34770, 27725,  3776, 35877, 40949, 40301, 46998, 16461,
        2858, 44994, 20654, 23638, 33369, 42229,  4129, 27391, 31143,
       45132,  1925, 42219, 25781, 21779,  9592, 37741, 46823, 23231,
       45515,  3176, 46276, 34352, 25401, 49743, 46446,  4640, 30707,
       23739, 33586,  1832, 30268, 20282, 36764, 45170, 36884, 31699,
        1406, 43364, 47928, 41127, 43180,  8263, 22318, 42398, 12896,
       18977, 23574,  9937,  6569, 49114, 31588, 10131, 47907, 49515,
       18606,  7218, 30413,  5625, 38248, 18628,  1001, 12208, 35113,
        2830, 27534, 42138, 13652,  5944,  1407, 41610, 39891, 15112,
       44844, 19721, 25024, 29202, 36276, 39958,   661, 18043, 22371,
       35641, 39853, 30911, 41955, 43857, 21949, 43746, 46648, 32494,
       14105, 38386, 20120, 28904, 35143,  4231, 13708, 43810, 42277,
        3283,  1953, 31515,  2155, 20498, 31683, 42040,  4234,   200,
       36365, 11911, 12635, 28395, 40031,  4606, 34394,  8215, 37759,
       14553, 14719, 18179, 45582, 43426,  5443,  9877,  7650, 12359,
        6664, 41049, 33178, 46123, 29436, 31784, 15060, 29097, 41641,
       24231, 28722, 13890, 15338, 37481, 22393, 10676,  3139, 24155,
       15351, 39511, 16387, 23082,  8877, 44342, 38396, 37261, 15349,
       49061, 17105, 23087,  1371, 34674, 20297, 24321, 23987, 46550,
       36069,  2100, 25234, 34786,  2729, 10025,  5159,  8431, 25396,
       26921, 37188, 48710, 31112, 44251, 16451, 21846,  4926, 28588,
        6096, 48854, 24882, 38671, 21024, 26139,  4389, 13802, 27708,
        1678, 47650, 19823, 26243, 36696, 27567,  5857, 19707, 32768,
       47513, 32739, 23108, 23616, 32381,   451,  3500, 23084, 15211,
       41707, 11233, 24722, 19731, 49636, 20977, 49200, 28252, 19798,
       26782,  1403, 27357, 47302, 31598, 20021,  6267, 47580,  2299,
       11647,  8363,  7911, 29426, 31629, 40921, 18309, 25681, 37564,
       32240, 17715, 46677, 12447, 49464, 30193, 24673, 24204, 45518,
       40549, 13965,  9241, 40302, 48070, 13306, 27240, 12004,  9084,
       43390, 23152, 23324, 27648,  1228, 46092, 32027, 31421,  7316,
       43764, 42641, 22746, 28859, 21292,  6686, 11729, 16883, 42046,
       12512, 40501, 40685, 36995, 39982, 39969,  3241, 22795, 49039,
       18615, 31175, 39161, 40437,  9994,  6273, 13209, 28998, 25430,
       12831,  8128,  5419, 28533, 44597, 14608, 48897, 44244, 13939,
        6750,  9208,  7064, 31140, 13455, 12168, 42616,  5689,  3070,
        1978,  2874, 39068, 42460, 30176, 38141,  6960,  1038, 49209,
        4659, 15185,  2267, 13878,  3686, 31118,  8697, 21588,  9303,
       20003, 39007, 26228, 49948, 17697, 28980, 26619, 35830, 23747,
        6294, 33630,  3341,  7473, 16442, 38786, 44999, 12293, 41466,
        9233,  9788, 35001, 25298, 39986,   862,  2941,  4421, 33244,
        7575, 38707, 49798, 18085, 13988, 29244, 28750,  8116, 33530,
       46631, 29343, 24445,  9925, 27080, 11451,  6904, 45348, 32479,
       17917,  5441, 44149, 41300, 21573, 10334, 43998, 12052, 42083,
       17453, 30257, 39149, 29732, 26723,  6609,  2034, 44709,  7674,
       13902, 11633,  1960,  7925, 46420, 26269, 33516, 22847, 38335,
       21758, 38217, 42984, 19386, 33003, 27706, 26824,   457, 25070,
       35603, 13110, 28126, 38921, 28360, 20127, 45190, 41435, 29336,
       24144, 21248, 38044, 40890, 13086, 32140, 41557,  3903,   940,
       12916,  8044, 34235, 25082, 24641, 15225, 41242, 26840, 13436,
       35179, 35269, 39914, 40648, 44443,  1594, 30968, 45393, 37955,
       47375, 43849, 14520,  5194,  5883,  7174, 27645, 22540, 41848,
       41055,  7251, 38036, 44770, 48489, 27562,  1234, 15466, 28408,
       13915, 16831, 40542,  8953, 15620, 19065, 49006, 24291, 29844,
       16116, 45621,  4940, 37693, 38307, 43657, 27004,  7168, 27850,
        3725, 28401, 19527, 38154, 20014, 20910, 46366, 17088,  8553,
       17107,  7212, 34917, 42103, 25270, 28003, 36149, 27758, 29641,
        6276,  6923, 42226, 26505, 38977, 19961, 39628, 44454, 35544,
       34448, 42199, 20015, 34421, 29772, 29888, 17156, 27873, 18436,
       40768,  4543, 49507, 36573, 25177, 28305, 25016, 22286, 41124,
       16903,   223,  5642, 11583, 34280,  9069, 37240, 26361, 23004,
       35381, 29578, 39083, 17226, 15735, 33368, 16191, 42241, 38341,
       49308, 10487,   905, 35798, 41019, 37199, 39435,  7537, 15509,
        7971, 10729, 18578, 11671,  5010, 36464, 49130, 44175, 48851,
       46362, 38433, 38222, 47678, 12634,  3438, 12785, 40287, 11178,
       33186, 43771, 48842, 19493, 27944,  7423, 30295,  1871,  7527,
       28515, 24685, 15255, 35859, 19371, 32538, 12509,  2345, 39606,
        3644, 21064, 45516, 32631, 33325, 34283, 35162, 22628, 26003,
       38320, 29665, 42998, 16346, 27724,  8136, 35201,   293, 28212,
       37718, 35683, 32432, 48464, 11446, 28256, 14403, 46869, 43229,
        5188,  5336, 45087,  9957, 42962, 14636,  6072, 34807, 36562,
       10547, 33524, 13022, 23298, 17069, 36495, 25617, 49141, 40119,
       26067,  5608,  7223, 49961,  4795,  5804,  2074, 43309, 20699,
       10529, 27248, 46097, 18828,  5313,  8859,  4933, 29526, 26895,
        7874, 43193, 44923,  7416, 15932, 17377, 21904, 42322, 38057,
       40567, 26166, 16654, 32286, 17804, 27583,  4325, 29441, 15824,
       17910, 35214, 30253, 15850,  7863, 25156,  2897,  4216, 34257,
       46172, 44753, 11951, 43060, 12394, 21967, 40439, 25410, 33850,
       18519,  4285, 14582,  5710, 33349, 37884,  1828,  4833, 25271,
       28201,  1577, 25197, 39328, 27952, 22678,  7609,  2063, 26515,
       25575, 24327, 18447, 23835, 39219, 25102,  8984, 37472, 24311,
       21301, 34240, 47125, 15081, 25228,  5536, 19408, 17129, 31528,
       45171, 45459, 27782, 38539, 22854, 42788, 34592,   956, 25916,
       22076,  3933, 20773,  1806, 33388, 37474, 30348, 10121, 24468,
        4871,  1808, 23753, 44845,  8907, 28264, 36833, 28206, 42831,
       19200, 47249, 44839, 37683, 34126, 37897, 47638, 37205, 49226,
       44514, 24235, 42338, 14095, 16868, 26107, 26562, 14144,  3261,
       25778, 20158, 49695, 33496,  1113, 16260, 28868,  5317,  8499,
         329, 16666,  2977, 31326, 17997, 12023,  5126, 41974,  6301,
       39782, 11278, 21876, 23435, 20476, 37635, 34569,  7798, 33203,
       36098, 42117,  5802, 49476, 28243,  9133, 14837,  6793, 25454,
       41195, 10988, 18041,  7186,  2480,  4872, 45833, 29800,  7845,
       28920, 31472, 12751, 14050, 44567, 14862, 33475, 16633,  8481,
       11214, 45999, 47285, 20218, 33553, 32148, 10552,  4809, 36034,
       40709, 40539,  7331, 28977, 29155, 16095,  2481, 24159,   382,
       11350,  9981, 39406,  7450, 10386,  9560, 46524, 42751,  4078,
       26669, 33860, 43531,  8391, 32992, 45407,  7725,  5560, 36829,
        6986]), [4, 6, 0, 7])]
Collaboration
DC 0, val_set_size=1000, COIs=[2, 8, 0, 7], M=tensor([2, 8, 0, 7], device='cuda:0'), Initial Performance: (0.222, 0.04485496115684509)
DC 1, val_set_size=1000, COIs=[5, 3, 0, 7], M=tensor([5, 3, 0, 7], device='cuda:0'), Initial Performance: (0.25, 0.044860809803009036)
DC 2, val_set_size=1000, COIs=[1, 9, 0, 7], M=tensor([1, 9, 0, 7], device='cuda:0'), Initial Performance: (0.221, 0.04458320701122284)
DC 3, val_set_size=1000, COIs=[4, 6, 0, 7], M=tensor([4, 6, 0, 7], device='cuda:0'), Initial Performance: (0.25, 0.04539239990711212)
D00: 1000 samples from classes {0, 7}
D01: 1000 samples from classes {0, 7}
D02: 1000 samples from classes {0, 7}
D03: 1000 samples from classes {0, 7}
D04: 1000 samples from classes {0, 7}
D05: 1000 samples from classes {0, 7}
D06: 1000 samples from classes {8, 2}
D07: 1000 samples from classes {8, 2}
D08: 1000 samples from classes {8, 2}
D09: 1000 samples from classes {8, 2}
D010: 1000 samples from classes {8, 2}
D011: 1000 samples from classes {8, 2}
D012: 1000 samples from classes {3, 5}
D013: 1000 samples from classes {3, 5}
D014: 1000 samples from classes {3, 5}
D015: 1000 samples from classes {3, 5}
D016: 1000 samples from classes {3, 5}
D017: 1000 samples from classes {3, 5}
D018: 1000 samples from classes {1, 9}
D019: 1000 samples from classes {1, 9}
D020: 1000 samples from classes {1, 9}
D021: 1000 samples from classes {1, 9}
D022: 1000 samples from classes {1, 9}
D023: 1000 samples from classes {1, 9}
D024: 1000 samples from classes {4, 6}
D025: 1000 samples from classes {4, 6}
D026: 1000 samples from classes {4, 6}
D027: 1000 samples from classes {4, 6}
D028: 1000 samples from classes {4, 6}
D029: 1000 samples from classes {4, 6}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO4']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.362, 0.06002717798948288) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.0816035498380661) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.304, 0.09341755312681198) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.0939168337881565) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.06308483767509461) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.08910550057888031) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.352, 0.13188596749305725) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.396, 0.11786670678853989) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.07532635158300399) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.289, 0.10358087551593781) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.397, 0.15207326704263688) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.15404279613494873) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.463, 0.105178024366498) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.11446061944961548) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.17986074367165567) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.19359765738248824) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.12571522931009532) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.351, 0.12731943529844283) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.423, 0.17620865178108217) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.41, 0.21025887111574412) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO2', '(DO3']
DC 1 --> ['(DO4', '(DO1']
DC 2 --> ['(DO0']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.47, 0.14941069076955318) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.357, 0.1479798818230629) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.408, 0.17783011223375797) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.425, 0.2532032940760255) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.15933214899897574) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.366, 0.16829006430506707) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.403, 0.21066069585829975) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.445, 0.24311571361124515) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.19129687175899743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.365, 0.16548691460490228) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.20810078812390567) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.433, 0.27205386623740196) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.1845419636927545) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.15964543256163596) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.425, 0.23719379422068596) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.429, 0.2844371333681047) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.456, 0.2080296144457534) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.367, 0.12997722536325454) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.431, 0.219299509100616) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.428, 0.3079838444199413) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[0, 7], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.694, 0.019268679186701776)
DC Expert-0, val_set_size=500, COIs=[8, 2], M=tensor([2, 8, 0, 7], device='cuda:0'), Initial Performance: (0.912, 0.007428182298317551)
DC Expert-1, val_set_size=500, COIs=[3, 5], M=tensor([5, 3, 0, 7], device='cuda:0'), Initial Performance: (0.734, 0.018887594819068907)
DC Expert-2, val_set_size=500, COIs=[1, 9], M=tensor([1, 9, 0, 7], device='cuda:0'), Initial Performance: (0.862, 0.010827436789870262)
DC Expert-3, val_set_size=500, COIs=[4, 6], M=tensor([4, 6, 0, 7], device='cuda:0'), Initial Performance: (0.856, 0.012435773532837629)
SUPER-DC 0, val_set_size=1000, COIs=[2, 8, 0, 7], M=tensor([2, 8, 0, 7], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 3, 0, 7], M=tensor([5, 3, 0, 7], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[1, 9, 0, 7], M=tensor([1, 9, 0, 7], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[4, 6, 0, 7], M=tensor([4, 6, 0, 7], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x775fe83e6880>, <fl_market.actors.data_consumer.DataConsumer object at 0x775ff4076e20>, <fl_market.actors.data_consumer.DataConsumer object at 0x775fe00a0280>, <fl_market.actors.data_consumer.DataConsumer object at 0x775fe83bb040>, <fl_market.actors.data_consumer.DataConsumer object at 0x775ff4111550>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8885, 0.009017482049763203) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004297792855650187) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.748, 0.02119672656059265) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.010797300979495049) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.904, 0.009403337262570859) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.497, 0.13103291171044112) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.509, 0.04391020053625107) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.551, 0.055669838845729826) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.457, 0.1263257073163986) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9205, 0.006324204462580383) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.948, 0.00515366542711854) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.746, 0.024218165040016174) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.010830807089805604) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.896, 0.010121155459433795) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.518, 0.07545800440013409) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.507, 0.05257538765668869) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.673, 0.03459341457486153) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.532, 0.060239147245883945) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.939, 0.004899242293089628) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.0050551714431494475) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.752, 0.022134942591190337) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.011840552330017089) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.010220091089606285) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.66, 0.03595777641236782) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.06409971638023854) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.692, 0.031260340064764025) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.551, 0.0560915507376194) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.937, 0.005223898976575583) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.00455085981823504) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.754, 0.02350652003288269) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.011237274013459683) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.888, 0.009657844051718711) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.664, 0.03996868115663529) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.06322136338055134) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.72, 0.030964350610971452) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.58, 0.04455454242229462) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9395, 0.005977630711859092) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004287715047597885) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.742, 0.02512884944677353) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.87, 0.01315960744023323) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.01239104825258255) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.683, 0.035877175599336626) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.503, 0.08104187764972448) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.697, 0.03229703626036644) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.54, 0.057586215138435363) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.939, 0.006084991808747873) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.0045615926440805196) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.746, 0.029541323482990264) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.013212281212210655) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.896, 0.010499152570962907) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.711, 0.03113323584198952) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.517, 0.07317690617963672) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.699, 0.03087443286180496) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.581, 0.04847087502479553) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.944, 0.005616168867098168) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.004710092140361667) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.732, 0.027466668486595153) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.015042437307536603) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.013117767853662372) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.688, 0.03329588544368744) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.554, 0.06919169783592224) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.717, 0.029657010078430177) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.601, 0.045189299941062924) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.942, 0.006503733289573575) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005198364500887692) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.762, 0.02651445209980011) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.01281659135222435) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.892, 0.012516068596858531) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.04824644324183464) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.543, 0.07041721229255199) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.662, 0.039193948119878766) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.627, 0.04482607507705688) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9495, 0.005315031007048674) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005708596316166222) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.758, 0.0260559241771698) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.872, 0.014272011205554009) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.012641754154115916) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.661, 0.042771487891674044) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.544, 0.07209303720295429) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.68, 0.03412531082332134) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.629, 0.04625026834011078) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.005955714851399534) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.005850264770211652) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.748, 0.029731225430965423) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.866, 0.014592355206608773) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.89, 0.012184992646798492) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.672, 0.040344163596630095) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.07392553390562534) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.707, 0.03146692110598087) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.584, 0.05290405142307281) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.005877966505213408) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005953696301206946) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.754, 0.02697554987668991) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.878, 0.011971519872546197) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.9, 0.01267752192541957) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.645, 0.048512087881565094) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.544, 0.07358797216415405) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.639, 0.049495531283318996) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.04876721954345703) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.005654444179075653) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005737656375393272) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.742, 0.028857150435447693) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.874, 0.013947157308459282) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.011848557744175196) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.619, 0.05433557453751564) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.06279208239912987) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.658, 0.0417011099755764) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.625, 0.05011731144785881) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9485, 0.006127284469344886) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.006265317015000619) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.734, 0.034678617179393766) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.014851408444344998) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.884, 0.013959551811218262) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.666, 0.04287031701207161) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.06686757481098175) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.68, 0.03831150540709496) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.611, 0.05242475324869156) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9505, 0.005114465093240142) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.005722355669364333) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.744, 0.030484130918979644) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.0162195645570755) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.908, 0.011855180766433477) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.647, 0.05097085815668106) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.573, 0.06109266865253449) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.653, 0.042188548550009725) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.637, 0.04785869997739792) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9505, 0.006526180666332948) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005305060749407857) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.756, 0.031028469860553742) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.015682812958955766) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.01229898259602487) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.628, 0.052030397891998294) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.568, 0.06825686467066407) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.62, 0.055236054718494416) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.652, 0.04510134744644165) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.005860555943989311) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.0051734285969287155) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.734, 0.031029437661170958) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.01689546862244606) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.9, 0.014089046061038972) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.625, 0.055314934879541394) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.575, 0.056315632477402684) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.646, 0.04943003697693348) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.638, 0.049603738009929656) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.95, 0.006198107082847855) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005943666495499201) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.756, 0.036121662616729736) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.017349191918969154) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.013969001106917858) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.659, 0.047363149255514145) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.566, 0.05924745510518551) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.651, 0.04430511170625687) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.649, 0.04912841528654099) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9505, 0.006527169869703357) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005911032074247487) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.752, 0.03651575803756714) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.015374836713075638) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.912, 0.013609345830976963) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.060529067367315295) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.564, 0.05864382387697697) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.615, 0.06017570652812719) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.664, 0.04825380671024322) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9545, 0.006236877883871785) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005554223378654569) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.76, 0.029170496225357057) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.016446656122803688) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.914, 0.013394506742246449) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.06327211627364158) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.577, 0.05862231661379337) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.601, 0.06710869045555591) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.587, 0.062054477274417874) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.007515407568400406) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005600333172827959) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.75, 0.027617149531841276) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.0160817628800869) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.894, 0.012147045396268367) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.622, 0.05516831009089947) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.0677955705076456) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.656, 0.0457243809401989) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.631, 0.048579619288444516) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9485, 0.00735480066146556) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006195930215530098) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.746, 0.03321506088972092) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.017079465091228485) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.904, 0.012588542014360427) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.59, 0.0696602246761322) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.05390386513620615) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.61, 0.06164540457725525) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.611, 0.05304210102558136) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.005464293666242156) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.006291248543886468) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.75, 0.032142294704914094) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.017290930271148683) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.9, 0.011357045870274305) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.575, 0.06225687238574028) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.05057204782962799) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.651, 0.04562803713977337) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.608, 0.05446404957771301) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.007022196061705472) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005804240017198026) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.772, 0.03343166273832321) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.014932791605591774) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.904, 0.015119241066277028) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.06640264859795571) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.575, 0.060479911640286446) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.65, 0.050494829759001734) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.606, 0.05661853063106537) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9485, 0.006135213394132734) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.006556540207238868) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.752, 0.03445323491096496) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.88, 0.015904546558856963) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.01663784915395081) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.571, 0.07329550984501838) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.616, 0.0477035790681839) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.641, 0.04598713132739067) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.591, 0.06143334859609604) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9535, 0.006602206971350825) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.006484678628097754) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.738, 0.036846770584583285) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.882, 0.01805600932240486) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.906, 0.012587792366743088) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.587, 0.07017443779110909) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.566, 0.05194247880578041) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.636, 0.050275278866291045) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.605, 0.056871474385261536) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.008077341988007901) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005859681808506139) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.762, 0.03274582433700562) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.864, 0.01625825569033623) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.904, 0.017618310138583185) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.613, 0.06923991093039512) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.06278990599513054) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.611, 0.05609486949443817) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.575, 0.06256235563755036) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.006873163755326459) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006613569148350507) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.758, 0.034571892082691194) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.014046821042895317) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.014331717845983803) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.545, 0.0762738646864891) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.614, 0.04357887616753578) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.626, 0.04854845689237118) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.607, 0.05645317083597183) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.007308193138175738) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.006201122699771076) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.736, 0.03679485189914703) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.013891585722565651) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.898, 0.016244356414303184) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.559, 0.07869742688536643) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.05349963596463204) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.632, 0.0539608428478241) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.616, 0.055208660185337065) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9495, 0.007551439927330648) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.00616200809320435) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.756, 0.036658850014209744) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.012481400169432163) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.904, 0.015550713865086436) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.615, 0.058148572921752927) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.053507970467209814) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.633, 0.04789190703630447) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.586, 0.06009140169620514) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9525, 0.007315370728250855) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.006954634671375971) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.742, 0.03916192615032196) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.0158594411816448) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.904, 0.013726473782211542) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.623, 0.06134472921490669) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.0491623375415802) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.683, 0.040614011839032176) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.615, 0.05096712166070938) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9555, 0.006689915317901978) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0060391399980289865) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.746, 0.03284591287374496) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.015003858998417854) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.908, 0.01575712923333049) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.614, 0.05718509009480476) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.607, 0.04803709127008915) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.623, 0.04710233406722546) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.613, 0.05801785865426064) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9495, 0.007902584345748892) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.006623186942189932) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.74, 0.04065038108825684) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.898, 0.01694205157458782) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.9, 0.017583593145012854) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.611, 0.06275354090332985) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.612, 0.05072023415565491) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.683, 0.03686046642065048) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.584, 0.06162384551763535) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.951, 0.006613556618140138) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.006071843766607344) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.736, 0.039555463433265686) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.886, 0.014584975384175778) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.912, 0.015580663412809372) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.64, 0.04799392780661583) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.547, 0.05390434217453003) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.661, 0.041891336679458616) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.599, 0.061634629279375075) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9535, 0.007277750954268413) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.006390455989632756) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.75, 0.033890890896320344) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.892, 0.016260871335864065) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.896, 0.01575482964515686) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.631, 0.04939744871854782) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.579, 0.05514831683039665) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.688, 0.03832539215683937) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.589, 0.061479236662387846) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.953, 0.007995588452595712) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006764117145095952) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.752, 0.030808127403259278) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.894, 0.014359250139445067) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.912, 0.015017238406464458) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.608, 0.06584734806418419) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.587, 0.055886105820536616) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.649, 0.044033429369330405) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.612, 0.05509016281366348) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9465, 0.008289666010326982) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.007684733581438195) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.754, 0.03362415808439255) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.89, 0.01513003295660019) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.896, 0.017304284192621706) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.572, 0.06401542477309703) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.555, 0.058314854711294176) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.676, 0.041246343314647674) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.598, 0.06264482629299163) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.948, 0.00958880123598533) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006267423614859581) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.746, 0.03766034257411957) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.876, 0.015503156617283822) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.906, 0.013398888386785985) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.605, 0.050273188710212705) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.497, 0.06591967406868934) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.697, 0.0378108706176281) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.539, 0.07235001474618911) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.949, 0.008367049012723327) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0063053463469259444) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.752, 0.0350168417096138) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.015728516317903996) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.906, 0.014352702528238297) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.634, 0.048056321203708646) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.0564614290073514) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.675, 0.0409713596701622) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.592, 0.0650595635175705) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.954, 0.008271441690425263) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.007153176225721836) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.75, 0.03570543271303177) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.884, 0.017258081153035162) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.912, 0.013095636643469334) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.624, 0.050370897084474564) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.568, 0.05988364142179489) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.692, 0.0377253373414278) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.583, 0.06576491719484329) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9505, 0.009228467183856993) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.006168263430707157) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.756, 0.03392372018098831) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.888, 0.015939206644892692) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.918, 0.013226704336702824) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.577, 0.06638425576686859) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.589, 0.055422200694680214) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.637, 0.05300222882628441) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.579, 0.07226234382390975) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.222, 0.04485496115684509), (0.362, 0.06002717798948288), (0.451, 0.06308483767509461), (0.463, 0.07532635158300399), (0.463, 0.105178024366498), (0.465, 0.12571522931009532), (0.47, 0.14941069076955318), (0.464, 0.15933214899897574), (0.469, 0.19129687175899743), (0.468, 0.1845419636927545), (0.456, 0.2080296144457534), (0.497, 0.13103291171044112), (0.518, 0.07545800440013409), (0.66, 0.03595777641236782), (0.664, 0.03996868115663529), (0.683, 0.035877175599336626), (0.711, 0.03113323584198952), (0.688, 0.03329588544368744), (0.634, 0.04824644324183464), (0.661, 0.042771487891674044), (0.672, 0.040344163596630095), (0.645, 0.048512087881565094), (0.619, 0.05433557453751564), (0.666, 0.04287031701207161), (0.647, 0.05097085815668106), (0.628, 0.052030397891998294), (0.625, 0.055314934879541394), (0.659, 0.047363149255514145), (0.609, 0.060529067367315295), (0.6, 0.06327211627364158), (0.622, 0.05516831009089947), (0.59, 0.0696602246761322), (0.575, 0.06225687238574028), (0.618, 0.06640264859795571), (0.571, 0.07329550984501838), (0.587, 0.07017443779110909), (0.613, 0.06923991093039512), (0.545, 0.0762738646864891), (0.559, 0.07869742688536643), (0.615, 0.058148572921752927), (0.623, 0.06134472921490669), (0.614, 0.05718509009480476), (0.611, 0.06275354090332985), (0.64, 0.04799392780661583), (0.631, 0.04939744871854782), (0.608, 0.06584734806418419), (0.572, 0.06401542477309703), (0.605, 0.050273188710212705), (0.634, 0.048056321203708646), (0.624, 0.050370897084474564), (0.577, 0.06638425576686859)]
TEST: 
[(0.21525, 0.0437830693423748), (0.36075, 0.05810387420654297), (0.442, 0.06061009407043457), (0.4575, 0.07175614351034164), (0.465, 0.09945581325888633), (0.45975, 0.11858130222558975), (0.4675, 0.140495234310627), (0.4625, 0.14899606961011885), (0.46325, 0.1790197706222534), (0.465, 0.17290244770050048), (0.4485, 0.19369042420387267), (0.509, 0.11806378921866417), (0.5175, 0.07218220072984695), (0.6515, 0.03594643679261208), (0.6515, 0.04093957306444645), (0.67875, 0.03566824012249708), (0.69625, 0.03335117048025131), (0.66425, 0.036906860649585725), (0.62425, 0.049423456802964214), (0.63675, 0.04552981636673212), (0.667, 0.04028078344464302), (0.63275, 0.04917961908876896), (0.61975, 0.05544967316091061), (0.645, 0.04545445370674133), (0.63575, 0.054436303615570066), (0.6195, 0.05545941071212292), (0.62725, 0.05561341264843941), (0.636, 0.049469930559396746), (0.59425, 0.06217368516325951), (0.60625, 0.06482233585417271), (0.63575, 0.05507913000881672), (0.588, 0.06944178175926209), (0.583, 0.06318581795692443), (0.606, 0.06521357730031013), (0.5635, 0.07186735606193542), (0.58675, 0.0700127333253622), (0.616, 0.06837788143754006), (0.5505, 0.07482714995741845), (0.569, 0.07780607621371746), (0.61, 0.057978017389774324), (0.60875, 0.06349316385388375), (0.6025, 0.05912351332604885), (0.5975, 0.06452120944857598), (0.6135, 0.05016860330849886), (0.62625, 0.04823044189065695), (0.5975, 0.06537387861311436), (0.577, 0.06213899084925652), (0.615, 0.05014467795938254), (0.62175, 0.04739807680249214), (0.62775, 0.04933970373868942), (0.57475, 0.06657504667341708)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.47      0.80      0.59      1000
           2       0.56      0.75      0.64      1000
           7       0.76      0.57      0.65      1000
           8       0.78      0.19      0.30      1000

    accuracy                           0.57      4000
   macro avg       0.64      0.57      0.55      4000
weighted avg       0.64      0.57      0.55      4000

Collaboration_DC_1
VAL: 
[(0.25, 0.044860809803009036), (0.25, 0.0816035498380661), (0.25, 0.08910550057888031), (0.289, 0.10358087551593781), (0.366, 0.11446061944961548), (0.351, 0.12731943529844283), (0.357, 0.1479798818230629), (0.366, 0.16829006430506707), (0.365, 0.16548691460490228), (0.373, 0.15964543256163596), (0.367, 0.12997722536325454), (0.509, 0.04391020053625107), (0.507, 0.05257538765668869), (0.524, 0.06409971638023854), (0.535, 0.06322136338055134), (0.503, 0.08104187764972448), (0.517, 0.07317690617963672), (0.554, 0.06919169783592224), (0.543, 0.07041721229255199), (0.544, 0.07209303720295429), (0.529, 0.07392553390562534), (0.544, 0.07358797216415405), (0.558, 0.06279208239912987), (0.546, 0.06686757481098175), (0.573, 0.06109266865253449), (0.568, 0.06825686467066407), (0.575, 0.056315632477402684), (0.566, 0.05924745510518551), (0.564, 0.05864382387697697), (0.577, 0.05862231661379337), (0.562, 0.0677955705076456), (0.598, 0.05390386513620615), (0.584, 0.05057204782962799), (0.575, 0.060479911640286446), (0.616, 0.0477035790681839), (0.566, 0.05194247880578041), (0.562, 0.06278990599513054), (0.614, 0.04357887616753578), (0.58, 0.05349963596463204), (0.584, 0.053507970467209814), (0.589, 0.0491623375415802), (0.607, 0.04803709127008915), (0.612, 0.05072023415565491), (0.547, 0.05390434217453003), (0.579, 0.05514831683039665), (0.587, 0.055886105820536616), (0.555, 0.058314854711294176), (0.497, 0.06591967406868934), (0.562, 0.0564614290073514), (0.568, 0.05988364142179489), (0.589, 0.055422200694680214)]
TEST: 
[(0.25, 0.04378252673149109), (0.25, 0.07828381612896919), (0.25, 0.08545986431837081), (0.2835, 0.09934729871153831), (0.34575, 0.10978565776348113), (0.357, 0.12227776199579239), (0.35475, 0.1418950108885765), (0.36225, 0.16125718301534653), (0.361, 0.15870756310224532), (0.36225, 0.15440980315208436), (0.36025, 0.12609325283765793), (0.50625, 0.042655156672000885), (0.512, 0.05302776227891445), (0.51575, 0.06504854273796082), (0.53125, 0.06433938424289226), (0.51, 0.08021790301799774), (0.519, 0.0735306706726551), (0.53475, 0.07328501760959626), (0.5415, 0.07422568219900132), (0.53725, 0.07408703669905663), (0.54175, 0.07644546496868133), (0.5435, 0.07554048481583596), (0.5435, 0.06712637911736966), (0.53975, 0.06838987416028977), (0.5645, 0.06385577668249608), (0.547, 0.07179908046126365), (0.5645, 0.05716985414922238), (0.5685, 0.06122744525969028), (0.56675, 0.062145228981971744), (0.554, 0.05971417579054832), (0.54375, 0.07127505171298981), (0.563, 0.05816256856918335), (0.573, 0.05327299204468727), (0.555, 0.0639974514245987), (0.5835, 0.05019523125886917), (0.56325, 0.05470936137437821), (0.54875, 0.06660418757796288), (0.58625, 0.046580067172646525), (0.57125, 0.055377617031335834), (0.5705, 0.05496169799566269), (0.57275, 0.050490926429629324), (0.578, 0.051558804586529734), (0.573, 0.05385897871851921), (0.55175, 0.056150600895285604), (0.56, 0.05954688028991222), (0.56525, 0.06109247225522995), (0.561, 0.05877105550467968), (0.50525, 0.06694783741235732), (0.53875, 0.06043732307851315), (0.5605, 0.06086544550955295), (0.567, 0.05912433633208275)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.85      0.80      0.83      1000
           3       0.52      0.43      0.47      1000
           5       0.57      0.30      0.39      1000
           7       0.43      0.73      0.54      1000

    accuracy                           0.57      4000
   macro avg       0.59      0.57      0.56      4000
weighted avg       0.59      0.57      0.56      4000

Collaboration_DC_2
VAL: 
[(0.221, 0.04458320701122284), (0.304, 0.09341755312681198), (0.352, 0.13188596749305725), (0.397, 0.15207326704263688), (0.413, 0.17986074367165567), (0.423, 0.17620865178108217), (0.408, 0.17783011223375797), (0.403, 0.21066069585829975), (0.425, 0.20810078812390567), (0.425, 0.23719379422068596), (0.431, 0.219299509100616), (0.551, 0.055669838845729826), (0.673, 0.03459341457486153), (0.692, 0.031260340064764025), (0.72, 0.030964350610971452), (0.697, 0.03229703626036644), (0.699, 0.03087443286180496), (0.717, 0.029657010078430177), (0.662, 0.039193948119878766), (0.68, 0.03412531082332134), (0.707, 0.03146692110598087), (0.639, 0.049495531283318996), (0.658, 0.0417011099755764), (0.68, 0.03831150540709496), (0.653, 0.042188548550009725), (0.62, 0.055236054718494416), (0.646, 0.04943003697693348), (0.651, 0.04430511170625687), (0.615, 0.06017570652812719), (0.601, 0.06710869045555591), (0.656, 0.0457243809401989), (0.61, 0.06164540457725525), (0.651, 0.04562803713977337), (0.65, 0.050494829759001734), (0.641, 0.04598713132739067), (0.636, 0.050275278866291045), (0.611, 0.05609486949443817), (0.626, 0.04854845689237118), (0.632, 0.0539608428478241), (0.633, 0.04789190703630447), (0.683, 0.040614011839032176), (0.623, 0.04710233406722546), (0.683, 0.03686046642065048), (0.661, 0.041891336679458616), (0.688, 0.03832539215683937), (0.649, 0.044033429369330405), (0.676, 0.041246343314647674), (0.697, 0.0378108706176281), (0.675, 0.0409713596701622), (0.692, 0.0377253373414278), (0.637, 0.05300222882628441)]
TEST: 
[(0.231, 0.043520193308591845), (0.31675, 0.08954067873954773), (0.36975, 0.12627470338344574), (0.4045, 0.14565940433740615), (0.3995, 0.17283708488941193), (0.41275, 0.16908486545085907), (0.3955, 0.17159293580055238), (0.3935, 0.20377891224622727), (0.42225, 0.20144247567653656), (0.41775, 0.2316529153585434), (0.41525, 0.21458645927906037), (0.533, 0.05632997907698154), (0.62925, 0.03774472846835852), (0.66475, 0.03356490585207939), (0.66, 0.03568269892036915), (0.6765, 0.03488312916457653), (0.66575, 0.03597061864286661), (0.69225, 0.03349281353503466), (0.64375, 0.04293398655951023), (0.66875, 0.03948394686728716), (0.68525, 0.036739380285143855), (0.636, 0.05422855494916439), (0.64675, 0.047444202497601506), (0.659, 0.04384075626730919), (0.64175, 0.04696997584402561), (0.618, 0.06129800166189671), (0.61375, 0.05697860108315945), (0.637, 0.04816288037598133), (0.59975, 0.0661789910942316), (0.58425, 0.07372507125139237), (0.6435, 0.05000343826413155), (0.59625, 0.06469246034324169), (0.6395, 0.04856472891569138), (0.63025, 0.05367337965965271), (0.6345, 0.049174214370548724), (0.618, 0.0525114716514945), (0.605, 0.058871415197849275), (0.6145, 0.0519861401244998), (0.6175, 0.057644555047154426), (0.628, 0.05132194656133652), (0.659, 0.04352392909675837), (0.60875, 0.05021517486870289), (0.6705, 0.04008010947704315), (0.6325, 0.04736782761663198), (0.6395, 0.043529857464134694), (0.64, 0.04688084471225738), (0.6475, 0.042559467576444146), (0.6665, 0.03902723438292742), (0.65, 0.04411421155929565), (0.6735, 0.04036007043719292), (0.606, 0.05414381581544876)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.57      0.76      0.65      1000
           1       0.77      0.47      0.58      1000
           7       0.84      0.49      0.62      1000
           9       0.48      0.71      0.57      1000

    accuracy                           0.61      4000
   macro avg       0.67      0.61      0.61      4000
weighted avg       0.67      0.61      0.61      4000

Collaboration_DC_3
VAL: 
[(0.25, 0.04539239990711212), (0.25, 0.0939168337881565), (0.396, 0.11786670678853989), (0.425, 0.15404279613494873), (0.428, 0.19359765738248824), (0.41, 0.21025887111574412), (0.425, 0.2532032940760255), (0.445, 0.24311571361124515), (0.433, 0.27205386623740196), (0.429, 0.2844371333681047), (0.428, 0.3079838444199413), (0.457, 0.1263257073163986), (0.532, 0.060239147245883945), (0.551, 0.0560915507376194), (0.58, 0.04455454242229462), (0.54, 0.057586215138435363), (0.581, 0.04847087502479553), (0.601, 0.045189299941062924), (0.627, 0.04482607507705688), (0.629, 0.04625026834011078), (0.584, 0.05290405142307281), (0.625, 0.04876721954345703), (0.625, 0.05011731144785881), (0.611, 0.05242475324869156), (0.637, 0.04785869997739792), (0.652, 0.04510134744644165), (0.638, 0.049603738009929656), (0.649, 0.04912841528654099), (0.664, 0.04825380671024322), (0.587, 0.062054477274417874), (0.631, 0.048579619288444516), (0.611, 0.05304210102558136), (0.608, 0.05446404957771301), (0.606, 0.05661853063106537), (0.591, 0.06143334859609604), (0.605, 0.056871474385261536), (0.575, 0.06256235563755036), (0.607, 0.05645317083597183), (0.616, 0.055208660185337065), (0.586, 0.06009140169620514), (0.615, 0.05096712166070938), (0.613, 0.05801785865426064), (0.584, 0.06162384551763535), (0.599, 0.061634629279375075), (0.589, 0.061479236662387846), (0.612, 0.05509016281366348), (0.598, 0.06264482629299163), (0.539, 0.07235001474618911), (0.592, 0.0650595635175705), (0.583, 0.06576491719484329), (0.579, 0.07226234382390975)]
TEST: 
[(0.25, 0.044226822316646576), (0.25025, 0.09019724974036217), (0.40925, 0.11303245615959168), (0.4305, 0.14766822332143784), (0.43575, 0.18539260530471802), (0.41875, 0.20131079435348512), (0.4305, 0.24464485538005828), (0.4475, 0.23358500742912292), (0.4465, 0.2583946565389633), (0.439, 0.27387344324588775), (0.4385, 0.2942857506275177), (0.46325, 0.12510263708233832), (0.5215, 0.0582461102604866), (0.535, 0.055184561163187026), (0.59625, 0.04143410348892212), (0.51425, 0.05842551264166832), (0.58875, 0.04801595349609852), (0.59375, 0.043348997555673126), (0.63825, 0.042449578776955606), (0.6295, 0.045721820294857024), (0.60725, 0.04942357833683491), (0.6265, 0.047030547976493836), (0.63975, 0.047298043236136435), (0.604, 0.04967977119982243), (0.63475, 0.04379389236867428), (0.64875, 0.0440600378960371), (0.64375, 0.04686324714124203), (0.64, 0.047120421662926674), (0.6405, 0.045782043740153315), (0.59325, 0.05953664955496788), (0.6485, 0.04487352826446295), (0.633, 0.049008394941687586), (0.6285, 0.05038919372856617), (0.58675, 0.05575970819592476), (0.60025, 0.0565937537625432), (0.61225, 0.054310168161988255), (0.59625, 0.05841769824922085), (0.61525, 0.05239162981510162), (0.6325, 0.0500702514052391), (0.583, 0.058743259638547894), (0.6285, 0.04771106632053852), (0.627, 0.05492952477931976), (0.5965, 0.05794139125943184), (0.60125, 0.05827446623146534), (0.57225, 0.059915895707905294), (0.6145, 0.052652133397758004), (0.588, 0.05907673736661673), (0.54125, 0.06771361583471298), (0.58225, 0.0610446869134903), (0.573, 0.062487676545977595), (0.57975, 0.06801601216197013)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.83      0.57      0.68      1000
           4       0.40      0.68      0.50      1000
           6       0.86      0.72      0.78      1000
           7       0.45      0.35      0.39      1000

    accuracy                           0.58      4000
   macro avg       0.63      0.58      0.59      4000
weighted avg       0.63      0.58      0.59      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [23]
name: alliance-4-dcs-23
score_metric: contrloss
aggregation: <function fed_avg at 0x76b2c7336c10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=23
Partitioning data
[[0, 6, 4, 1], [5, 7, 4, 1], [9, 8, 4, 1], [3, 2, 4, 1]]
[(array([32659,  7396,   871,  8190, 33489, 44742,  8515,  5082,   965,
       11208,   700, 39329, 40740, 22304, 44430,   974, 28487, 44470,
       46964, 24750, 26632, 28921, 43184,  6663, 28273, 30029, 46367,
       39397, 38683, 15767, 48108, 33229, 13937, 41040,   276,  9831,
       45344,   927, 35963,  8842, 30591, 38617, 45559, 43659, 27768,
        2617, 28750,  1234,  5549, 34259, 20709, 33584, 29856, 34421,
       39553, 49734, 33380, 49130, 11897,  7445,  5003, 11958, 18763,
       26018,   600, 46936, 26667, 12477, 25023, 32870, 43077, 35570,
        9077, 16939, 46873, 32990, 37041, 39271, 47483,    93,  9121,
       33662, 34724, 33478, 49672, 12807, 45297,  6328, 20985, 35633,
       46625, 12674, 31391, 16554,  3347,    77, 48059, 30820,  9945,
       18102, 22924, 12297, 48777, 17222,  5557, 21815, 36415, 42231,
       12944, 47726, 47068, 30888,  7256, 33268, 22951, 42165, 39628,
       29380,  3564, 21143, 37035,  2427, 28000,  2010,   757,  7203,
       22118, 21803, 41745,  5349, 28272, 47329,  7925, 47339,  1715,
       21458, 16196, 38711, 12492, 17316,  5414, 48001, 21964,  1831,
       28305, 11218, 24113,  5642,  7263, 47994, 25199, 43548, 40956,
       42456,  4567,  4726,  9614, 28716, 32233,  2622, 13656, 27202,
       11372, 38791, 22458, 26656, 43427, 17620,  3731, 45232,  5093,
       45594, 20452, 16553, 11122, 14666, 44552, 24983, 19094,  6952,
       13850, 14434, 32107, 43475, 26764, 38307,  1376, 22194,  3721,
        1887, 43253, 34989, 10141, 10334,  4239, 17200, 33573,   348,
       14732, 31450, 46419, 39196, 41880, 14325, 11002, 38748, 45652,
       29881, 16647, 13031, 11615,  3987, 32056, 23776, 21469, 40556,
       20574, 49349, 13348,  1757, 29593, 44770, 13229, 10265,  2459,
       31083, 41242, 46749, 19004, 42929, 11682, 16235, 33258,  7772,
       48317, 37646, 17237,  5640,   436, 37078, 42829, 16632, 36386,
       38217, 44806, 26918, 39774, 45166, 43045, 28014, 37774, 45607,
       29833,  8942, 35340, 16404, 26339,  2252, 39512,  6096,   745,
       36348, 13137, 46342,  7848,  6581,  5785, 23691,  5176, 40551,
       24184,  6941,  5708, 42510, 18816, 20104, 13825, 39091, 17072,
       41444, 14293, 31042, 24955, 39889, 17957,  3307, 34684, 27177,
       15726, 14678, 49462, 33216, 46404, 23050, 45862,  8950, 19228,
       40879, 35455, 35585,  6093, 47497, 26275, 30222, 23451, 17105,
       31894, 42415, 40675, 18924, 42671, 25551, 47821, 10327, 13188,
       34804, 20310,  4094, 15552, 41672, 11070, 23539, 44258, 20141,
       18334, 31820, 12930, 21831, 15220,  5497, 13838, 23683,  5263,
        3655, 36913, 36192, 39148,  6521, 20734,  5419,  9125, 42046,
       16611,  7824,  9862,  9237,  5055, 42712, 31808, 40568, 34986,
        4785, 21155, 23760, 42850,   914, 34056, 19230, 35435, 21192,
       22909, 28106, 35600, 49747, 14024, 40929, 42778, 23390, 42460,
       11303,  8466, 35891,  6179, 33278,  7825, 15664, 12411, 48526,
       44422, 25680, 29381,  7632, 15517, 31806,  7335, 10431, 29241,
        3403, 22157, 30800,  3823,  4959, 44314, 10176, 31780, 38747,
       44612, 45637, 38864, 22986,  3435, 38893, 47513,  3841,   819,
        7715,  7705, 35444, 45361, 48176,  1154, 36704, 48460, 39756,
        6750, 30212,  3976, 47352, 23412, 19949, 39464, 35977, 47962,
        1529, 14274, 46322, 47276, 35900, 27240, 20771,  6968,  4926,
       34394, 29391, 28860, 24394, 49088, 21572, 41311, 36110,  2179,
       32060, 39326, 23517,  7025, 15536,  9021, 35480, 14940, 33690,
       42679, 33298, 32340, 38853, 26085,  2898, 11204, 47858, 26355,
       14163, 25793, 25430, 12641, 26415, 28501, 26478, 19239, 36396,
       45140, 30588,   863, 21941, 48027, 22295,  1035, 21958, 44573,
       35131, 38771, 34447, 39010,  7137, 44579, 35577,  3528, 48664,
        2810, 39537, 10665, 45895, 42836, 34829, 22332, 24673, 34139,
       12407, 24577,  3554, 42311,  1053, 31775, 32608, 35185, 30911,
        8254, 44215, 31281, 36711, 43486, 42665, 47605, 45665,  1269,
       36457, 34752, 45749, 38906,  1310,  9937, 34450, 43714,  3112,
        3225, 43032, 48447, 40088, 15777,  8375, 45767, 30600,  5677,
       43172, 47570, 42700, 23603,  6022, 39626, 22438,  4666, 47348,
       37656, 40827,  7822,  7362, 43758, 13778, 30435, 40529, 39501,
       25879, 11565, 42979,  1315, 41148, 16505,  9013, 36182, 29891,
       21382, 22139,  5173, 48334,  6844,  3442, 46333, 30950, 19585,
       22902,   158, 35839, 33872, 21161, 21659, 18363, 31902, 26710,
       15963, 36631, 18507, 38640,  6603, 48367, 39932, 22259, 49605,
       48881,  6569, 34425, 32302, 47015,  8804, 28649,  1826,   520,
       29771, 11586, 43914, 15667, 44465, 43683, 31932,  5742, 30501,
       35413,  5726, 48889, 38708, 48159, 39803, 29083, 27787, 20511,
       33998,  3360, 32388, 12416, 23525, 21473, 33038, 21281, 25153,
       21702, 27644, 32026,  4497,  5786, 22183,  2415, 27946, 47571,
       16893, 25781,    82,   429, 46660,  1804,  3077, 38593, 39169,
       34819,  3999, 41897, 24529,  4741,  9873, 20439, 18699, 24874,
        7163,  1923, 17608, 13959, 14258, 10925, 37172, 38794, 18293,
        2573, 20828, 41058, 44717,  2629, 44760,  3577,  2832, 16383,
       14857, 10076, 35318, 25024, 13464, 25444, 45812, 32784, 45644,
       17420, 15009, 49055, 31431, 45609,  2486, 34881, 35324,  6770,
       37004, 17513, 42915, 39833, 44572, 46521, 47318,  6990, 43975,
       22949,  3322, 17161, 10477,  4857, 10766, 31239,  8792, 42675,
        1569, 47530,    89, 43055, 31514, 23958, 10890, 14075,  2078,
       34816, 48521, 15415, 35915, 26926, 37089, 44850, 41249, 32786,
       39036, 23591,  8405, 22246, 33066, 12136, 28823, 32797, 28074,
       43228, 13310,  5607, 27428, 44516, 35187, 46036, 26844, 24239,
       31751, 48997,  6879, 18875,   343, 17266, 39356, 14490, 13903,
        4561, 19358, 46234, 46649, 24428, 18698, 44549, 26293, 24979,
       45470, 41404, 29318, 34327, 23317, 39769, 49314, 18562, 22270,
       22335,  2800,   547, 45258, 14035, 22067, 44824, 43359,  4326,
        1242, 10236, 29473, 14005, 43600, 11558, 29876, 24625, 37652,
       48113, 40520, 31288, 27089,  5301, 27994, 44266, 45455, 14904,
        6889, 36251, 36107, 32502, 36067, 12743, 37849, 14117, 36929,
       11955, 35291, 15756, 15299,  7992,  8374, 37260,  4909,   212,
       49234, 22816, 35613, 22422,  8244,  6908, 16848, 17986,   606,
       25710, 21615,  4171, 16882,  5282, 42007, 36400, 32462, 44488,
        7312, 23685, 31072, 20595,  5616,  4648, 30999,  3678, 31951,
        6122, 11037, 43932,    45, 15506, 49787, 49075, 47836,  1660,
       42802, 12914, 34097, 10248, 36611, 20079, 25463, 24470,  2306,
       40758, 48610, 11134, 19522,  3085, 33832, 16673, 19866, 31100,
       34802, 41607, 47824, 48294,  1565, 10122, 37098,  5885,    64,
        9610, 11744, 17539, 17950,  5287, 32235,  9524, 15797, 13240,
       49768,  9717,  5144, 11403,  7408, 14336, 46078,  4244, 29616,
       11308, 19564, 30218, 30712,  8339, 20536, 16755,   119, 40751,
       26937, 31147,  8093, 17014,  3034, 30445, 21850, 40311, 45201,
       44311, 42658,  9781,  4500, 38735, 19382, 26065, 26652, 36614,
       40431, 19341, 11163, 30103, 43290, 12080,  7587, 37428, 24541,
       19481, 21181, 48215, 47362,  1421, 32295, 10450, 28798, 27576,
       22419, 41922, 19125, 41768, 11109, 31720, 15447, 35357, 17772,
       19581,  4533, 13884, 12275, 40352, 41908,   561, 10678, 30154,
       30553,  4040, 29205, 17460, 17128, 37743, 19454, 29143, 18276,
       42663, 15814, 24873, 34118, 42976, 17017, 26294, 11954, 23020,
       31297, 18994, 38031, 12348, 14512, 34721, 35134, 11972, 48749,
       47001,  7019, 36040, 25388, 38192, 25135, 40250, 22499, 32988,
       29044, 16077, 41807, 15114, 43893,  9088, 12946, 12169, 29911,
       27371]), [0, 6, 4, 1]), (array([33576,  1922,   839, 20020, 33494,  4878,  8287,  3388,  4639,
       26548, 44538, 47095,  9288, 24553, 17293, 25774, 18928, 25740,
       29970, 33454, 31240, 18574,   339, 49070,  7518, 32122,  8282,
       17670, 25356,  9284,  8261, 46436, 47209, 14026, 47757,  8020,
       48181, 32199,  9391, 47884, 47211, 45043,  7846, 48843, 49552,
       22099, 42348,  5755, 15186, 37632, 23641, 47463, 41115,  4319,
       30140, 31439, 42645,  1849,  9537, 25722, 26836, 11560,  4579,
        4990, 15980,  1198, 22059, 26969, 43455, 39929, 47386, 44532,
       19012,  1280, 35534,  6338, 38600, 15301, 27691, 28659, 45731,
       21186, 29256, 49380, 49327, 15371, 34755, 18083, 14437,  3810,
        4920, 25583, 45108, 35034, 16852, 48189, 23234, 31782, 30196,
       11201, 47388, 24344, 10803, 14932, 28730, 20669,  5439, 29608,
       48555,   173, 36227, 43399, 35762, 26747, 23874, 19756, 40620,
       13872, 12912, 22890, 45215, 38366, 43979,  6958, 42088, 34763,
       22779, 25057,  8252,  4312, 29961, 48011, 23515, 46916,    27,
       41062, 29713, 41789, 13121, 17740, 37749, 25515, 28660, 11770,
       45448,  9665,  6697, 49188, 36954, 27377, 13073, 44706, 31355,
       18131, 22932,   784, 42776, 48207, 19182,  1618, 18569, 26418,
       46708, 16470,  2686, 29857, 41358, 10768, 19334, 24744,  7981,
       32598,  8240, 30931, 35892, 26587, 23689, 22941, 22473, 48161,
       46264, 19757,  8198, 25903, 27094, 45636,  1860,  1993, 43654,
       37921,  5729, 30370, 14875,  5561,  8894, 25870, 40934, 37451,
       47661,  3229, 16510, 22590, 21233,  7140, 49867, 22021, 26143,
       11347, 14033, 31781,  4862, 39757,  5305, 32957,  2916, 32889,
        6140, 39662, 41406, 41759, 30042, 37434, 35640,  7082, 25517,
        5690, 28306, 43599, 30235, 33578, 49661, 29302, 32364, 27633,
       33534,  5411,   670, 12051, 28321, 30523, 33142,  4641, 19652,
       17343,  1928, 10597, 41442, 39742, 26501, 46030, 47046, 46816,
       32347,  1898, 15073, 19466, 21995, 22115, 10584, 27374, 33469,
       49231,  2319, 34592, 25090, 20094, 20618, 12994, 30434, 39408,
       37683,  5258, 23773, 42598, 41795,   968, 42338, 26249, 38142,
       17627, 32374, 23486,  1783,  5710, 30883, 37695, 12985, 45147,
        8238, 49630, 23000, 12163, 38678, 47777, 48773, 29359, 39064,
       36495, 42284, 26733,  4088,  8343, 28262, 26217, 13442,   329,
       14319,  8907, 49109, 10277, 13039, 45127, 21394, 21576,  8498,
        7969, 46709,  7922, 45523, 47056, 14219, 21820, 26166, 35980,
       16582, 22980, 16187, 18802, 25241, 25132, 40010, 21409, 17129,
       29036, 32995,    11, 37100,  3213, 23828, 21857, 15647,  3364,
        5989, 19931, 33775, 36205, 33712, 49310, 32660, 17113, 31350,
       25370, 33921, 44986, 44374,  5812, 29937, 49116, 33585, 31341,
       12690, 45561, 27529, 28152, 23794,  3138, 34900, 44378,  4782,
       19372,  8744, 20576, 24948, 35407,  1927, 28257,  8360, 22625,
       30030,  4513, 10772, 46368,  6164, 13237, 31876, 40570, 18590,
        2119, 39136, 16091, 11414,  3473, 33527, 13547, 43229, 27916,
       27039, 31276, 22457, 24729, 44091, 17913,  3146,  1544,  4554,
        2211, 49648, 30797,   114, 23903,  1900, 28639, 36174, 15850,
       29557, 27098, 27910, 45747, 49103, 41473, 28392, 18938, 34990,
       22901, 27574, 25594, 42651, 24311,   825, 38196, 17616,  3627,
        6235, 33299,  5116, 31924,  9347, 49925, 37980, 47321, 44228,
       43724,  1113, 26422,  2620, 28977, 48579, 34260,  5792, 18339,
       29872, 46228, 29369, 41180, 38194, 10927, 40998, 18431,   388,
        6734, 23627, 14947, 38667,  6398, 21096, 20170,  9906,  9472,
       22031, 16162, 13284, 10687, 38286, 38225, 22669, 11260, 20175,
       40478, 39246, 10259, 29859, 15072, 47275, 48960, 12432,  3054,
       20423, 12364, 23585, 38094, 29486,  9719, 11990, 16025, 42887,
       42238,  3151, 43322, 13950, 29499, 30917,   669, 30794, 29582,
       13815,  8630,  2293, 40156,  6249, 24884, 46319, 30228,  3494,
        4103, 34852, 44963, 42662, 16426, 33667,  7563, 31075, 14090,
       14474, 31227, 24080, 46132, 10593, 41294, 13636,  7536, 37577,
       22673, 40523, 13164, 12108,  3952, 45221, 12406, 16194,  1557,
       26745, 13764, 10415, 16135, 11649, 32186, 39047, 40377, 43926,
       36945,  6212, 32969,  9963, 14744, 13282,  3936, 44705, 15503,
       10404, 47244, 46276, 37775, 49154, 21016, 33458, 18669, 31003,
        4973, 41216, 11862,  5067,  9962, 46415,  4286, 32224, 15715,
       31701, 38995, 40597, 37345, 16689, 14789, 25785, 40798,  2673,
       33666,  9695,  7595, 18014,  5919, 14957, 33370, 41101,  9254,
       42849, 41665,  5051, 27479, 48801, 48488, 29237, 38263, 13231,
       41093, 15131, 37469,  9780, 32229, 34144, 47735, 42312,  7100,
       27461, 40240, 19178, 46476,  8639, 13363, 39473, 18314, 33482,
       49204, 38824,  7365, 46789, 21222, 21081, 47868, 49902, 48018,
        6234, 14113, 31755,  4058, 22946,  2131, 15663, 39505, 20303,
       25692, 17437, 20060, 28407, 49597, 17845, 21708, 25834, 18473,
        7383, 47622,  7984, 44761, 44002, 23231, 21694, 30799,  9776,
       41191, 13102, 31429,  3695, 29635, 16659, 17123, 20373,  9153,
       28387, 23179, 41202,  9544, 28076, 37639, 25941, 48191, 27801,
       16020,  9342, 34119, 44747, 14469, 18690, 48309, 22580, 29254,
       46968, 30156,  6505, 46940, 36372, 18434, 16254, 43567, 15036,
       41100, 23637, 45645, 29404,  1952, 16073, 20054, 15668, 36671,
       45205, 34671,  3507,  4156, 30593,  1238,   674, 32590,   435,
       33971, 42313, 38092, 23022, 49951, 18912, 23421, 35598, 32280,
       26047,  2787,  5178, 31033, 41931, 49355, 28785, 32284, 15196,
       19721, 25383, 13663, 44891, 40116, 39853, 21642, 12333, 28614,
        3560,  9915, 30915, 33862, 23200, 41246,  5940, 20904,   831,
       40757, 17634, 49421, 35605, 12412, 25117,  6851, 15275, 43050,
        9582, 27238,  5076,   375,  3512, 34982, 27272, 49509,  8450,
        4261,  7738,   250, 23504, 40509, 11776, 31079,  5224, 22013,
       24486, 33640, 18885, 25479, 37611, 16711,  1724,  4206, 31395,
       43583, 26452, 26878, 44309, 44499, 31345,   493, 49326, 17409,
       36686, 34830, 20057, 49483, 38596, 27402, 29146, 48279,  9938,
       19226, 44575, 34533, 40518,  5084,   565, 12800, 40024, 18594,
        9407,  7147, 17626,  5947, 20731,  5971, 25459,  9232, 28640,
       16813, 31634, 44876, 37248, 49491, 24075,  8432, 30943, 38610,
       14501, 34902, 27716, 23489, 13217, 15456, 46135, 23107, 14524,
        8371,  9946, 29894, 41869, 34778,  5515,  5841, 12920, 11799,
       43251, 45672,  3212,  5629, 40386, 25278, 45602, 21245, 26450,
        9584, 47980, 18057, 44647, 14851, 19202, 49221, 41602, 21661,
        5666, 21199, 36086, 34813,  8124, 24043, 27404, 44589,  7401,
       26292, 11532, 32679, 42669,   427, 27330, 34400, 21627,  7659,
        7605, 12516, 14182,  3020,  6074, 31348,  5559, 18345, 39820,
        5791,  1973, 34064, 28393, 33194, 43398, 47540, 13843, 16272,
       21020, 25162,  3523, 40713, 15610, 46408, 39132,  6928, 34385,
       48182, 45834, 24580, 34940, 13877, 13849,  8995, 26267, 28626,
       21154,  6589,   599, 10298,   761, 47423, 17726, 16937, 34701,
       25852, 31351, 36058, 18134,  8944, 39349,  2259, 35033,   874,
       10479, 37844, 16576, 38650,  9646, 46240,  5567, 39692, 45988,
       24923, 33261, 27607, 14409, 17995, 13615, 30792, 48799, 26115,
       49409, 32168,  8625, 33561, 38911, 35783,  5834, 19265, 30716,
       27792, 26405, 33198,  6040, 20353, 17334, 22358, 36941, 42335,
       41259,  6736, 13701, 27970, 43984, 29875,  9889, 40422,  4021,
        9442, 49588, 43608, 20616, 21933, 29999, 41948, 44217, 12964,
       17309, 46219, 43609, 45866, 29974, 42785,  8305, 17205, 21808,
       40076]), [5, 7, 4, 1]), (array([ 2818, 19498, 43780, 29418, 43121, 36659, 32407, 28871, 21289,
       37575, 23657, 40875,  4041, 10219,  5140, 42253, 41920, 27900,
       23111, 40015, 28713,  4675, 47967, 42420, 39043, 24627, 21008,
       41905, 22482, 19724, 15215, 13711, 41089, 26016, 31291, 44860,
        2637,  2985, 26222, 47315,  7571, 37413,  2853,  1612,  9434,
       10806, 16396, 31881, 37678, 43903, 39581, 41899, 16046, 17404,
       18796, 17142, 30823,  5776,   428, 15082, 21960, 31956, 22285,
       17176, 41840, 17158, 38406, 26828,  6671,  3977, 47262,   269,
       33568, 22240, 22538, 28010, 29059, 15936,  7708,  1638, 22464,
       13429,   915, 41128, 18878, 27025, 38793, 22127,   408, 32766,
       38017, 36713, 19805, 22182,  4489, 33784, 49360, 47073, 40638,
       31869, 20518, 26561,  1767, 16803, 39930,  7583, 18779, 22623,
        4121, 42473,  7302, 38378,   270, 33483,  2313, 49583, 17289,
       27311, 46280, 22199, 47512, 35218,   867,  5668, 37074, 41415,
       39371, 40976, 24076,   881, 18963, 28747, 41771,  2609, 15901,
       36549, 13407, 20380, 46346, 30220,   672, 49831, 29073, 37040,
       15652, 21614, 34767, 39264, 21887, 44404, 10698, 14970, 35197,
       25094,  4584, 41161, 43882,  8992,  1000, 33813, 26769, 18943,
       21745, 36250,  6903, 41363, 20172,  3846,  2308, 48816, 26101,
        2890, 42994, 27975, 47697,  1078, 40043, 34223, 21170, 26511,
       23908, 19086, 42058, 29000,  2281,  9229, 26622, 48606, 19876,
       37070, 46654, 38481, 37762, 27755,  2829, 28783, 22543, 39449,
       33538, 34390,  1043, 12568,  9880, 17201, 15252, 11139, 15495,
       49057, 10762, 40819, 15027,  4289, 26012, 49208,  9511, 25736,
        9028, 25433, 28390, 40166,  6258, 44605, 23840, 36031, 35019,
       42139, 27815, 14112,  6364,  1940,  2671,  5203, 47435, 36797,
       19824, 28514, 39567, 24027, 49783, 21401, 16078, 31233, 34402,
       15249,  9432, 15031, 13705,  6984, 26506, 10309,  4003, 13157,
       42617,  6238,   190, 44059, 39613, 26623, 47713, 40070, 31462,
       46453,  1479, 14054, 17167, 27352, 35896,  6667, 30087, 18644,
        4272,  1512, 28016, 30639, 44416, 39336, 20762, 43408, 22340,
        9068, 26440,  3305,  9960, 14728, 43705, 12544, 39702, 30601,
       16700, 20942, 10876, 10139, 13233,  7455, 49253, 26105, 17130,
        4222, 35671, 23669, 27410, 29600, 47378, 21028, 36803, 23883,
       36744, 39075, 43175, 39139, 17617, 28768, 45617, 34615, 21541,
       14710,  3482, 33124,  4828,  1914, 48374,  9648, 20873, 36892,
       25783, 15322, 28134,  5600, 15630, 27153,  9567, 22518, 46068,
       48959, 27618, 21998, 42371, 21710, 48805,  3430, 25632, 31037,
       33225, 48081, 37710, 34468, 26538, 10688, 42270, 23960,  5687,
       15951, 37335, 25047, 28707, 32465,  7287,  8229,  5855,  1357,
       18593, 40270,  2721, 35726, 36108, 41240, 23872, 48641,  1261,
       39802, 23148, 43699, 15699, 27027, 18295,  6459,  4916,  1721,
       13355, 10016, 21568, 36255,   987, 35778, 31938, 29362, 39916,
       33119, 47791, 37064, 45635, 22862, 36876,  1289,  3681, 22440,
       14049, 13684, 12128, 18307, 47391, 45608, 28985, 32207, 44362,
       41281, 22147,    62, 33888, 17854,  4287, 30483, 45115, 44570,
       35909, 12319, 33651,  4474, 22600, 38808,  1702, 48457,  3313,
       30488,  3770, 13836, 13883, 46392, 32974, 21716, 23707, 40881,
        4794,  8678,   901, 19748, 23845, 20962, 45784,  7443, 10004,
         259, 38339, 14866,  3765, 34825, 48874, 46386, 35828, 41647,
       34248,  7535, 20740, 37641, 29734, 21830, 23399, 19768,  9269,
       41403, 30210, 15914, 24270,  2663, 49567, 41941, 10763, 19336,
        8831, 39350,   430, 26108, 48943, 34527, 13720, 11993, 23011,
       28450, 17252, 33570, 38322,  5904,  8384, 32281, 35628, 23818,
       12286, 28295, 41187,  8968, 30660,   971, 37483, 10245, 22218,
       27436, 21164,  2640,  4936, 46617,  7218,  7751, 23279, 42434,
        3296, 18627, 44270, 43549, 22050, 41265,   336, 19180, 35151,
        7110, 35443, 16749, 23133, 49352, 26512, 16636, 29471, 15625,
       15155, 31053, 33408, 32300, 24067, 11045, 11254, 30967, 49981,
       31549, 15773, 30815, 47611, 14649, 41554, 28200,  2488, 31939,
       40707, 41878, 33480, 47144, 12921,  1624,  2842, 49685,  2691,
       36523, 39977, 28711, 17145, 24659, 43239,    28,  7634, 15958,
       46077, 44450, 14921, 22820, 47591, 43586,   979, 15643,  2703,
       37466, 43238, 38869, 20810, 48235, 37214, 37987, 37977, 11997,
       42967, 31547, 48505, 18512, 22749, 24664, 12997, 18550, 23817,
       37234, 28219, 13005, 41788, 15409, 46049,  5844, 26351, 15864,
       43460, 10489, 15737, 45255, 13772, 37154, 18841, 47126, 40285,
       22677,  5998, 47374, 48969, 32017, 37960, 40111, 22750, 30125,
       20253, 43529, 40981, 28084, 43682, 46884, 15140, 34255, 19426,
       36965, 37241, 22203, 26360,   526, 20173,  8279,  1595,  9550,
       47855, 20769, 39752, 17402,  8361,  8504, 46185, 35564,  3957,
       34193,  8541,  6415, 47344, 49292, 42629, 26953, 20343,  1813,
       13466, 27535,  7382, 45729, 29670, 17150, 30425, 28228, 29329,
        8551,  8233,  2155, 36160, 16897,  5957,  5187, 26114, 39067,
       23654, 40486, 39596, 23671,  4924, 47384, 27417, 11810, 45712,
        5790, 28260, 47664, 36428,  2442, 12124, 32519, 44942, 37875,
       44162,  8939, 10237, 28769, 29376, 20485,  2461, 13855, 39005,
       43746, 39467,   372, 31476,  2327, 14787, 10447, 34784, 25017,
       19307,  4527, 30869, 37805,  7119, 19990, 25147, 24658, 45764,
       29361, 42643, 48860, 28892, 22985,  3383, 37288, 44563, 27860,
       34744,  8263,  4989, 24241, 28744, 29278, 36976, 11957,  1526,
       28903,  5501, 39565, 12261,  5931, 30681, 42769, 32929, 19746,
       45226, 25429, 41822, 16058, 19452, 15256, 42938, 18502, 24799,
       49390,  1974, 33209, 29121, 46507, 25817, 47519,  2339, 37512,
       48000,   617, 14659, 34832, 27506, 42113, 34862, 36594, 15779,
       21585, 21333, 17515,   261, 14871, 28784, 31899,  7706, 14859,
       23338, 33063, 16286, 32161, 29330,  7077, 24566, 44412, 15512,
       35105, 10100,   227,  1052, 49381, 45977, 31890, 29633,  4609,
       39256, 11890, 22450, 47968,  3485, 36938, 42292, 28958,  8221,
        1631, 42065,  4101, 15740, 32736,   168,  8475, 36388, 40046,
       23598, 11580, 22882, 16250, 35145, 29314, 25608, 16850, 49616,
       20777, 14713, 25700, 26059, 15891, 26737, 19616, 38278, 41289,
       11664,  2769, 38420,  6295, 48617, 40537,  8175,  4230, 19710,
       33114,   699,   396, 42281, 16275, 19399, 36672, 11683,  1571,
       36751, 36033,  2390,  8812, 19867, 20197, 48995, 15205,  6429,
        6191, 48636, 37269, 24284, 29311, 42918, 30014, 32603, 20214,
       43447, 34145, 40576, 28347,   311, 22165,  1410, 46731, 18926,
       38974, 20510, 26578, 39109, 44541, 45704, 31512, 38725, 41279,
       19628, 49894, 16855, 48970, 21437, 14900, 41794, 17856, 29325,
       25652, 39204, 49705, 20783,  5959, 10491, 11791, 18063, 43217,
       38892,  6272, 20988, 37995,  6120, 10393, 48740, 20914, 11427,
       17755,  8294,  3386, 24398,  2039, 28906, 43641, 29188, 49494,
       13526, 34068, 38571, 26637, 45580,  2268,  1694, 45859, 42211,
       26794, 15832,  2280,  9549,  7827, 23662, 21153,  4423, 12923,
       25150, 30245,  5759, 41322, 38549, 29285, 32410, 42734, 34426,
       43847,  6371, 14722, 22529,  6701,  2173, 22586, 18495, 10748,
       39240,  3840, 31107,   947, 11579,  6899, 37203, 14067, 46259,
       46359, 28539, 43132, 30486, 22163, 19500, 35454,  8875, 15496,
       21073, 12774, 33327, 23311, 41356, 19609, 11698, 39974, 12622,
       20874, 48845, 10953, 23578, 49121, 36085, 45011, 49436, 18206,
        2511, 15704, 43605,  9765,  8571, 12373,  3414,  2227, 36872,
        3059]), [9, 8, 4, 1]), (array([ 6304, 37804, 27012, 40216, 43318, 23884,  9846,  6309, 25738,
        3864, 20721, 17440,  3813, 15453, 26707,  4197, 47851,  2127,
       29685, 20447, 10869, 41892, 32250,   846,  8210,  6607, 40165,
       41826, 44173, 15421, 19820, 16619,  9293, 11998, 11477,  6014,
       40246, 37973, 43462, 37662, 11387, 16366, 46825,  7753,  7086,
        1608, 46058,  6405, 34099, 41285,   998, 13700, 16301, 15603,
        9034, 11564, 22420, 38304, 12428, 28953, 25490, 15383,  4558,
       27233, 35124,  1510,  8319, 30868, 33133, 29810, 42752, 16547,
       22030, 49970, 24726,  1958, 43922,  4674, 18814, 37770,  6619,
       12111,  8356, 13001, 30817, 38992,  1057,  7432, 35092, 22620,
        9024, 18127, 29012, 14368, 31178, 18998, 28864, 21686, 43843,
       46121, 21053, 22913, 14108, 26849, 10193, 24749, 47100, 10746,
       41412, 35128, 25882, 32451,  1539, 40584, 12174, 42930, 39108,
       27407, 15974, 10575, 47901, 23376, 11231, 13349, 20252, 13013,
       11120, 22406, 41016,  8781,  6913, 30266,  4402, 49079,  1556,
       34984, 44263, 27909, 20329,  5321, 47554, 38679, 21085, 19126,
       32297, 46542, 31179, 32846, 22919, 14118, 28253, 25281, 14070,
       32541, 33092,  2447,  2011, 20820,  1895, 23498, 22793, 49741,
       46182, 41229, 10958, 29215, 43411,  7906, 39294, 15545,  5455,
        2353, 45502, 11047, 39885, 11714, 27275, 48907, 29705, 20770,
       46215,  3571, 26357,  5823, 36211, 29520, 33267,  3905, 22294,
       36422, 32936,  8645,  9830, 11105, 43328,  7671,  6649, 46157,
       14089, 29839, 48040, 14171, 14703, 40232, 26044, 24060, 38364,
       17615,  1961, 17243, 45646, 16070,  3194, 24081, 48999, 11724,
       37906, 15053, 16295, 23145, 41917, 10942, 17047, 33463,  9720,
        4025,  5520, 44550, 22662, 27114,  5218, 22411,  1573, 38781,
       35323, 15247, 38932, 46417, 46605,  6540, 47722, 17018, 26559,
       45382, 38634,  9183, 23594, 18570, 21510, 42744, 14472, 27738,
        7151, 37255,  2784, 32899, 11137, 20558, 15977, 37686, 18143,
       25200,  9940, 44644, 32377, 33474, 40372, 33083, 30632, 11286,
       16833, 48733, 14595, 32807,  6626, 16739, 26425, 21099, 14811,
        7466, 12583, 17981, 45951, 39499, 20651, 31621, 36423, 38349,
       31040, 38038, 37856, 20673, 43826,  8055,  6616,  2957, 11454,
       16215, 20038, 30940, 10926, 40924, 28664,  2789, 14619, 10440,
        4616, 16462,  1258, 16581,   802, 11270, 34307, 43990, 34488,
       25101, 38592, 15314, 33519, 14785,  4038,  1107, 14157, 33054,
       35653, 30572, 24773, 38801, 44535, 20073,  3963, 14383, 38216,
        3569, 29625, 19280, 14285, 10671,   912, 14601,  4168, 15398,
       15789,   990, 28667, 26386, 47171, 18163,  3484, 37373, 41024,
       13934, 33183,  7807, 47936,  1475, 13487,   885,  2514, 17693,
       27239, 42227,  2004, 30437, 11003, 24927, 29779, 33175, 33350,
        6028, 30786, 26999, 39368,  7521, 20009, 30631, 27186, 17302,
       43244, 31884, 23673, 40398, 27546, 25440, 40403, 31454, 41884,
       11843, 42894, 34203, 13290,  7330, 27948, 42428, 47710,  4511,
       48779, 30665, 17834, 36502, 26976, 24064, 39847, 15443, 14106,
       40348, 33498, 40217,  9803, 46564, 43572, 21262, 29180, 34222,
       48169, 22511, 26588, 46819, 11884, 16446, 18204, 14229, 27498,
        6334, 26176, 43823, 33531, 32409,    90, 25982, 49914, 13814,
       11779, 10412,   522, 23062, 15369, 32763, 32480, 15589, 44634,
         804, 38459, 11089, 13273, 34873,  8706, 11587, 32057, 34885,
       21500, 13081,  2191, 27865, 47368, 27685, 33842, 36382,  5647,
       44879, 48414, 25030, 39062,  1798, 28834, 46575,   283,  3558,
       38182, 48092, 48608, 11443, 46214, 27594, 25398, 23453,  4237,
       26308,  6177, 43287, 10378,  5219, 31361,  4656, 17955,  8219,
        4634, 17273, 24623, 45533, 15929, 16609,  5384, 43692, 21063,
       26630, 19615, 33776, 47584, 12060,  2450, 32094, 22619,  5704,
        6943, 18099, 23634, 28174, 47908, 32478, 22743, 16908, 45133,
        8688,  7903, 14134, 40530, 40794, 28885, 33383, 45598,  5706,
       45786, 41625, 38354, 47795,  5298, 24221, 48195, 40098, 27017,
        5676,   621, 46153, 26497, 29070, 36142, 26329, 19059, 15126,
        9018,  1284, 27372, 14786, 30522, 34969, 20280,  8913,  8631,
        1704,  9750, 21052, 10604, 45022,  7043, 45110, 26943, 49687,
        6652, 12717,   477,  1348, 14853, 35378,  4137, 49586, 39945,
       36552,  1657, 30725, 21693, 33099,  9592,  4642, 41770, 43420,
       16105, 16880, 23943, 21794, 17450, 10299, 32523, 48179, 31306,
        8701, 11845, 17707, 23076, 48360, 30614, 48605, 41608, 39209,
       40301,  1984, 43897, 19704, 26882, 39878, 29852,  9364, 47576,
       46145, 27531, 41077, 39293,  9500,  1396, 45396, 21582, 45410,
        2190,  5225, 33221, 45756, 21188, 44994,  9507, 35395,  3556,
       14726,   959, 25779, 41103, 31954, 17613, 26886, 33215,  1937,
       45293,  9041, 27259, 18409,  9285, 14798, 36345, 24265, 43494,
        1339, 29128, 15734,  5829, 15454, 20341,  3283,  4846,  8987,
       40036, 22274,  7670, 49475, 37528,  9273, 19528, 17260, 43222,
       42695, 34776, 41097, 34727, 20629, 24925, 32494,  8137, 22385,
       20967, 22369,  3046, 38728, 16672,  5227, 37677, 47232, 48796,
       21649, 33080, 10500, 36071, 36731,  8114, 30077,  9779,  4885,
       11794, 47147, 44750, 47120, 32201, 20435, 18197, 44061,  2528,
       32253, 16263, 25798, 24572, 26975, 42907,  6344,  7394, 25142,
        3509, 40219, 17039,  5880, 48429, 46591, 47796,  2105,  7957,
       35457,  8066, 37741, 31898, 32029, 12975, 39946, 15238, 14635,
        9250,  4199, 15909, 16101,  9749, 18185, 15730, 27303, 44283,
       25643,  2297, 18264, 26233, 23739, 39541,  1406,  1640,   381,
       33688, 27762,  3913, 40077, 30243, 16525, 35050,  6049, 44525,
       17552, 48945, 31588, 37588,  5769, 38929, 27584, 25300, 35694,
       33390,   524,    99,  8085, 28089, 36331, 19575, 26424,  3836,
        3103, 45856, 18458,   301, 31102, 13108, 31445,  1240, 48669,
       23533, 28559,  7300,  8556, 34530, 31251, 30039, 15662, 39277,
       12741, 49769,  8001, 20840,  5897, 14593,  6375, 10403, 16023,
       36236, 33625, 36703, 43145, 19063, 33650, 49820, 17030,  4764,
       44818, 42707, 20605, 14519, 11007, 20442, 14598, 33126, 33624,
       44415, 16834, 42606, 12894,  2958,  4299,  2656, 48375, 41984,
       17621, 41888, 25596, 33050, 16964, 34263, 38078, 26741, 16588,
        8690, 30471, 17732, 47178, 29029, 14643,  9085, 22697, 16962,
       31523, 26746,  6779, 24268,  4942, 31329, 45001, 31830, 13002,
       17860, 22745,  6241,  8893, 47060, 17268, 22303, 41579, 31245,
       49230,  7540,  7375, 24620, 44725, 27636,  1494, 29394, 31587,
       43195, 33399, 31711,  3009,   226,  4578, 42031, 49447,   126,
       22545, 28394, 11817, 13037, 42121,  4464, 25377, 46323, 11242,
       14662, 45105, 11829,  4661, 23546, 13470, 42341,  6988, 12893,
       30182, 18927, 33615, 19301, 45946,  7214, 25910,  1251, 27194,
       37239,  5635, 28245, 46679, 32348, 16745, 32737, 19983, 24068,
        7503, 25961, 44971,  3233, 14964, 26471,  6495,  8779, 45715,
       42991, 20273, 23142, 40749, 32311, 38902, 30062, 13444, 45968,
       21888, 46730, 45097, 13311, 33598, 19880, 12560, 23088, 28957,
        8090, 26230,  1548, 30244, 41563, 22689, 41736, 25827, 29230,
        9012, 37755, 37320, 33237,  6456, 47453, 21893, 24561, 38682,
        5975, 49875, 35878, 46928, 22145,  4607, 41862, 26074, 21530,
       34791, 37253,  2067, 27228, 34618, 19484, 33569,  2597, 32042,
       13899, 24157,  8201,  2320,  4352, 30165,  9318, 33646,  9459,
       18223, 19242, 21230, 17274, 13647, 49392, 41739, 41026, 38838,
       17457, 28344, 27773, 41701, 47208, 30086, 33196, 39824, 41378,
       40762]), [3, 2, 4, 1])]
Collaboration
DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.255, 0.04439314603805542)
DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.245, 0.04447525298595428)
DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.244, 0.045177888870239255)
DC 3, val_set_size=1000, COIs=[3, 2, 4, 1], M=tensor([3, 2, 4, 1], device='cuda:0'), Initial Performance: (0.209, 0.04494925320148468)
D00: 1000 samples from classes {1, 4}
D01: 1000 samples from classes {1, 4}
D02: 1000 samples from classes {1, 4}
D03: 1000 samples from classes {1, 4}
D04: 1000 samples from classes {1, 4}
D05: 1000 samples from classes {1, 4}
D06: 1000 samples from classes {0, 6}
D07: 1000 samples from classes {0, 6}
D08: 1000 samples from classes {0, 6}
D09: 1000 samples from classes {0, 6}
D010: 1000 samples from classes {0, 6}
D011: 1000 samples from classes {0, 6}
D012: 1000 samples from classes {5, 7}
D013: 1000 samples from classes {5, 7}
D014: 1000 samples from classes {5, 7}
D015: 1000 samples from classes {5, 7}
D016: 1000 samples from classes {5, 7}
D017: 1000 samples from classes {5, 7}
D018: 1000 samples from classes {8, 9}
D019: 1000 samples from classes {8, 9}
D020: 1000 samples from classes {8, 9}
D021: 1000 samples from classes {8, 9}
D022: 1000 samples from classes {8, 9}
D023: 1000 samples from classes {8, 9}
D024: 1000 samples from classes {2, 3}
D025: 1000 samples from classes {2, 3}
D026: 1000 samples from classes {2, 3}
D027: 1000 samples from classes {2, 3}
D028: 1000 samples from classes {2, 3}
D029: 1000 samples from classes {2, 3}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO4', '(DO2']
DC 2 --> ['(DO1']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.276, 0.06242937207221985) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.25, 0.06935057532787323) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.25, 0.10054781334102154) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.257, 0.09110854172706603) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.44, 0.07027108192443848) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.352, 0.07565721029043197) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.252, 0.10466695122420788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.323, 0.11809220957756042) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.455, 0.08399075710773468) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.405, 0.08414981603622436) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.38, 0.10775397792458534) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.355, 0.1430300070643425) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.09190552899241447) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.389, 0.10456353434920311) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.413, 0.15356273053586483) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.356, 0.16536943846940994) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.11256972487270832) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.376, 0.11904050388932227) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.395, 0.17955271257087588) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.354, 0.1911297039538622) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO5', '(DO2']
DC 1 --> ['(DO0', '(DO3']
DC 2 --> ['(DO4']
DC 3 --> ['(DO1']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.464, 0.12555305766686797) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.392, 0.12016960312426091) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.436, 0.20252362125553192) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.376, 0.1672702697366476) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.13640131926350296) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.382, 0.12200692869722843) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.1993113804385066) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.384, 0.1897293770611286) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.1368831439372152) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.384, 0.1383049213439226) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.441, 0.22230591307021677) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.387, 0.205445771753788) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.15904230438452213) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.416, 0.13590771255642176) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.251293695660308) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.379, 0.1797317993193865) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.469, 0.15739924981538206) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.422, 0.15744038719683887) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.456, 0.24946794794872404) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.393, 0.2138650175333023) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[1, 4], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.692, 0.020670207858085633)
DC Expert-0, val_set_size=500, COIs=[0, 6], M=tensor([0, 6, 4, 1], device='cuda:0'), Initial Performance: (0.938, 0.005567504147067666)
DC Expert-1, val_set_size=500, COIs=[5, 7], M=tensor([5, 7, 4, 1], device='cuda:0'), Initial Performance: (0.844, 0.013536606922745704)
DC Expert-2, val_set_size=500, COIs=[8, 9], M=tensor([9, 8, 4, 1], device='cuda:0'), Initial Performance: (0.912, 0.008059985995292663)
DC Expert-3, val_set_size=500, COIs=[2, 3], M=tensor([3, 2, 4, 1], device='cuda:0'), Initial Performance: (0.786, 0.01843729305267334)
SUPER-DC 0, val_set_size=1000, COIs=[0, 6, 4, 1], M=tensor([0, 6, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 7, 4, 1], M=tensor([5, 7, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[9, 8, 4, 1], M=tensor([9, 8, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[3, 2, 4, 1], M=tensor([3, 2, 4, 1], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x76b2b8061880>, <fl_market.actors.data_consumer.DataConsumer object at 0x76b2b8189df0>, <fl_market.actors.data_consumer.DataConsumer object at 0x76b2b820fee0>, <fl_market.actors.data_consumer.DataConsumer object at 0x76b28c5f5ac0>, <fl_market.actors.data_consumer.DataConsumer object at 0x76b28c6894f0>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.795, 0.013594387948513031) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004199446882354096) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.01160474380850792) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.006100144477561117) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.814, 0.018861411333084107) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.594, 0.051467184841632846) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.473, 0.05724928307533264) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.583, 0.056047844752669336) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.423, 0.05902448618412018) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9255, 0.006801596997771412) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.003964884998917114) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.012402707509696484) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.006653165928088129) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.808, 0.021150469481945038) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.644, 0.043863730162382125) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.05246051198244095) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.648, 0.0463257787823677) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.427, 0.0683288446366787) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9555, 0.003580851184204221) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.003663376725919079) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.013196781635284423) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.006131447539664805) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.81, 0.01912663644552231) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.723, 0.030807052940130232) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.06217485703527927) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.713, 0.03311613842844963) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.516, 0.06273820409923792) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.952, 0.004336522354045883) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.005445231348483503) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.87, 0.01324596694111824) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.922, 0.008552911043632776) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.794, 0.022361165761947632) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.701, 0.038078264564275745) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.516, 0.0720658098757267) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.671, 0.03681181578338146) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.497, 0.0686497095823288) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9635, 0.0029246034980751575) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.003898142425226979) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.014425043128430843) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.007994936424307526) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.798, 0.021853789627552033) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.689, 0.04441331979632378) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.06056152833998203) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.717, 0.02835196453332901) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.51, 0.0758630507439375) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9625, 0.003356696457718499) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.00419177254292299) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.015075370267033577) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.00849454750912264) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.798, 0.021499280512332915) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.718, 0.03847121565043926) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.588, 0.059691231474280355) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.718, 0.03316137959063053) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.544, 0.07299188117682934) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.0032588496909011157) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004912590324769553) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.013601159572601319) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.932, 0.009347277418244631) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.798, 0.021724790096282958) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.692, 0.04305425921827555) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.0776826820075512) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.711, 0.03475248958170414) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.522, 0.08426463341712952) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9645, 0.003762572229214129) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.00412982404010836) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.01653130280971527) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.008450390953104943) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.796, 0.018902255475521088) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.661, 0.06664366569370031) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.10116060478240252) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.694, 0.03859878826141357) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.503, 0.11136553245782853) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.002725627273961436) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.00450546680422849) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.018802045896649362) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.00794482355332002) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.818, 0.01799912562966347) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.736, 0.037897088155150416) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.07287695077061652) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.693, 0.03699357694387436) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.545, 0.08213913640007377) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.0031567311502149094) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004517162030424515) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.01641799598187208) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.008395101914647967) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.804, 0.020511021673679353) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.662, 0.05757951978035271) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.0872968789562583) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.615, 0.05856676339358091) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.547, 0.08361987232044339) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9675, 0.0034207853492007416) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004782483891860465) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.015375951007008552) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.00887919262633659) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.818, 0.025912582278251647) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.657, 0.06229338051751256) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.556, 0.08526467325538396) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.645, 0.04714084534347057) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.526, 0.09573384264111519) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.964, 0.0029850795347301754) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005079700218993821) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.017960748299956322) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.928, 0.009986141865607352) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.802, 0.025284680426120757) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.692, 0.05372409975528717) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.559, 0.07966644436866045) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.677, 0.04030956010520458) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.562, 0.09010484267771245) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9665, 0.00324560279630532) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.005071672961559671) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.864, 0.018150676220655443) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.008918952342006377) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.808, 0.023245200216770172) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.694, 0.05213857774436474) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.574, 0.07063023909926415) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.676, 0.04374094121158123) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.549, 0.08438677062094212) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.0037365900413287817) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005082220703498024) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.01824603049457073) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.008619934735470451) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.812, 0.02297342050075531) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.675, 0.0658614626377821) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.08540059484541417) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.594, 0.0681470917314291) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.537, 0.09395713527454064) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9725, 0.0032532684601392246) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004899766767306573) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.017533672794699667) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.009468731703294906) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.816, 0.02145589655637741) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.688, 0.05626877185702324) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.561, 0.09655217663943767) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.646, 0.05417554976791143) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.524, 0.09478746940381826) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.0034166958988280384) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004732515699048236) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.89, 0.018172179266810418) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.007646556346677244) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.808, 0.023810354948043823) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.729, 0.04114553748071194) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.59, 0.07377449983358383) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.636, 0.05489017649367452) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.553, 0.08296396810933948) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.967, 0.003799651808280032) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.00620516440105348) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.017833906661719082) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.007771952577400953) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.802, 0.027319962322711944) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.746, 0.03635142085701227) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.633, 0.055454658523201944) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.576, 0.07691042716428638) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.583, 0.06909551439620555) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.97, 0.0031957980009719904) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.005448519860423403) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.01688642494380474) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.007728820707183331) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.814, 0.026848995447158814) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.724, 0.04434730754047632) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.6, 0.06710333196818828) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.617, 0.05995112732797861) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.574, 0.07543066402897239) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9745, 0.0033227580566081087) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.005957155048396089) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.018258533105254174) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.008126910293241963) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.804, 0.03144659370183945) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.683, 0.05854573379456997) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.533, 0.09357550022006035) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.661, 0.049516352485865354) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.571, 0.08230548187345266) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.00324915295340179) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005886108754537418) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.878, 0.020971816409379243) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.00852372579392977) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.81, 0.02554738348722458) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.699, 0.04860167840868235) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.583, 0.07382475884258748) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.633, 0.07320124753564596) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.539, 0.08717309763655066) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.0030535176281664463) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005725312862074133) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.888, 0.016432073593139647) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.008242369258543476) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.808, 0.02712833309173584) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.731, 0.039192063763737676) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.599, 0.06997238719463349) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.627, 0.061843223448842764) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.582, 0.07287285409867764) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.974, 0.003302078610196077) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.006163131121124934) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.017320848878473044) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.008168741074041464) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.822, 0.030048605382442476) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.726, 0.04401694733649492) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.08006536009162664) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.59, 0.08524585388321429) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.571, 0.07104285535216331) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.0034727188320393906) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.006031822417873627) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.019593187168240548) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.008394268348813056) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.812, 0.029345535814762114) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.718, 0.04306544279307127) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.555, 0.07936314749717713) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.672, 0.045620070688426495) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.561, 0.08849761410802602) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.004562299428403321) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.956, 0.006274704012790608) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.872, 0.018351032607257366) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.009845794178720097) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.818, 0.02602631193399429) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.663, 0.06584175670333206) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.559, 0.09035921368934215) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.612, 0.06211752519011497) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.533, 0.09874813477881253) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9715, 0.003624380563301656) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005717757521362273) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.016579279765486718) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.008394215850508771) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.816, 0.023164491295814514) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.705, 0.048305738478899005) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.09518963605165481) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.624, 0.07008354194834829) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.54, 0.09844984780065716) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.003972602346277654) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005525605961980546) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.876, 0.017674057081341742) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.008170035484828987) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.812, 0.023567962467670442) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.695, 0.05214636294543743) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.547, 0.09549585355818271) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.597, 0.07390842138975859) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.534, 0.10490062907710672) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.0033521885507852857) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0076351742277328185) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.01768321028351784) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.008801260674954393) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.808, 0.025143262386322023) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.738, 0.04526833648979664) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.592, 0.07428599946945906) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.623, 0.06706994019635022) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.551, 0.0843046133518219) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9695, 0.004380470518543916) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.00539049849406274) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.882, 0.017971893697977068) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.010512701821920928) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.814, 0.025121979117393493) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.672, 0.058420941855758426) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.539, 0.10013783052563667) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.669, 0.04938983228057623) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.534, 0.11143588774697855) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9705, 0.0034889195911528077) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0065357353479635096) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.01758617828786373) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.009249027245212347) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.814, 0.025469512820243834) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.684, 0.05590513555705547) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.543, 0.107726468116045) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.66, 0.05953462942317128) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.549, 0.09507564569264651) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.969, 0.004394009752903912) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.006739088206604265) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.886, 0.015935146406292917) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.0092163819167763) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.82, 0.0272165966629982) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.702, 0.0517084992825985) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.552, 0.08725151464343071) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.619, 0.06420395309850574) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.551, 0.08928520199563354) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.971, 0.0038310421464070716) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.007852016620796291) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.01593153092265129) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.011372034727042775) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.814, 0.023222050547599793) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.686, 0.057700800210237504) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.549, 0.08672708347439766) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.638, 0.060016704466193914) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.533, 0.1049615674316883) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.004416083134677308) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.007192622021305738) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.017983801305294037) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.010694451463496079) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.814, 0.026770399034023284) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.674, 0.0619677846673876) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.10643987825512886) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.662, 0.05071088765561581) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.542, 0.08872351581603288) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9745, 0.0038341973764345026) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.007260342413020965) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.020510063841938972) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.008272862268844619) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.812, 0.026220645427703856) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.69, 0.056130789395421744) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.54, 0.09672835090756417) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.684, 0.04689658886566758) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.564, 0.08601281375437975) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.004190573336201964) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.006048923002467518) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.019658845022320747) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.011250993099616607) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.814, 0.025446595191955565) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.671, 0.06015094558522105) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.558, 0.10225674263387918) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.645, 0.06314047473669052) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.552, 0.09581564490124583) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.9735, 0.004350891638855501) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.007196567568447762) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.018658989772200586) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.010968120031204308) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.826, 0.028341419994831085) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.646, 0.07248200491815805) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.498, 0.11615245322883129) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.685, 0.045208358090370895) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.518, 0.11095990849938243) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.972, 0.004517299780255314) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.00765508792686154) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.88, 0.018481393456459046) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.009981865778041539) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.822, 0.02513160228729248) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.693, 0.06085996127035469) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.1084063328653574) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.651, 0.05635213004052639) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.537, 0.10764810177311301) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.976, 0.00438471945576805) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.97, 0.007407381690689022) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.866, 0.01966294617950916) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.009591208881000056) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.826, 0.023472002029418945) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.665, 0.05983309491118416) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.545, 0.09214763559401035) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.664, 0.059060312990099194) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.519, 0.1030595711749047) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.977, 0.004790473207981677) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.0075557948077512125) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.868, 0.017986381709575654) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.926, 0.011915007236690143) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.822, 0.02272103551030159) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.668, 0.057270337246358397) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.512, 0.11193337536603212) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.663, 0.05820565201714635) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.545, 0.0886077049029991) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.975, 0.005218812349309651) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.007116074332734568) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.884, 0.019493865355849266) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.93, 0.010088047209748766) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.816, 0.02369481724500656) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.665, 0.06351489692367614) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.574, 0.08185693728923797) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.62, 0.06662887777015566) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.548, 0.09550420293980279) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.975, 0.004608568637080196) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.007421398872822461) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.874, 0.01663198424875736) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.009644478906935547) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.806, 0.025641255140304565) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.637, 0.07352664207341149) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.529, 0.09794292380288243) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.671, 0.05140670317225158) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.523, 0.1275918675386347) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.255, 0.04439314603805542), (0.276, 0.06242937207221985), (0.44, 0.07027108192443848), (0.455, 0.08399075710773468), (0.465, 0.09190552899241447), (0.459, 0.11256972487270832), (0.464, 0.12555305766686797), (0.461, 0.13640131926350296), (0.475, 0.1368831439372152), (0.461, 0.15904230438452213), (0.469, 0.15739924981538206), (0.594, 0.051467184841632846), (0.644, 0.043863730162382125), (0.723, 0.030807052940130232), (0.701, 0.038078264564275745), (0.689, 0.04441331979632378), (0.718, 0.03847121565043926), (0.692, 0.04305425921827555), (0.661, 0.06664366569370031), (0.736, 0.037897088155150416), (0.662, 0.05757951978035271), (0.657, 0.06229338051751256), (0.692, 0.05372409975528717), (0.694, 0.05213857774436474), (0.675, 0.0658614626377821), (0.688, 0.05626877185702324), (0.729, 0.04114553748071194), (0.746, 0.03635142085701227), (0.724, 0.04434730754047632), (0.683, 0.05854573379456997), (0.699, 0.04860167840868235), (0.731, 0.039192063763737676), (0.726, 0.04401694733649492), (0.718, 0.04306544279307127), (0.663, 0.06584175670333206), (0.705, 0.048305738478899005), (0.695, 0.05214636294543743), (0.738, 0.04526833648979664), (0.672, 0.058420941855758426), (0.684, 0.05590513555705547), (0.702, 0.0517084992825985), (0.686, 0.057700800210237504), (0.674, 0.0619677846673876), (0.69, 0.056130789395421744), (0.671, 0.06015094558522105), (0.646, 0.07248200491815805), (0.693, 0.06085996127035469), (0.665, 0.05983309491118416), (0.668, 0.057270337246358397), (0.665, 0.06351489692367614), (0.637, 0.07352664207341149)]
TEST: 
[(0.2515, 0.043328432649374006), (0.2885, 0.05995646780729294), (0.4465, 0.06739776784181595), (0.4545, 0.0804382722377777), (0.46875, 0.08793868029117584), (0.46075, 0.10819000673294067), (0.46475, 0.12106116461753845), (0.46375, 0.1310386021733284), (0.475, 0.13236147272586823), (0.4615, 0.15334183567762374), (0.46875, 0.1515594100356102), (0.58975, 0.04921727125346661), (0.63875, 0.04193907761573792), (0.7125, 0.03054058912396431), (0.69, 0.03780336159467697), (0.6865, 0.0435387114956975), (0.70875, 0.03770370757579804), (0.69025, 0.04264739307016134), (0.6445, 0.06889935506135225), (0.724, 0.037375298775732514), (0.6565, 0.05764345175027847), (0.66525, 0.0604006036221981), (0.68775, 0.04983314231038093), (0.687, 0.050445395976305006), (0.683, 0.061603230088949204), (0.679, 0.05521797430515289), (0.70875, 0.040642078258097174), (0.74275, 0.03517749252542853), (0.72475, 0.04250642643868923), (0.6835, 0.05664067340642214), (0.69875, 0.04660006649792194), (0.72725, 0.035922460965812204), (0.71675, 0.04138923718780279), (0.70875, 0.042670837789773944), (0.662, 0.06476494608819484), (0.707, 0.0457706501185894), (0.69025, 0.05033670791983604), (0.727, 0.041032765947282314), (0.659, 0.05684466518461704), (0.683, 0.0538777135387063), (0.70075, 0.047634824700653555), (0.6875, 0.053055244594812395), (0.66125, 0.05696147997677326), (0.6865, 0.04921257616579532), (0.669, 0.0558347377628088), (0.62975, 0.07207452440261841), (0.66875, 0.0568335372954607), (0.64975, 0.0590801395624876), (0.67025, 0.051173621013760565), (0.65825, 0.05842517828941345), (0.6325, 0.07155036048591137)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.81      0.42      0.55      1000
           1       0.88      0.86      0.87      1000
           4       0.45      0.96      0.61      1000
           6       0.82      0.29      0.43      1000

    accuracy                           0.63      4000
   macro avg       0.74      0.63      0.62      4000
weighted avg       0.74      0.63      0.62      4000

Collaboration_DC_1
VAL: 
[(0.245, 0.04447525298595428), (0.25, 0.06935057532787323), (0.352, 0.07565721029043197), (0.405, 0.08414981603622436), (0.389, 0.10456353434920311), (0.376, 0.11904050388932227), (0.392, 0.12016960312426091), (0.382, 0.12200692869722843), (0.384, 0.1383049213439226), (0.416, 0.13590771255642176), (0.422, 0.15744038719683887), (0.473, 0.05724928307533264), (0.558, 0.05246051198244095), (0.58, 0.06217485703527927), (0.516, 0.0720658098757267), (0.598, 0.06056152833998203), (0.588, 0.059691231474280355), (0.54, 0.0776826820075512), (0.523, 0.10116060478240252), (0.562, 0.07287695077061652), (0.541, 0.0872968789562583), (0.556, 0.08526467325538396), (0.559, 0.07966644436866045), (0.574, 0.07063023909926415), (0.561, 0.08540059484541417), (0.561, 0.09655217663943767), (0.59, 0.07377449983358383), (0.633, 0.055454658523201944), (0.6, 0.06710333196818828), (0.533, 0.09357550022006035), (0.583, 0.07382475884258748), (0.599, 0.06997238719463349), (0.58, 0.08006536009162664), (0.555, 0.07936314749717713), (0.559, 0.09035921368934215), (0.562, 0.09518963605165481), (0.547, 0.09549585355818271), (0.592, 0.07428599946945906), (0.539, 0.10013783052563667), (0.543, 0.107726468116045), (0.552, 0.08725151464343071), (0.549, 0.08672708347439766), (0.532, 0.10643987825512886), (0.54, 0.09672835090756417), (0.558, 0.10225674263387918), (0.498, 0.11615245322883129), (0.546, 0.1084063328653574), (0.545, 0.09214763559401035), (0.512, 0.11193337536603212), (0.574, 0.08185693728923797), (0.529, 0.09794292380288243)]
TEST: 
[(0.23225, 0.043421665340662004), (0.25, 0.06652063864469528), (0.345, 0.07260621449351311), (0.4005, 0.08074362868070603), (0.388, 0.10054817044734955), (0.381, 0.11419055730104447), (0.39725, 0.11531978017091751), (0.3775, 0.1179304256439209), (0.38325, 0.13432818323373794), (0.41475, 0.1329578117132187), (0.41625, 0.15371146613359452), (0.4715, 0.05537429392337799), (0.563, 0.05086654880642891), (0.5815, 0.05880254945158959), (0.53075, 0.0679128203690052), (0.63, 0.055007729172706604), (0.608, 0.05444713808596134), (0.56225, 0.07197673130035401), (0.53, 0.09378070983290672), (0.596, 0.06548027029633521), (0.5535, 0.08268473970890045), (0.57675, 0.07804343274235725), (0.582, 0.07092198845744133), (0.60325, 0.06449106439948082), (0.58, 0.0807127203643322), (0.5795, 0.09170672518014908), (0.6055, 0.06918335917592049), (0.65225, 0.05164797578752041), (0.62225, 0.06309322272241115), (0.55975, 0.08745049035549164), (0.597, 0.06729910543560982), (0.614, 0.06572770889103413), (0.596, 0.07482888190448284), (0.578, 0.07428202259540558), (0.57575, 0.08690565618872642), (0.5725, 0.090032439827919), (0.556, 0.09119307973980903), (0.59425, 0.07220714667439461), (0.54375, 0.09757866564393043), (0.54875, 0.10250440865755081), (0.5705, 0.08318909281492233), (0.5865, 0.08046024841070175), (0.54075, 0.1005496085882187), (0.55725, 0.08998369893431664), (0.5645, 0.09595633280277252), (0.523, 0.10773491796851158), (0.56325, 0.1012795572578907), (0.55825, 0.0877519374191761), (0.52475, 0.1073695180118084), (0.59025, 0.07867211282253266), (0.54575, 0.09320002949237824)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.93      0.88      0.90      1000
           4       0.37      0.97      0.54      1000
           5       0.69      0.17      0.27      1000
           7       0.80      0.17      0.28      1000

    accuracy                           0.55      4000
   macro avg       0.70      0.55      0.50      4000
weighted avg       0.70      0.55      0.50      4000

Collaboration_DC_2
VAL: 
[(0.244, 0.045177888870239255), (0.25, 0.10054781334102154), (0.252, 0.10466695122420788), (0.38, 0.10775397792458534), (0.413, 0.15356273053586483), (0.395, 0.17955271257087588), (0.436, 0.20252362125553192), (0.441, 0.1993113804385066), (0.441, 0.22230591307021677), (0.464, 0.251293695660308), (0.456, 0.24946794794872404), (0.583, 0.056047844752669336), (0.648, 0.0463257787823677), (0.713, 0.03311613842844963), (0.671, 0.03681181578338146), (0.717, 0.02835196453332901), (0.718, 0.03316137959063053), (0.711, 0.03475248958170414), (0.694, 0.03859878826141357), (0.693, 0.03699357694387436), (0.615, 0.05856676339358091), (0.645, 0.04714084534347057), (0.677, 0.04030956010520458), (0.676, 0.04374094121158123), (0.594, 0.0681470917314291), (0.646, 0.05417554976791143), (0.636, 0.05489017649367452), (0.576, 0.07691042716428638), (0.617, 0.05995112732797861), (0.661, 0.049516352485865354), (0.633, 0.07320124753564596), (0.627, 0.061843223448842764), (0.59, 0.08524585388321429), (0.672, 0.045620070688426495), (0.612, 0.06211752519011497), (0.624, 0.07008354194834829), (0.597, 0.07390842138975859), (0.623, 0.06706994019635022), (0.669, 0.04938983228057623), (0.66, 0.05953462942317128), (0.619, 0.06420395309850574), (0.638, 0.060016704466193914), (0.662, 0.05071088765561581), (0.684, 0.04689658886566758), (0.645, 0.06314047473669052), (0.685, 0.045208358090370895), (0.651, 0.05635213004052639), (0.664, 0.059060312990099194), (0.663, 0.05820565201714635), (0.62, 0.06662887777015566), (0.671, 0.05140670317225158)]
TEST: 
[(0.22875, 0.04402987053990364), (0.25, 0.09631045669317245), (0.25525, 0.0999217810332775), (0.39225, 0.10327717873454094), (0.41875, 0.14785195577144622), (0.405, 0.17185581016540527), (0.43225, 0.1947092747092247), (0.43925, 0.19275807696580888), (0.451, 0.21640128707885742), (0.4615, 0.24251160019636153), (0.452, 0.24052831733226776), (0.57475, 0.05484951885044575), (0.65675, 0.04448143258690834), (0.72425, 0.031156053587794302), (0.6845, 0.03302995775640011), (0.731, 0.025927248783409594), (0.72325, 0.03056872084364295), (0.724, 0.03298020265996456), (0.69225, 0.0363890623152256), (0.7075, 0.03530725152790547), (0.627, 0.056359536543488506), (0.645, 0.04552898209542036), (0.69125, 0.037658337593078614), (0.66725, 0.0433318530023098), (0.61425, 0.06468041081726551), (0.6385, 0.055320115022361276), (0.64825, 0.05228676351904869), (0.60225, 0.07397928515076638), (0.63, 0.05823353292047977), (0.66725, 0.0495634186193347), (0.638, 0.07338423034548759), (0.6515, 0.059935865491628645), (0.60325, 0.08343433032929898), (0.67825, 0.04446661166101694), (0.61925, 0.061773659646511075), (0.63225, 0.06754886589944363), (0.60575, 0.07295076191425323), (0.645, 0.06209358625113964), (0.675, 0.0456030540317297), (0.6605, 0.05497546315193176), (0.62675, 0.06186802113056183), (0.64275, 0.055039325028657916), (0.6665, 0.04918123319745064), (0.68425, 0.04501206471771002), (0.6585, 0.06062144488096237), (0.68925, 0.043004599660634996), (0.6725, 0.051770754247903825), (0.66175, 0.055205322563648226), (0.6825, 0.05685276632010937), (0.62375, 0.06362531863152981), (0.68825, 0.047443673610687256)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.54      0.77      0.63      1000
           4       0.74      0.97      0.84      1000
           8       0.90      0.62      0.74      1000
           9       0.69      0.40      0.50      1000

    accuracy                           0.69      4000
   macro avg       0.72      0.69      0.68      4000
weighted avg       0.72      0.69      0.68      4000

Collaboration_DC_3
VAL: 
[(0.209, 0.04494925320148468), (0.257, 0.09110854172706603), (0.323, 0.11809220957756042), (0.355, 0.1430300070643425), (0.356, 0.16536943846940994), (0.354, 0.1911297039538622), (0.376, 0.1672702697366476), (0.384, 0.1897293770611286), (0.387, 0.205445771753788), (0.379, 0.1797317993193865), (0.393, 0.2138650175333023), (0.423, 0.05902448618412018), (0.427, 0.0683288446366787), (0.516, 0.06273820409923792), (0.497, 0.0686497095823288), (0.51, 0.0758630507439375), (0.544, 0.07299188117682934), (0.522, 0.08426463341712952), (0.503, 0.11136553245782853), (0.545, 0.08213913640007377), (0.547, 0.08361987232044339), (0.526, 0.09573384264111519), (0.562, 0.09010484267771245), (0.549, 0.08438677062094212), (0.537, 0.09395713527454064), (0.524, 0.09478746940381826), (0.553, 0.08296396810933948), (0.583, 0.06909551439620555), (0.574, 0.07543066402897239), (0.571, 0.08230548187345266), (0.539, 0.08717309763655066), (0.582, 0.07287285409867764), (0.571, 0.07104285535216331), (0.561, 0.08849761410802602), (0.533, 0.09874813477881253), (0.54, 0.09844984780065716), (0.534, 0.10490062907710672), (0.551, 0.0843046133518219), (0.534, 0.11143588774697855), (0.549, 0.09507564569264651), (0.551, 0.08928520199563354), (0.533, 0.1049615674316883), (0.542, 0.08872351581603288), (0.564, 0.08601281375437975), (0.552, 0.09581564490124583), (0.518, 0.11095990849938243), (0.537, 0.10764810177311301), (0.519, 0.1030595711749047), (0.545, 0.0886077049029991), (0.548, 0.09550420293980279), (0.523, 0.1275918675386347)]
TEST: 
[(0.20325, 0.043842192202806475), (0.2555, 0.08742595201730728), (0.3265, 0.11303198945522308), (0.359, 0.13696498495340348), (0.36825, 0.15801430696249008), (0.3605, 0.18254355472326278), (0.382, 0.1589944772720337), (0.3905, 0.17838659977912902), (0.39525, 0.1921792615056038), (0.3875, 0.17070769482851028), (0.395, 0.20351121479272843), (0.45025, 0.05600716817378998), (0.44275, 0.06636409860849381), (0.50675, 0.0654287048280239), (0.489, 0.06868428280949593), (0.5135, 0.07704280593991279), (0.53575, 0.07565879631042481), (0.5115, 0.08824002537131309), (0.50275, 0.11327805489301682), (0.54575, 0.08462917518615723), (0.551, 0.08630788689851761), (0.52825, 0.0972886984348297), (0.54475, 0.0946157858967781), (0.52925, 0.0887247516810894), (0.529, 0.09713592398166657), (0.52775, 0.09930387637019157), (0.54425, 0.08505428701639176), (0.5715, 0.07301555788516999), (0.5615, 0.0795986088514328), (0.54775, 0.08881186330318451), (0.53725, 0.08977076232433319), (0.566, 0.07456606966257096), (0.5655, 0.07407875412702561), (0.54425, 0.09010385057330131), (0.5285, 0.10519302439689636), (0.531, 0.10229617884755135), (0.54225, 0.10778404530882836), (0.54525, 0.08928690674901009), (0.518, 0.11798967260122299), (0.5285, 0.09847493547201157), (0.54225, 0.09411294457316399), (0.536, 0.10832315433025361), (0.532, 0.09336476868391037), (0.533, 0.09233720070123673), (0.547, 0.09985920768976211), (0.508, 0.11693463531136512), (0.5295, 0.11388250258564948), (0.5145, 0.10666819703578949), (0.5305, 0.09547691902518272), (0.54325, 0.10115653216838837), (0.51075, 0.1304891173839569)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.94      0.85      0.89      1000
           2       0.47      0.04      0.08      1000
           3       0.54      0.18      0.27      1000
           4       0.36      0.97      0.53      1000

    accuracy                           0.51      4000
   macro avg       0.58      0.51      0.44      4000
weighted avg       0.58      0.51      0.44      4000

do_assignment: None
checkpoint_path: /mnt/lia/scratch/pohl/SP-Experiments/pohl_checkpoints
result_path: /mnt/lia/scratch/pohl/SP-Experiments/results
seeds: [12]
name: alliance-4-dcs-12
score_metric: contrloss
aggregation: <function fed_avg at 0x71c9cd94cc10>
alliance_size: 4
communication_rounds: 50
communication_rounds_before_merging: 10
local_epochs: 5
batch_size: 32
dataset: cifar10
num_classes_in_dataset: 10
public_dataset_size: 5000
do_train_set_size: 1000
dc_val_set_size: 1000
partitioning: ((2, 6), [(2, 6), (2, 6), (2, 6), (2, 6)])
dir_alpha: 1.0
fed_prox_mu: None
ekd_lr: 0.001
ekd_batch_size: 32
ekd_epochs: 10
ekd_temperature: 1.0
ekd_hard_loss_weight: 0.0
ekd_selection_frequency: 11
ekd_selection_ratio: 0.6
matching_frequency: 5
realistic_filtering: False
RUN FOR SEED=12
Partitioning data
[[6, 1, 7, 4], [5, 0, 7, 4], [8, 3, 7, 4], [9, 2, 7, 4]]
[(array([40565, 32339, 39457, 16008, 30469, 15600, 36274, 46340, 27651,
       15424,  4824,  4421,  6548, 25892,  7823, 36318, 14392, 32170,
       25039,  1579, 38310, 46334,  6486,  2424, 38993, 15349, 25294,
       15048, 20755,  9406, 18256, 28382, 47734, 42975, 20535,  3401,
       25877,  3412, 11070, 32198, 33824, 45836, 20800,  7137, 49129,
       49110, 36522,  4201, 49584,   234, 25526, 49754,  7481, 43933,
       46846,  8734, 10007, 32904, 32752, 25704, 28177,  4118, 45759,
       14701, 26235,  6101,  2978, 18816, 45863, 35591, 20025, 42881,
       21719, 19834, 35577,   248, 37835, 22889, 35615, 39258, 36625,
       15657,  3122, 34245, 30390, 33367, 28504, 42209, 16064, 36613,
       23105, 48229, 16960, 22004, 30031,  6581, 36348, 46550,   298,
       30759, 12254, 45140,  1159,  8344, 17554,  1222,  2596,  3018,
        3676, 10783, 22567,  8053, 18025, 37045, 25448, 25569, 46134,
       14959, 41142,  3223, 12635, 49966,  8820, 33990, 36062, 17173,
       17858, 20324, 22058, 24231,  3873,  1585, 22363, 27843, 38337,
       27708,  9130,  4105, 25612, 25155,  2583, 13390, 42380,  5206,
       48206, 29214,  7247, 20789, 35780, 26542, 23760, 41491, 13176,
       34602,  4731, 15517, 37748, 27454,  9303, 20278,  1917, 33812,
       27602, 16508, 10176, 44573,  8171, 47961,   620, 25641,  8015,
        2777, 34564,  2439,  5497, 21730, 37796, 34264, 45637,  3926,
       31099, 43624,  5679, 39469, 42960, 31651, 12221, 21673, 37715,
        2179, 49808, 39337, 18396, 47407,  8867, 29089, 37404,  2043,
       32637, 33178,   738, 38786, 25551, 14291, 42584, 30089,  8827,
       30249, 46418, 29357, 21965, 32497, 40824, 49094, 40929, 34552,
        7044, 35060,   132,  5964, 23927, 39133, 41275, 41045,  6046,
       38959, 10757,  3554, 18571, 30100, 26231, 40206, 31826, 22295,
       11917, 26502, 15338, 42015,  4805, 11233, 27087, 25035, 30292,
       39808,  8170,  5169, 11310,  8128, 20717, 16927, 27868, 35550,
       13763, 14074, 34287, 19144, 44273, 37301, 39990, 33228, 23514,
       32603, 41576, 33112, 41445, 39936, 21088, 28347, 30923, 11580,
         466, 32418,  8124, 28847, 19310,  5211, 30676, 46219,  7722,
       44541, 28193, 34258, 30269, 30166, 12384,  7326, 16925, 43600,
       32458, 10363,   873,  9946, 34830, 49993,  9138, 45672, 42655,
       12741, 23275, 29966, 34376, 39097, 41087, 14101, 44097, 45569,
       16233, 25363, 36046, 38812, 13755, 22532, 24450, 40178, 36614,
        3231, 37153,  5171,   841, 39338, 13839,  5685, 18117,  2584,
       44109, 12865, 16083, 23933,  9118, 42734,  7735, 36736, 38756,
        4360,  2582, 13899, 12064, 23923, 13488, 20312, 23648,   262,
       37235, 26994, 33452,  7475, 26452, 13056, 39654, 45704, 10112,
       48612, 31512, 13897, 44483, 44971, 21933, 34637, 20475, 24873,
       39848, 26578, 40987, 21706, 36595,  5810, 11421, 21073, 42709,
        1304,   536, 31113,  9754, 19574,  4358, 37620, 18333,   690,
       19810, 21796, 14005,  6895, 28137, 27002, 24297, 32959,  7841,
       41525, 15295, 16320, 31072, 28882, 45488, 10271,  2910, 34700,
       37461, 32979,  4132, 44217, 40972, 31747, 10236, 19930, 44185,
       48294, 34064, 49186,  7622, 24284, 12676, 31195, 44686,  6318,
       26080, 24219, 36107, 49416, 46519, 40505, 34238, 16673, 27538,
       18066,  1694, 45444, 22762, 49937, 23100, 21651, 24417,   364,
       44134, 39805, 43251, 47540, 44726, 46177, 29791, 15655, 44993,
       45988, 19500,  8573, 37666, 24121, 21547, 11782, 27132,    75,
       33114, 15883, 28122, 32684, 19301, 15557, 34690, 29314, 29894,
       16368, 16275, 26637, 46396,  1781, 12516,  3620, 10614,  6068,
       48465, 38858, 12923, 30049,   815, 40751, 31539, 48312, 24466,
       44524, 11646, 15756, 20376, 38047, 36813, 23714, 12054, 49225,
       18832,  7130, 19546,  7832, 45736, 27871, 38950, 33859, 44487,
       37979, 44311, 16800, 14507, 29309, 38286, 33712,  1445,  9587,
       38703, 14641, 19461, 24917, 35908, 23785, 31777, 36114, 10281,
       10573, 40637, 24850, 17181, 15079, 29272, 17737, 10873, 26991,
       18165,  6573,  8897,  4037, 48567, 48055, 48041, 16026, 16750,
       37150, 26046, 34707,  8859, 27006,  7813, 44922,  7223, 33825,
       38195, 20678, 37930, 39259, 14680, 35682, 49903, 16537, 49745,
       12995, 27288,   181, 32374, 26107, 38279, 49058, 24197, 17436,
        3135, 10278,  8129, 26420, 38243,   972,  9133, 48837,  3637,
        5326, 11088,  5774, 39378, 18447, 42485, 41758,  2135, 48104,
       15070, 15511, 14449, 12658, 42895, 17521, 39408, 21660, 48697,
        9110, 41526, 12889, 42514, 49226, 26166, 43625, 13569, 28328,
        3955,  3157, 10858, 28987, 46754, 43360, 27985, 45000, 39307,
       10602, 21107, 36608, 23186, 15245, 42268,   797, 40300,  1930,
       13703, 28366, 17463, 48204, 44353, 25079, 37842, 28498, 20780,
       13039, 26017, 48256, 29196, 46620, 32208, 40820, 35696,  7969,
       31944,  3050, 39257, 14775,  8106, 21482, 42593, 19844, 45999,
       22204,  7331,  1742,   289, 41805, 44493,  6329, 39711,  1218,
       36278,  8595, 11996, 17636, 29535, 20109, 38693,  1071, 12787,
        1042, 31357, 40912, 20565, 17910,  6677, 46412,  8038, 16504,
       49925, 18227, 18468, 49183, 11426,  9805, 29644, 46672, 36589,
       35241, 28197, 35290,  5363, 40692,  9374, 21335, 34876, 27775,
        5842, 48848, 31117, 26733, 36369,  5269, 14493, 14290, 26947,
       37262, 36936, 42463, 37896, 10818, 34584, 43060, 40356, 22569,
        7621,  1309, 12900, 26056, 19200,  2801, 27187, 49627,  2458,
       35483, 18579, 12961, 16171, 33485, 40893, 16914,  6449, 23571,
         771, 44610, 46988, 44798, 35164, 28639, 10235,  1908, 34044,
       15235, 30422, 21535, 38667, 44424,  2103, 18517,  1308, 23910,
       24901, 31520,   973, 39633, 19638, 12244, 20016, 16654, 35869,
       21269,  2301, 38308, 45767, 15196, 44147, 43595, 31464, 14670,
        1169, 48505, 10250,  6838, 44336, 40146,  1387, 38135,  7436,
       23545, 28250, 24883, 28167, 19528, 33171,  9724, 15168, 45170,
       35066, 23779, 10861,  6144,  9529, 36671, 27494, 14557, 43549,
       49687, 10865, 39005, 46883, 49390, 44450, 11092, 46753, 19742,
       27461, 24448, 28837, 20515, 13453, 32818, 30464, 25579, 45515,
        2123, 26819, 28576, 19643, 45315, 10633, 10392, 10539, 35774,
       44400, 15024, 17801, 40228, 47716, 28464, 13846, 45723, 23460,
         856, 25779, 26638,  3577, 42750, 30616,  2106,  1022,   665,
       44125,  1882, 40962, 15482, 37214, 19264, 37854,  3945, 19846,
        1784, 47144, 37089, 23634, 28561, 22944, 13987, 10519, 28666,
       23329, 36294, 22243, 18409,  3897,  3936, 37431, 32893, 48634,
       20888,  1813,  9349, 23337,  7365, 41237, 31689, 47287,  5229,
        4057, 31033, 15703, 41825, 12444, 45408, 13469, 40674, 18404,
       29291, 49504, 20412, 18340, 41518,  1863, 23817, 23468, 33666,
       18052, 37274,  6654, 26584, 25434, 18875, 25620, 46940, 27384,
       26118, 14911, 30300, 40259, 39470, 16073, 30230, 30715,  6344,
        1575,  6844, 12136, 33342, 16802, 22536,  5998, 42725,  9848,
       30077,   153, 44705, 26772,  8523, 23211, 48429,  1516, 24046,
        1069,  2839, 32928, 26911, 19689, 18490, 32415,  6234, 46789,
       41263, 48211, 40210, 47345,  2842, 45323,  2835, 43553,  5405,
        6270, 41523, 26854, 17623,  1814, 10925, 21753, 40531, 45820,
       16323, 33408, 22461,  2647, 36976, 18304, 20782, 49402, 49954,
       10536, 41101, 38350, 24308, 47798,   572, 15515,   660, 19475,
       14262, 33140, 49902, 34670, 27817,  9061, 27558, 35714, 48997,
        1832, 43926,  1238, 14726,   868, 46528,  5514, 12522, 14019,
         490, 17211, 46446, 36523, 36317,  7656, 11033, 38395, 20543,
        8541, 13292, 40033, 30020, 32413, 12505, 41852, 31256,   934,
        8473]), [6, 1, 7, 4]), (array([ 9035, 26874, 11513, 48114, 30657, 23488, 40417, 41232, 30871,
       32769, 18463,  3173, 13676,  2864,  8494, 43759, 36121, 42293,
       34833, 19274, 41046, 28982, 42866, 41094, 31646, 29268, 27653,
       44838,  4125, 15861,  4627, 33957, 14495, 46608, 37114, 32849,
       32530, 16949, 40069, 20801, 27991, 42194, 20348, 45992, 46428,
       49267, 44284, 46165, 37508, 15653, 31409,  6580, 13251,  6436,
       16626,  1091, 20207,  6982,  9871, 41139, 27947,  5367, 26194,
       23273,  3451,  3080, 21677, 11995, 40632,  9337,  4890, 45553,
       38990, 28779, 30155, 26079,  7090, 18966, 14004, 45215, 39817,
       44358, 25333, 22890, 37899, 48935, 48832, 46310,  6561, 47218,
       12177, 16580, 17009, 22059, 30723, 37880, 40434, 38632,  5993,
       46853, 31059,  2089, 40051, 43038, 13579, 47103, 49988, 14627,
        2148, 18399, 27348, 40904,  8252, 40763,  6220, 13283, 43651,
        6807, 25751, 18347, 32044,  1199, 14259, 41340, 24877,  1545,
       29450,  2182, 19169, 24656, 47602,  9772, 47919, 11740, 32559,
       45587, 39124, 32658, 28425,  4120, 15450, 21233, 46130, 14992,
       27265, 39783, 47920, 41499, 26072, 46022,  6689, 42613, 36202,
       49547,  2374,  7398, 38944, 41886, 47659,  6058, 29630, 15544,
       34112, 34728, 49675, 27223, 47647, 31120, 49316, 11980,  1355,
       28090,  7140,  9445, 29422,  4990, 27941, 42549, 32124,   424,
       38739,  1061, 21598, 11396, 40688, 20493, 48754, 38403, 15019,
       21046, 37708, 48083, 30231, 47230,  8274, 20809, 29804, 28889,
       22483, 29836,  2194, 20075,  7446, 30952,  6037, 38709, 30054,
       10258, 23392, 24648, 18852,  8511, 27397, 20421, 19652, 36123,
       18772, 13057, 27666, 34299, 30162, 24909, 25552, 20171, 28438,
       19933, 20277, 36235, 33079, 18986, 49810,  9929, 33039, 41817,
       47555,  6707, 22908, 15336, 11600, 30393, 24953, 40105, 39846,
       24650, 46483, 46690, 43599, 27305,  4632, 40765,  2895, 28767,
       40432, 48556, 31023, 38312, 43659, 47560, 21930,  8753, 34070,
       16375, 38716, 15959, 43253,  3609, 28138, 38307,  1329, 28919,
       16513, 19409, 46137, 23010, 35528, 42183, 39396,  7808,  5684,
       41164, 21573,  8102, 45390, 20295, 43731, 46192, 44731, 10578,
       28003, 22123, 31481, 13964, 49130, 15153, 22271, 19273, 44967,
       12469,  2490, 20902,  4295, 18694,  8437, 44592, 25489, 10804,
       43138,  8660,  2156, 47805, 11218, 27278, 25668, 34711, 29732,
       16316, 16065, 15115, 34748, 29189,  2863, 37068, 29039, 26546,
       46934, 17221,  6904, 39819,  4721, 25477,   782, 28063,  6688,
       32363, 13714, 12936, 19945, 19974, 44430, 37788, 10031, 39138,
        9171, 38321, 18614, 43077, 31600, 24111,  3909, 45357, 13060,
        7588, 10486,  4619,  7643, 35873, 36530, 43245, 17082, 38320,
       11431, 37664, 16752, 48148, 34341, 11056, 21322, 35546, 11276,
        3987, 19493, 37146, 23629, 47557,   650, 27211, 36323, 47994,
       30539,  4952, 37165, 11709, 35952, 27901, 32140,  7599, 11178,
       38586, 11114,  9435, 20396,   284, 35608, 29293, 49513,  9236,
        5738, 48069, 19106, 22026, 16146, 21472, 32683, 12028,  4746,
       36497, 45186, 48239, 49799, 41167, 14468, 41212, 29518, 27042,
       46041,  4861, 31015, 46303, 22993,  4524,  2617, 48096, 29544,
       18864, 33530, 28650, 14216,  6518,  1708,  6187,  3582, 49340,
       31955, 42190, 29620, 45438,  4523, 11304, 32056, 25905, 47931,
       47645, 23746, 12542, 33386, 26081, 42844, 28068, 20743, 34568,
       28467, 38190, 49375,  7985, 39311, 40012, 11721, 17994,  3585,
       17154, 41030,  4096, 33711,  6090, 35028, 14958, 33062, 38580,
         332, 30888, 18763, 47271, 36007, 31246, 48059, 49992,  5441,
       45182, 37712,  2959, 48378, 30467, 38334, 30909, 48677,  1187,
       23984, 10326, 41879,  9058, 27080, 33831, 24713, 49346,  5331,
       34705, 28515, 26342, 48554,  8242, 37467, 34900,  2716, 37787,
       14143, 24026, 38142, 18997,   470, 36806, 20884, 49277,   948,
       12751, 36829, 40942, 42909, 11323, 27374, 28184, 44101, 16074,
       24267, 39717, 38225, 43066,  8966, 17913, 49809, 16719, 42184,
        5114, 14818, 21109,  6850,  9778, 17095,  6283, 47092, 27565,
       25742, 41347, 33096, 19250, 32920, 40120, 23290, 11046,  9071,
         318,  1165, 35284, 26494, 11393, 29718, 10927, 36897, 20550,
        9795, 43617, 20436, 32568, 46671, 46719, 27242, 17951,  6821,
       43435, 40684, 47923, 10226, 17494,  2612, 19418, 24782, 10685,
        9560, 22730, 12980, 43229, 24025, 35986,  5307, 13147, 23577,
       22207, 37190, 26397, 49610, 21100, 24970, 42651, 37131, 47880,
       31800, 26821, 17499,  4396, 48343,  8664, 39262, 35815, 30036,
       49229, 34273, 47835, 23356,  6460, 34191, 24371, 39261, 47438,
       35972, 26749, 27015, 36597, 31262, 23034, 18874, 29481, 41780,
       15853, 22291, 14603,  2814, 41370,  9239, 12838, 44911,  7482,
       19883, 32514, 49454, 45929,  6695, 43906, 38512, 43528, 41584,
       10884, 13272, 14191,  8745, 39799, 35421, 32888, 33949, 38155,
       26811,  6072, 49037, 30278, 47856,   688,  4555, 11284, 44081,
       37637, 25197, 15472,   575, 44115, 45376,   329, 26067, 46691,
       13540, 39155, 35962, 47169, 32354, 33884, 27078, 39017, 39720,
        7895, 19997, 20369, 10598, 23490, 11502, 13136, 34113, 43830,
       33349, 33636, 46996,  4903, 33155, 10106, 24954, 30972, 44286,
         746, 36034, 11102, 21329, 15551, 32688, 23880,   362, 43305,
        8764,  7717, 43671, 18037, 48029, 43782,  8643, 24088,  9768,
       23835, 23842,  1857, 17872, 12844,  5798, 33045, 18881, 12848,
       47590, 43242, 34826,  6653, 46834,  1395,   294, 23331, 33908,
       46982, 11260, 20099, 25304,  2000, 36037, 30410, 23778, 42251,
       49010, 12402,  4513, 22723, 40585, 26702, 33556, 34528, 33139,
        1046, 37497,  6696, 21899, 31939, 20120, 30915, 19936, 24889,
       13412,  4231, 19033, 26717,  1855, 43756, 33221, 28795, 37124,
       37778, 41500, 42403, 38455, 20253,  6465, 49669, 13834, 48529,
       46398, 47300,  9907, 30794, 25371, 45598, 24513, 37465, 41219,
       23247,  5259,  6785,  3668, 34410, 27897, 16383, 49421, 25114,
       10641,   130, 43714, 11573, 26062, 29070, 47482, 19822,  3252,
       38794, 34840, 35432, 39734, 42072, 24548,  1388, 47796, 14244,
       47415,  4297, 23279, 13023, 46339, 32088, 10640, 15909, 22949,
       13882, 30917, 36466, 33352, 35868, 26830, 21788, 22727, 14858,
       11967, 10706, 39169, 41453, 23748, 39573,  5940, 27074, 40707,
       14943, 18553, 14036, 17083, 42809,  9749, 17199,  1639,  9506,
       30942, 10979, 30435, 30911, 20306, 28100,  7368, 43306, 32533,
       43203, 29098, 43499, 15625, 24171, 11994, 16294, 42040, 44563,
       16241,  3314, 32989, 22288, 23512, 27247,  3288,  7052,  9615,
       48179,  1018, 45136, 21980, 22740,  4695, 28913, 21676,  1968,
       16433, 14111, 31305,  3522, 15596, 17958, 40770, 17412, 46241,
       36646, 20108, 16865, 37438, 28489, 30094, 32069, 17671, 23394,
       46164, 11209, 38787, 43327, 31767, 20325, 20407,  5285, 47139,
       44851, 24040, 27489, 45801, 36420, 29849, 44572, 34331, 10851,
       21666, 39945, 39344, 48738, 34589, 14256,  3112,  3360, 21708,
       42640, 30970, 37442, 18043, 31476, 34450, 46891, 47855, 21176,
       15582, 41246, 19657, 34082, 29420, 35135, 32518, 40301, 38593,
       15377, 42695,  5173, 41337, 22886, 27276, 22241, 40665, 48616,
       27180, 44040, 17420, 31627, 14181, 27017,  4429,  3761,  5495,
        8631, 35563, 15339, 14827, 16659, 13269, 28793,  9888, 22509,
       13764,  6378, 26586, 25147, 20060, 40071, 34051, 40793,  3176,
       49243, 11757, 38070, 38953,  6562, 19426, 47975, 48716, 16891,
       15256, 18418,  5788, 12022, 33129, 17719, 29024, 46255, 21629,
       14051]), [5, 0, 7, 4]), (array([25450, 26156, 12543, 48247,  1236,  2881, 16730, 20375, 23858,
        2417, 35995, 12400, 19402, 18221, 37437, 34655, 14767, 25987,
       27370,  4245,  9117,  2979, 29747, 19696, 40389, 47021, 15161,
        1770, 17416, 31714, 35315, 18691, 45697, 30087, 48469,  6632,
       40784, 16042, 14427, 10098, 48133, 43807, 19123, 27281, 16381,
       33626, 31879, 46392, 21477, 43662, 15260, 19603, 38867, 32896,
       40368, 10606, 46736,  5648, 45138, 32447,  5985, 18623, 32463,
       20668, 21383, 39981, 42467, 30770, 16607, 30835, 45168,  9960,
       43423,  4670, 32728, 17799,  2929, 22696, 28450, 42497, 40691,
         418, 27398, 33372, 12418,  9076, 16512, 24456, 46926, 43899,
        7112,  7387, 24787, 47114, 46175,  6337, 43001, 43883,  1479,
       41070, 48445, 28006, 23818, 12725, 40671, 22440, 49372, 41585,
       37318, 40881, 48698,  2489, 48794, 22374, 46938,  3130, 28284,
       25096, 20812, 17617, 19374, 23294, 26437, 28205, 13266,  5278,
       12650,  2820, 20405, 31692,  8489, 39838, 43344, 25756, 21257,
       48552, 32327, 42300, 42203, 12258, 21249, 45025, 40789,  8270,
        9572, 43540,  9669,  4502, 16692, 45354, 26195, 17519, 19167,
       42840,  8961, 26328, 43119,  7561, 21312, 17348, 45309, 34171,
       35173, 19720, 41674, 14211, 16483, 36229, 32207, 41008, 44937,
       37557, 29056,  4602,  9972, 42826,   562, 35864,  8211, 29968,
       37858, 43755,  5941,  6139, 27640, 49166,  9599, 13694, 32246,
       39895, 17007, 13139, 39802,  9651, 42965,  4713, 40495, 25951,
       29476, 34212, 16727,  2483, 38689,  8022,   222, 22649, 13886,
        8831, 32465, 32138, 48475, 35588, 16861, 16084, 49253, 24629,
       26694, 23172, 36742, 34893, 23085, 29929, 26254, 46735, 43895,
       33287, 49625, 25902,  4723, 27056, 29877, 19314, 21565, 48125,
       22682, 36803, 39621, 49916, 49553, 38469, 45853, 38809, 42763,
       25629, 16049,   244, 41298, 10191, 31317, 28800, 24652, 20638,
       33525, 16289,  1104, 34160,  4542, 35769, 49408, 33917,  3679,
       19830, 30995, 33007, 24733, 19101,  3429, 15313, 19578, 31400,
       44621, 18414,  4954,  7906, 48296, 13541, 14047, 25633,  5190,
       24696, 24751, 31463,  1461, 43490, 32732,  9794, 36351,  9164,
       11409, 42504, 22891,  1070,  7651,  8710,  6482,  8101, 26688,
       30586,  1010, 22011, 29523,  9298, 26224, 12039, 38068, 10506,
       34199, 11564, 14466, 28330, 22239, 13530, 30839, 19327, 49975,
       25307, 15122, 39021, 14993, 37187, 14161, 32405,  2141,  4151,
       27608, 20301, 22850, 40249,  7006, 19922, 21299, 36230,  6849,
        8330,  7050, 32702,   266,  4402, 44987,  2165, 46094,  9006,
        8028, 33280, 36440, 41713, 33964, 41269, 14574,  9700, 11357,
       22905, 24099, 34661, 35937,  7680, 44173, 20639, 35715, 23094,
       32139, 11386, 19221, 16731, 35772, 22411, 38364,  9008,  3997,
       24174, 44943, 42738, 35229, 42736, 34889,  2412, 29442, 34710,
       46425, 24434, 16851, 33600, 20625, 14187, 40973, 11902, 43827,
       31414, 43784,  4221,  6883, 30774, 49319, 15640, 16495, 36772,
       30346, 44381,   639, 16561, 20326, 32085, 42953, 20577,   922,
       17920,  5592, 29654,  3355, 25769, 44576, 11268,  5945, 49140,
       29464,  8408, 29960, 10017,  3571, 38647, 46347, 43737,  1265,
        2127, 47062, 44226, 34999, 44768, 31542, 42837, 17828, 46023,
       45670, 15080,  8246, 18520, 41271, 10931, 20882, 48346, 14253,
        7200, 25471, 13862, 41910, 20330, 36212,   857, 10414, 16646,
       43440, 27274, 37965, 20685,  9343, 40232,  9897, 43744, 17074,
       45402, 34185, 16271, 13824, 26900, 47838,  8790, 24148, 23372,
       29050, 39778, 10634, 10869, 27906, 15224, 42496,  2892, 14239,
       10066,  8041, 48772,  6724, 49918, 16341, 37370, 39158, 12517,
       38215, 47654, 26907,  5744, 48376, 12147, 17155, 38006,   207,
       13235, 27475, 47717, 28880,  8105, 30265,  1135, 26931, 10384,
       46045,  5620, 42063,  5995, 49843, 38176, 31336,  7817, 39107,
       32068, 10988, 45103, 28520, 32487,  7023, 26810, 34260, 43436,
       22601, 40646,  7756, 34511, 27600, 10295, 16557, 23344, 33681,
       39395,  6326, 43791, 33773, 43144, 27573, 44935, 40694, 23889,
       38595, 12722, 43091, 27824,  3611, 46211, 14614, 13132, 39400,
       27423, 45353, 25741,  7940, 12229, 46383, 29824, 10333, 20879,
       39304, 32440, 38121, 29195, 44076, 31818, 31671, 13732, 36407,
       39345, 17040, 36805, 48548, 19542, 24311,  2139, 15579, 47082,
       44784, 18481, 37636,  2807, 37302, 26767, 27400,  9486,  7682,
        4820, 29719, 29441, 29588,  3434, 36239, 17464, 41185, 22501,
       16399,  6966, 36959, 27043, 45343, 40709, 49470,  9891,  2244,
        1226, 28553, 27189, 42421, 44292,  1758, 27568, 30199,  6481,
       36256, 27226, 44363, 24505,  2113, 14222,  2750, 31432,  2278,
       21601, 21367, 33191, 40497, 43579,  4886, 10823, 20900, 13430,
       10216, 22901, 42052,  5579, 26916, 34308, 13974, 43309, 25131,
       18519, 25343, 26895, 24439, 39797, 12466, 13224, 40916, 47003,
       40295,  5200, 32211, 15979, 35982, 39111, 47828, 43940,   739,
       37055, 34955, 41700,  3279, 38539, 35698, 18405,  3914, 27748,
       33722, 10121, 32700, 15317, 31389,  3996,  8646, 46154, 48714,
       16184, 12901, 31331, 48307,  7016,  3623, 12163,  2776, 46517,
       20614,  9741, 10916, 47572, 28951, 40266, 44356, 15881, 23794,
       20158, 39700, 39276, 10621, 35003, 43215, 38732, 30066,  8946,
       39767,  5368, 47890, 25876, 41205, 47575, 14837, 16348, 12565,
        5863, 26633, 36136, 44222, 10712, 25116, 44135,  3150, 20960,
       22312, 35511, 29320,   722, 40837,  3699, 37670, 18828, 19361,
       13790, 40897, 42353, 17001,  2119, 26983,  3213,  9726, 33176,
       26104,  3933, 11329, 36417,  9672, 35153, 27036, 35002, 23330,
       37405, 37001, 22403, 36740, 41382, 41958, 17077,  2995,  4707,
       49488,  7933,  6314, 26838, 27790, 10888, 14186, 30559, 39501,
       22617,  1903, 37928, 31902, 32384, 31143,   831, 35522, 47155,
       28881,  2515, 10055, 37731, 14618, 42982, 34604, 31860, 44974,
       39413, 49352,  2210, 40277, 12213, 20778, 44656, 14310,  6168,
       23826, 18977, 44465, 47147, 26048, 35915,  5909,  7283, 24866,
        1762, 10015, 10179, 17513, 44900, 12417, 16240,  9638, 40943,
       31799,  3182, 19585, 18142, 31227,  5225, 14560, 20409, 18523,
        5815, 43758, 45586, 48046, 42795, 40195, 25014,  1569, 14082,
       27135, 19569, 20192, 43983, 41878, 49718,  3556, 17350, 43139,
       32100, 45623, 37530, 13310, 30306, 35186, 33674, 45397, 28696,
        7100, 10059, 38242, 22318, 42544, 30206, 45279, 41623,  4106,
        3397, 48235, 32654, 37123, 37960, 28796, 19006, 40067,  6720,
       46076, 27519,  6266, 26434, 42285, 48876, 32169, 13636, 45812,
       16155,  8331, 32643, 25703, 47348, 31306, 36755,  3322, 30841,
        2488, 31446, 46036, 30674, 45121, 30228,  3952, 22820, 23393,
        8913, 15525, 22210,  3189, 45749, 33027, 27613, 27199,  9153,
       27029, 25576, 47908, 13984, 46778, 39036, 27853, 35151,  4725,
       46011, 45688, 30242, 42603, 27100, 30425, 39576,  1303,  8447,
       46947, 40219, 29377, 43950, 39459, 43196,  9791,  5488, 31290,
        7636, 26380,  2175, 10196, 14635, 33401, 40088, 18829, 22443,
       26307, 27624, 39146, 49252,  2293, 30061, 26844,  2088, 29830,
       27227, 13803, 31053, 43765, 15744, 45712, 20978, 14914, 28658,
       47605,  1937, 47408,  5264, 28191, 40085,  2703, 33057, 33947,
        7984, 10765,  6894, 16136, 33657, 19170, 32373, 36694, 19548,
       44360, 37075, 10570, 34298, 38660,  1889, 28603, 20303, 35743,
       29582, 25863, 15917,  8361, 19627, 33979, 36105, 13427, 22513,
       30131, 21033,  7207, 14415, 33334, 36142,  5193, 32018, 35443,
       48223]), [8, 3, 7, 4]), (array([46298, 15758,  3526, 26668,  4622, 30615,   147, 28019,  5228,
        1504, 30038, 26888, 45235, 30578, 14599, 22252,  9683,  6643,
       46611,  4675,  1384, 28353, 21159, 32563,  3895, 27837,  9229,
       30973, 28358, 24107,  3237, 10405, 29356, 27815, 41419, 48140,
       18642, 20944, 41244,   278,  6293,   768, 17163, 25308, 28337,
       38426, 36321,  2767, 47641, 21300, 49249, 26394, 34515, 18260,
       36280,   657,  1231, 23410, 49287, 24638, 29529, 17139, 40875,
       29697, 32163,  3199, 39699, 30764, 39806, 17600, 18963, 33671,
       13782, 32838, 41765, 47765, 36521, 43619,  6158, 39319,  8841,
       43406, 23305, 14071, 47340, 45405, 39917, 31870, 30223, 29258,
       24550, 14205,  1332, 29193, 33248, 26945,  2158, 42414, 39117,
       18344, 37017,  2303, 26136, 32040, 17896, 20765, 26021, 47187,
        2585, 18539, 28656, 30503, 32072, 43141,  8402, 32156, 34409,
       26229, 30934, 47660, 31077, 44052,  8303,  7652, 13780, 43673,
       32552, 34077, 16267, 19670, 43614,  6934,  1390, 20129,  7752,
       35545, 12902, 27957, 30903,  3086, 38357, 19620, 11096, 27668,
       43539, 19369, 26518,     1, 32192, 11552, 34502,  5394, 29753,
        9501, 18430, 27791, 36508, 24133, 26506, 45984,  4618, 42102,
       24391,  8323, 36549, 29221, 48839, 29585, 33620, 12262, 21907,
        2838, 36288, 47896, 33676, 42362, 45871, 12011, 36887, 45673,
        3755, 33483,   360, 26600,   541, 21696, 24880,  7943,  9434,
       47875, 45684, 35218, 23075, 27965, 36504, 48751,  9513, 33224,
       41976, 13662, 36668,  2129, 19055, 44716, 33672, 14813, 49035,
       25483, 49266, 14626, 15616,  5133, 18666, 12186, 32245, 13380,
       24407, 23359, 26530, 10144, 44969, 42556,  1045, 30180, 17903,
        3977, 31176,  3298, 17716, 40387, 35423, 23856, 35184, 10622,
       38402, 11814, 28879, 22652, 40412, 15725, 42420, 37538, 48308,
       43591, 40693, 46723,  5138, 41580,  5300, 44280, 21189,  7410,
       19806, 34959, 41556, 42542, 18163, 14180,  7633, 35575, 45953,
       39687, 15118, 19969,  6315,  3193, 24210, 42489, 47563,  3454,
       36710, 16439, 13081,  9093,  8881, 40189, 34600, 13794, 10457,
       34433, 16357, 17114, 32361, 12720,  3520, 31074, 38170,    47,
       37515, 24500, 48504,  4525, 15977, 27685, 25268, 32470, 14500,
       29625, 40487, 34555, 21316, 33282, 27204,  1921, 22012, 38565,
        5634,  8789,  7340, 46633, 42155, 33564,  3161, 25379, 40444,
        1207,  3847, 48870, 27738, 29650, 34609, 37990, 20570,  1812,
       44658,  9989, 34866, 24994, 34398, 11500, 28689, 26146, 39339,
       49800,   646,  5087, 28426,  6477, 17344, 16742, 26956, 20226,
       31307, 31486, 37353,  1884, 21031,  2948, 42536, 19343, 25222,
       46967, 45277, 18457,  2533, 47940, 20660, 25628, 18964, 29902,
       16372, 49159, 15697, 47172, 39928, 13757, 36972, 13985, 15088,
        8037,  6073, 38129,  3944, 12043, 26651, 44070, 15430, 19899,
       26213, 30513,  2372, 22279, 42120, 42990, 23843, 18573, 16593,
       31475, 46405, 47632, 31333, 28452, 26990, 18269, 48498, 31636,
       19964,  5836, 46800,  5576,  9144, 16683, 46014, 46179, 44879,
       14588, 41407, 46258,  9952,  7074, 24610,  4811, 37166, 28227,
       44008, 27046,  2833, 48092, 27542, 31308,  8092, 48146, 26893,
        9408, 37811, 42995, 39935,  8121, 33531, 14172, 43441, 34088,
       35486, 48973, 30333,  9558, 30746, 48480, 16569, 47500, 44739,
       45803, 32582, 48733, 49397, 35420, 30371, 47360, 24719, 21210,
       31285, 10512, 16462, 42889, 33503,  1024,  2678,  5127, 35723,
       25095,   630, 16757, 41401, 10725, 45218, 18720, 36152, 29126,
       42588, 26760, 38638, 17339, 21063, 32706, 45021,  2354, 33834,
       23222, 21039,   957, 35947, 33403, 45335, 43367, 10018,  7258,
        3164, 35063, 21090,  5189, 20051,  5618, 35834, 18888, 42427,
       10594, 17188, 19918, 14583, 44181, 21301, 26276, 11867, 40955,
       15567, 18712, 24596, 17648, 19058,    43, 45708, 36513, 10552,
       26451,  2541, 46301,  6752, 43536,  8238, 49737, 34228, 42117,
       39030, 47406, 22405,  6939, 45610, 30484, 26165, 46792, 28048,
       23435, 24812, 16618, 49350, 40563,  1927, 45747, 48453, 12925,
       30150, 28502, 31789, 36691, 42386,  8052, 34412, 26826, 49396,
       28759, 32414,  1276,  6839, 46117, 15916, 35922, 43768,  7672,
       11469,  2616,  7565, 17720,  7463, 43902,  8465, 34329, 36562,
        7613, 20423,  5846, 25917, 13566, 48727, 42959,  8927, 24370,
       10957, 33150, 23364, 10348, 43532, 28195, 49796, 18292, 19600,
       24568, 48099, 30996, 40730, 27255, 25724, 13622, 32296, 33299,
        4582, 20707, 22854, 28771,  5403, 38500, 40025,  3711,  5116,
       17177,  7400, 36987,  8924, 17840, 13547, 25617, 44958, 42598,
       25778, 16025, 11621, 45281, 49774, 21317, 15550,  6726,  7141,
       11742, 17549, 48586, 10779, 48578, 24013, 40541, 23547,  3151,
       29018, 24995,  6827,  8984, 34788, 41473,  6335, 30890, 42812,
        8751, 23784, 38109, 26208, 29592, 25182, 14202, 35008, 28453,
       18124,  4078, 42951, 17989, 37935, 28429, 47039, 47665, 32053,
        6291, 15413, 43150,  7564, 33450, 47246, 25592,  4401, 49592,
       44762,  7101, 19064, 43322, 23486, 23909, 23609,  5761, 20314,
        4241, 33660, 13597, 37319,  8604, 37902, 45945, 33775, 27969,
       10034,  6641, 45150, 41390, 25836, 24233, 30556, 31529, 19744,
       18339, 45336, 44560,  7836,  2370, 19599,  9369, 11581, 29975,
       11414, 46857,  6513, 28965, 31224, 34469, 47422, 38325, 32329,
       22624,  8580, 22806, 21324,  9279, 31217, 22309, 17678, 41846,
        8109, 12627, 10041,  6680,  3733, 26752, 38965, 46167,   647,
       18139,  6979,  6864, 17159,  5996, 14431, 24090, 25226,   994,
       18914, 26411, 49850, 36144, 38284, 43168, 22687, 28911, 12888,
       27915, 25269, 24715, 30822, 45689, 12580, 18912, 29756, 48399,
       47205,  1435, 22205, 22149, 21251, 23853, 20276, 34793, 46521,
       16908,  9777, 23532, 49237, 37719, 45441, 25898, 27193, 35276,
       41766, 35624, 12510, 43335, 15656,  1347, 37875, 41737, 30528,
       12179, 33209, 43473, 32947,  9074, 16173, 46245, 40162, 14022,
       43632, 38115, 23655,  1804, 43304, 34808, 18161, 29218, 17507,
       13231, 10350, 14440, 13903,  5826, 13978,  6298,  9776,  1407,
       33744, 45867, 38720, 11586, 41934, 34852, 14283, 14316, 37591,
       29787, 39067,  4058, 37850,  2976, 48478,  4173,  1349, 25403,
       33501, 27390, 10561, 45517,  3587, 35811, 37777, 33796,  6603,
       36327, 21382, 16636,  3664, 17550, 15777, 39377, 41688, 22133,
       20698, 21252, 42910,  9655,  3560, 32285,  3757, 41590, 44703,
        7855,  2990, 46531, 41005, 41249,   725,  2830, 22990, 15538,
       23739, 30857, 18403, 30967, 26706,  3879, 28260, 31160, 48087,
       48484,   528, 35648, 14087, 12261, 28076,  6274,  3519, 22153,
       25299,  8379, 48985, 48447,  1654, 48937, 42938, 41451, 36690,
        8423, 39866, 19180, 29161,  5029, 43529, 48807,  3913, 28596,
       27282, 38995, 17179,  9155, 44846, 44095,  9389, 47399,  3695,
        6594, 22202, 44586, 17707, 49475,  8263,  6636,  6569, 49564,
        3690, 26713, 17845,  4978,  6652,  8271, 41186, 20446, 42675,
       14744, 19857, 39698, 42313,  6535, 11081, 24421, 48367, 21229,
       25860,  4280, 13428,  4642, 33740, 39891, 35175, 35077,  9019,
        3377, 45605, 16640, 18628, 28903, 13038, 41302, 32757, 18185,
       20826, 25776, 30291, 19882, 48008, 35185,  3326,  7043, 38103,
       17728, 21659, 40842, 45311,  6770, 38810, 43637, 43694, 21453,
       45221, 18434, 41777, 25401,  5742, 45396, 42769, 34352,  1324,
       12324, 44517, 34816,  8208, 10904, 25292, 39772, 18054, 34882,
       19980, 35797, 37193, 25099,  5358, 12331,  5813, 36247, 46174,
       48521]), [9, 2, 7, 4])]
Collaboration
DC 0, val_set_size=1000, COIs=[6, 1, 7, 4], M=tensor([6, 1, 7, 4], device='cuda:0'), Initial Performance: (0.25, 0.04443984019756317)
DC 1, val_set_size=1000, COIs=[5, 0, 7, 4], M=tensor([5, 0, 7, 4], device='cuda:0'), Initial Performance: (0.262, 0.04436455535888672)
DC 2, val_set_size=1000, COIs=[8, 3, 7, 4], M=tensor([8, 3, 7, 4], device='cuda:0'), Initial Performance: (0.275, 0.04432411301136017)
DC 3, val_set_size=1000, COIs=[9, 2, 7, 4], M=tensor([9, 2, 7, 4], device='cuda:0'), Initial Performance: (0.225, 0.044532188177108765)
D00: 1000 samples from classes {4, 7}
D01: 1000 samples from classes {4, 7}
D02: 1000 samples from classes {4, 7}
D03: 1000 samples from classes {4, 7}
D04: 1000 samples from classes {4, 7}
D05: 1000 samples from classes {4, 7}
D06: 1000 samples from classes {1, 6}
D07: 1000 samples from classes {1, 6}
D08: 1000 samples from classes {1, 6}
D09: 1000 samples from classes {1, 6}
D010: 1000 samples from classes {1, 6}
D011: 1000 samples from classes {1, 6}
D012: 1000 samples from classes {0, 5}
D013: 1000 samples from classes {0, 5}
D014: 1000 samples from classes {0, 5}
D015: 1000 samples from classes {0, 5}
D016: 1000 samples from classes {0, 5}
D017: 1000 samples from classes {0, 5}
D018: 1000 samples from classes {8, 3}
D019: 1000 samples from classes {8, 3}
D020: 1000 samples from classes {8, 3}
D021: 1000 samples from classes {8, 3}
D022: 1000 samples from classes {8, 3}
D023: 1000 samples from classes {8, 3}
D024: 1000 samples from classes {9, 2}
D025: 1000 samples from classes {9, 2}
D026: 1000 samples from classes {9, 2}
D027: 1000 samples from classes {9, 2}
D028: 1000 samples from classes {9, 2}
D029: 1000 samples from classes {9, 2}
FL ROUND 1
DC-DO Matching
DC 0 --> ['(DO3', '(DO0']
DC 1 --> ['(DO2', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.25, 0.07179966375231743) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.251, 0.062282080620527265) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.387, 0.0866992349922657) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.25, 0.10071668189764023) --> New model used: True
FL ROUND 2
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.294, 0.09029780164361) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.373, 0.0706805988252163) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.09509955850243569) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.269, 0.12162477773427963) --> New model used: True
FL ROUND 3
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.405, 0.10274842362105846) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.463, 0.0916736244559288) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.448, 0.12655268445611) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.416, 0.14859290887415408) --> New model used: True
FL ROUND 4
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.404, 0.1201973113194108) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.459, 0.11129003369808196) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.451, 0.17445171085000039) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.441, 0.18057729303836823) --> New model used: True
FL ROUND 5
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.13035795422643423) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.451, 0.13108988019824028) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2068759422376752) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.446, 0.2312651706188917) --> New model used: True
FL ROUND 6
DC-DO Matching
DC 0 --> ['(DO0', '(DO2']
DC 1 --> ['(DO3', '(DO1']
DC 2 --> ['(DO4']
DC 3 --> ['(DO5']
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.451, 0.1425810459293425) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.443, 0.158301618501544) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.464, 0.2383120637461543) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.463, 0.24315960404276848) --> New model used: True
FL ROUND 7
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.461, 0.1451124506518245) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.457, 0.15763706554472445) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.2743811624888331) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.455, 0.2798636357784271) --> New model used: True
FL ROUND 8
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.459, 0.1760368304941803) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.461, 0.17171229308471084) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.468, 0.3013828208167106) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.467, 0.29754252534732223) --> New model used: True
FL ROUND 9
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.468, 0.1761757607832551) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.446, 0.17199157116003336) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.3272228060346097) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.469, 0.30223537323623895) --> New model used: True
FL ROUND 10
DC-DO Matching
TRAINING: DC 0
New Model (Val Acc, Val Loss) = (0.465, 0.19416647404991091) --> New model used: True
TRAINING: DC 1
New Model (Val Acc, Val Loss) = (0.448, 0.17022600414045155) --> New model used: True
TRAINING: DC 2
New Model (Val Acc, Val Loss) = (0.47, 0.3739811360947788) --> New model used: True
TRAINING: DC 3
New Model (Val Acc, Val Loss) = (0.468, 0.2888700136058033) --> New model used: True
CREATING ALLIANCE...
/mnt/lia/scratch/pohl/miniconda3/envs/semester_project_tests/lib/python3.8/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(

DC Alliance, val_set_size=2000, COIs=[4, 7], M=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0'), Initial Performance: (0.5475, 0.021328159302473067)
DC Expert-0, val_set_size=500, COIs=[1, 6], M=tensor([6, 1, 7, 4], device='cuda:0'), Initial Performance: (0.93, 0.006120357226580381)
DC Expert-1, val_set_size=500, COIs=[0, 5], M=tensor([5, 0, 7, 4], device='cuda:0'), Initial Performance: (0.896, 0.007846746120601893)
DC Expert-2, val_set_size=500, COIs=[8, 3], M=tensor([8, 3, 7, 4], device='cuda:0'), Initial Performance: (0.94, 0.005073083020746708)
DC Expert-3, val_set_size=500, COIs=[9, 2], M=tensor([9, 2, 7, 4], device='cuda:0'), Initial Performance: (0.936, 0.0057009288892149925)
SUPER-DC 0, val_set_size=1000, COIs=[6, 1, 7, 4], M=tensor([6, 1, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-0', 'DCAlliance']
SUPER-DC 1, val_set_size=1000, COIs=[5, 0, 7, 4], M=tensor([5, 0, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-1', 'DCAlliance']
SUPER-DC 2, val_set_size=1000, COIs=[8, 3, 7, 4], M=tensor([8, 3, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-2', 'DCAlliance']
SUPER-DC 3, val_set_size=1000, COIs=[9, 2, 7, 4], M=tensor([9, 2, 7, 4], device='cuda:0'), , Expert DCs: ['DCExpert-3', 'DCAlliance']
RECRUITING: 
[<fl_market.actors.data_consumer.DataConsumer object at 0x71c9c045d820>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c97c13e490>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c9b0281fa0>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c9b02bfd30>, <fl_market.actors.data_consumer.DataConsumer object at 0x71c9c00c96d0>]
FL ROUND 11
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.576, 0.02032484343647957) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004757704786956311) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.005675251021981239) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.005198483375832438) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.946, 0.005608893979340792) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.475, 0.0846301537156105) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.502, 0.07719845002889633) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.465, 0.20944385898672044) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.477, 0.1182997468188405) --> New model used: True
FL ROUND 12
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8035, 0.013345799706876278) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004009699840098619) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.006048367170616985) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.0069474382679909465) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.00668133369460702) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.582, 0.042315167754888534) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.585, 0.051967587560415265) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.497, 0.2567575519979) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.51, 0.07816052938997746) --> New model used: True
FL ROUND 13
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.833, 0.012803037714213133) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004242773976176977) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.0057515558619052175) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.005543063748627901) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.94, 0.006957396179437638) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.574, 0.04757797908782959) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.564, 0.05358584405481815) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.495, 0.12321048349142075) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.516, 0.0738716722279787) --> New model used: True
FL ROUND 14
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.833, 0.014256251242011785) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004185790874063969) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.005258220691233873) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.005955673939548433) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.946, 0.006895134523510933) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.04587965139746666) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.627, 0.039896142482757566) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.544, 0.07420881590247154) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.549, 0.06576673026382923) --> New model used: True
FL ROUND 15
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.844, 0.013171238236129285) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004339927610009908) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.944, 0.006118899118155241) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.005800210644025356) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.006918091325089335) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.585, 0.05289100030064583) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.629, 0.04343206167221069) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.534, 0.08572100147604943) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.56, 0.05998837101459503) --> New model used: True
FL ROUND 16
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.85, 0.012744808074086905) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004884677551686764) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.005687152199447155) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.007520068538375199) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.007224782001227141) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.576, 0.054619480341672896) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.598, 0.0616098066419363) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.517, 0.08972707148641348) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.534, 0.08952646129578352) --> New model used: True
FL ROUND 17
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.857, 0.015293334141373634) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004784792389720678) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.00585559150390327) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.007708461941685528) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.95, 0.007496076242532581) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.601, 0.042672060012817384) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.613, 0.045416498094797135) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.596, 0.05652818042039871) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.589, 0.0503250267803669) --> New model used: True
FL ROUND 18
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.859, 0.013808263193815947) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.004376363158226013) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.007182621503248811) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.006695517924148589) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.94, 0.007330468800850212) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.044973959386348725) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.637, 0.04753617852926254) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.543, 0.07968518191576004) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.549, 0.061072509378194806) --> New model used: True
FL ROUND 19
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8545, 0.014344924757257104) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.954, 0.004772701930254698) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.005844024920836091) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.00655885815853253) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.007837367257568985) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.617, 0.044344561696052553) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.643, 0.047554197579622266) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.538, 0.07254925668239594) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.591, 0.053079565539956096) --> New model used: True
FL ROUND 20
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.848, 0.017575377711094915) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.00535124777443707) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.005374130427837372) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.95, 0.006131111715920269) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.958, 0.0068498170599341395) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.617, 0.04330801299214363) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.562, 0.057559849381446836) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.558, 0.05528815531730652) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.597, 0.046521203368902204) --> New model used: True
FL ROUND 21
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8545, 0.015128600619733333) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.95, 0.005707006981596351) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.005251866988837719) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.00633627344109118) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006703903545159847) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.627, 0.04255084428191185) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.623, 0.04626948809623718) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.526, 0.07467444288730621) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.567, 0.052771809190511706) --> New model used: True
FL ROUND 22
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8545, 0.015587988825514912) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.00519925835262984) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.006349090401083231) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.006757613980676979) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.00796636725141434) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.04843812611699104) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.542, 0.06774675013124942) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.533, 0.07001174649596215) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.597, 0.05094253021478653) --> New model used: True
FL ROUND 23
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.86, 0.01704106624983251) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.004903978180140257) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.006816603959538043) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.007957097405917012) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.0060728678349405525) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.621, 0.042263079941272735) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.575, 0.052388162344694136) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.557, 0.051316569209098814) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.564, 0.04951502585411072) --> New model used: True
FL ROUND 24
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.866, 0.016940700998529793) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.004822443166398444) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.005802755822427571) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.00600433502253145) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.007995822328375653) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.567, 0.05525292414426804) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.584, 0.05113502663373947) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.575, 0.05264648103713989) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.617, 0.044406283736228944) --> New model used: True
FL ROUND 25
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.857, 0.018420903788879513) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.005089866753667593) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.942, 0.00700607573799789) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.95, 0.007301070926710963) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.944, 0.007484483920037747) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.607, 0.0408896122276783) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.511, 0.06659309449791909) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.461, 0.07942746245861053) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.565, 0.05015604156255722) --> New model used: True
FL ROUND 26
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8565, 0.019599553141742946) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005406043296679854) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.00603691410087049) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.006683038609102368) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.0076779299119953065) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.617, 0.04681454655528069) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.535, 0.05883785635232926) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.505, 0.06983091044425964) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.622, 0.044321644306182864) --> New model used: True
FL ROUND 27
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8605, 0.018233037235215306) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005585783798247576) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.0060306361690163615) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.007668923589284532) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.942, 0.00811589543009177) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.05492640188336372) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.555, 0.05856065157055855) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.544, 0.0651718635559082) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.616, 0.04597525006532669) --> New model used: True
FL ROUND 28
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8605, 0.018466551054269077) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0047655815770849584) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.007611991436453536) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.94, 0.006508830139879137) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.007651745555456728) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.6, 0.04958549511432648) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.631, 0.044962218105793) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.565, 0.05661990559101105) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.612, 0.04211514782905579) --> New model used: True
FL ROUND 29
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8585, 0.018819629513192923) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005352231945842505) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.007011254524812102) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.007395132803125307) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.954, 0.006466053659562022) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.595, 0.04668757539987564) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.493, 0.0733136351108551) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.527, 0.0704604412317276) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.593, 0.04694643896818161) --> New model used: True
FL ROUND 30
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.865, 0.016362720964476465) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005086703132838011) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.0064460944198945075) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.00800638529410935) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.95, 0.00802477233018726) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.04425796237587929) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.546, 0.0564834118783474) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.586, 0.05951714983582496) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.605, 0.0431532202064991) --> New model used: True
FL ROUND 31
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8665, 0.018756290055811406) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.004814210545271635) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.96, 0.006281876090914011) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.008335572914103978) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.946, 0.008028654447756708) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.645, 0.04224618935585022) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.55, 0.0508819540143013) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.516, 0.0667247640490532) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.526, 0.06338445040583611) --> New model used: True
FL ROUND 32
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8655, 0.02022589528001845) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.00493615403608419) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.005584420241415501) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.008118760001845658) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.007453002378344536) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.603, 0.04322687640786171) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.495, 0.06728473794460296) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.486, 0.07678681659698486) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.597, 0.04563046091794968) --> New model used: True
FL ROUND 33
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8625, 0.018409045622684063) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.005924389041028917) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.946, 0.007186209131032228) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.008317686002017582) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.008580019008601085) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.613, 0.052088760137557984) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.501, 0.07416269916296005) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.51, 0.06863044118881226) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.538, 0.07553837579488755) --> New model used: True
FL ROUND 34
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.869, 0.01727849878370762) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.006133734941482544) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.006860093087423593) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.006876939910463988) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.944, 0.009580338388041128) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.619, 0.04898043191432953) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.532, 0.060786819100379946) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.496, 0.07545511674880981) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.527, 0.10252989901602268) --> New model used: True
FL ROUND 35
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8655, 0.019147434243932365) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.004442057471722364) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.962, 0.006800371426157653) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.008179470583098009) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.008293047916493379) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.609, 0.045245510309934615) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.494, 0.07558438158035279) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.546, 0.058958629667758944) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.543, 0.06112434524297714) --> New model used: True
FL ROUND 36
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.869, 0.017701029997318984) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.968, 0.005270228949346346) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.007006882477551699) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.0065517873654025605) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.956, 0.007558602187084034) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.618, 0.04189877200126648) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.491, 0.07544886896014213) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.546, 0.05615606462955475) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.59, 0.051186521649360654) --> New model used: True
FL ROUND 37
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.874, 0.0194050108268857) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.004586440918035805) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.95, 0.006808205196633935) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.007713223986327648) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.00730416338192299) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.63, 0.04342683172225952) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.496, 0.0713254200220108) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.585, 0.05558304476737976) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.636, 0.04100181540846825) --> New model used: True
FL ROUND 38
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8695, 0.01785001878067851) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.0055718728620558974) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.007091099116951227) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.008725497311912477) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.95, 0.008430123302503489) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.602, 0.05895741194486618) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.524, 0.06434599235653878) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.49, 0.07173479008674621) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.552, 0.07555354154109954) --> New model used: True
FL ROUND 39
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.873, 0.018842114510014652) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.964, 0.005191267159767449) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.005771921269595623) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.942, 0.00860726070511737) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.944, 0.008922839252394624) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.586, 0.05819120070338249) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.526, 0.058777602910995486) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.53, 0.06675324070453643) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.55, 0.08665113750100135) --> New model used: True
FL ROUND 40
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8735, 0.01781016219034791) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.005683277463540435) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.007450508706271649) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.936, 0.009975287129549542) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.008188828675076365) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.051484933465719224) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.58, 0.053896558344364166) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.523, 0.08103418719768524) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.601, 0.0612029808908701) --> New model used: True
FL ROUND 41
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8695, 0.01792680050432682) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.0065833560745668365) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.007202738128602505) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.01010620107795694) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.008901333997608162) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.591, 0.058262347474694255) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.51, 0.07139192363619805) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.51, 0.07674978053569793) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.592, 0.05864804089069366) --> New model used: True
FL ROUND 42
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8705, 0.018341709890402853) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.007840551602188498) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.007297185405157507) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.944, 0.008814224204605125) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.00786829878250137) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.575, 0.06426442958414555) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.557, 0.056615793585777285) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.467, 0.10991459530591964) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.645, 0.04447048828005791) --> New model used: True
FL ROUND 43
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8685, 0.018369014479219913) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.96, 0.006587141874129884) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.958, 0.006912099128589034) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.00811423214711249) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.007997918768087402) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.615, 0.04764125415682793) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.525, 0.06361882835626602) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.534, 0.07516131293773651) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.548, 0.06916070801019669) --> New model used: True
FL ROUND 44
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.863, 0.02044096842035651) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.0066682769451290366) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.948, 0.008070049681002275) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.946, 0.008533178027690155) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.008046454016119241) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.62, 0.04189593267440796) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.523, 0.06522099846601487) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.52, 0.06648805093765259) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.568, 0.05964649277925491) --> New model used: True
FL ROUND 45
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8695, 0.019083950150758026) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.0071345941631589084) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.94, 0.008364922311156988) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.956, 0.007996075965464115) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.946, 0.008962547609582544) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.04780129927396774) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.542, 0.05840812665224075) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.529, 0.06433080071210862) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.572, 0.060221251368522646) --> New model used: True
FL ROUND 46
DC-DO Matching
DC Alliance --> []
DC Expert-0 --> []
DC Expert-1 --> []
DC Expert-2 --> []
DC Expert-3 --> []
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.867, 0.020066280771046878) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.962, 0.0071218976977688725) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.008649418354034424) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.948, 0.008007204721448943) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.009567500219447538) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.594, 0.04824873851239681) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.541, 0.05914467692375183) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.516, 0.07526116633415222) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.561, 0.059661232858896254) --> New model used: True
FL ROUND 47
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8645, 0.019981508638709785) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.952, 0.007464068644680083) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.00820368068665266) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.952, 0.009597479428746737) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.95, 0.009377376889809967) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.612, 0.04910453453660011) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.539, 0.06299619692564011) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.551, 0.06385153722763061) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.561, 0.070363175958395) --> New model used: True
FL ROUND 48
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8665, 0.01870619214512408) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.966, 0.00821868033905048) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.954, 0.008271451854147018) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.948, 0.009204809546470643) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.95, 0.010071191581431776) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.616, 0.048813894033432004) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.581, 0.05818929642438889) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.581, 0.058176871120929716) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.545, 0.10462762951850892) --> New model used: True
FL ROUND 49
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.872, 0.02106668890081346) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.008218438254669308) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.952, 0.007879849519580602) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.934, 0.01093171752616763) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.948, 0.010007919082418083) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.606, 0.045600790143013) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.51, 0.07388946783542633) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.532, 0.0745121745467186) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.55, 0.07830906948447228) --> New model used: True
FL ROUND 50
DC-DO Matching
TRAINING: DC Alliance
New Model (Val Acc, Val Loss) = (0.8655, 0.020454647921957076) --> New model used: True
TRAINING: DC Expert-0
New Model (Val Acc, Val Loss) = (0.958, 0.008981159074173774) --> New model used: True
TRAINING: DC Expert-1
New Model (Val Acc, Val Loss) = (0.956, 0.007195764558389783) --> New model used: True
TRAINING: DC Expert-2
New Model (Val Acc, Val Loss) = (0.938, 0.012171303902752697) --> New model used: True
TRAINING: DC Expert-3
New Model (Val Acc, Val Loss) = (0.952, 0.010103790824301542) --> New model used: True
TRAINING: SUPER-DC 0
New Model (Val Acc, Val Loss) = (0.65, 0.04063942913711071) --> New model used: True
TRAINING: SUPER-DC 1
New Model (Val Acc, Val Loss) = (0.508, 0.07187153866887093) --> New model used: True
TRAINING: SUPER-DC 2
New Model (Val Acc, Val Loss) = (0.54, 0.0779086880683899) --> New model used: True
TRAINING: SUPER-DC 3
New Model (Val Acc, Val Loss) = (0.584, 0.05682645490765571) --> New model used: True
Results: Collaboration
Collaboration_DC_0
VAL: 
[(0.25, 0.04443984019756317), (0.25, 0.07179966375231743), (0.294, 0.09029780164361), (0.405, 0.10274842362105846), (0.404, 0.1201973113194108), (0.451, 0.13035795422643423), (0.451, 0.1425810459293425), (0.461, 0.1451124506518245), (0.459, 0.1760368304941803), (0.468, 0.1761757607832551), (0.465, 0.19416647404991091), (0.475, 0.0846301537156105), (0.582, 0.042315167754888534), (0.574, 0.04757797908782959), (0.586, 0.04587965139746666), (0.585, 0.05289100030064583), (0.576, 0.054619480341672896), (0.601, 0.042672060012817384), (0.618, 0.044973959386348725), (0.617, 0.044344561696052553), (0.617, 0.04330801299214363), (0.627, 0.04255084428191185), (0.6, 0.04843812611699104), (0.621, 0.042263079941272735), (0.567, 0.05525292414426804), (0.607, 0.0408896122276783), (0.617, 0.04681454655528069), (0.6, 0.05492640188336372), (0.6, 0.04958549511432648), (0.595, 0.04668757539987564), (0.618, 0.04425796237587929), (0.645, 0.04224618935585022), (0.603, 0.04322687640786171), (0.613, 0.052088760137557984), (0.619, 0.04898043191432953), (0.609, 0.045245510309934615), (0.618, 0.04189877200126648), (0.63, 0.04342683172225952), (0.602, 0.05895741194486618), (0.586, 0.05819120070338249), (0.612, 0.051484933465719224), (0.591, 0.058262347474694255), (0.575, 0.06426442958414555), (0.615, 0.04764125415682793), (0.62, 0.04189593267440796), (0.612, 0.04780129927396774), (0.594, 0.04824873851239681), (0.612, 0.04910453453660011), (0.616, 0.048813894033432004), (0.606, 0.045600790143013), (0.65, 0.04063942913711071)]
TEST: 
[(0.26325, 0.043459017485380176), (0.25, 0.06910176756978036), (0.28625, 0.08651740956306457), (0.405, 0.09793887826800346), (0.40425, 0.11450791484117508), (0.45, 0.12330296683311462), (0.4495, 0.13521357721090316), (0.46975, 0.1373603984117508), (0.46, 0.1669597494006157), (0.47575, 0.16677658635377884), (0.46975, 0.18435369455814363), (0.481, 0.07941830489039421), (0.597, 0.040064811408519745), (0.5875, 0.044312307327985766), (0.60075, 0.04198802195489407), (0.615, 0.047502949774265286), (0.6075, 0.04935572777688503), (0.62925, 0.03784619437158108), (0.64375, 0.04001400637626648), (0.627, 0.03965674403309822), (0.63825, 0.039171585731208325), (0.6395, 0.0389041443541646), (0.6385, 0.0427683040946722), (0.6435, 0.03755366586148739), (0.58975, 0.049079876989126206), (0.6435, 0.03590170086175203), (0.64275, 0.04048305168747902), (0.6175, 0.050082306951284405), (0.625, 0.04208857275545597), (0.62325, 0.04243734885752201), (0.63, 0.03998979452997446), (0.64575, 0.03915405049920082), (0.6205, 0.041804459504783155), (0.62125, 0.048016702845692635), (0.62425, 0.0461509892642498), (0.628, 0.040791534304618834), (0.63325, 0.03787758664786816), (0.64425, 0.03902895047515631), (0.6085, 0.0538522879332304), (0.6, 0.0537050196826458), (0.633, 0.04699514508247375), (0.62125, 0.05145147407054901), (0.58125, 0.058477160215377806), (0.64, 0.04350796988606453), (0.6455, 0.03593208385258913), (0.6535, 0.041591294787824154), (0.621, 0.04288887191563845), (0.64225, 0.043727082915604114), (0.63625, 0.04452706979215145), (0.6435, 0.04011050051450729), (0.64975, 0.03787371683120728)]
DETAILED: 
              precision    recall  f1-score   support

           1       0.79      0.92      0.85      1000
           4       0.51      0.56      0.54      1000
           6       0.51      0.51      0.51      1000
           7       0.81      0.60      0.69      1000

    accuracy                           0.65      4000
   macro avg       0.66      0.65      0.65      4000
weighted avg       0.66      0.65      0.65      4000

Collaboration_DC_1
VAL: 
[(0.262, 0.04436455535888672), (0.251, 0.062282080620527265), (0.373, 0.0706805988252163), (0.463, 0.0916736244559288), (0.459, 0.11129003369808196), (0.451, 0.13108988019824028), (0.443, 0.158301618501544), (0.457, 0.15763706554472445), (0.461, 0.17171229308471084), (0.446, 0.17199157116003336), (0.448, 0.17022600414045155), (0.502, 0.07719845002889633), (0.585, 0.051967587560415265), (0.564, 0.05358584405481815), (0.627, 0.039896142482757566), (0.629, 0.04343206167221069), (0.598, 0.0616098066419363), (0.613, 0.045416498094797135), (0.637, 0.04753617852926254), (0.643, 0.047554197579622266), (0.562, 0.057559849381446836), (0.623, 0.04626948809623718), (0.542, 0.06774675013124942), (0.575, 0.052388162344694136), (0.584, 0.05113502663373947), (0.511, 0.06659309449791909), (0.535, 0.05883785635232926), (0.555, 0.05856065157055855), (0.631, 0.044962218105793), (0.493, 0.0733136351108551), (0.546, 0.0564834118783474), (0.55, 0.0508819540143013), (0.495, 0.06728473794460296), (0.501, 0.07416269916296005), (0.532, 0.060786819100379946), (0.494, 0.07558438158035279), (0.491, 0.07544886896014213), (0.496, 0.0713254200220108), (0.524, 0.06434599235653878), (0.526, 0.058777602910995486), (0.58, 0.053896558344364166), (0.51, 0.07139192363619805), (0.557, 0.056615793585777285), (0.525, 0.06361882835626602), (0.523, 0.06522099846601487), (0.542, 0.05840812665224075), (0.541, 0.05914467692375183), (0.539, 0.06299619692564011), (0.581, 0.05818929642438889), (0.51, 0.07388946783542633), (0.508, 0.07187153866887093)]
TEST: 
[(0.25325, 0.0434291261434555), (0.25125, 0.06006230038404465), (0.37275, 0.067833630412817), (0.46325, 0.08763139560818672), (0.46075, 0.1062627123594284), (0.45225, 0.12552101576328278), (0.4415, 0.1515355521440506), (0.457, 0.1510611606836319), (0.45775, 0.1647727227807045), (0.44675, 0.16469195330142974), (0.45125, 0.16312419641017914), (0.48825, 0.07445451048016548), (0.594, 0.04654846957325935), (0.585, 0.0483674428910017), (0.64575, 0.0371773382872343), (0.64275, 0.03893299628794193), (0.61025, 0.055058519050478934), (0.6385, 0.04200383242964745), (0.63325, 0.043366703316569326), (0.63, 0.043356469452381134), (0.5755, 0.05297853136062622), (0.62375, 0.04351329433917999), (0.55125, 0.061966993257403374), (0.58875, 0.04946529147028923), (0.60525, 0.04660102593898773), (0.5205, 0.06340930350124836), (0.54775, 0.056159222796559334), (0.55675, 0.05576710118353367), (0.64175, 0.04159385079145431), (0.51425, 0.06811753684282303), (0.5605, 0.053912179425358774), (0.57025, 0.048696455508470536), (0.52325, 0.06381499716639519), (0.5195, 0.06739110842347146), (0.557, 0.05811555315554142), (0.52, 0.0706172431409359), (0.51025, 0.06692884603142739), (0.52325, 0.06465459391474723), (0.54325, 0.05868071223795414), (0.5525, 0.052941009044647214), (0.59825, 0.04968204653263092), (0.524, 0.06690212422609329), (0.57975, 0.052563680842518806), (0.55, 0.05850947213172913), (0.5355, 0.061165643453598026), (0.56275, 0.052078941017389295), (0.5635, 0.055421349331736564), (0.5535, 0.05999045163393021), (0.5935, 0.05479534707963467), (0.51275, 0.07034620541334152), (0.51325, 0.0667553973942995)]
DETAILED: 
              precision    recall  f1-score   support

           0       0.88      0.38      0.53      1000
           4       0.37      0.41      0.39      1000
           5       0.45      0.82      0.58      1000
           7       0.72      0.45      0.55      1000

    accuracy                           0.51      4000
   macro avg       0.60      0.51      0.51      4000
weighted avg       0.60      0.51      0.51      4000

Collaboration_DC_2
VAL: 
[(0.275, 0.04432411301136017), (0.387, 0.0866992349922657), (0.451, 0.09509955850243569), (0.448, 0.12655268445611), (0.451, 0.17445171085000039), (0.464, 0.2068759422376752), (0.464, 0.2383120637461543), (0.47, 0.2743811624888331), (0.468, 0.3013828208167106), (0.467, 0.3272228060346097), (0.47, 0.3739811360947788), (0.465, 0.20944385898672044), (0.497, 0.2567575519979), (0.495, 0.12321048349142075), (0.544, 0.07420881590247154), (0.534, 0.08572100147604943), (0.517, 0.08972707148641348), (0.596, 0.05652818042039871), (0.543, 0.07968518191576004), (0.538, 0.07254925668239594), (0.558, 0.05528815531730652), (0.526, 0.07467444288730621), (0.533, 0.07001174649596215), (0.557, 0.051316569209098814), (0.575, 0.05264648103713989), (0.461, 0.07942746245861053), (0.505, 0.06983091044425964), (0.544, 0.0651718635559082), (0.565, 0.05661990559101105), (0.527, 0.0704604412317276), (0.586, 0.05951714983582496), (0.516, 0.0667247640490532), (0.486, 0.07678681659698486), (0.51, 0.06863044118881226), (0.496, 0.07545511674880981), (0.546, 0.058958629667758944), (0.546, 0.05615606462955475), (0.585, 0.05558304476737976), (0.49, 0.07173479008674621), (0.53, 0.06675324070453643), (0.523, 0.08103418719768524), (0.51, 0.07674978053569793), (0.467, 0.10991459530591964), (0.534, 0.07516131293773651), (0.52, 0.06648805093765259), (0.529, 0.06433080071210862), (0.516, 0.07526116633415222), (0.551, 0.06385153722763061), (0.581, 0.058176871120929716), (0.532, 0.0745121745467186), (0.54, 0.0779086880683899)]
TEST: 
[(0.27825, 0.043275490760803226), (0.38925, 0.08313762971758842), (0.45525, 0.09075938367843628), (0.461, 0.11947788047790528), (0.466, 0.16523100256919862), (0.4715, 0.1961016225218773), (0.4705, 0.22676054126024245), (0.47375, 0.25869140833616255), (0.4735, 0.28333983933925627), (0.47325, 0.3097177770137787), (0.47275, 0.3541496697664261), (0.47625, 0.20204253494739532), (0.51, 0.21710336902737618), (0.51075, 0.11277931737899781), (0.56175, 0.0684041743427515), (0.55275, 0.08006400698423385), (0.53725, 0.08517849424481391), (0.60425, 0.05193529751896858), (0.56925, 0.0749715416431427), (0.5555, 0.06814444184303284), (0.57475, 0.05263919572532177), (0.54125, 0.07071842455863953), (0.55325, 0.06239312806725502), (0.60175, 0.04586411674320698), (0.596, 0.04971393832564354), (0.4835, 0.07772637489438057), (0.51725, 0.06664848850667476), (0.55375, 0.06593088898062706), (0.57525, 0.053893879771232604), (0.54625, 0.0631393421292305), (0.592, 0.0569782075881958), (0.54875, 0.06199437171220779), (0.5095, 0.0721014910042286), (0.5325, 0.06378911630809307), (0.515, 0.06993100272119045), (0.57925, 0.055417772948741915), (0.58375, 0.05228043732047081), (0.604, 0.05070423222333193), (0.52725, 0.06758192083239556), (0.53975, 0.06350025348365307), (0.51575, 0.07416268277168274), (0.50175, 0.07479566037654876), (0.4675, 0.10310611587762833), (0.56475, 0.0694191841930151), (0.54925, 0.06433450332283974), (0.55225, 0.06205763053894043), (0.53775, 0.068914355173707), (0.5595, 0.060570773094892505), (0.585, 0.05637839376926422), (0.53375, 0.06931599228084087), (0.54525, 0.07444460394978523)]
DETAILED: 
              precision    recall  f1-score   support

           3       0.40      0.82      0.54      1000
           4       0.43      0.28      0.34      1000
           7       0.74      0.39      0.51      1000
           8       0.91      0.69      0.79      1000

    accuracy                           0.55      4000
   macro avg       0.62      0.55      0.54      4000
weighted avg       0.62      0.55      0.54      4000

Collaboration_DC_3
VAL: 
[(0.225, 0.044532188177108765), (0.25, 0.10071668189764023), (0.269, 0.12162477773427963), (0.416, 0.14859290887415408), (0.441, 0.18057729303836823), (0.446, 0.2312651706188917), (0.463, 0.24315960404276848), (0.455, 0.2798636357784271), (0.467, 0.29754252534732223), (0.469, 0.30223537323623895), (0.468, 0.2888700136058033), (0.477, 0.1182997468188405), (0.51, 0.07816052938997746), (0.516, 0.0738716722279787), (0.549, 0.06576673026382923), (0.56, 0.05998837101459503), (0.534, 0.08952646129578352), (0.589, 0.0503250267803669), (0.549, 0.061072509378194806), (0.591, 0.053079565539956096), (0.597, 0.046521203368902204), (0.567, 0.052771809190511706), (0.597, 0.05094253021478653), (0.564, 0.04951502585411072), (0.617, 0.044406283736228944), (0.565, 0.05015604156255722), (0.622, 0.044321644306182864), (0.616, 0.04597525006532669), (0.612, 0.04211514782905579), (0.593, 0.04694643896818161), (0.605, 0.0431532202064991), (0.526, 0.06338445040583611), (0.597, 0.04563046091794968), (0.538, 0.07553837579488755), (0.527, 0.10252989901602268), (0.543, 0.06112434524297714), (0.59, 0.051186521649360654), (0.636, 0.04100181540846825), (0.552, 0.07555354154109954), (0.55, 0.08665113750100135), (0.601, 0.0612029808908701), (0.592, 0.05864804089069366), (0.645, 0.04447048828005791), (0.548, 0.06916070801019669), (0.568, 0.05964649277925491), (0.572, 0.060221251368522646), (0.561, 0.059661232858896254), (0.561, 0.070363175958395), (0.545, 0.10462762951850892), (0.55, 0.07830906948447228), (0.584, 0.05682645490765571)]
TEST: 
[(0.23075, 0.04349978512525558), (0.25, 0.09680754977464676), (0.27325, 0.11629943352937698), (0.42225, 0.14138725328445434), (0.44, 0.17147793173789977), (0.4455, 0.21885153269767763), (0.4645, 0.2289725457429886), (0.4565, 0.2658118067979813), (0.46975, 0.28245870041847226), (0.472, 0.28643305253982543), (0.475, 0.2728337438106537), (0.4715, 0.11190707716345787), (0.522, 0.07232482293248177), (0.5265, 0.06706263494491577), (0.54175, 0.06176917681097984), (0.559, 0.055896089434623715), (0.525, 0.0830163795351982), (0.58375, 0.04652697402238846), (0.54775, 0.05694413411617279), (0.57225, 0.050892528235912324), (0.6085, 0.04607420092821121), (0.58325, 0.050361112639307974), (0.59625, 0.04867145952582359), (0.5835, 0.0472723244279623), (0.6255, 0.04210454784333706), (0.59175, 0.04635233868658543), (0.61725, 0.04268210488557816), (0.62, 0.04497873851656914), (0.624, 0.040765797071158884), (0.60125, 0.04600771263241768), (0.63175, 0.040535593315958976), (0.5475, 0.059638138100504876), (0.6025, 0.04548358596861363), (0.5475, 0.06989041905105114), (0.5225, 0.09493674331903458), (0.565, 0.05917803652584553), (0.6135, 0.04934067666530609), (0.63575, 0.04054306623339653), (0.556, 0.07202834816277028), (0.548, 0.08049383562803268), (0.5815, 0.059452481016516685), (0.60225, 0.05714819855988026), (0.6405, 0.043148768499493596), (0.554, 0.06634790766239167), (0.5815, 0.055578105732798574), (0.59475, 0.053967582911252976), (0.59025, 0.05563084352016449), (0.58, 0.06551630367338658), (0.5445, 0.09854278406500816), (0.57775, 0.07421476078033447), (0.58525, 0.05401590964198112)]
DETAILED: 
              precision    recall  f1-score   support

           2       0.41      0.81      0.55      1000
           4       0.47      0.25      0.33      1000
           7       0.75      0.49      0.59      1000
           9       0.92      0.79      0.85      1000

    accuracy                           0.59      4000
   macro avg       0.64      0.59      0.58      4000
weighted avg       0.64      0.59      0.58      4000
